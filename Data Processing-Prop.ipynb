{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle, sys, glob, requests\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from orderedset import OrderedSet\n",
    "from copy import deepcopy\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "datatypes = ['string', 'boolean', 'decimal', 'float', 'double', 'dateTime', 'time', 'date', 'gYearMonth', 'gYear',\n",
    "             'gMonthDay', 'gDay', 'gMonth', 'hexBinary', 'base64Binary', 'anyURI', 'normalizedString', 'token', 'language', \n",
    "             'NMTOKEN', 'Name', 'NCName', 'integer', 'nonPositiveInteger', 'negativeInteger', 'long', 'int', 'short', 'byte',\n",
    "             'nonNegativeInteger', 'unsignedLong', 'unsignedInt', 'unsignedShort', 'unsignedByte', 'positiveInteger']\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.construct_mapping_dict()\n",
    "        \n",
    "        self.parents_dict = {}\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()        \n",
    "    \n",
    "    def construct_mapping_dict(self):\n",
    "        self.mapping_dict = {self.extract_ID(el, False): self.get_child_node(el, \"rdfs:label\")[0].firstChild.nodeValue for el in self.root.getElementsByTagName(\"owl:Class\") if self.get_child_node(el, \"rdfs:label\")}\n",
    "        self.mapping_dict_inv = {self.mapping_dict[key]: key for key in self.mapping_dict}\n",
    "        return\n",
    "        \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        subclasses = self.get_subclasses()\n",
    "        self.parents_dict = {}\n",
    "        for (a,b,c,d) in subclasses:\n",
    "            if c == \"subclass_of\" and a!=\"Thing\" and b!=\"Thing\":\n",
    "                if b not in self.parents_dict:\n",
    "                    self.parents_dict[b] = [a]\n",
    "                else:\n",
    "                    self.parents_dict[b].append(a)\n",
    "        return [(b,a,c,d) for (a,b,c,d) in subclasses]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = [(prop, \"Object Property\") for prop in self.object_properties]\n",
    "        data_props = [(prop, \"Datatype Property\") for prop in self.data_properties]\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop, prop_type in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop), prop_type) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop), prop_type))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode, \"subclass_of\", \"Subclass\"))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    restriction = el.getElementsByTagName(\"owl:Restriction\")\n",
    "                    if not restriction:\n",
    "                        continue\n",
    "                    prop = self.get_child_node(restriction[0], \"owl:onProperty\")\n",
    "                    some_vals = self.get_child_node(restriction[0], \"owl:someValuesFrom\")\n",
    "                    \n",
    "                    if not prop or not some_vals:\n",
    "                        continue\n",
    "#                     print(self.extract_ID(el), \"**\", self.extract_ID(some_vals[0]), \"**\", self.extract_ID(prop[0]))\n",
    "                    try:\n",
    "                        if self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
    "                        elif self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
    "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
    "                        elif not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
    "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
    "                        else:\n",
    "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
    "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
    "                    except:\n",
    "                        try:\n",
    "                            if not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
    "                                subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
    "                            elif not self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
    "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
    "                                class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                                subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
    "                        except Exception as e:\n",
    "                            print (e)\n",
    "                            continue\n",
    "                else:\n",
    "                    if self.extract_ID(level1_class[0]):\n",
    "                        subclass_pairs.append((level1_class[0], el.parentNode, \"subclass_of\", \"Subclass\"))\n",
    "                    else:\n",
    "#                         level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "#                         subclass_pairs.extend([(elem, el.parentNode, \"subclass_of\", \"Subclass\") for elem in level2classes if self.extract_ID(elem)])\n",
    "                        continue\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        subclasses = [(self.extract_ID(a), self.extract_ID(b), c, d) for (a,b,c,d) in self.subclasses]\n",
    "        return [el for el in subclasses if el[0] and el[1] and el[2] and el[0]!=\"Thing\" and el[1]!=\"Thing\"]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element, check_coded = True):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        element_id = element_id.split(\"#\")[-1]\n",
    "        if len(list(filter(str.isdigit, element_id))) >= 3 and \"_\" in element_id and check_coded:\n",
    "            return self.mapping_dict[element_id]\n",
    "        return element_id.replace(\"UNDEFINED_\", \"\").replace(\"DO_\", \"\")\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-2] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"AML-conference/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2'), doc.getElementsByTagName('measure')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource'), c.firstChild.nodeValue) for (a,b,c) in ls])\n",
    "    return alignments\n",
    "\n",
    "# Extracting USE embeddings\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\")\n",
    "    embeds = model(words)\n",
    "    return embeds.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "\n",
    "reference_alignments = load_alignments(alignment_folder)\n",
    "\n",
    "# ra_anatomy_coded = load_alignments(\"../Anatomy/Alignments/\")\n",
    "# ra_anatomy = []\n",
    "# ont1 = Ontology(\"../Anatomy/Ontologies/mouse.owl\")\n",
    "# ont2 = Ontology(\"../Anatomy/Ontologies/human.owl\")\n",
    "# for elem in ra_anatomy_coded:\n",
    "#     pre1, pre2 = elem[0].split(\"#\")[0].split(\".\")[0].split(\"/\")[-1], elem[1].split(\"#\")[0].split(\".\")[0].split(\"/\")[-1]\n",
    "#     elem1, elem2 = elem[0].split(\"#\")[-1], elem[1].split(\"#\")[-1]\n",
    "#     ra_anatomy.append(( pre1 + \"#\" + ont1.mapping_dict[elem1], pre2 + \"#\" + ont2.mapping_dict[elem2]))\n",
    "\n",
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "# gt_mappings.extend(ra_anatomy)\n",
    "\n",
    "\n",
    "ontologies_in_alignment = pickle.load(open(\"../data_generic.pkl\", \"rb\"))[-1][:-1]\n",
    "# ontologies_in_alignment += [[\"../Anatomy/Ontologies/human.owl\", \"../Anatomy/Ontologies/mouse.owl\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "# Combinatorial mapping generation\n",
    "ent_mappings = []\n",
    "prop_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "\n",
    "    ent_mapping = list(itertools.product(ent1, ent2))\n",
    "    prop_mapping = list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    ent_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in ent_mapping])\n",
    "    prop_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in prop_mapping])\n",
    "\n",
    "aml_data_ent = {mapping: 0 for mapping in ent_mappings}\n",
    "aml_data_prop = {mapping: 0 for mapping in prop_mappings}\n",
    "\n",
    "\n",
    "s_ent = set(ent_mappings)\n",
    "s_prop = set(prop_mappings)\n",
    "for mapping in set(gt_mappings):\n",
    "    if mapping[:2] in s_ent:\n",
    "        aml_data_ent[tuple(mapping[:2])] = mapping[-1]\n",
    "    elif mapping[:2] in s_prop:\n",
    "        aml_data_prop[tuple(mapping[:2])] = mapping[-1]\n",
    "    else:\n",
    "        print (mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left ('Member_PC', 'Program_Committee') PC Program_Committee\n",
      "left ('Member_PC', 'Program_Chair') PC Program_Chair\n",
      "left ('Member_PC', 'Program_Committee_member') PCM Program_Committee\n",
      "left ('Chair_PC', 'Program_Committee') PC Program_Committee\n",
      "left ('Chair_PC', 'Program_Chair') PC Program_Chair\n",
      "left ('Chair_PC', 'Program_Committee_member') PCM Program_Committee\n",
      "left ('Member_PC', 'Presenter_city') PC Presenter_city\n",
      "left ('Chair_PC', 'Presenter_city') PC Presenter_city\n",
      "left ('OC_Member', 'Organizing_Committee_member') OCM Organizing_Committee\n",
      "left ('OC_Member', 'Organizing_Committee') OC Organizing_Committee\n",
      "left ('OC_Chair', 'Organizing_Committee_member') OCM Organizing_Committee\n",
      "left ('OC_Chair', 'Organizing_Committee') OC Organizing_Committee\n",
      "left ('PC_Member', 'Program_Committee') PC Program_Committee\n",
      "left ('PC_Member', 'Program_Chair') PC Program_Chair\n",
      "left ('PC_Member', 'Program_Committee_member') PCM Program_Committee\n",
      "left ('PC_Chair', 'Program_Committee') PC Program_Committee\n",
      "left ('PC_Chair', 'Program_Chair') PC Program_Chair\n",
      "left ('PC_Chair', 'Program_Committee_member') PCM Program_Committee\n",
      "right ('Passive_conference_participant', 'Member_PC') PCP Passive_conference\n",
      "right ('Passive_conference_participant', 'Chair_PC') PCP Passive_conference\n",
      "right ('Program_committee', 'Member_PC') PC Program_committee\n",
      "right ('Program_committee', 'Chair_PC') PC Program_committee\n",
      "left ('OC_Member', 'One_conference_day') OCD One_conference\n",
      "left ('OC_Chair', 'One_conference_day') OCD One_conference\n",
      "left ('SC_Member', 'Sponsor_company_house') SCH Sponsor_company\n",
      "left ('SC_Member', 'Sponsor_city') SC Sponsor_city\n",
      "left ('SC_Member', 'Session_chair') SC Session_chair\n",
      "left ('SC_Member', 'Brief_introduction_for_Session_chair') BIFSC Session_chair\n",
      "left ('PC_Member', 'Presenter_city') PC Presenter_city\n",
      "left ('PC_Chair', 'Presenter_city') PC Presenter_city\n",
      "left ('Member_PC', 'PC_Chair') PC PC_Chair\n",
      "left ('Chair_PC', 'PC_Chair') PC PC_Chair\n",
      "right ('Organizing_committee', 'OC_Member') OC Organizing_committee\n",
      "right ('Organizing_committee', 'OC_Chair') OC Organizing_committee\n",
      "right ('Steering_committee', 'SC_Member') SC Steering_committee\n",
      "right ('Submitted_contribution', 'SC_Member') SC Submitted_contribution\n",
      "right ('Passive_conference_participant', 'PC_Member') PCP Passive_conference\n",
      "right ('Passive_conference_participant', 'PC_Chair') PCP Passive_conference\n",
      "right ('Program_committee', 'PC_Member') PC Program_committee\n",
      "right ('Program_committee', 'PC_Chair') PC Program_committee\n"
     ]
    }
   ],
   "source": [
    "# Abbrevation resolution preprocessing\n",
    "\n",
    "abbreviations_dict = {}\n",
    "final_dict = {}\n",
    "\n",
    "for mapping in ent_mappings:\n",
    "    mapping = tuple([el.split(\"#\")[1] for el in mapping])\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[0])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\"_\")])\n",
    "        if is_abb.group() in abbreviation:\n",
    "            \n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[1].split(\"_\")[start:end])\n",
    "            print (\"left\", mapping, abbreviation, fullform)\n",
    "            \n",
    "            rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[1].split(\"_\")[:start] + mapping[1].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[1])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\"_\")])\n",
    "        \n",
    "        if is_abb.group() in abbreviation:\n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[0].split(\"_\")[start:end])\n",
    "            print (\"right\", mapping, abbreviation, fullform)\n",
    "\n",
    "            rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[0].split(\"_\")[:start] + mapping[0].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n",
    "\n",
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
    "filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error list index out of range\n",
      "registation deadline registration deadline\n",
      "viza viz\n",
      "reviewes reviewed\n",
      "memeber registration fee member registration fee\n",
      "accpet if room rating accept if room rating\n",
      "programme programmer\n",
      "scientifically organised by scientifically organized by\n",
      "sponzorship sponsorship\n",
      "has programme has programmer\n",
      "sponzor sponsor\n",
      "author cd proceedings included author cd proceedings included\n",
      "is a topis of conference parts is a tops of conference parts\n",
      "site URL site URL\n",
      "holded by holed by\n",
      "accepts hardcopy submissions accepts hard copy submissions\n",
      "organised by organized by\n",
      "technic activity technical activity\n",
      "logo URL logo URL\n",
      "sponzor fee sponsor fee\n",
      "was a committe co-chair of was a committee co-chair of\n",
      "author attendee cd registration fee author attendee cd registration fee\n",
      "is writen by is write by\n",
      "cd proceening cd proceeding\n",
      "has VAT has VAT\n",
      "webmaster web master\n",
      "technically organises technically organists\n",
      "programme brochure programmer brochure\n",
      "LCD projector LCD projector\n",
      "computer networks aapplications topic computer networks applications topic\n",
      "nonauthor registration fee non author registration fee\n",
      "registeered applicant registered applicant\n",
      "IASTED member IASTED member\n",
      "operating topicsystems operating topic systems\n",
      "has an ISBN has an ISBN\n",
      "organisation organization\n",
      "powerline transmission topic power line transmission topic\n",
      "cheque exchequer\n",
      "hardcopy mailing manifests printed by hard copy mailing manifests printed by\n",
      "technical commitee technical committee\n",
      "SC member SC member\n",
      "conference www conference www\n",
      "IASTED non member IASTED non member\n",
      "registration SIGMOD member registration SIGMOD member\n",
      "paper ID paper ID\n",
      "any URI any URI\n",
      "NGO NGO\n",
      "flyer flayer\n",
      "organising agency organizing agency\n",
      "has a commtitee has a committee\n",
      "organizator organization\n",
      "contribution 1th-author contribution th-author\n",
      "scientifically organises scientifically organists\n",
      "initial manuscipt initial manuscript\n",
      "print hardcopy mailing manifests print hard copy mailing manifests\n",
      "technically organised by technically organized by\n",
      "registration SIGKDD member registration SIGKDD member\n",
      "has a URL has a URL\n",
      "TProgram committee member TProgram committee member\n",
      "presentationed by presentation ed by\n",
      "organises organists\n",
      "CAD topic CAD topic\n",
      "is the 1th part of is the th part of\n",
      "modelling modeling\n",
      "ACM SIGKDD ACM SIGKDD\n",
      "coctail reception cocktail reception\n",
      "Total number of extracted unique classes and properties from entire RA set:  829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples())))\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "inp = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "\n",
    "# Resolving abbreviations to full forms\n",
    "inp_resolved = []\n",
    "for concept in inp:\n",
    "    for key in filtered_dict:\n",
    "        concept = concept.replace(key, filtered_dict[key])\n",
    "    final_list = []\n",
    "    # Lowering case except in abbreviations\n",
    "    for word in concept.split(\" \"):\n",
    "        if not re.search(\"[A-Z][A-Z]+\", word):\n",
    "            final_list.append(word.lower())\n",
    "        else:\n",
    "            final_list.append(word)\n",
    "    concept = \" \".join(final_list)\n",
    "    inp_resolved.append(concept)\n",
    "\n",
    "\n",
    "url = \"https://montanaflynn-spellcheck.p.rapidapi.com/check/\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"montanaflynn-spellcheck.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"9965b01207msh06291e57d6f2c55p1a6a16jsn0fb016da4a62\"\n",
    "    }\n",
    "\n",
    "inp_spellchecked = []\n",
    "for concept in inp_resolved:\n",
    "    querystring = {\"text\": concept}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring).json()\n",
    "    if response[\"suggestion\"] != concept:\n",
    "        resolved = str(concept)\n",
    "        final_list = []\n",
    "        for word in concept.split(\" \"):\n",
    "            if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "                final_list.append(word.lower())\n",
    "            else:\n",
    "                final_list.append(word)\n",
    "        resolved = \" \".join(final_list)\n",
    "#         print (resolved, \"suggestion\", response)\n",
    "        for word in response[\"corrections\"]:\n",
    "            if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "                resolved = resolved.replace(word.lower(), response[\"corrections\"][word][0].lower())\n",
    "                \n",
    "        \n",
    "        print (concept, resolved)\n",
    "        inp_spellchecked.append(resolved)\n",
    "    else:\n",
    "        inp_spellchecked.append(concept)\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "extracted_elems = [\"<UNK>\"] + extracted_elems\n",
    "\n",
    "# stopwords = [\"has\", \"is\", \"a\", \"an\", \"the\"]\n",
    "stopwords = [\"has\"]\n",
    "inp_stemmed = []\n",
    "for elem in inp_spellchecked:\n",
    "    words = \" \".join([word for word in elem.split() if word not in stopwords])\n",
    "    words = words.replace(\"-\", \" \")\n",
    "    inp_stemmed.append(words)\n",
    "\n",
    "\n",
    "embeds = np.array(extractUSEEmbeddings(inp_stemmed))\n",
    "embeds = np.array([np.zeros(embeds.shape[1],)] + list(embeds))\n",
    "# embeds = np.array([np.zeros(512,)] + list(extractUSEEmbeddings(inp_spellchecked)))\n",
    "embeddings = dict(zip(extracted_elems, embeds))\n",
    "\n",
    "\n",
    "emb_vals = list(embeddings.values())\n",
    "emb_indexer = {key: i for i, key in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: key for i, key in enumerate(list(embeddings.keys()))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "def path_to_root(elem, ont_mappings, curr = [], rootpath=[]):\n",
    "    curr.append(elem)\n",
    "    if elem not in ont_mappings or not ont_mappings[elem]:\n",
    "        rootpath.append(curr)\n",
    "        return\n",
    "    for node in ont_mappings[elem]:\n",
    "        curr_orig = deepcopy(curr)\n",
    "        _ = path_to_root(node, ont_mappings, curr, rootpath)\n",
    "        curr = curr_orig\n",
    "    return rootpath\n",
    "\n",
    "def get_one_hop_neighbours(ont, prop=False):\n",
    "    ont_obj = Ontology(\"conference_ontologies/\" + ont + \".owl\")\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c,d) in triples]\n",
    "    neighbours_dict = {elem: [[] for i in range(4)] for elem in list(set(flatten(entities)))}\n",
    "    for (e1, e2, p, d) in triples:\n",
    "        if e1==e2:\n",
    "            continue\n",
    "        if d == \"Object Property\":\n",
    "            neighbours_dict[e1][2].append([e2])\n",
    "            neighbours_dict[e2][2].append([e1])\n",
    "        elif d == \"Datatype Property\":\n",
    "            neighbours_dict[e1][3].append([e2])\n",
    "            neighbours_dict[e2][3].append([e1])\n",
    "        elif d == \"Subclass\":\n",
    "            neighbours_dict[e2][1].append([e1])\n",
    "        else:\n",
    "            print (\"Error wrong value of d: \", d)\n",
    "    \n",
    "    rootpath_dict = ont_obj.parents_dict\n",
    "    rootpath_dict_new = {}\n",
    "    for elem in rootpath_dict:\n",
    "        rootpath_dict_new[elem] = path_to_root(elem, rootpath_dict, [], [])\n",
    "    ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    \n",
    "    for entity in neighbours_dict:\n",
    "        if entity in rootpath_dict_new and len(rootpath_dict_new[entity]) > 0:\n",
    "            neighbours_dict[entity][0].extend(rootpath_dict_new[entity])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if prop:\n",
    "        prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "        neighbours_dict_props = {c: [[c], [], []] for a,b,c,d in prop_triples}\n",
    "        for e1, e2, p, d in prop_triples:\n",
    "            neighbours_dict_props[p][1].extend([e1])\n",
    "#             else:\n",
    "#                 ne1ighbours_dict_props[p][1].extend([e1])\n",
    "#             if e2 not in datatypes:\n",
    "            neighbours_dict_props[p][2].extend([e2])\n",
    "#             else:\n",
    "#                 neighbours_dict_props[p][2].extend([e2])\n",
    "        neighbours_dict_props = {ont + \"#\" + p: [list(OrderedSet([ont + \"#\" + e for e in elem])) for elem in neighbours_dict_props[p]] for p in neighbours_dict_props}\n",
    "        return neighbours_dict_props\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "    neighbours_dict = {ont + \"#\" + el: [[tuple([ont + \"#\" + node for node in path]) for path in nbr_type]\n",
    "                                        for nbr_type in neighbours_dict[el]] \n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: [[list(path) for path in nbr_type] for nbr_type in neighbours_dict[el]]\n",
    "                       for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "neighbours_dicts = {}\n",
    "for ont in list(set(flatten(ontologies_in_alignment))):\n",
    "    neighbours_dicts = {**neighbours_dicts, **get_one_hop_neighbours(ont)}\n",
    "\n",
    "neighbours_dicts_prop = {}\n",
    "for ont in list(set(flatten(ontologies_in_alignment))):\n",
    "    neighbours_dicts_prop = {**neighbours_dicts_prop, **get_one_hop_neighbours(ont, True)}\n",
    "\n",
    "    \n",
    "\n",
    "# max_neighbours = np.max(flatten([[len(el[e]) for e in el] for el in neighbours_dicts.values()]))\n",
    "\n",
    "# neighbours_dicts_prop = {ont.split(\"/\")[-1].split(\".\")[0]: get_one_hop_neighbours(ont, True) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "# max_neighbours_prop = np.max(flatten([[[len(elem) for elem in el[e]] for e in el] for el in neighbours_dicts_prop.values()]))\n",
    "\n",
    "# max_neighbours = np.max([max_neighbours, max_neighbours_prop])\n",
    "# neighbours_dicts = {ont: {key: neighbours_dicts[ont][key] + [\"<UNK>\" for i in range(max_neighbours -len(neighbours_dicts[ont][key]))]\n",
    "#               for key in neighbours_dicts[ont]} for ont in neighbours_dicts}\n",
    "\n",
    "# neighbours_dicts_prop = {ont: {key: [e + [\"<UNK>\" for i in range(max_neighbours -len(e))] for e in neighbours_dicts_prop[ont][key]]\n",
    "#               for key in neighbours_dicts_prop[ont]} for ont in neighbours_dicts_prop}\n",
    "\n",
    "# ontologies_in_alignment = [[el.split(\"/\")[1].split(\".\")[0] for el in ont] for ont in ontologies_in_alignment]\n",
    "emb_indexer, emb_indexer_inv, emb_vals = pickle.load(open(\"Input/data_conf.pkl\", \"rb\"))[1:4]\n",
    "\n",
    "# f = open(\"Input/data_conf_oaei_german_aml_prop_thresh.pkl\", \"wb\")\n",
    "# pickle.dump([data_ent, data_prop, aml_data_ent, aml_data_prop, data_german, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts, neighbours_dicts_prop, ontologies_in_alignment], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Input/data_conf_oaei_german_aml_prop_thresh.pkl\", \"rb\")\n",
    "data_ent, data_prop, aml_data_ent, aml_data_prop, data_german, emb_indexer, emb_indexer_inv, emb_vals, _, neighbours_dicts_prop, ontologies_in_alignment = pickle.load(f)\n",
    "f.close()\n",
    "f = open(\"Input/data_conf_oaei_german_aml_prop_thresh.pkl\", \"wb\")\n",
    "pickle.dump([data_ent, data_prop, aml_data_ent, aml_data_prop, data_german, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts, neighbours_dicts_prop, ontologies_in_alignment], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, '1.0'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_ent.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ontologies_in_alignment = pickle.load(open(\"data_path.pkl\", \"rb\"))[-1]\n",
    "ontologies_in_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6148052215576172"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(*extractUSEEmbeddings(['late paid applicant', 'late registered participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iasted-sigkdd.rdf', 'cmt-ekaw.rdf']\n",
      "0.7916666666666666 0.7307692307692307 0.76 0.7421874999999999 0.7786885245901639\n",
      "['conference-edas.rdf', 'cmt-sigkdd.rdf']\n",
      "0.8181818181818182 0.6206896551724138 0.7058823529411765 0.6521739130434783 0.7692307692307693\n",
      "['conference-sigkdd.rdf', 'conference-confOf.rdf']\n",
      "0.7727272727272727 0.5666666666666667 0.6538461538461539 0.5985915492957746 0.7203389830508474\n",
      "['edas-iasted.rdf', 'cmt-conference.rdf']\n",
      "0.75 0.5294117647058824 0.6206896551724139 0.5625 0.6923076923076923\n",
      "['edas-sigkdd.rdf', 'ekaw-iasted.rdf']\n",
      "0.875 0.56 0.6829268292682927 0.603448275862069 0.7865168539325843\n",
      "['cmt-confOf.rdf', 'edas-ekaw.rdf']\n",
      "0.7916666666666666 0.48717948717948717 0.6031746031746031 0.5277777777777778 0.7037037037037036\n",
      "['conference-ekaw.rdf', 'cmt-iasted.rdf']\n",
      "0.7142857142857143 0.6896551724137931 0.7017543859649122 0.6944444444444444 0.7092198581560283\n",
      "Final Results: [0.78764688 0.59776743 0.67546771 0.62587478 0.73714377]\n"
     ]
    }
   ],
   "source": [
    "# AML test\n",
    "def is_test(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) in test_onto\n",
    "\n",
    "results = []\n",
    "# all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "for i in list(range(0, len(ontologies_in_alignment)-1, 3)):\n",
    "    test_onto = ontologies_in_alignment[i+1:i+3]\n",
    "    for ont_pair in test_onto:\n",
    "        a, b, c = ont_pair[0], ont_pair[1], ont_pair[0] + \"-\" + ont_pair[1]\n",
    "        java_command = \"java -jar AML_v3.1/AgreementMakerLight.jar -s conference_ontologies/\" + a + \".owl\" + \\\n",
    "                            \" -t conference_ontologies/\" + b + \".owl -o AML-test-results/\" + c + \".rdf -a\"\n",
    "        process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "    print (os.listdir(\"AML-test-results/\"))\n",
    "    pred_aml = load_alignments(\"AML-test-results/\")\n",
    "    pred_aml = [tuple([el.split(\"/\")[-1] for el in key]) for key in pred_aml]\n",
    "    tp = len([elem for elem in pred_aml if data[elem]])\n",
    "    fn = len([key for key in gt_mappings if key not in set(pred_aml) and is_test(test_onto, key)])\n",
    "    fp = len([elem for elem in pred_aml if not data[elem]])\n",
    "\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = 2 * precision * recall / (precision + recall)\n",
    "    f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "    f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    print (precision, recall, f1score, f2score, f0_5score)\n",
    "    \n",
    "    metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "    results.append(metrics)\n",
    "    \n",
    "    _ = [os.remove(f) for f in glob.glob('AML-test-results/*')]\n",
    "    \n",
    "print (\"Final Results:\", np.mean(results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76 0.7307692307692307 0.7450980392156863 0.7364341085271318 0.753968253968254\n",
      "0.8461538461538461 0.7586206896551724 0.8 0.7746478873239436 0.8270676691729323\n",
      "0.7857142857142857 0.7333333333333333 0.7586206896551724 0.7432432432432431 0.7746478873239436\n",
      "0.75 0.4411764705882353 0.5555555555555556 0.4807692307692307 0.6578947368421052\n",
      "0.7777777777777778 0.56 0.6511627906976745 0.5932203389830509 0.7216494845360826\n",
      "0.7727272727272727 0.4358974358974359 0.5573770491803278 0.4775280898876404 0.6692913385826772\n",
      "0.5925925925925926 0.5517241379310345 0.5714285714285714 0.5594405594405594 0.583941605839416\n",
      "Final Results: [0.75499511 0.6016459  0.66274896 0.62361192 0.71263728]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AML test\n",
    "def is_test(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) in test_onto\n",
    "\n",
    "results = []\n",
    "prefix = \"/data/Vivek/IBM/IBM-Internship/conference_ontologies/\"\n",
    "for i in list(range(0, len(ontologies_in_alignment)-1, 3)):\n",
    "    test_onto = ontologies_in_alignment[i+1:i+3]\n",
    "    tp_tot, fn_tot, fp_tot = [], [], []\n",
    "    for ont_pair in test_onto:\n",
    "        a, b, c = prefix + ont_pair[0], prefix + ont_pair[1], ont_pair[0] + \"-\" + ont_pair[1]\n",
    "        !mkdir $c\n",
    "        java_command = \"java -jar logmap-matcher/target/logmap-matcher-4.0.jar MATCHER file:\" +  a + \".owl file:\" + b + \".owl \" + \\\n",
    "                        \"/data/Vivek/IBM/IBM-Internship/\" + c + \"/ false\"\n",
    "        process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "        \n",
    "        pred_aml = [l.strip().split(\"\\t\")[:2] for l in open(c + \"/logmap2_mappings.tsv\", \"r\").read().split(\"\\n\")[:-1]]\n",
    "        pred_aml = [tuple([el.split(\"/\")[-1] for el in key]) for key in pred_aml]\n",
    "        tp = [elem for elem in pred_aml if data[elem]]\n",
    "        fn = [key for key in gt_mappings if key not in set(pred_aml) and is_test([tuple(ont_pair)], key)]\n",
    "        fp = [elem for elem in pred_aml if not data[elem]]\n",
    "        \n",
    "        tp_tot.extend(tp)\n",
    "        fn_tot.extend(fn)\n",
    "        fp_tot.extend(fp)\n",
    "        \n",
    "        !rm -rf $c\n",
    "   \n",
    "    precision = len(tp_tot)/(len(tp_tot)+len(fp_tot))\n",
    "    recall = len(tp_tot)/(len(tp_tot)+len(fn_tot))\n",
    "    f1score = 2 * precision * recall / (precision + recall)\n",
    "    f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "    f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    print (precision, recall, f1score, f2score, f0_5score)\n",
    "    \n",
    "    metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "    results.append(metrics)\n",
    "    \n",
    "    \n",
    "    \n",
    "print (\"Final Results:\", np.mean(results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ontologies_in_alignment = pickle.load(open(\"data_multi_rootpath.pkl\", \"rb\"))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best thresholds: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print (\"Best thresholds: {}\".format([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 1240)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:int(sys.argv[1])] for el in neighbours_dicts[ont]\n",
    "       if count_non_unk(neighbours_dicts[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['confOf', 'sigkdd'],\n",
       " ['iasted', 'sigkdd'],\n",
       " ['cmt', 'ekaw'],\n",
       " ['confOf', 'iasted'],\n",
       " ['conference', 'edas'],\n",
       " ['cmt', 'sigkdd'],\n",
       " ['ekaw', 'sigkdd'],\n",
       " ['conference', 'confOf'],\n",
       " ['conference', 'sigkdd'],\n",
       " ['confOf', 'edas'],\n",
       " ['cmt', 'conference'],\n",
       " ['edas', 'iasted'],\n",
       " ['conference', 'iasted'],\n",
       " ['edas', 'sigkdd'],\n",
       " ['ekaw', 'iasted'],\n",
       " ['cmt', 'edas'],\n",
       " ['edas', 'ekaw'],\n",
       " ['cmt', 'confOf'],\n",
       " ['confOf', 'ekaw'],\n",
       " ['conference', 'ekaw'],\n",
       " ['cmt', 'iasted']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontologies_in_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conference', 'iasted'),\n",
       " ('ekaw', 'sigkdd'),\n",
       " ('cmt', 'sigkdd'),\n",
       " ('edas', 'ekaw'),\n",
       " ('conference', 'sigkdd'),\n",
       " ('iasted', 'sigkdd'),\n",
       " ('confOf', 'sigkdd'),\n",
       " ('ekaw', 'iasted'),\n",
       " ('conference', 'ekaw'),\n",
       " ('cmt', 'ekaw'),\n",
       " ('edas', 'sigkdd'),\n",
       " ('confOf', 'iasted'),\n",
       " ('cmt', 'conference'),\n",
       " ('cmt', 'confOf'),\n",
       " ('cmt', 'iasted'),\n",
       " ('conference', 'edas'),\n",
       " ('confOf', 'edas'),\n",
       " ('edas', 'iasted'),\n",
       " ('confOf', 'ekaw'),\n",
       " ('conference', 'confOf'),\n",
       " ('cmt', 'edas')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registeered applicant registered applicant\n",
      "technically organised by technically organized by\n",
      "ngo no\n",
      "sponzorship sponsorship\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://montanaflynn-spellcheck.p.rapidapi.com/check/\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"montanaflynn-spellcheck.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"9965b01207msh06291e57d6f2c55p1a6a16jsn0fb016da4a62\"\n",
    "    }\n",
    "\n",
    "# inp_spellchecked = []\n",
    "for concept in inp[731:]:\n",
    "    querystring = {\"text\": concept}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring).json()\n",
    "    if response[\"suggestion\"] != concept:\n",
    "        resolved = str(concept)\n",
    "        for word in response[\"corrections\"]:\n",
    "            if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "                resolved = resolved.replace(word, response[\"corrections\"][word][0])\n",
    "        \n",
    "        inp_spellchecked.append(resolved)\n",
    "        print (concept, resolved)\n",
    "    else:\n",
    "        inp_spellchecked.append(concept)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24726325273513794"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(*extractUSEEmbeddings([\"die lebensmittel\", \"food\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7796"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_spellchecked, fp_spellchecked = [dict(el) for el in pickle.load(open(\"test_v2.pkl\", \"rb\"))]\n",
    "fn_baseline, fp_baseline = [dict(el) for el in pickle.load(open(\"test_best.pkl\", \"rb\"))]\n",
    "fn_unhas, fp_unhas = [dict(el) for el in pickle.load(open(\"test_unhas.pkl\", \"rb\"))]\n",
    "fn_resolved, fp_resolved = [dict(el) for el in pickle.load(open(\"test_resolved.pkl\", \"rb\"))]\n",
    "\n",
    "fn_dict, fp_dict = {}, {}\n",
    "def create_comparison_file(file, idx):\n",
    "    fn, fp = [dict(el) for el in pickle.load(open(file, \"rb\"))]\n",
    "    \n",
    "    for key in fn:\n",
    "        if key in fn_dict:\n",
    "            fn_dict[key][idx] = fn[key]\n",
    "        else:\n",
    "            fn_dict[key] = [\"N/A\" for i in range(4)]\n",
    "            fn_dict[key][idx] = fn[key]\n",
    "    \n",
    "    for key in fp:\n",
    "        if key in fp_dict:\n",
    "            fp_dict[key][idx] = fp[key]\n",
    "        else:\n",
    "            fp_dict[key] = [\"N/A\" for i in range(4)]\n",
    "            fp_dict[key][idx] = fp[key]\n",
    "    \n",
    "\n",
    "create_comparison_file(\"test_best.pkl\", 0)\n",
    "create_comparison_file(\"test_unhas.pkl\", 1)\n",
    "create_comparison_file(\"test_v2.pkl\", 2)\n",
    "create_comparison_file(\"test_resolved.pkl\", 3)\n",
    "\n",
    "open(\"fn - comparison.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(el) for el in flatten(el)]) for el in fn_dict.items()]))\n",
    "open(\"fp - comparison.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(el) for el in flatten(el)]) for el in fp_dict.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['confOf', 'sigkdd'],\n",
       " ['iasted', 'sigkdd'],\n",
       " ['cmt', 'ekaw'],\n",
       " ['confOf', 'iasted'],\n",
       " ['conference', 'edas'],\n",
       " ['cmt', 'sigkdd'],\n",
       " ['ekaw', 'sigkdd'],\n",
       " ['conference', 'confOf'],\n",
       " ['conference', 'sigkdd'],\n",
       " ['confOf', 'edas'],\n",
       " ['cmt', 'conference'],\n",
       " ['edas', 'iasted'],\n",
       " ['conference', 'iasted'],\n",
       " ['edas', 'sigkdd'],\n",
       " ['ekaw', 'iasted'],\n",
       " ['cmt', 'edas'],\n",
       " ['edas', 'ekaw'],\n",
       " ['cmt', 'confOf'],\n",
       " ['confOf', 'ekaw'],\n",
       " ['conference', 'ekaw'],\n",
       " ['cmt', 'iasted']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontologies_in_alignment = pickle.load(open(\"data_path.pkl\", \"rb\"))[-1]\n",
    "ontologies_in_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['confOf#Organization', 'sigkdd#Organizator', '1', '2', '3', '4'],\n",
       " ['iasted#Document', 'sigkdd#Document', '5', '6', '78', '8']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {('confOf#Organization', 'sigkdd#Organizator'): (1,2,3,4),\n",
    " ('iasted#Document', 'sigkdd#Document'): (5,6,78,8)}\n",
    "[[str(el) for el in flatten(el)] for el in d.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left ('Chair_PC', 'Program_Chair') PC Program_Chair\n",
      "left ('Chair_PC', 'Program_Committee') PC Program_Committee\n",
      "left ('Chair_PC', 'Program_Committee_member') PCM Program_Committee\n",
      "left ('Member_PC', 'Program_Chair') PC Program_Chair\n",
      "left ('Member_PC', 'Program_Committee') PC Program_Committee\n",
      "left ('Member_PC', 'Program_Committee_member') PCM Program_Committee\n",
      "left ('Chair_PC', 'Presenter_city') PC Presenter_city\n",
      "left ('Member_PC', 'Presenter_city') PC Presenter_city\n",
      "left ('OC_Member', 'Organizing_Committee_member') OCM Organizing_Committee\n",
      "left ('OC_Member', 'Organizing_Committee') OC Organizing_Committee\n",
      "left ('PC_Member', 'Program_Chair') PC Program_Chair\n",
      "left ('PC_Member', 'Program_Committee') PC Program_Committee\n",
      "left ('PC_Member', 'Program_Committee_member') PCM Program_Committee\n",
      "left ('OC_Chair', 'Organizing_Committee_member') OCM Organizing_Committee\n",
      "left ('OC_Chair', 'Organizing_Committee') OC Organizing_Committee\n",
      "left ('PC_Chair', 'Program_Chair') PC Program_Chair\n",
      "left ('PC_Chair', 'Program_Committee') PC Program_Committee\n",
      "left ('PC_Chair', 'Program_Committee_member') PCM Program_Committee\n",
      "right ('Passive_conference_participant', 'Chair_PC') PCP Passive_conference\n",
      "right ('Passive_conference_participant', 'Member_PC') PCP Passive_conference\n",
      "right ('Program_committee', 'Chair_PC') PC Program_committee\n",
      "right ('Program_committee', 'Member_PC') PC Program_committee\n",
      "left ('SC_Member', 'Brief_introduction_for_Session_chair') BIFSC Session_chair\n",
      "left ('SC_Member', 'Session_chair') SC Session_chair\n",
      "left ('SC_Member', 'Sponsor_city') SC Sponsor_city\n",
      "left ('SC_Member', 'Sponsor_company_house') SCH Sponsor_company\n",
      "left ('OC_Member', 'One_conference_day') OCD One_conference\n",
      "left ('PC_Member', 'Presenter_city') PC Presenter_city\n",
      "left ('OC_Chair', 'One_conference_day') OCD One_conference\n",
      "left ('PC_Chair', 'Presenter_city') PC Presenter_city\n",
      "left ('Chair_PC', 'PC_Chair') PC PC_Chair\n",
      "left ('Member_PC', 'PC_Chair') PC PC_Chair\n",
      "right ('Submitted_contribution', 'SC_Member') SC Submitted_contribution\n",
      "right ('Steering_committee', 'SC_Member') SC Steering_committee\n",
      "right ('Passive_conference_participant', 'PC_Member') PCP Passive_conference\n",
      "right ('Passive_conference_participant', 'PC_Chair') PCP Passive_conference\n",
      "right ('Program_committee', 'PC_Member') PC Program_committee\n",
      "right ('Program_committee', 'PC_Chair') PC Program_committee\n",
      "right ('Organizing_committee', 'OC_Member') OC Organizing_committee\n",
      "right ('Organizing_committee', 'OC_Chair') OC Organizing_committee\n",
      "left ('paperID', 'is_dated_on') IDO is_dated\n"
     ]
    }
   ],
   "source": [
    "abbreviations_dict = {}\n",
    "final_dict = {}\n",
    "\n",
    "for mapping in all_mappings:\n",
    "    mapping = tuple([el.split(\"#\")[1] for el in mapping])\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[0])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\"_\")])\n",
    "        if is_abb.group() in abbreviation:\n",
    "            \n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[1].split(\"_\")[start:end])\n",
    "            print (\"left\", mapping, abbreviation, fullform)\n",
    "            \n",
    "            rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[1].split(\"_\")[:start] + mapping[1].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[1])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\"_\")])\n",
    "        \n",
    "        if is_abb.group() in abbreviation:\n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[0].split(\"_\")[start:end])\n",
    "            print (\"right\", mapping, abbreviation, fullform)\n",
    "\n",
    "            rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[0].split(\"_\")[:start] + mapping[0].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n",
    "\n",
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
    "filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n",
    "inp_resolved = []\n",
    "for concept in inp:\n",
    "    for key in filtered_dict:\n",
    "        concept = concept.replace(key, filtered_dict[key])\n",
    "    inp_resolved.append(concept)\n",
    "inp_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8085169792175293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(*extractUSEEmbeddings([\"Conference Banquet\", \"Dinner Banquet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('confOf#hasEmail', 'sigkdd#E-mail'), 0.9161555776735063),\n",
       " (('confOf#Chair_PC', 'sigkdd#Program_Chair'), 0.8290806880788957),\n",
       " (('iasted#Student_registration_fee', 'sigkdd#Registration_Student'),\n",
       "  0.9156892709934972),\n",
       " (('iasted#Deadline_for_notification_of_acceptance',\n",
       "   'sigkdd#Deadline_Author_notification'),\n",
       "  0.5085231269229767),\n",
       " (('iasted#Nonmember_registration_fee', 'sigkdd#Registration_Non-Member'),\n",
       "  0.8098175068174337),\n",
       " (('cmt#ConferenceMember', 'ekaw#Conference_Participant'), 0.3532655254301476),\n",
       " (('cmt#Author', 'ekaw#Paper_Author'), 0.9288647440450568),\n",
       " (('cmt#writtenBy', 'ekaw#reviewWrittenBy'), 0.7592434303979944),\n",
       " (('cmt#hasBeenAssigned', 'ekaw#reviewerOfPaper'), 0.2502871547766108),\n",
       " (('cmt#assignedTo', 'ekaw#hasReviewer'), 0.3499445900379554),\n",
       " (('cmt#PaperFullVersion', 'ekaw#Regular_Paper'), 0.8379052493933772),\n",
       " (('confOf#Administrative_event', 'iasted#Activity_before_conference'),\n",
       "  0.37740041094526244),\n",
       " (('conference#has_the_first_name', 'edas#hasFirstName'), 0.880357115623567),\n",
       " (('conference#Conference_part', 'edas#ConferenceEvent'), 0.8634459315564949),\n",
       " (('conference#has_a_review_expertise', 'edas#hasRating'), 0.3899073903898582),\n",
       " (('cmt#ConferenceChair', 'sigkdd#General_Chair'), 0.8078542602095952),\n",
       " (('cmt#submitPaper', 'sigkdd#submit'), 0.7581630948456032),\n",
       " (('ekaw#PC_Chair', 'sigkdd#Program_Chair'), 0.8432266698142893),\n",
       " (('conference#Conference_fees', 'sigkdd#Fee'), 0.6188697294990487),\n",
       " (('conference#has_an_email', 'sigkdd#E-mail'), 0.7713571936527924),\n",
       " (('conference#is_given_by', 'sigkdd#presentationed_by'), 0.3609120954885683),\n",
       " (('conference#gives_presentations', 'sigkdd#presentation'),\n",
       "  0.6056676430562652),\n",
       " (('conference#has_an_email', 'confOf#hasEmail'), 0.8262672905906706),\n",
       " (('conference#Conference_contribution', 'confOf#Contribution'),\n",
       "  0.8299805404727582),\n",
       " (('conference#has_a_track-workshop-tutorial_topic', 'confOf#hasTopic'),\n",
       "  0.2714890208534184),\n",
       " (('confOf#reviewes', 'edas#isReviewing'), 0.6861398267050898),\n",
       " (('confOf#writtenBy', 'edas#isWrittenBy'), 0.8407108246292888),\n",
       " (('confOf#Working_event', 'edas#AcademicEvent'), 0.8097133404737215),\n",
       " (('confOf#Member_PC', 'edas#TPCMember'), 0.678868106101965),\n",
       " (('confOf#writes', 'edas#hasRelatedPaper'), 0.297312720126214),\n",
       " (('edas#WelcomeTalk', 'iasted#Welcome_address'), 0.7917928042109413),\n",
       " (('edas#Attendee', 'iasted#Delegate'), 0.5473781390424541),\n",
       " (('edas#SocialEvent', 'iasted#Social_program'), 0.3743878054700725),\n",
       " (('edas#SlideSet', 'iasted#Transparency'), 0.22831568660972779),\n",
       " (('edas#Paper', 'iasted#Submission'), 0.5257373243903976),\n",
       " (('edas#ConferenceVenuePlace', 'iasted#Conference_building'),\n",
       "  0.7506334913541919),\n",
       " (('edas#DiningPlace', 'iasted#Conference_restaurant'), 0.8535001980925233),\n",
       " (('cmt#Preference', 'conference#Review_preference'), 0.783329808487203),\n",
       " (('cmt#email', 'conference#has_an_email'), 0.8262672905906706),\n",
       " (('cmt#Co-author', 'conference#Contribution_co-author'), 0.819271045480498),\n",
       " (('cmt#assignedByReviewer', 'conference#invited_by'), 0.48464419365137834),\n",
       " (('cmt#assignExternalReviewer', 'conference#invites_co-reviewers'),\n",
       "  0.4054012020450263),\n",
       " (('edas#hasCostAmount', 'sigkdd#Price'), 0.7047592167957165),\n",
       " (('edas#hasName', 'sigkdd#Name_of_conference'), 0.4805131633714943),\n",
       " (('edas#ConferenceVenuePlace', 'sigkdd#Conference_hall'), 0.6088660708614848),\n",
       " (('edas#AccommodationPlace', 'sigkdd#Hotel'), 0.5872493203395058),\n",
       " (('edas#startDate', 'sigkdd#Start_of_conference'), 0.528186177578591),\n",
       " (('edas#ConferenceChair', 'sigkdd#General_Chair'), 0.8688377867265579),\n",
       " (('edas#endDate', 'sigkdd#End_of_conference'), 0.5493031244333424),\n",
       " (('edas#Attendee', 'sigkdd#Listener'), 0.5273161396977948),\n",
       " (('conference#Passive_conference_participant', 'iasted#Listener'),\n",
       "  0.24072285668445437),\n",
       " (('conference#Active_conference_participant', 'iasted#Speaker'),\n",
       "  0.0043877585366876425),\n",
       " (('conference#Conference_fees', 'iasted#Fee'), 0.6188697294990487),\n",
       " (('conference#contributes', 'iasted#write'), 0.37880804066855067),\n",
       " (('conference#Conference_part', 'iasted#Conference_activity'),\n",
       "  0.754729668736523),\n",
       " (('conference#Submitted_contribution', 'iasted#Submission'),\n",
       "  0.6515994788116265),\n",
       " (('conference#Camera_ready_contribution', 'iasted#Final_manuscript'),\n",
       "  0.20353872337293488),\n",
       " (('conference#Conference_proceedings', 'iasted#Publication'),\n",
       "  0.1916452210219224),\n",
       " (('ekaw#Conference_Banquet', 'iasted#Dinner_banquet'), 0.9146926474654158),\n",
       " (('cmt#writePaper', 'confOf#writes'), 0.6220006428478738),\n",
       " (('cmt#ConferenceMember', 'confOf#Member'), 0.8568252252381775),\n",
       " (('cmt#PaperFullVersion', 'confOf#Paper'), 0.6356338598318733),\n",
       " (('cmt#hasBeenAssigned', 'confOf#reviewes'), 0.40867753235276827),\n",
       " (('cmt#hasAuthor', 'confOf#writtenBy'), 0.5261010208677641),\n",
       " (('cmt#hasSubjectArea', 'confOf#dealsWith'), 0.25596982134585333),\n",
       " (('cmt#hasConferenceMember', 'edas#hasMember'), 0.6716821946490479),\n",
       " (('cmt#memberOfConference', 'edas#isMemberOf'), 0.5384339627563817),\n",
       " (('cmt#hasAuthor', 'edas#isWrittenBy'), 0.3940901098620504),\n",
       " (('cmt#hasBeenAssigned', 'edas#isReviewing'), 0.4039842265275537),\n",
       " (('cmt#assignedTo', 'edas#isReviewedBy'), 0.5476728328223632),\n",
       " (('edas#isReviewedBy', 'ekaw#hasReviewer'), 0.38891106779153306),\n",
       " (('edas#AcademiaOrganization', 'ekaw#Academic_Institution'),\n",
       "  0.7638726986015515),\n",
       " (('edas#isReviewing', 'ekaw#reviewerOfPaper'), 0.4574440998156941),\n",
       " (('edas#isLocationOf', 'ekaw#locationOf'), 0.7440274368714033),\n",
       " (('edas#Programme', 'ekaw#Programme_Brochure'), 0.5409891762364093),\n",
       " (('edas#Attendee', 'ekaw#Conference_Participant'), 0.8768055189198933),\n",
       " (('edas#hasLocation', 'ekaw#heldIn'), 0.3689859558260715),\n",
       " (('conference#Information_for_participants', 'ekaw#Programme_Brochure'),\n",
       "  -0.1194315228734015),\n",
       " (('conference#has_a_review', 'ekaw#hasReview'), 0.8414144924885054),\n",
       " (('conference#Late_paid_applicant', 'ekaw#Late-Registered_Participant'),\n",
       "  0.654309404831223),\n",
       " (('conference#Early_paid_applicant', 'ekaw#Early-Registered_Participant'),\n",
       "  0.5991906447371812),\n",
       " (('conference#Poster', 'ekaw#Poster_Paper'), 0.7686016382074798),\n",
       " (('conference#Conference_www', 'ekaw#Web_Site'), 0.44282246019334814),\n",
       " (('conference#contributes', 'ekaw#authorOf'), 0.3016937274326247),\n",
       " (('conference#Accepted_contribution', 'ekaw#Accepted_Paper'),\n",
       "  0.8357883000723147),\n",
       " (('conference#Reviewed_contribution', 'ekaw#Evaluated_Paper'),\n",
       "  0.6112944103257296),\n",
       " (('conference#Submitted_contribution', 'ekaw#Submitted_Paper'),\n",
       "  0.6904896027545419),\n",
       " (('confOf#Poster', 'ekaw#Poster_Paper'), 0.7685522235623811),\n",
       " (('confOf#Contribution', 'ekaw#Paper'), 0.639392230902997),\n",
       " (('confOf#Trip', 'ekaw#Conference_Trip'), 0.8240761817569572),\n",
       " (('confOf#Scholar', 'ekaw#Student'), 0.5104832530935325)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pay',\n",
       " 'rejected by',\n",
       " 'Registration SIGMOD Member',\n",
       " 'is connected with',\n",
       " 'NGO',\n",
       " 'overhead projector',\n",
       " 'name of conference',\n",
       " 'call for participation',\n",
       " 'coffee break',\n",
       " 'scientifically organised by',\n",
       " 'volunteer',\n",
       " 'publisher',\n",
       " 'regular',\n",
       " 'add program committee member',\n",
       " 'contact email',\n",
       " 'part of event',\n",
       " 'dinner banquet',\n",
       " 'social program',\n",
       " 'decision',\n",
       " 'is sent by',\n",
       " 'was a committee chair of',\n",
       " 'organisation',\n",
       " 'paper',\n",
       " 'assign external reviewer',\n",
       " 'was a member of',\n",
       " 'has cost amount',\n",
       " 'session chair',\n",
       " 'single level conference',\n",
       " 'hotel',\n",
       " 'deadline hotel reservation',\n",
       " 'country',\n",
       " 'important dates',\n",
       " 'paper',\n",
       " 'has parts',\n",
       " 'event',\n",
       " 'paper due on',\n",
       " 'is the 1th part of',\n",
       " 'has surname',\n",
       " 'wireless communications topic',\n",
       " 'two level conference',\n",
       " 'has first name',\n",
       " 'bid',\n",
       " 'is designed for',\n",
       " 'author',\n",
       " 'double hotel room',\n",
       " 'review form',\n",
       " 'presentation',\n",
       " 'subject area',\n",
       " 'positive integer',\n",
       " 'deadline abstract submission',\n",
       " 'has a degree',\n",
       " 'is present in',\n",
       " 'related to event',\n",
       " 'reviews',\n",
       " 'topic',\n",
       " 'e-mail',\n",
       " 'has keyword',\n",
       " 'university',\n",
       " 'has street',\n",
       " 'reviewed contribution',\n",
       " 'call',\n",
       " 'occupy',\n",
       " 'ends on',\n",
       " 'presenter university',\n",
       " 'rejected paper',\n",
       " 'obtain',\n",
       " 'invited by',\n",
       " 'paper',\n",
       " 'updated version of',\n",
       " 'company',\n",
       " 'topic',\n",
       " 'call for paper',\n",
       " 'student',\n",
       " 'has reviewer',\n",
       " 'has updated version',\n",
       " 'invited talk',\n",
       " 'free time break',\n",
       " 'non academic event',\n",
       " 'best student paper supporter',\n",
       " 'publisher of',\n",
       " 'submit paper',\n",
       " 'author',\n",
       " 'program committee',\n",
       " 'TPC Member',\n",
       " 'listener',\n",
       " 'hotel fee',\n",
       " 'int',\n",
       " 'default choice',\n",
       " 'review of paper',\n",
       " 'author',\n",
       " 'poster',\n",
       " 'has amount of',\n",
       " 'web site',\n",
       " 'has location',\n",
       " 'subclass of',\n",
       " 'conference days',\n",
       " 'has a track-workshop-tutorial topic',\n",
       " 'welcome address',\n",
       " 'sponzorship',\n",
       " 'program committee',\n",
       " 'read by meta-reviewer',\n",
       " 'conference proceedings',\n",
       " 'regular contribution',\n",
       " 'reception',\n",
       " 'abstract',\n",
       " 'print hardcopy mailing manifests',\n",
       " 'Chair PC',\n",
       " 'has co-author',\n",
       " 'card',\n",
       " 'location',\n",
       " 'conference hall',\n",
       " 'is writen by',\n",
       " 'has email',\n",
       " 'antennas topic',\n",
       " 'trip city',\n",
       " 'has contributions',\n",
       " 'review',\n",
       " 'was a committee of',\n",
       " 'call for manuscripts',\n",
       " 'worker lecturer',\n",
       " 'video presentation',\n",
       " 'has email',\n",
       " 'reception',\n",
       " 'meta-review',\n",
       " 'inverse of part of 7',\n",
       " 'computer networks topic',\n",
       " 'paper presentation',\n",
       " 'coctail reception',\n",
       " 'slide set',\n",
       " 'has biography',\n",
       " 'subclass of',\n",
       " 'has an email',\n",
       " 'location',\n",
       " 'chair',\n",
       " 'receiving manuscript',\n",
       " 'user',\n",
       " 'has menu',\n",
       " 'author of',\n",
       " 'tip',\n",
       " 'is an abstract submission date',\n",
       " 'date',\n",
       " 'session room',\n",
       " 'committee',\n",
       " 'details entered by',\n",
       " 'committee',\n",
       " 'nonauthor registration fee',\n",
       " 'workshop',\n",
       " 'personal history',\n",
       " 'has call',\n",
       " 'program committee',\n",
       " 'speak in',\n",
       " 'programme brochure',\n",
       " 'end date',\n",
       " 'accepted contribution',\n",
       " 'enable virtual meeting',\n",
       " 'topic covered by',\n",
       " 'sponzor',\n",
       " 'has city',\n",
       " 'has topic',\n",
       " 'scientific event',\n",
       " 'has phone',\n",
       " 'co-chair',\n",
       " 'written contribution',\n",
       " 'string',\n",
       " 'submit until',\n",
       " 'personal review history',\n",
       " 'location of',\n",
       " 'has an ISBN',\n",
       " 'subclass of',\n",
       " 'date',\n",
       " 'registration',\n",
       " 'remark',\n",
       " 'state',\n",
       " 'building',\n",
       " 'city of conference',\n",
       " 'possible reviewer',\n",
       " 'personal publication history',\n",
       " 'conference banquet',\n",
       " 'gives presentations',\n",
       " 'tax',\n",
       " 'email',\n",
       " 'has end date time',\n",
       " 'SC Member',\n",
       " 'conference part',\n",
       " 'date',\n",
       " 'session',\n",
       " 'contributes',\n",
       " 'assigned paper',\n",
       " 'external reviewer',\n",
       " 'Registration SIGKDD Member',\n",
       " 'submissions deadline',\n",
       " 'prepare',\n",
       " 'radio communications topic',\n",
       " 'PC Member',\n",
       " 'introduction',\n",
       " 'steering committee',\n",
       " 'security topic',\n",
       " 'has title',\n",
       " 'travel grant',\n",
       " 'submit',\n",
       " 'science worker',\n",
       " 'set max papers',\n",
       " 'invited speaker',\n",
       " 'has review',\n",
       " 'poster',\n",
       " 'author of paper student',\n",
       " 'camera ready manuscript deadline',\n",
       " 'modelling',\n",
       " 'is held after',\n",
       " 'powerline transmission topic',\n",
       " 'name',\n",
       " 'read paper',\n",
       " 'computer networks management topic',\n",
       " 'review question',\n",
       " 'presentation',\n",
       " 'negative review',\n",
       " 'communications topic',\n",
       " 'co-write paper',\n",
       " 'PC Chair',\n",
       " 'has phone',\n",
       " 'memeber registration fee',\n",
       " 'conference volume',\n",
       " 'date time',\n",
       " 'date',\n",
       " 'worker non speaker',\n",
       " 'review',\n",
       " 'invitation letter',\n",
       " 'conference venue place',\n",
       " 'has related document',\n",
       " 'presenter city',\n",
       " 'delegate',\n",
       " 'workshop session',\n",
       " 'research institute',\n",
       " 'has a location',\n",
       " 'multi-author volume',\n",
       " 'industrial session',\n",
       " 'currency',\n",
       " 'lists event',\n",
       " 'car',\n",
       " 'held in',\n",
       " 'has gender',\n",
       " 'has member',\n",
       " 'conference trip',\n",
       " 'presentationed by',\n",
       " 'one day presenter',\n",
       " 'city',\n",
       " 'follows',\n",
       " 'hotel room',\n",
       " 'satellite and space communications topic',\n",
       " 'conference session',\n",
       " 'computer networks enterprise topic',\n",
       " 'referenced in',\n",
       " 'associated chair',\n",
       " 'contribution co-author',\n",
       " 'coffee break',\n",
       " 'obtain',\n",
       " 'has attendee',\n",
       " 'is paid with',\n",
       " 'covers topic',\n",
       " 'document',\n",
       " 'organizing committee member',\n",
       " 'plenary lecture speaker',\n",
       " 'conference participant',\n",
       " 'can stay in',\n",
       " 'has a review reference or expertise',\n",
       " 'conference hall',\n",
       " 'track',\n",
       " 'has a program committee',\n",
       " 'start date',\n",
       " 'working event',\n",
       " 'student non speaker',\n",
       " 'academia organization',\n",
       " 'invited speaker',\n",
       " 'workshop',\n",
       " 'is a date of camera ready paper submission',\n",
       " 'registration of participants event',\n",
       " 'has subject area',\n",
       " 'contribution',\n",
       " 'reviewing event',\n",
       " 'paper assignment tools run by',\n",
       " 'sponsor state',\n",
       " 'evaluated paper',\n",
       " 'sponsor',\n",
       " 'technic activity',\n",
       " 'accepted paper',\n",
       " 'conference',\n",
       " 'parallel with',\n",
       " 'unsigned long',\n",
       " 'IASTED non member',\n",
       " 'review rating',\n",
       " 'conference document',\n",
       " 'has author',\n",
       " 'workshop',\n",
       " 'document',\n",
       " 'program committee chair',\n",
       " 'registration fee',\n",
       " 'organizational meeting',\n",
       " 'assigned by administrator',\n",
       " 'book proceeding',\n",
       " 'has part',\n",
       " 'has conflict of interest',\n",
       " 'sponsor company house',\n",
       " 'string',\n",
       " 'has short title',\n",
       " 'enter review criteria',\n",
       " 'int',\n",
       " 'boolean',\n",
       " 'is needed for',\n",
       " 'author',\n",
       " 'name',\n",
       " 'payed by',\n",
       " 'int',\n",
       " 'has street',\n",
       " 'write',\n",
       " 'tutorial speaker',\n",
       " 'invited talk abstract',\n",
       " 'has rating',\n",
       " 'paper abstract',\n",
       " 'presenter state',\n",
       " 'has tracks',\n",
       " 'member of program committee',\n",
       " 'has last name',\n",
       " 'author information form',\n",
       " 'max papers',\n",
       " 'one conference day',\n",
       " 'is reviewing',\n",
       " 'social event',\n",
       " 'logo URL',\n",
       " 'has a volume',\n",
       " 'invites co-reviewers',\n",
       " 'session chair',\n",
       " 'communication theory topic',\n",
       " 'has review history',\n",
       " 'conference applicant',\n",
       " 'operating topicsystems',\n",
       " 'has a topic or a submission contribution',\n",
       " 'welcome talk',\n",
       " 'webmaster',\n",
       " 'cryptography topic',\n",
       " 'sponsorship',\n",
       " 'person',\n",
       " 'has submission instructions',\n",
       " 'paid applicant',\n",
       " 'workshop chair',\n",
       " 'activity after conference',\n",
       " 'organizing committee',\n",
       " 'conference',\n",
       " 'bank transfer',\n",
       " 'assigned by reviewer',\n",
       " 'has city',\n",
       " 'assistant',\n",
       " 'Member PC',\n",
       " 'banquet',\n",
       " 'poster session',\n",
       " 'technically organised by',\n",
       " 'computer networks sensor topic',\n",
       " 'is member of',\n",
       " 'is given by',\n",
       " 'done till',\n",
       " 'review written by',\n",
       " 'belongs to event',\n",
       " 'provided by',\n",
       " 'late-registered participant',\n",
       " 'study at',\n",
       " 'administrator',\n",
       " 'starts on',\n",
       " 'has important dates',\n",
       " 'sponzor fee',\n",
       " 'conference chair',\n",
       " 'audiovisual equipment',\n",
       " 'assign reviewer',\n",
       " 'has start date time',\n",
       " 'has a steering committee',\n",
       " 'reject rating',\n",
       " 'lecture',\n",
       " 'person',\n",
       " 'is visited by',\n",
       " 'demo paper',\n",
       " 'gold supporter',\n",
       " 'conference contributor',\n",
       " 'attendee',\n",
       " 'has country',\n",
       " 'is submitted at',\n",
       " 'credit card',\n",
       " 'deals with',\n",
       " 'textual review question',\n",
       " 'best research paper award',\n",
       " 'dining place',\n",
       " 'neutral review',\n",
       " 'nation',\n",
       " 'abstract',\n",
       " 'accepted by',\n",
       " 'is topic of',\n",
       " 'best paper awards committee',\n",
       " 'has a publisher',\n",
       " 'tutorial abstract',\n",
       " 'author',\n",
       " 'deadline paper submission',\n",
       " 'meal event',\n",
       " 'committee member',\n",
       " 'conference airport',\n",
       " 'registration due on',\n",
       " 'exhibitor',\n",
       " 'CAD Topic',\n",
       " 'review preference',\n",
       " 'volume contains paper',\n",
       " 'has a submitted contribution',\n",
       " 'reviewer',\n",
       " 'camera ready contribution',\n",
       " 'place',\n",
       " 'is prepared by',\n",
       " 'has a date of issue',\n",
       " 'trip day',\n",
       " 'conference fees',\n",
       " 'is signed by',\n",
       " 'demo session',\n",
       " 'string',\n",
       " 'platinum supporter',\n",
       " 'pay',\n",
       " 'administrative event',\n",
       " 'is held in',\n",
       " 'computer networks optical topic',\n",
       " 'final manuscript',\n",
       " 'accepted paper',\n",
       " 'author of paper',\n",
       " 'has postal code',\n",
       " 'contribution 1th-author',\n",
       " 'is present',\n",
       " 'was a program committee of',\n",
       " 'author cd proceedings included',\n",
       " 'multimedia topic',\n",
       " 'sponsor city',\n",
       " 'speaker',\n",
       " 'write paper',\n",
       " 'numerical review question',\n",
       " 'research topic',\n",
       " 'co-author',\n",
       " 'fee for extra trip',\n",
       " 'was a track-workshop chair of',\n",
       " 'subclass of',\n",
       " 'contributed talk',\n",
       " 'hold',\n",
       " 'has title',\n",
       " 'van',\n",
       " 'acceptance',\n",
       " 'abstract',\n",
       " 'scholar',\n",
       " 'conference activity',\n",
       " 'adjust bid',\n",
       " 'hotel registration form',\n",
       " 'conference city',\n",
       " 'read by reviewer',\n",
       " 'government organization',\n",
       " 'reviewing results event',\n",
       " 'active paper',\n",
       " 'workshop',\n",
       " 'mobile computing topic',\n",
       " 'int',\n",
       " 'document',\n",
       " 'publication',\n",
       " 'excursion',\n",
       " 'student registration fee',\n",
       " 'reviewer',\n",
       " 'has a URL',\n",
       " 'written by',\n",
       " 'for event',\n",
       " 'finalize paper assignment',\n",
       " 'has been assigned',\n",
       " 'cd proceening',\n",
       " 'paper',\n",
       " 'registration student',\n",
       " 'deadline for notification of acceptance',\n",
       " 'has authors',\n",
       " 'computer architecture topic',\n",
       " 'is reviewed by',\n",
       " 'agency staff member',\n",
       " 'paper full version',\n",
       " 'introduction of speaker',\n",
       " 'registration form',\n",
       " 'person',\n",
       " 'computer networks switching topic',\n",
       " 'reviewer',\n",
       " 'has submission deadline',\n",
       " 'organization',\n",
       " 'accept rating',\n",
       " 'hotel presenter',\n",
       " 'subclass of',\n",
       " 'programme',\n",
       " 'conference',\n",
       " 'listener',\n",
       " 'related to paper',\n",
       " 'has members',\n",
       " 'submission event',\n",
       " 'event',\n",
       " 'award',\n",
       " 'conference',\n",
       " 'country',\n",
       " 'conference contribution',\n",
       " 'is sent after',\n",
       " 'go through',\n",
       " 'is given by',\n",
       " 'organising agency',\n",
       " 'closing talk',\n",
       " 'is dated on',\n",
       " 'start reviewer bidding',\n",
       " 'title',\n",
       " 'cheque',\n",
       " 'camera ready paper',\n",
       " 'end of conference',\n",
       " 'form',\n",
       " 'is made from',\n",
       " 'paper ID',\n",
       " 'is paid by',\n",
       " 'holded by',\n",
       " 'extended abstract',\n",
       " 'write review',\n",
       " 'is provider of',\n",
       " 'silver supporter',\n",
       " 'meta-reviewer',\n",
       " 'paper',\n",
       " 'end review',\n",
       " 'conference hiker',\n",
       " 'is occupied by',\n",
       " 'paper',\n",
       " 'member',\n",
       " 'is paid in',\n",
       " 'conference proceedings',\n",
       " 'is review history of',\n",
       " 'industry organization',\n",
       " 'time zone',\n",
       " 'has been assigned a review reference',\n",
       " 'call for papers',\n",
       " 'trip',\n",
       " 'single hotel room',\n",
       " 'boolean',\n",
       " 'meal break',\n",
       " 'information for participants',\n",
       " 'conference dinner',\n",
       " 'review expertise',\n",
       " 'organizer',\n",
       " 'string',\n",
       " 'is written by',\n",
       " 'poster paper',\n",
       " 'is a topis of conference parts',\n",
       " 'early paid applicant',\n",
       " 'belongs to reviewers',\n",
       " 'simulating',\n",
       " 'conference event',\n",
       " 'topic',\n",
       " 'tutorial chair',\n",
       " 'sign',\n",
       " 'has event',\n",
       " 'week reject rating',\n",
       " 'regular paper',\n",
       " 'power point presentation',\n",
       " 'industrial paper',\n",
       " 'award',\n",
       " 'search',\n",
       " 'has home page',\n",
       " 'has an abstract',\n",
       " 'departure tax',\n",
       " 'early registration',\n",
       " 'presenter',\n",
       " 'invited speaker',\n",
       " 'hardcopy mailing manifests printed by',\n",
       " 'written by',\n",
       " 'has the last name',\n",
       " 'talk event',\n",
       " 'camera ready event',\n",
       " 'has country',\n",
       " 'person',\n",
       " 'is used by',\n",
       " 'social event',\n",
       " 'enter conference details',\n",
       " 'is a full paper submission date',\n",
       " 'notification until',\n",
       " 'session',\n",
       " 'is sent before',\n",
       " 'is a date of acceptance announcement',\n",
       " 'was a steering committee of',\n",
       " 'conference hotel',\n",
       " 'expert on',\n",
       " 'person',\n",
       " 'meal menu',\n",
       " 'shuttle bus',\n",
       " 'taxi',\n",
       " 'rejection',\n",
       " 'initiates',\n",
       " 'academic institution',\n",
       " 'has topic',\n",
       " 'is given to',\n",
       " 'paper assignment finalized by',\n",
       " 'main office',\n",
       " 'has the first name',\n",
       " 'is initiated by',\n",
       " 'student lecturer',\n",
       " 'has bid',\n",
       " 'presenter',\n",
       " 'passive conference participant',\n",
       " 'belong to a conference volume',\n",
       " 'bronze supporter',\n",
       " 'site URL',\n",
       " 'ACM SIGKDD',\n",
       " 'conference member',\n",
       " 'accept paper',\n",
       " 'conference announcement',\n",
       " 'author attendee book registration fee',\n",
       " 'active conference participant',\n",
       " 'invited talk',\n",
       " 'individual presentation',\n",
       " 'part of',\n",
       " 'meeting room place',\n",
       " 'registration non-member',\n",
       " 'withdrawn paper',\n",
       " 'medicine topic',\n",
       " 'registation deadline',\n",
       " 'start of conference',\n",
       " 'date time',\n",
       " 'is a starting date',\n",
       " 'relates to',\n",
       " 'conference www',\n",
       " 'departure',\n",
       " 'full day tour',\n",
       " 'any URI',\n",
       " 'microelectronics topic',\n",
       " 'issues',\n",
       " 'attendee at',\n",
       " 'is an ending date',\n",
       " 'run paper assignment tools',\n",
       " 'mailing list',\n",
       " 'signal processing topic',\n",
       " 'value added tax',\n",
       " 'academic event',\n",
       " 'event on list',\n",
       " 'reject paper',\n",
       " 'review',\n",
       " 'is location of',\n",
       " 'record of attendance',\n",
       " 'registration fee',\n",
       " 'presenter house',\n",
       " 'late paid applicant',\n",
       " 'is held before',\n",
       " 'tutorial',\n",
       " 'computer networks measurements topic',\n",
       " 'conference',\n",
       " 'track-workshop chair',\n",
       " 'conference session',\n",
       " 'participant',\n",
       " 'is situated in',\n",
       " 'track',\n",
       " 'technically organises',\n",
       " 'adjusted by',\n",
       " 'nonmember registration fee',\n",
       " 'is equipped by',\n",
       " 'conference chair',\n",
       " 'technical commitee',\n",
       " 'fee',\n",
       " 'lecturer',\n",
       " 'video cassette player',\n",
       " 'program committee member',\n",
       " 'OC Chair',\n",
       " 'item',\n",
       " 'organises',\n",
       " 'research',\n",
       " 'best student paper award',\n",
       " 'organization',\n",
       " 'price',\n",
       " 'accepts hardcopy submissions',\n",
       " 'conference',\n",
       " 'author not reviewer',\n",
       " 'student',\n",
       " 'design',\n",
       " 'general chair',\n",
       " 'organization',\n",
       " 'has first name',\n",
       " 'organised by',\n",
       " 'has program committee member',\n",
       " 'manuscript due on',\n",
       " 'conference restaurant',\n",
       " 'paper in volume',\n",
       " 'review criteria entered by',\n",
       " 'tutorial',\n",
       " 'money',\n",
       " 'positive review',\n",
       " 'computer',\n",
       " 'parallel and distributed computing topic',\n",
       " 'has related paper',\n",
       " 'assigned to',\n",
       " 'non speaker',\n",
       " 'pending paper',\n",
       " 'proceedings',\n",
       " 'has decision',\n",
       " 'has an expertise',\n",
       " 'city',\n",
       " 'has a review',\n",
       " 'organizing committee',\n",
       " 'added by',\n",
       " 'activity before conference',\n",
       " 'employed by',\n",
       " 'main office',\n",
       " 'contact information',\n",
       " 'was an organizing committee of',\n",
       " 'scientifically organises',\n",
       " 'max choice',\n",
       " 'has workshops',\n",
       " 'is paid for',\n",
       " 'computer networks security topic',\n",
       " 'published paper',\n",
       " 'social event',\n",
       " 'author attendee cd registration fee',\n",
       " 'date time',\n",
       " 'has fax',\n",
       " 'is used for',\n",
       " 'time',\n",
       " 'references',\n",
       " 'workshop paper',\n",
       " 'document',\n",
       " 'has administrative event',\n",
       " 'has cost currency',\n",
       " 'conference participant',\n",
       " 'has an organizing committee',\n",
       " 'mark conflict of interest',\n",
       " 'brief introduction for session chair',\n",
       " 'rejected paper',\n",
       " 'IASTED member',\n",
       " 'submission',\n",
       " 'has a committee co-chair',\n",
       " 'plenary lecture',\n",
       " 'give',\n",
       " 'designed by',\n",
       " 'conference paper',\n",
       " 'demo chair',\n",
       " 'person',\n",
       " 'program chair',\n",
       " 'virtual meeting enabled by',\n",
       " 'member of conference',\n",
       " 'searched by',\n",
       " 'need',\n",
       " 'author book proceedings included',\n",
       " 'best applications paper award',\n",
       " 'has name',\n",
       " 'flyer',\n",
       " 'submitted paper',\n",
       " 'place',\n",
       " 'has postal code',\n",
       " 'has programme',\n",
       " 'review',\n",
       " 'review',\n",
       " 'conference building',\n",
       " 'reviewer',\n",
       " 'reviewer of paper',\n",
       " 'has a committee chair',\n",
       " 'tutorial',\n",
       " 'deadline author notification',\n",
       " 'awarded by',\n",
       " 'conference state',\n",
       " 'initial manuscipt',\n",
       " 'is menu of',\n",
       " 'submitted contribution',\n",
       " 'activity',\n",
       " 'has conference member',\n",
       " 'fee',\n",
       " 'document',\n",
       " 'has tutorials',\n",
       " 'place',\n",
       " 'belongs to a review reference',\n",
       " 'paper presented as',\n",
       " 'written by',\n",
       " 'has a commtitee',\n",
       " 'reviews per paper',\n",
       " 'review',\n",
       " 'presentation of paper',\n",
       " 'string',\n",
       " 'abstract',\n",
       " 'preference',\n",
       " 'reviewer bidding started by',\n",
       " 'viza',\n",
       " 'renting',\n",
       " 'transparency',\n",
       " 'payment document',\n",
       " 'tutorial',\n",
       " 'organizator',\n",
       " 'performance topic',\n",
       " 'paper author',\n",
       " 'computer networks aapplications topic',\n",
       " 'was a committe co-chair of',\n",
       " 'program committee member',\n",
       " 'break event',\n",
       " 'short paper',\n",
       " 'accepting manuscript',\n",
       " 'speaker',\n",
       " 'has a name',\n",
       " 'session chair',\n",
       " 'subclass of',\n",
       " 'regular author',\n",
       " 'reviewes',\n",
       " 'transport vehicle',\n",
       " 'int',\n",
       " 'LCD projector',\n",
       " 'writes',\n",
       " 'OC Member',\n",
       " 'chairman',\n",
       " 'test only topic',\n",
       " 'currency',\n",
       " 'rejected contribution',\n",
       " 'has VAT',\n",
       " 'has a track-workshop-tutorial chair',\n",
       " 'registeered applicant',\n",
       " 'accpet if room rating',\n",
       " 'university',\n",
       " 'refusing manuscript',\n",
       " 'proceedings publisher',\n",
       " 'send',\n",
       " 'has a review expertise',\n",
       " 'speaker lecture',\n",
       " 'call for reviews',\n",
       " 'administrator',\n",
       " 'is part of conference volumes',\n",
       " 'name of sponsor',\n",
       " 'deadline',\n",
       " 'presentation',\n",
       " 'deadline',\n",
       " 'early-registered participant',\n",
       " 'rated papers',\n",
       " 'person',\n",
       " 'regular session',\n",
       " 'accommodation place',\n",
       " 'min choice',\n",
       " 'subclass of']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_case_handled = []\n",
    "for concept in inp:\n",
    "    final_list = []\n",
    "    for word in concept.split(\" \"):\n",
    "        if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "            final_list.append(word.lower())\n",
    "        else:\n",
    "            final_list.append(word)\n",
    "    case_resolved = \" \".join(final_list)\n",
    "    inp_case_handled.append(case_resolved)\n",
    "    \n",
    "inp_case_handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eef59b087748c58eb55ef798baa1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a020ec334845459b0b659592f901d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7977b08c854e328bb05253633ce659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435778770.0, style=ProgressStyle(descri"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14206647872924805\n"
     ]
    }
   ],
   "source": [
    "# from transformers import XLNetTokenizer, XLNetModel\n",
    "# import torch\n",
    "# import scipy\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "import time\n",
    "\n",
    "t = time.time()\n",
    "input_ids = torch.tensor(tokenizer.encode(\"fastigial nucleus\", add_special_tokens=True)).unsqueeze(0)\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0].mean(1)\n",
    "\n",
    "print (t-time.time())\n",
    "# input_ids = torch.tensor(tokenizer.encode(\"femur\", add_special_tokens=True)).unsqueeze(0) \n",
    "\n",
    "# outputs1 = model(input_ids)\n",
    "# last_hidden_states1 = outputs1[0].mean(1)\n",
    "\n",
    "# cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "# cos(last_hidden_states, last_hidden_states1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'bert-large-nli-mean-tokens'. Make sure that:\n\n- 'bert-large-nli-mean-tokens' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-large-nli-mean-tokens' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-455cefa0d73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-large-nli-mean-tokens\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mword_embedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Apply mean pooling to get one fixed sized sentence vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             )\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'bert-large-nli-mean-tokens'. Make sure that:\n\n- 'bert-large-nli-mean-tokens' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'bert-large-nli-mean-tokens' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=False,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=True)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2498054802417755"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-large-nli-mean-tokens')\n",
    "\n",
    "cos_sim(*model.encode([\"My brother plays guitar\", \"The sun is shining\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1406574696302414"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(*extractUSEEmbeddings([\"My brother plays guitar\", \"The sun is shining\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
