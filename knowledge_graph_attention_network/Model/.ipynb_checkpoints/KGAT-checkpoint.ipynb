{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utility.load_data import Data\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "import random as rd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_type = \"si\"\n",
    "alg_type = \"bi\" \n",
    "dataset = \"yelp2018\" \n",
    "regs = [1e-5,1e-5]\n",
    "layer_size = [64,32,16]\n",
    "embed_size = 64\n",
    "lr = 0.0001 \n",
    "epoch = 1000 \n",
    "verbose = 50 \n",
    "save_flag = 1 \n",
    "pretrain = -1 \n",
    "batch_size = 1024 \n",
    "node_dropout = [0.1]\n",
    "mess_dropout = [0.1,0.1,0.1]\n",
    "use_att = True \n",
    "use_kge = True\n",
    "\n",
    "path = \"../Data/yelp2018/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        kg_file = path + '/kg_final.txt'\n",
    "\n",
    "        # ----------get number of users and items & then load rating data from train_file & test_file------------.\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "\n",
    "        ''' Just count number of distinct users, items, prepare a dict '''\n",
    "        self.train_data, self.train_user_dict = self._load_ratings(train_file)\n",
    "        self.test_data, self.test_user_dict = self._load_ratings(test_file)\n",
    "        self.exist_users = self.train_user_dict.keys()\n",
    "\n",
    "        self._statistic_ratings()\n",
    "\n",
    "        # ----------get number of entities and relations & then load kg data from kg_file ------------.\n",
    "        \n",
    "        self.n_relations, self.n_entities, self.n_triples = 0, 0, 0\n",
    "        ''' Simply counting max number of relations '''\n",
    "        self.kg_data, self.kg_dict, self.relation_dict = self._load_kg(kg_file)\n",
    "\n",
    "        # ----------print the basic info about the dataset-------------.\n",
    "        self.batch_size_kg = self.n_triples // (self.n_train // self.batch_size)\n",
    "        self._print_data_info()\n",
    "\n",
    "    # reading train & test interaction data.\n",
    "    def _load_ratings(self, file_name):\n",
    "        user_dict = dict()\n",
    "        inter_mat = list()\n",
    "\n",
    "        lines = open(file_name, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmps = l.strip()\n",
    "            inters = [int(i) for i in tmps.split(' ')]\n",
    "\n",
    "            u_id, pos_ids = inters[0], inters[1:]\n",
    "            pos_ids = list(set(pos_ids))\n",
    "\n",
    "            for i_id in pos_ids:\n",
    "                inter_mat.append([u_id, i_id])\n",
    "\n",
    "            if len(pos_ids) > 0:\n",
    "                user_dict[u_id] = pos_ids\n",
    "        return np.array(inter_mat), user_dict\n",
    "\n",
    "    def _statistic_ratings(self):\n",
    "        self.n_users = max(max(self.train_data[:, 0]), max(self.test_data[:, 0])) + 1\n",
    "        self.n_items = max(max(self.train_data[:, 1]), max(self.test_data[:, 1])) + 1\n",
    "        self.n_train = len(self.train_data)\n",
    "        self.n_test = len(self.test_data)\n",
    "\n",
    "    # reading train & test interaction data.\n",
    "    def _load_kg(self, file_name):\n",
    "        def _construct_kg(kg_np):\n",
    "            kg = collections.defaultdict(list)\n",
    "            rd = collections.defaultdict(list)\n",
    "\n",
    "            for head, relation, tail in kg_np:\n",
    "                kg[head].append((tail, relation))\n",
    "                rd[relation].append((head, tail))\n",
    "            return kg, rd\n",
    "\n",
    "        kg_np = np.loadtxt(file_name, dtype=np.int32)\n",
    "        kg_np = np.unique(kg_np, axis=0)\n",
    "\n",
    "        # self.n_relations = len(set(kg_np[:, 1]))\n",
    "        # self.n_entities = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "        self.n_relations = max(kg_np[:, 1]) + 1\n",
    "        self.n_entities = max(max(kg_np[:, 0]), max(kg_np[:, 2])) + 1\n",
    "        self.n_triples = len(kg_np)\n",
    "\n",
    "        kg_dict, relation_dict = _construct_kg(kg_np)\n",
    "\n",
    "        return kg_np, kg_dict, relation_dict\n",
    "\n",
    "    def _print_data_info(self):\n",
    "        print('[n_users, n_items]=[%d, %d]' % (self.n_users, self.n_items))\n",
    "        print('[n_train, n_test]=[%d, %d]' % (self.n_train, self.n_test))\n",
    "        print('[n_entities, n_relations, n_triples]=[%d, %d, %d]' % (self.n_entities, self.n_relations, self.n_triples))\n",
    "        print('[batch_size, batch_size_kg]=[%d, %d]' % (self.batch_size, self.batch_size_kg))\n",
    "\n",
    "    def _generate_train_cf_batch(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_user_dict[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_i_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "\n",
    "                if neg_i_id not in self.train_user_dict[u] and neg_i_id not in neg_items:\n",
    "                    neg_items.append(neg_i_id)\n",
    "            return neg_items\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_user_dict.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_user_dict[uid]\n",
    "            test_iids = self.test_user_dict[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "\n",
    "        return split_uids, split_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGAT_loader(Data):\n",
    "    def __init__(self, path):\n",
    "        super().__init__(path)\n",
    "\n",
    "        # generate the sparse adjacency matrices for user-item interaction & relational kg data.\n",
    "        ''' Create a sparse adjacency matrix of users + entities + relations '''\n",
    "        self.adj_list, self.adj_r_list = self._get_relational_adj_list()\n",
    "\n",
    "        # generate the sparse laplacian matrices.\n",
    "        ''' DIRECT COPY: Create a sparse adjacency matrix of users + entities + relations '''\n",
    "        self.lap_list = self._get_relational_lap_list()\n",
    "\n",
    "        # generate the triples dictionary, key is 'head', value is '(tail, relation)'.\n",
    "        self.all_kg_dict = self._get_all_kg_dict()\n",
    "\n",
    "        self.all_h_list, self.all_r_list, self.all_t_list, self.all_v_list = self._get_all_kg_data()\n",
    "\n",
    "\n",
    "    def _get_relational_adj_list(self):\n",
    "        t1 = time()\n",
    "        adj_mat_list = []\n",
    "        adj_r_list = []\n",
    "\n",
    "        def _np_mat2sp_adj(np_mat, row_pre, col_pre):\n",
    "            n_all = self.n_users + self.n_entities\n",
    "            # single-direction\n",
    "            a_rows = np_mat[:, 0] + row_pre\n",
    "            a_cols = np_mat[:, 1] + col_pre\n",
    "            a_vals = [1.] * len(a_rows)\n",
    "\n",
    "            b_rows = a_cols\n",
    "            b_cols = a_rows\n",
    "            b_vals = [1.] * len(b_rows)\n",
    "\n",
    "            a_adj = sp.coo_matrix((a_vals, (a_rows, a_cols)), shape=(n_all, n_all))\n",
    "            b_adj = sp.coo_matrix((b_vals, (b_rows, b_cols)), shape=(n_all, n_all))\n",
    "\n",
    "            return a_adj, b_adj\n",
    "        \n",
    "        \n",
    "        R, R_inv = _np_mat2sp_adj(self.train_data, row_pre=0, col_pre=self.n_users)\n",
    "        adj_mat_list.append(R)\n",
    "        adj_r_list.append(0)\n",
    "\n",
    "        adj_mat_list.append(R_inv)\n",
    "        adj_r_list.append(self.n_relations + 1)\n",
    "        print('\\tconvert ratings into adj mat done.')\n",
    "\n",
    "        for r_id in self.relation_dict.keys():\n",
    "            K, K_inv = _np_mat2sp_adj(np.array(self.relation_dict[r_id]), row_pre=self.n_users, col_pre=self.n_users)\n",
    "            adj_mat_list.append(K)\n",
    "            adj_r_list.append(r_id + 1)\n",
    "\n",
    "            adj_mat_list.append(K_inv)\n",
    "            adj_r_list.append(r_id + 2 + self.n_relations)\n",
    "        print('\\tconvert %d relational triples into adj mat done. @%.4fs' %(len(adj_mat_list), time()-t1))\n",
    "\n",
    "        self.n_relations = len(adj_r_list)\n",
    "        # print('\\tadj relation list is', adj_r_list)\n",
    "\n",
    "        return adj_mat_list, adj_r_list\n",
    "\n",
    "    def _get_relational_lap_list(self):\n",
    "        def _bi_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def _si_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if adj_type == 'bi':\n",
    "            lap_list = [_bi_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate bi-normalized adjacency matrix.')\n",
    "        else:\n",
    "            lap_list = [_si_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate si-normalized adjacency matrix.')\n",
    "        return lap_list\n",
    "\n",
    "    def _get_all_kg_dict(self):\n",
    "        all_kg_dict = collections.defaultdict(list)\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "\n",
    "            rows = lap.row\n",
    "            cols = lap.col\n",
    "\n",
    "            for i_id in range(len(rows)):\n",
    "                head = rows[i_id]\n",
    "                tail = cols[i_id]\n",
    "                relation = self.adj_r_list[l_id]\n",
    "\n",
    "                all_kg_dict[head].append((tail, relation))\n",
    "        return all_kg_dict\n",
    "\n",
    "    def _get_all_kg_data(self):\n",
    "        def _reorder_list(org_list, order):\n",
    "            new_list = np.array(org_list)\n",
    "            new_list = new_list[order]\n",
    "            return new_list\n",
    "\n",
    "        all_h_list, all_t_list, all_r_list = [], [], []\n",
    "        all_v_list = []\n",
    "\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "            all_h_list += list(lap.row)\n",
    "            all_t_list += list(lap.col)\n",
    "            all_v_list += list(lap.data)\n",
    "            all_r_list += [self.adj_r_list[l_id]] * len(lap.row)\n",
    "\n",
    "        assert len(all_h_list) == sum([len(lap.data) for lap in self.lap_list])\n",
    "\n",
    "        # resort the all_h/t/r/v_list,\n",
    "        # ... since tensorflow.sparse.softmax requires indices sorted in the canonical lexicographic order\n",
    "        print('\\treordering indices...')\n",
    "        org_h_dict = dict()\n",
    "\n",
    "        for idx, h in enumerate(all_h_list):\n",
    "            if h not in org_h_dict.keys():\n",
    "                org_h_dict[h] = [[],[],[]]\n",
    "\n",
    "            org_h_dict[h][0].append(all_t_list[idx])\n",
    "            org_h_dict[h][1].append(all_r_list[idx])\n",
    "            org_h_dict[h][2].append(all_v_list[idx])\n",
    "        print('\\treorganize all kg data done.')\n",
    "\n",
    "        sorted_h_dict = dict()\n",
    "        for h in org_h_dict.keys():\n",
    "            org_t_list, org_r_list, org_v_list = org_h_dict[h]\n",
    "            sort_t_list = np.array(org_t_list)\n",
    "            sort_order = np.argsort(sort_t_list)\n",
    "\n",
    "            sort_t_list = _reorder_list(org_t_list, sort_order)\n",
    "            sort_r_list = _reorder_list(org_r_list, sort_order)\n",
    "            sort_v_list = _reorder_list(org_v_list, sort_order)\n",
    "\n",
    "            sorted_h_dict[h] = [sort_t_list, sort_r_list, sort_v_list]\n",
    "        print('\\tsort meta-data done.')\n",
    "\n",
    "        od = collections.OrderedDict(sorted(sorted_h_dict.items()))\n",
    "        new_h_list, new_t_list, new_r_list, new_v_list = [], [], [], []\n",
    "\n",
    "        for h, vals in od.items():\n",
    "            new_h_list += [h] * len(vals[0])\n",
    "            new_t_list += list(vals[0])\n",
    "            new_r_list += list(vals[1])\n",
    "            new_v_list += list(vals[2])\n",
    "\n",
    "\n",
    "        assert sum(new_h_list) == sum(all_h_list)\n",
    "        assert sum(new_t_list) == sum(all_t_list)\n",
    "        assert sum(new_r_list) == sum(all_r_list)\n",
    "        # try:\n",
    "        #     assert sum(new_v_list) == sum(all_v_list)\n",
    "        # except Exception:\n",
    "        #     print(sum(new_v_list), '\\n')\n",
    "        #     print(sum(all_v_list), '\\n')\n",
    "        print('\\tsort all data done.')\n",
    "\n",
    "\n",
    "        return new_h_list, new_r_list, new_t_list, new_v_list\n",
    "\n",
    "    def _generate_train_A_batch(self):\n",
    "        exist_heads = self.all_kg_dict.keys()\n",
    "\n",
    "        if self.batch_size_kg <= len(exist_heads):\n",
    "            heads = rd.sample(exist_heads, self.batch_size_kg)\n",
    "        else:\n",
    "            heads = [rd.choice(exist_heads) for _ in range(self.batch_size_kg)]\n",
    "\n",
    "        def sample_pos_triples_for_h(h, num):\n",
    "            pos_triples = self.all_kg_dict[h]\n",
    "            n_pos_triples = len(pos_triples)\n",
    "\n",
    "            pos_rs, pos_ts = [], []\n",
    "            while True:\n",
    "                if len(pos_rs) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "\n",
    "                t = pos_triples[pos_id][0]\n",
    "                r = pos_triples[pos_id][1]\n",
    "\n",
    "                if r not in pos_rs and t not in pos_ts:\n",
    "                    pos_rs.append(r)\n",
    "                    pos_ts.append(t)\n",
    "            return pos_rs, pos_ts\n",
    "\n",
    "        def sample_neg_triples_for_h(h, r, num):\n",
    "            neg_ts = []\n",
    "            while True:\n",
    "                if len(neg_ts) == num: break\n",
    "\n",
    "                t = np.random.randint(low=0, high=self.n_users + self.n_entities, size=1)[0]\n",
    "                if (t, r) not in self.all_kg_dict[h] and t not in neg_ts:\n",
    "                    neg_ts.append(t)\n",
    "            return neg_ts\n",
    "\n",
    "        pos_r_batch, pos_t_batch, neg_t_batch = [], [], []\n",
    "\n",
    "        for h in heads:\n",
    "            pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "            pos_r_batch += pos_rs\n",
    "            pos_t_batch += pos_ts\n",
    "\n",
    "            neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "            neg_t_batch += neg_ts\n",
    "\n",
    "        return heads, pos_r_batch, pos_t_batch, neg_t_batch\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        users, pos_items, neg_items = self._generate_train_cf_batch()\n",
    "\n",
    "        batch_data = {}\n",
    "        batch_data['users'] = users\n",
    "        batch_data['pos_items'] = pos_items\n",
    "        batch_data['neg_items'] = neg_items\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def generate_train_feed_dict(self, model, batch_data):\n",
    "        feed_dict = {\n",
    "            model.users: batch_data['users'],\n",
    "            model.pos_items: batch_data['pos_items'],\n",
    "            model.neg_items: batch_data['neg_items'],\n",
    "\n",
    "            model.mess_dropout: eval(mess_dropout),\n",
    "            model.node_dropout: eval(node_dropout),\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def generate_train_A_batch(self):\n",
    "        heads, relations, pos_tails, neg_tails = self._generate_train_A_batch()\n",
    "\n",
    "        batch_data = {}\n",
    "\n",
    "        batch_data['heads'] = heads\n",
    "        batch_data['relations'] = relations\n",
    "        batch_data['pos_tails'] = pos_tails\n",
    "        batch_data['neg_tails'] = neg_tails\n",
    "        return batch_data\n",
    "\n",
    "    def generate_train_A_feed_dict(self, model, batch_data):\n",
    "        feed_dict = {\n",
    "            model.h: batch_data['heads'],\n",
    "            model.r: batch_data['relations'],\n",
    "            model.pos_t: batch_data['pos_tails'],\n",
    "            model.neg_t: batch_data['neg_tails'],\n",
    "\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "\n",
    "    def generate_test_feed_dict(self, model, user_batch, item_batch, drop_flag=True):\n",
    "\n",
    "        feed_dict ={\n",
    "            model.users: user_batch,\n",
    "            model.pos_items: item_batch,\n",
    "            model.mess_dropout: [0.] * len(eval(layer_size)),\n",
    "            model.node_dropout: [0.] * len(eval(layer_size)),\n",
    "\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[45919, 45538]\n",
      "[n_train, n_test]=[930032, 253578]\n",
      "[n_entities, n_relations, n_triples]=[136499, 42, 1853704]\n",
      "[batch_size, batch_size_kg]=[1024, 2041]\n",
      "\tconvert ratings into adj mat done.\n",
      "\tconvert 86 relational triples into adj mat done. @1.5698s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgenerate si-normalized adjacency matrix.\n",
      "\treordering indices...\n",
      "\treorganize all kg data done.\n",
      "\tsort meta-data done.\n",
      "\tsort all data done.\n"
     ]
    }
   ],
   "source": [
    "data_generator = KGAT_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5567472"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_interactions():\n",
    "    user_train = open(path + \"/train.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of columns at line 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8a4130ad43b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 raise ValueError(\"Wrong number of columns at line %d\"\n\u001b[0;32m-> 1084\u001b[0;31m                                  % line_num)\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of columns at line 2"
     ]
    }
   ],
   "source": [
    "class KGAT(object):\n",
    "    def __init__(self, data_config, pretrain_data, args):\n",
    "        self._parse_args(data_config, pretrain_data, args)\n",
    "        '''\n",
    "        *********************************************************\n",
    "        Create Placeholder for Input Data & Dropout.\n",
    "        '''\n",
    "        self._build_inputs()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Create Model Parameters for CF & KGE parts.\n",
    "        \"\"\"\n",
    "        self.weights = self._build_weights()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Compute Graph-based Representations of All Users & Items & KG Entities via Message-Passing Mechanism of Graph Neural Networks.\n",
    "        Different Convolutional Layers:\n",
    "            1. bi: defined in 'KGAT: Knowledge Graph Attention Network for Recommendation', KDD2019;\n",
    "            2. gcn:  defined in 'Semi-Supervised Classification with Graph Convolutional Networks', ICLR2018;\n",
    "            3. graphsage: defined in 'Inductive Representation Learning on Large Graphs', NeurIPS2017.\n",
    "        \"\"\"\n",
    "        self._build_model_phase_I()\n",
    "        \"\"\"\n",
    "        Optimize Recommendation (CF) Part via BPR Loss.\n",
    "        \"\"\"\n",
    "        self._build_loss_phase_I()\n",
    "\n",
    "        \"\"\"\n",
    "        *********************************************************\n",
    "        Compute Knowledge Graph Embeddings via TransR.\n",
    "        \"\"\"\n",
    "        self._build_model_phase_II()\n",
    "        \"\"\"\n",
    "        Optimize KGE Part via BPR Loss.\n",
    "        \"\"\"\n",
    "        self._build_loss_phase_II()\n",
    "\n",
    "        self._statistics_params()\n",
    "\n",
    "    def _parse_args(self, data_config, pretrain_data, args):\n",
    "        # argument settings\n",
    "        self.model_type = 'kgat'\n",
    "\n",
    "        self.pretrain_data = pretrain_data\n",
    "\n",
    "        self.n_users = data_config['n_users']\n",
    "        self.n_items = data_config['n_items']\n",
    "        self.n_entities = data_config['n_entities']\n",
    "        self.n_relations = data_config['n_relations']\n",
    "\n",
    "        self.n_fold = 100\n",
    "\n",
    "        # initialize the attentive matrix A for phase I.\n",
    "        self.A_in = data_config['A_in']\n",
    "\n",
    "        self.all_h_list = data_config['all_h_list']\n",
    "        self.all_r_list = data_config['all_r_list']\n",
    "        self.all_t_list = data_config['all_t_list']\n",
    "        self.all_v_list = data_config['all_v_list']\n",
    "\n",
    "        self.adj_uni_type = args.adj_uni_type\n",
    "\n",
    "        self.lr = args.lr\n",
    "\n",
    "        # settings for CF part.\n",
    "        self.emb_dim = args.embed_size\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        # settings for KG part.\n",
    "        self.kge_dim = args.kge_size\n",
    "        self.batch_size_kg = args.batch_size_kg\n",
    "\n",
    "        self.weight_size = eval(args.layer_size)\n",
    "        self.n_layers = len(self.weight_size)\n",
    "\n",
    "        self.alg_type = args.alg_type\n",
    "        self.model_type += '_%s_%s_%s_l%d' % (args.adj_type, args.adj_uni_type, args.alg_type, self.n_layers)\n",
    "\n",
    "        self.regs = eval(args.regs)\n",
    "        self.verbose = args.verbose\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        # placeholder definition\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        # for knowledge graph modeling (TransD)\n",
    "        self.A_values = tf.placeholder(tf.float32, shape=[len(self.all_v_list)], name='A_values')\n",
    "\n",
    "        self.h = tf.placeholder(tf.int32, shape=[None], name='h')\n",
    "        self.r = tf.placeholder(tf.int32, shape=[None], name='r')\n",
    "        self.pos_t = tf.placeholder(tf.int32, shape=[None], name='pos_t')\n",
    "        self.neg_t = tf.placeholder(tf.int32, shape=[None], name='neg_t')\n",
    "\n",
    "        # dropout: node dropout (adopted on the ego-networks);\n",
    "        # message dropout (adopted on the convolution operations).\n",
    "        self.node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    def _build_weights(self):\n",
    "        all_weights = dict()\n",
    "\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        if self.pretrain_data is None:\n",
    "            all_weights['user_embed'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embed')\n",
    "            all_weights['entity_embed'] = tf.Variable(initializer([self.n_entities, self.emb_dim]), name='entity_embed')\n",
    "            print('using xavier initialization')\n",
    "        else:\n",
    "            all_weights['user_embed'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
    "                                                    name='user_embed', dtype=tf.float32)\n",
    "\n",
    "            item_embed = self.pretrain_data['item_embed']\n",
    "            other_embed = initializer([self.n_entities - self.n_items, self.emb_dim])\n",
    "\n",
    "            all_weights['entity_embed'] = tf.Variable(initial_value=tf.concat([item_embed, other_embed], 0),\n",
    "                                                      trainable=True, name='entity_embed', dtype=tf.float32)\n",
    "            print('using pretrained initialization')\n",
    "\n",
    "        all_weights['relation_embed'] = tf.Variable(initializer([self.n_relations, self.kge_dim]),\n",
    "                                                    name='relation_embed')\n",
    "        all_weights['trans_W'] = tf.Variable(initializer([self.n_relations, self.emb_dim, self.kge_dim]))\n",
    "\n",
    "        self.weight_size_list = [self.emb_dim] + self.weight_size\n",
    "\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            all_weights['W_gc_%d' %k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_gc_%d' % k)\n",
    "            all_weights['b_gc_%d' %k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_gc_%d' % k)\n",
    "\n",
    "            all_weights['W_bi_%d' % k] = tf.Variable(\n",
    "                initializer([self.weight_size_list[k], self.weight_size_list[k + 1]]), name='W_bi_%d' % k)\n",
    "            all_weights['b_bi_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k + 1]]), name='b_bi_%d' % k)\n",
    "\n",
    "            all_weights['W_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([2 * self.weight_size_list[k], self.weight_size_list[k+1]]), name='W_mlp_%d' % k)\n",
    "            all_weights['b_mlp_%d' % k] = tf.Variable(\n",
    "                initializer([1, self.weight_size_list[k+1]]), name='b_mlp_%d' % k)\n",
    "\n",
    "        return all_weights\n",
    "\n",
    "    def _build_model_phase_I(self):\n",
    "        if self.alg_type in ['bi']:\n",
    "            self.ua_embeddings, self.ea_embeddings = self._create_bi_interaction_embed()\n",
    "\n",
    "        elif self.alg_type in ['gcn']:\n",
    "            self.ua_embeddings, self.ea_embeddings = self._create_gcn_embed()\n",
    "\n",
    "        elif self.alg_type in ['graphsage']:\n",
    "            self.ua_embeddings, self.ea_embeddings = self._create_graphsage_embed()\n",
    "\n",
    "        self.u_e = tf.nn.embedding_lookup(self.ua_embeddings, self.users)\n",
    "        self.pos_i_e = tf.nn.embedding_lookup(self.ea_embeddings, self.pos_items)\n",
    "        self.neg_i_e = tf.nn.embedding_lookup(self.ea_embeddings, self.neg_items)\n",
    "\n",
    "        self.batch_predictions = tf.matmul(self.u_e, self.pos_i_e, transpose_a=False, transpose_b=True)\n",
    "\n",
    "    def _build_model_phase_II(self):\n",
    "        self.h_e, self.r_e, self.pos_t_e, self.neg_t_e = self._get_kg_inference(self.h, self.r, self.pos_t, self.neg_t)\n",
    "        self.A_kg_score = self._generate_transE_score(h=self.h, t=self.pos_t, r=self.r)\n",
    "        self.A_out = self._create_attentive_A_out()\n",
    "\n",
    "    def _get_kg_inference(self, h, r, pos_t, neg_t):\n",
    "        embeddings = tf.concat([self.weights['user_embed'], self.weights['entity_embed']], axis=0)\n",
    "        embeddings = tf.expand_dims(embeddings, 1)\n",
    "\n",
    "        # head & tail entity embeddings: batch_size *1 * emb_dim\n",
    "        h_e = tf.nn.embedding_lookup(embeddings, h)\n",
    "        pos_t_e = tf.nn.embedding_lookup(embeddings, pos_t)\n",
    "        neg_t_e = tf.nn.embedding_lookup(embeddings, neg_t)\n",
    "\n",
    "        # relation embeddings: batch_size * kge_dim\n",
    "        r_e = tf.nn.embedding_lookup(self.weights['relation_embed'], r)\n",
    "\n",
    "        # relation transform weights: batch_size * kge_dim * emb_dim\n",
    "        trans_M = tf.nn.embedding_lookup(self.weights['trans_W'], r)\n",
    "\n",
    "        # batch_size * 1 * kge_dim -> batch_size * kge_dim\n",
    "        h_e = tf.reshape(tf.matmul(h_e, trans_M), [-1, self.kge_dim])\n",
    "        pos_t_e = tf.reshape(tf.matmul(pos_t_e, trans_M), [-1, self.kge_dim])\n",
    "        neg_t_e = tf.reshape(tf.matmul(neg_t_e, trans_M), [-1, self.kge_dim])\n",
    "        \n",
    "        # Remove the l2 normalization terms\n",
    "        # h_e = tf.math.l2_normalize(h_e, axis=1)\n",
    "        # r_e = tf.math.l2_normalize(r_e, axis=1)\n",
    "        # pos_t_e = tf.math.l2_normalize(pos_t_e, axis=1)\n",
    "        # neg_t_e = tf.math.l2_normalize(neg_t_e, axis=1)\n",
    "\n",
    "        return h_e, r_e, pos_t_e, neg_t_e\n",
    "\n",
    "    def _build_loss_phase_I(self):\n",
    "        pos_scores = tf.reduce_sum(tf.multiply(self.u_e, self.pos_i_e), axis=1)\n",
    "        neg_scores = tf.reduce_sum(tf.multiply(self.u_e, self.neg_i_e), axis=1)\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(self.u_e) + tf.nn.l2_loss(self.pos_i_e) + tf.nn.l2_loss(self.neg_i_e)\n",
    "        regularizer = regularizer / self.batch_size\n",
    "\n",
    "        # Using the softplus as BPR loss to avoid the nan error.\n",
    "        base_loss = tf.reduce_mean(tf.nn.softplus(-(pos_scores - neg_scores)))\n",
    "        # maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        # base_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "        self.base_loss = base_loss\n",
    "        self.kge_loss = tf.constant(0.0, tf.float32, [1])\n",
    "        self.reg_loss = self.regs[0] * regularizer\n",
    "        self.loss = self.base_loss + self.kge_loss + self.reg_loss\n",
    "\n",
    "        # Optimization process.RMSPropOptimizer\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "    def _build_loss_phase_II(self):\n",
    "        def _get_kg_score(h_e, r_e, t_e):\n",
    "            kg_score = tf.reduce_sum(tf.square((h_e + r_e - t_e)), 1, keepdims=True)\n",
    "            return kg_score\n",
    "\n",
    "        pos_kg_score = _get_kg_score(self.h_e, self.r_e, self.pos_t_e)\n",
    "        neg_kg_score = _get_kg_score(self.h_e, self.r_e, self.neg_t_e)\n",
    "        \n",
    "        # Using the softplus as BPR loss to avoid the nan error.\n",
    "        kg_loss = tf.reduce_mean(tf.nn.softplus(-(neg_kg_score - pos_kg_score)))\n",
    "        # maxi = tf.log(tf.nn.sigmoid(neg_kg_score - pos_kg_score))\n",
    "        # kg_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "\n",
    "        kg_reg_loss = tf.nn.l2_loss(self.h_e) + tf.nn.l2_loss(self.r_e) + \\\n",
    "                      tf.nn.l2_loss(self.pos_t_e) + tf.nn.l2_loss(self.neg_t_e)\n",
    "        kg_reg_loss = kg_reg_loss / self.batch_size_kg\n",
    "\n",
    "        self.kge_loss2 = kg_loss\n",
    "        self.reg_loss2 = self.regs[1] * kg_reg_loss\n",
    "        self.loss2 = self.kge_loss2 + self.reg_loss2\n",
    "\n",
    "        # Optimization process.\n",
    "        self.opt2 = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss2)\n",
    "\n",
    "    def _create_bi_interaction_embed(self):\n",
    "        A = self.A_in\n",
    "        # Generate a set of adjacency sub-matrix.\n",
    "        A_fold_hat = self._split_A_hat(A)\n",
    "\n",
    "        ego_embeddings = tf.concat([self.weights['user_embed'], self.weights['entity_embed']], axis=0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for k in range(0, self.n_layers):\n",
    "            # A_hat_drop = tf.nn.dropout(A_hat, 1 - self.node_dropout[k], [self.n_users + self.n_items, 1])\n",
    "            temp_embed = []\n",
    "            for f in range(self.n_fold):\n",
    "                temp_embed.append(tf.sparse_tensor_dense_matmul(A_fold_hat[f], ego_embeddings))\n",
    "\n",
    "            # sum messages of neighbors.\n",
    "            side_embeddings = tf.concat(temp_embed, 0)\n",
    "\n",
    "            add_embeddings = ego_embeddings + side_embeddings\n",
    "\n",
    "            # transformed sum messages of neighbors.\n",
    "            sum_embeddings = tf.nn.leaky_relu(\n",
    "                tf.matmul(add_embeddings, self.weights['W_gc_%d' % k]) + self.weights['b_gc_%d' % k])\n",
    "\n",
    "\n",
    "            # bi messages of neighbors.\n",
    "            bi_embeddings = tf.multiply(ego_embeddings, side_embeddings)\n",
    "            # transformed bi messages of neighbors.\n",
    "            bi_embeddings = tf.nn.leaky_relu(\n",
    "                tf.matmul(bi_embeddings, self.weights['W_bi_%d' % k]) + self.weights['b_bi_%d' % k])\n",
    "\n",
    "            ego_embeddings = bi_embeddings + sum_embeddings\n",
    "            # message dropout.\n",
    "            ego_embeddings = tf.nn.dropout(ego_embeddings, 1 - self.mess_dropout[k])\n",
    "\n",
    "            # normalize the distribution of embeddings.\n",
    "            norm_embeddings = tf.math.l2_normalize(ego_embeddings, axis=1)\n",
    "\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = tf.concat(all_embeddings, 1)\n",
    "\n",
    "        ua_embeddings, ea_embeddings = tf.split(all_embeddings, [self.n_users, self.n_entities], 0)\n",
    "        return ua_embeddings, ea_embeddings\n",
    "\n",
    " \n",
    "    def _split_A_hat(self, X):\n",
    "        A_fold_hat = []\n",
    "\n",
    "        fold_len = (self.n_users + self.n_entities) // self.n_fold\n",
    "\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold - 1:\n",
    "                end = self.n_users + self.n_entities\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            A_fold_hat.append(self._convert_sp_mat_to_sp_tensor(X[start:end]))\n",
    "        return A_fold_hat\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        indices = np.mat([coo.row, coo.col]).transpose()\n",
    "        return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "    def _create_attentive_A_out(self):\n",
    "        indices = np.mat([self.all_h_list, self.all_t_list]).transpose()\n",
    "        A = tf.sparse.softmax(tf.SparseTensor(indices, self.A_values, self.A_in.shape))\n",
    "        return A\n",
    "\n",
    "    def _generate_transE_score(self, h, t, r):\n",
    "        embeddings = tf.concat([self.weights['user_embed'], self.weights['entity_embed']], axis=0)\n",
    "        embeddings = tf.expand_dims(embeddings, 1)\n",
    "\n",
    "        h_e = tf.nn.embedding_lookup(embeddings, h)\n",
    "        t_e = tf.nn.embedding_lookup(embeddings, t)\n",
    "\n",
    "        # relation embeddings: batch_size * kge_dim\n",
    "        r_e = tf.nn.embedding_lookup(self.weights['relation_embed'], r)\n",
    "\n",
    "        # relation transform weights: batch_size * kge_dim * emb_dim\n",
    "        trans_M = tf.nn.embedding_lookup(self.weights['trans_W'], r)\n",
    "\n",
    "        # batch_size * 1 * kge_dim -> batch_size * kge_dim\n",
    "        h_e = tf.reshape(tf.matmul(h_e, trans_M), [-1, self.kge_dim])\n",
    "        t_e = tf.reshape(tf.matmul(t_e, trans_M), [-1, self.kge_dim])\n",
    "\n",
    "        # l2-normalize\n",
    "        # h_e = tf.math.l2_normalize(h_e, axis=1)\n",
    "        # r_e = tf.math.l2_normalize(r_e, axis=1)\n",
    "        # t_e = tf.math.l2_normalize(t_e, axis=1)\n",
    "\n",
    "        kg_score = tf.reduce_sum(tf.multiply(t_e, tf.tanh(h_e + r_e)), 1)\n",
    "\n",
    "        return kg_score\n",
    "\n",
    "    def _statistics_params(self):\n",
    "        # number of params\n",
    "        total_parameters = 0\n",
    "        for variable in self.weights.values():\n",
    "            shape = variable.get_shape()  # shape is an array of tf.Dimension\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        if self.verbose > 0:\n",
    "            print(\"#params: %d\" % total_parameters)\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        return sess.run([self.opt, self.loss, self.base_loss, self.kge_loss, self.reg_loss], feed_dict)\n",
    "\n",
    "    def train_A(self, sess, feed_dict):\n",
    "        return sess.run([self.opt2, self.loss2, self.kge_loss2, self.reg_loss2], feed_dict)\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "        batch_predictions = sess.run(self.batch_predictions, feed_dict)\n",
    "        return batch_predictions\n",
    "\n",
    "    \"\"\"\n",
    "    Update the attentive laplacian matrix.\n",
    "    \"\"\"\n",
    "    def update_attentive_A(self, sess):\n",
    "        fold_len = len(self.all_h_list) // self.n_fold\n",
    "        kg_score = []\n",
    "\n",
    "        for i_fold in range(self.n_fold):\n",
    "            start = i_fold * fold_len\n",
    "            if i_fold == self.n_fold - 1:\n",
    "                end = len(self.all_h_list)\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "\n",
    "            feed_dict = {\n",
    "                self.h: self.all_h_list[start:end],\n",
    "                self.r: self.all_r_list[start:end],\n",
    "                self.pos_t: self.all_t_list[start:end]\n",
    "            }\n",
    "            A_kg_score = sess.run(self.A_kg_score, feed_dict=feed_dict)\n",
    "            kg_score += list(A_kg_score)\n",
    "\n",
    "        kg_score = np.array(kg_score)\n",
    "\n",
    "        new_A = sess.run(self.A_out, feed_dict={self.A_values: kg_score})\n",
    "        new_A_values = new_A.values\n",
    "        new_A_indices = new_A.indices\n",
    "\n",
    "        rows = new_A_indices[:, 0]\n",
    "        cols = new_A_indices[:, 1]\n",
    "        self.A_in = sp.coo_matrix((new_A_values, (rows, cols)), shape=(self.n_users + self.n_entities,\n",
    "                                                                       self.n_users + self.n_entities))\n",
    "        if self.alg_type in ['org', 'gcn']:\n",
    "            self.A_in.setdiag(1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
