{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.argv = [\"main\", \"../Input/data_webdir_leb.pkl\", 2, 4, \"Models/temp2_4.pt\", \"Output/test2_4.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbours: ../Input/data_webdir_leb.pkl\n",
      "Epoch: 0 Idx: 0 Loss: 0.21420708574637012\n",
      "Training size: 2270 Val size: 323\n",
      "Epoch: 0 Idx: 0 Loss: 0.24104197013788822\n",
      "Len (direct inputs):  8\n",
      "-0.439 1.004\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  17\n",
      "Training size: 2269 Val size: 323\n",
      "Epoch: 0 Idx: 0 Loss: 0.14860147830078327\n",
      "Len (direct inputs):  5\n",
      "-0.555 1.02\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  23\n",
      "Training size: 2269 Val size: 323\n",
      "Epoch: 0 Idx: 0 Loss: 0.09775426626506283\n",
      "Len (direct inputs):  11\n",
      "-0.494 1.02\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  27\n",
      "Training size: 2269 Val size: 323\n",
      "Epoch: 0 Idx: 0 Loss: 0.21477732189603463\n",
      "Len (direct inputs):  5\n",
      "-0.424 1.02\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  16\n",
      "Training size: 2268 Val size: 323\n",
      "Epoch: 0 Idx: 0 Loss: 0.13420242747499614\n",
      "Len (direct inputs):  11\n",
      "-0.473 0.901\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  30\n",
      "Final Results: [0.43181818 0.84285714 0.56872102 0.70503796 0.47763287]\n",
      "Threshold:  0.529\n"
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle, operator, random\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[1], \"rb\")\n",
    "data_small, data_large, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts = pickle.load(f)\n",
    "max_paths = int(sys.argv[2])\n",
    "max_pathlen = int(sys.argv[3])\n",
    "max_types = 2\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "def test():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, all_metrics, direct_inputs, direct_targets, threshold_results, emb_indexer_inv, emb_vals\n",
    "    all_results = OrderedDict()    \n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (round(pred_elem, 3), targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (round(sim, 3), direct_targets[idx])\n",
    "    return (test_data_t, all_results)\n",
    "\n",
    "def optimize_threshold(emb_indexer_inv, emb_vals):\n",
    "    global batch_size, val_data_t, val_data_f, model, optimizer, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(val_data_t)\n",
    "        np.random.shuffle(val_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(val_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(val_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (round(pred_elem, 3), targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (round(sim, 3), direct_targets[idx])\n",
    "        \n",
    "        low_threshold = round(np.min([el[0] for el in all_results.values()]) - 0.02, 3)\n",
    "        high_threshold = round(np.max([el[0] for el in all_results.values()]) + 0.02, 3)\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        print (low_threshold, high_threshold)\n",
    "        while threshold < high_threshold:\n",
    "            threshold = round(threshold, 3)\n",
    "#             print (threshold)\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [(key, all_results[key][0]) for key in val_data_t if key not in set(res)]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            # print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "            if threshold in threshold_results:\n",
    "                #print (threshold, len(threshold_results[threshold]))\n",
    "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
    "            else:\n",
    "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
    "            threshold += step\n",
    "        \n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        s = set(res)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in s]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, emb_vals, threshold=0.9):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.n_neighbours = max_types\n",
    "        self.max_paths = max_paths\n",
    "        self.max_pathlen = max_pathlen\n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.threshold = nn.Parameter(torch.DoubleTensor([threshold]))\n",
    "        self.threshold.requires_grad = False\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
    "        \n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(self.max_pathlen) for i in range(self.max_pathlen)]))\n",
    "        self.w_rootpath = nn.Parameter(torch.DoubleTensor([0.5]))\n",
    " \n",
    "    def forward(self, nodes, features):\n",
    "        '''\n",
    "        Arguments:\n",
    "            - nodes: batch_size * 2\n",
    "            - features: batch_size * 2 * 4 * max_paths * max_pathlen\n",
    "        '''\n",
    "        results = []\n",
    "        nodes = nodes.permute(1,0) # 2 * batch_size\n",
    "        features = features.permute(1,0,2,3,4) # 2 * batch_size * 4 * max_paths * max_pathlen\n",
    "        for i in range(2):\n",
    "            node_emb = self.name_embedding(nodes[i]) # batch_size * 512\n",
    "            feature_emb = self.name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512\n",
    "            \n",
    "            feature_emb_reshaped = feature_emb.permute(0,4,1,2,3).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_paths * self.max_pathlen)\n",
    "            path_weights = torch.bmm(node_emb[:, None, :], feature_emb_reshaped)\n",
    "            path_weights = path_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_paths, self.max_pathlen)\n",
    "            path_weights = torch.sum(path_weights, dim=-1)\n",
    "            best_path_indices = torch.max(path_weights, dim=-1)[1][(..., ) + (None, ) * 3]\n",
    "            best_path_indices = best_path_indices.expand(-1, -1, -1, self.max_pathlen,  self.embedding_dim)\n",
    "            best_path = torch.gather(feature_emb, 2, best_path_indices).squeeze(2) # batch_size * 4 * max_pathlen * 512\n",
    "            # Another way: \n",
    "            # path_weights = masked_softmax(path_weights)\n",
    "            # best_path = torch.sum(path_weights[:, :, :, None, None] * feature_emb, dim=2)\n",
    "\n",
    "            best_path_reshaped = best_path.permute(0,3,1,2).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_pathlen)\n",
    "            node_weights = torch.bmm(node_emb.unsqueeze(1), best_path_reshaped) # batch_size * 4 * max_pathlen\n",
    "            node_weights = masked_softmax(node_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_pathlen)) # batch_size * 4 * max_pathlen\n",
    "            attended_path = node_weights.unsqueeze(-1) * best_path # batch_size * 4 * max_pathlen * 512\n",
    "\n",
    "            distance_weighted_path = torch.sum((self.v[None,None,:,None] * attended_path), dim=2) # batch_size * 4 * 512\n",
    "\n",
    "            self.w_children = (1-self.w_rootpath)\n",
    "            context_emb = self.w_rootpath * distance_weighted_path[:,0,:] \\\n",
    "                        + self.w_children * distance_weighted_path[:,1,:]\n",
    "\n",
    "            contextual_node_emb = torch.cat((node_emb, context_emb), dim=1)\n",
    "            output_node_emb = self.output(contextual_node_emb)\n",
    "            results.append(output_node_emb)\n",
    "        sim = self.cosine_sim_layer(results[0], results[1])\n",
    "        return sim\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    return [emb_indexer[elem] for elem in elem_tuple]\n",
    "\n",
    "def embedify(seq):\n",
    "    for item in seq:\n",
    "        if isinstance(item, list):\n",
    "            yield list(embedify(item))\n",
    "        else:\n",
    "            yield emb_indexer[item]\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return list(embedify([neighbours_dicts[elem] for elem in elem_tuple]))\n",
    "\n",
    "def to_feature(inputs):\n",
    "    inputs_lenpadded = [[[[path[:max_pathlen] + [0 for i in range(max_pathlen -len(path[:max_pathlen]))]\n",
    "                                    for path in nbr_type[:max_paths]]\n",
    "                                for nbr_type in ent[:max_types]]\n",
    "                            for ent in elem]\n",
    "                        for elem in inputs]\n",
    "    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]\n",
    "                             for i in range(max_paths - len(nbr_type))]\n",
    "                            for nbr_type in ent] for ent in elem]\n",
    "                        for elem in inputs_lenpadded]\n",
    "    return inputs_pathpadded\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets, nodes = [], [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            nodes.append(generate_data_neighbourless(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return inputs, nodes, targets\n",
    "\n",
    "print(\"Number of neighbours: \" + str(sys.argv[1]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 1\n",
    "weight_decay = 0.001\n",
    "batch_size = 32\n",
    "dropout = 0.3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_t = {elem: data_large[elem] for elem in data_large if data_large[elem]}\n",
    "data_f = {elem: data_large[elem] for elem in data_large if not data_large[elem]}\n",
    "\n",
    "train_data_t = list(data_t.keys())\n",
    "train_data_f = list(data_f.keys())\n",
    "np.random.shuffle(train_data_f)\n",
    "train_data_f = train_data_f[:150000]\n",
    "\n",
    "train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "\n",
    "model = SiameseNetwork(emb_vals).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "    inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "    inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "    targets_all = list(targets_pos) + list(targets_neg)\n",
    "    nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "    \n",
    "    all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "    all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "    inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled[:10]))\n",
    "\n",
    "    batch_size = min(batch_size, len(inputs_all))\n",
    "    num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        \n",
    "        inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "        targets = np.array(targets_all[batch_start: batch_end])\n",
    "        nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "        \n",
    "        inp_elems = torch.LongTensor(inputs).to(device)\n",
    "        targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "        node_elems = torch.LongTensor(nodes).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(node_elems, inp_elems)\n",
    "        loss = F.mse_loss(outputs, targ_elems)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx%5000 == 0:\n",
    "            print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "torch.save(model.state_dict(), sys.argv[4])\n",
    "\n",
    "for i in range(5):\n",
    "    data_t = {elem: data_small[elem] for elem in data_small if data_small[elem]}\n",
    "    data_f = {elem: data_small[elem] for elem in data_small if not data_small[elem]}\n",
    "\n",
    "    data_t_items = list(data_t.keys())\n",
    "    data_f_items = list(data_f.keys())\n",
    "\n",
    "    test_data_t = data_t_items[int((0.2*i)*len(data_t)):int((0.2*i + 0.2)*len(data_t))]\n",
    "    test_data_f = data_f_items[int((0.2*i)*len(data_f)):int((0.2*i + 0.2)*len(data_f))]\n",
    "\n",
    "    train_data_tt = data_t_items[:int(0.2*i*len(data_t))] + data_t_items[int(0.2*(i+1)*len(data_t)):]\n",
    "    train_data_ff = data_f_items[:int(0.2*i*len(data_f))] + data_f_items[int(0.2*(i+1)*len(data_f)):]\n",
    "\n",
    "    np.random.shuffle(train_data_tt)\n",
    "    np.random.shuffle(train_data_ff)\n",
    "\n",
    "    val_data_t = train_data_tt[:int(0.1*len(data_t))]\n",
    "    val_data_f = train_data_ff[:int(0.1*len(data_f))]\n",
    "\n",
    "    train_data_t = train_data_tt[int(0.1*len(data_t)):]\n",
    "    train_data_f = train_data_ff[int(0.1*len(data_f)):]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    train_data_f = train_data_f[:150000]\n",
    "\n",
    "    print (\"Training size:\", len(train_data_t) + len(train_data_f) , \"Val size:\", len(val_data_t) + len(val_data_f))\n",
    "\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    \n",
    "    model = SiameseNetwork(emb_vals).to(device)\n",
    "    pretrained_dict = torch.load(sys.argv[4])\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k!=\"name_embedding.weight\"}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))[:10]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    optimize_threshold(emb_indexer_inv, emb_vals)\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    final_results.append(test())\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "\n",
    "os.remove(sys.argv[4])\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)\n",
    "pickle.dump([final_results, threshold_results], open(sys.argv[-1], \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Supermarkt': 1,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Geschenke': 2,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Ahr': 3,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Kraeuter-Gewuerze': 4,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Honig': 5,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Pfalz': 6,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Nahe': 7,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke': 8,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Tee-Kaffee': 9,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Oesterreichisch': 10,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Home-Service': 11,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten': 12,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Franzoesisch': 13,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen_Whisky': 14,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Oekologischer-Weinbau': 15,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Gewuerze-und-Kraeuter': 16,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Italienisch': 17,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Rheingau': 18,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Tabakwaren': 19,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wasser': 20,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Zustellservice': 21,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Schweizer': 22,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Frankreich': 23,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Accessoires': 24,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Franzoesisch': 25,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel': 26,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Kaffee-und-Tee': 27,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Kleinstanbaugebiete': 28,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Mosel': 29,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland': 30,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Spanien': 31,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Kaffee-und-Tee_Tee': 32,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke': 33,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Saale-Unstrut': 34,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Bier_Heimbrauen': 35,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Bier': 36,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Deutsch': 37,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Schweiz': 38,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Fertigprodukte': 39,\n",
       " 'google_lebensmittel#Subclass': 40,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner': 41,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Kaffee-und-Tee_Kaffee': 42,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Suedafrikanisch': 43,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Naturkost': 44,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Cocktails': 45,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Hohenlohe': 46,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Spezialitaeten': 47,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Tiefkuehlkost': 48,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch': 49,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Weinpraesente': 50,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Weltweit': 51,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Ungarn': 52,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Delikatessen': 53,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen_Absinth': 54,\n",
       " 'google_lebensmittel#subclass_of': 55,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Oesterreich': 56,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Franken': 57,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Saar': 58,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Portugal': 59,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Griechisch': 60,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Tiefkuehlkost': 61,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Suesswaren': 62,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Kroatien': 63,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Franken': 64,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Rheingau': 65,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Baden': 66,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Oesterreichisch': 67,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Fisch': 68,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Rheinhessen': 69,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen': 70,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Wuerttemberg': 71,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Baden': 72,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Mittelrhein': 73,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Fleisch-Wurst': 74,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Vegetarisch_Vegan': 75,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Home-Service_Pizza': 76,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Mosel-Saar-Ruwer': 77,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Champagner': 78,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Spirituosen': 79,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Obst-Gemuese': 80,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Essig-und-Oel_Olivenoel': 81,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Headshops-Hanfprodukte': 82,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch': 83,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Bier': 84,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Essig-und-Oel': 85,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Kaese-Milchprodukte': 86,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Pfalz': 87,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein': 88,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Asiatisch': 89,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Fisch-und-Meeresfruechte': 90,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen_Wodka': 91,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Internationale-Kueche': 92,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Vegetarisch': 93,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Bodensee': 94,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Spanisch': 95,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Biokost': 96,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops': 97,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Kaese': 98,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Backwaren_Weihnachtsgebaeck': 99,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Italienisch': 100,\n",
       " 'web_lebensmittel#subclass_of': 101,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Rheinhessen': 102,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Sekt': 103,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Spanisch': 104,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Landwirtschaftliche-Erzeugnisse': 105,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen': 106,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Back-Suesswaren': 107,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Fleisch-und-Wurst': 108,\n",
       " 'web_lebensmittel#Subclass': 109,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken': 110,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Nahe': 111,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch': 112,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Biowein': 113,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Italien': 114,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Backwaren': 115,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Erfrischungsgetraenke': 116}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_onto, test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in set(res)]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "\n",
    "\n",
    "final_results1, threshold_results1 = pickle.load(open(\"Output/conf6_2_part1.pkl\", \"rb\"))\n",
    "final_results2, threshold_results2 = pickle.load(open(\"Output/conf6_2_part2.pkl\", \"rb\"))\n",
    "threshold_results = {}\n",
    "for key in threshold_results1:\n",
    "    threshold_results[key] = threshold_results1[key]\n",
    "    if key in threshold_results2:\n",
    "        threshold_results[key].extend(threshold_results2[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "final_results = final_results1 + final_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for [('confOf', 'sigkdd')] is : (0.8333333333333334, 0.7142857142857143, 0.7692307692307692, 0.7352941176470589, 0.8064516129032258)\n",
      "Performance for [('iasted', 'sigkdd')] is : (0.8, 0.8, 0.8000000000000002, 0.8, 0.8)\n",
      "Performance for [('cmt', 'ekaw')] is : (0.625, 0.45454545454545453, 0.5263157894736842, 0.4807692307692307, 0.5813953488372092)\n",
      "Performance for [('conference', 'iasted')] is : (0.6666666666666666, 0.2857142857142857, 0.4, 0.3225806451612903, 0.5263157894736842)\n",
      "Performance for [('edas', 'sigkdd')] is : (0.7, 0.4666666666666667, 0.56, 0.5, 0.6363636363636365)\n",
      "Performance for [('ekaw', 'iasted')] is : (1.0, 0.6, 0.7499999999999999, 0.6521739130434783, 0.8823529411764706)\n",
      "Final Results: [0.77083333 0.55353535 0.63425776 0.58180298 0.70547989]\n",
      "Threshold:  0.934\n"
     ]
    }
   ],
   "source": [
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3240"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickle.load(open(\"Input/data_lebensmittel.pkl\", \"rb\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('german_datasets_copy/webdirectory/dmoz.owl', 'german_datasets_copy/webdirectory/google.owl')\n",
      "('german_datasets_copy/webdirectory/dmoz.owl', 'german_datasets_copy/webdirectory/web.owl')\n"
     ]
    }
   ],
   "source": [
    "arr = [('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/google.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/web.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl')]\n",
    "\n",
    "for ont_pair in arr:\n",
    "    print (ont_pair)\n",
    "    a, b, c = ont_pair[0], ont_pair[1], ont_pair[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"-\" + ont_pair[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    !rm -rf $c\n",
    "    os.mkdir(c)\n",
    "    java_command = \"java -jar logmap-matcher/target/logmap-matcher-4.0.jar MATCHER file:\" +  os.path.abspath(a) + \\\n",
    "                     \" file:\" + os.path.abspath(b) + \" \" + \"/data/Vivek/IBM/IBM-Internship/\" + c + \"/ false\"\n",
    "    process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "pred_logmap = [[el.split(\"/\")[-1] for el in l.split(\"\\t\")[:-1]] for l in open(os.path.abspath(c + \"/\") + \"/logmap2_mappings.tsv\",  \"r\").read().split(\"\\n\")[:-1] if not l.startswith(\"Optional\")]\n",
    "gt_mappings = []\n",
    "for elem in pred_logmap:\n",
    "    gt_mappings.append(tuple([el.split(\"#\")[0].replace(\".v2\", \"\").rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"#\" + el.split(\"#\")[1] for el in elem]))\n",
    "data_orig = pickle.load(open(\"Input/data_webdir.pkl\", \"rb\"))[0]\n",
    "data_logmap = {}\n",
    "for key in data_orig:\n",
    "    data_logmap[key] = False\n",
    "s = set(list(data_logmap.keys()))\n",
    "gt_mappings = [tuple(pair) for pair in gt_mappings]\n",
    "for mapping in gt_mappings:\n",
    "    \n",
    "    if mapping in s:\n",
    "        data_logmap[mapping] = True\n",
    "    else:\n",
    "        mapping = tuple([el.replace(\",-\", \"_\") for el in mapping])\n",
    "        if mapping in s:\n",
    "            data_logmap[mapping] = True\n",
    "        else:\n",
    "            print (mapping)\n",
    "all_metrics = []\n",
    "def return_test_data(data, i):\n",
    "    data_t = {elem: data[elem] for elem in data if data[elem]}\n",
    "    data_f = {elem: data[elem] for elem in data if not data[elem]}\n",
    "\n",
    "    data_t_items = list(data_t.keys())\n",
    "    data_f_items = list(data_f.keys())\n",
    "\n",
    "    test_data_t = data_t_items[int((0.2*i)*len(data_t)):int((0.2*i + 0.2)*len(data_t))]\n",
    "    test_data_f = data_f_items[int((0.2*i)*len(data_f)):int((0.2*i + 0.2)*len(data_f))]\n",
    "    \n",
    "    test_data = {}\n",
    "    for elem in test_data_t:\n",
    "        test_data[elem] = True\n",
    "    for elem in test_data_f:\n",
    "        test_data[elem] = False\n",
    "    return test_data\n",
    "\n",
    "for i in range(5):\n",
    "    test_gt = return_test_data(data_orig, i)\n",
    "    test_logmap = {elem: data_logmap[elem] for elem in test_gt}\n",
    "    tp = len([elem for elem in test_gt if test_gt[elem] and test_logmap[elem]])\n",
    "    fp = len([elem for elem in test_logmap if not test_gt[elem] and test_logmap[elem]])\n",
    "    fn = len([elem for elem in test_logmap if test_gt[elem] and not test_logmap[elem]])\n",
    "    \n",
    "    try:\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "        f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        continue\n",
    "    all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    \n",
    "np.mean(all_metrics, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
