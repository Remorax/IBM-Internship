{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle, sys, glob, requests\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from orderedset import OrderedSet\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.acceptable_props = [\"part_of\", \"is_a\", \"has_part\"]\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.construct_mapping_dict()\n",
    "        \n",
    "        self.parents_dict = {}\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()               \n",
    "    \n",
    "    def construct_mapping_dict(self):\n",
    "        self.mapping_dict = {self.extract_ID(el, False): self.get_child_node(el, \"rdfs:label\")[0].firstChild.nodeValue for el in self.root.getElementsByTagName(\"owl:Class\") if self.get_child_node(el, \"rdfs:label\")}\n",
    "        self.mapping_dict_inv = {self.mapping_dict[key]: key for key in self.mapping_dict}\n",
    "        return\n",
    "        \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        subclasses = self.get_subclasses()\n",
    "        self.parents_dict = {}\n",
    "        for (a,b,c,d) in subclasses:\n",
    "            if c == \"subclass_of\" and a!=\"Thing\" and b!=\"Thing\":\n",
    "                if b not in self.parents_dict:\n",
    "                    self.parents_dict[b] = [a]\n",
    "                else:\n",
    "                    self.parents_dict[b].append(a)\n",
    "        return [(b,a,c,d) for (a,b,c,d) in subclasses]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = [(prop, \"Object Property\") for prop in self.object_properties]\n",
    "        data_props = [(prop, \"Datatype Property\") for prop in self.data_properties]\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop, prop_type in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop), prop_type) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop), prop_type))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        \n",
    "        return [elem for elem in list(set(all_triples)) if elem[-1] == \"Subclass\" or elem[-2] in self.acceptable_props]\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode, \"subclass_of\", \"Subclass\"))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    restriction = el.getElementsByTagName(\"owl:Restriction\")\n",
    "                    if not restriction:\n",
    "                        continue\n",
    "                    prop = self.get_child_node(restriction[0], \"owl:onProperty\")\n",
    "                    some_vals = self.get_child_node(restriction[0], \"owl:someValuesFrom\")\n",
    "                    \n",
    "                    if not prop or not some_vals:\n",
    "                        continue\n",
    "#                     print(self.extract_ID(el), \"**\", self.extract_ID(some_vals[0]), \"**\", self.extract_ID(prop[0]))\n",
    "                    try:\n",
    "                        if self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
    "                        elif self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
    "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
    "                        elif not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
    "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
    "                        else:\n",
    "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
    "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
    "                    except:\n",
    "                        try:\n",
    "                            if not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
    "                                subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
    "                            elif not self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
    "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
    "                                class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                                subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
    "                        except Exception as e:\n",
    "                            print (e)\n",
    "                            continue\n",
    "                else:\n",
    "                    if self.extract_ID(level1_class[0]):\n",
    "                        subclass_pairs.append((level1_class[0], el.parentNode, \"subclass_of\", \"Subclass\"))\n",
    "                    else:\n",
    "#                         level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "#                         subclass_pairs.extend([(elem, el.parentNode, \"subclass_of\", \"Subclass\") for elem in level2classes if self.extract_ID(elem)])\n",
    "                        continue\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        subclasses = [(self.extract_ID(a), self.extract_ID(b), c, d) for (a,b,c,d) in self.subclasses]\n",
    "        return [el for el in subclasses if el[0] and el[1] and el[2] and el[0]!=\"Thing\" and el[1]!=\"Thing\"]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element, check_coded = True):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        element_id = element_id.split(\"#\")[-1]\n",
    "        if len(list(filter(str.isdigit, element_id))) >= 3 and \"_\" in element_id and check_coded:\n",
    "            return self.mapping_dict[element_id]\n",
    "        return element_id.replace(\"UNDEFINED_\", \"\").replace(\"DO_\", \"\")\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Anatomy/Alignmentsreference.rdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f217644bbf15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mra_anatomy_coded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_alignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mgt_mappings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mont1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOntology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Anatomy/Ontologies/mouse.owl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f217644bbf15>\u001b[0m in \u001b[0;36mload_alignments\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminidom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetElementsByTagName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'entity1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetElementsByTagName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'entity2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0malignments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAttribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rdf:resource'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAttribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rdf:resource'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/xml/dom/minidom.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, parser, bufsize)\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexpatbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpulldom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/xml/dom/expatbuilder.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(file, namespaces)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Anatomy/Alignmentsreference.rdf'"
     ]
    }
   ],
   "source": [
    "alignment_folder = \"../Anatomy/Alignments\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "\n",
    "# Extracting USE embeddings\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\")\n",
    "    embeds = model(words)\n",
    "    return embeds.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "\n",
    "ra_anatomy_coded = load_alignments(alignment_folder)\n",
    "gt_mappings = []\n",
    "ont1 = Ontology(\"../Anatomy/Ontologies/mouse.owl\")\n",
    "ont2 = Ontology(\"../Anatomy/Ontologies/human.owl\")\n",
    "for elem in ra_anatomy_coded:\n",
    "    pre1, pre2 = elem[0].split(\"#\")[0].split(\".\")[0].split(\"/\")[-1], elem[1].split(\"#\")[0].split(\".\")[0].split(\"/\")[-1]\n",
    "    elem1, elem2 = elem[0].split(\"#\")[-1], elem[1].split(\"#\")[-1]\n",
    "    gt_mappings.append(( pre1 + \"#\" + ont1.mapping_dict[elem1], pre2 + \"#\" + ont2.mapping_dict[elem2]))\n",
    "\n",
    "\n",
    "\n",
    "# ontologies_in_alignment = pickle.load(open(\"../data_generic.pkl\", \"rb\"))[-1][:-1]\n",
    "ontologies_in_alignment = [[\"../Anatomy/Ontologies/mouse.owl\", \"../Anatomy/Ontologies/human.owl\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(l[0])\n",
    "    ont2 = Ontology(l[1])\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2))\n",
    "\n",
    "    \n",
    "    all_mappings.extend([(l[0].split(\"/\")[-1].split(\".\")[0] + \"#\" + el[0], l[1].split(\"/\")[-1].split(\".\")[0] + \"#\" + el[1]) for el in mappings])\n",
    "    \n",
    "\n",
    "data = {mapping: False for mapping in all_mappings}\n",
    "for mapping in set(gt_mappings):\n",
    "    data[mapping] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Abbrevation resolution preprocessing\n",
    "\n",
    "abbreviations_dict = {}\n",
    "final_dict = {}\n",
    "all_mappings_new = []\n",
    "for mapping in all_mappings:\n",
    "    mapping = tuple([el.split(\"#\")[1].replace(\"_\", \" \") for el in mapping])\n",
    "    mapping_new = []\n",
    "    for elem in mapping:\n",
    "        word_replace = []\n",
    "        digit = \"\"\n",
    "        for word in elem.split():\n",
    "            if re.search(\"\\d\", word):\n",
    "                word_sub = re.sub(\"\\d\", \"\", word)\n",
    "                digit = re.search(\"\\d\", word).group()\n",
    "            else:\n",
    "                word_sub = word\n",
    "            word_replace.append(word_sub)\n",
    "        if digit:\n",
    "            word_replace.append(digit)\n",
    "        word_replace = [el for el in word_replace if el]\n",
    "        mapping_new.append(\" \".join(word_replace))\n",
    "    all_mappings_new.append(mapping_new)\n",
    "    mapping = tuple(mapping_new)\n",
    "    is_abb = re.search(\"[A-Z]([A-Z]+| )\", mapping[0])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\" \")])\n",
    "        if is_abb.group().strip() in abbreviation:\n",
    "            \n",
    "            start = abbreviation.find(is_abb.group().strip())\n",
    "            end = start + len(is_abb.group().strip())\n",
    "            fullform = \" \".join(mapping[1].split(\" \")[start:end])\n",
    "            \n",
    "            rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\" \") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[1].split(\" \")[:start] + mapping[1].split(\" \")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "    is_abb = re.search(\"[A-Z]([A-Z]+| )\", mapping[1])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\" \")])\n",
    "        \n",
    "        if is_abb.group().strip() in abbreviation:\n",
    "            start = abbreviation.find(is_abb.group().strip())\n",
    "            end = start + len(is_abb.group().strip())\n",
    "            fullform = \" \".join(mapping[0].split(\" \")[start:end])\n",
    "\n",
    "            rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\" \") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[0].split(\" \")[:start] + mapping[0].split(\" \")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n",
    "\n",
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
    "filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing T  with thoracic in T Vertebra 1\n",
      "Replacing C  with cervical in C Vertebra 2\n",
      "Replacing T  with thoracic in T Vertebra 4\n",
      "Replacing T  with thoracic in T Vertebra 1\n",
      "Replacing L  with lumbar in L Vertebra 4\n",
      "Replacing T  with thoracic in T Vertebra 5\n",
      "Replacing T  with thoracic in T Vertebra 9\n",
      "Replacing T  with thoracic in T Vertebra 1\n",
      "Replacing C  with cervical in C Vertebra 7\n",
      "Replacing C  with cervical in C Vertebra 5\n",
      "Replacing S  with sacral in S Vertebra 2\n",
      "Replacing L  with lumbar in L Vertebra 5\n",
      "Replacing C  with cervical in C Vertebra 4\n",
      "Replacing S  with sacral in S Vertebra 1\n",
      "Replacing T  with thoracic in T Vertebra 3\n",
      "Replacing C  with cervical in C Vertebra 6\n",
      "Replacing S  with sacral in S Vertebra 3\n",
      "Replacing L  with lumbar in L Vertebra 3\n",
      "Replacing C  with cervical in C Vertebra 3\n",
      "Replacing C  with cervical in C Vertebra 1\n",
      "Replacing T  with thoracic in T Vertebra 6\n",
      "Replacing T  with thoracic in T Vertebra 1\n",
      "Replacing T  with thoracic in T Vertebra 2\n",
      "Replacing S  with sacral in S Vertebra 5\n",
      "Replacing T  with thoracic in T Gamma-Delta Lymphocyte\n",
      "Replacing T  with thoracic in T Vertebra 8\n",
      "Replacing L  with lumbar in L Vertebra 1\n",
      "Replacing S  with sacral in S Vertebra 4\n",
      "Replacing L  with lumbar in L Vertebra 2\n",
      "Replacing T  with thoracic in T Vertebra 7\n",
      "Total number of extracted unique classes and properties from entire RA set:  6048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    \n",
    "    ont = Ontology(ont_name)\n",
    "    entities = ont.get_entities()\n",
    "    entities += list(set(flatten([(a,b) for (a,b,c,d) in ont.get_triples()])))\n",
    "    extracted_elems.extend([ont_name.split(\"/\")[-1].split(\".\")[0] + \"#\" + elem for elem in entities])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "inp_anatomy = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "\n",
    "roman_regex = \"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n",
    "inp_anatomy = [\" \".join([word.replace(\"/\", \" or \").replace(\"_\", \" \") for word in elem.split()]) for elem in inp_anatomy]\n",
    "inp_numbered = []\n",
    "for elem in inp_anatomy:\n",
    "    word_replace = []\n",
    "    digit = \"\"\n",
    "    for word in elem.split():\n",
    "        if re.search(\"\\d\", word):\n",
    "            word_sub = re.sub(\"\\d\", \"\", word)\n",
    "            digit = re.search(\"\\d\", word).group()\n",
    "        else:\n",
    "            word_sub = word\n",
    "        word_replace.append(word_sub)\n",
    "    if digit:\n",
    "        word_replace.append(digit)\n",
    "    inp_numbered.append(\" \".join(word_replace))\n",
    "\n",
    "inp_resolved = []\n",
    "for concept in inp_numbered:\n",
    "    for key in filtered_dict:\n",
    "        if key in concept:\n",
    "            print (\"Replacing {} with {} in {}\".format(key, filtered_dict[key], concept))\n",
    "        concept = concept.replace(key, filtered_dict[key])\n",
    "    final_list = []\n",
    "    # Lowering case except in abbreviations\n",
    "    for word in concept.split(\" \"):\n",
    "        if not re.search(\"[A-Z][A-Z]+\", word):\n",
    "            final_list.append(word.lower())\n",
    "        else:\n",
    "            final_list.append(word)\n",
    "    \n",
    "    concept = \" \".join(final_list)\n",
    "    \n",
    "    inp_resolved.append(concept)\n",
    "\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "extracted_elems = [\"<UNK>\"] + extracted_elems\n",
    "\n",
    "embeds = np.array(extractUSEEmbeddings(inp_resolved))\n",
    "embeds = np.array([np.zeros(embeds.shape[1],)] + list(embeds))\n",
    "# embeds = np.array([np.zeros(512,)] + list(extractUSEEmbeddings(inp_spellchecked)))\n",
    "embeddings = dict(zip(extracted_elems, embeds))\n",
    "\n",
    "emb_vals = list(embeddings.values())\n",
    "emb_indexer = {key: i for i, key in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: key for i, key in enumerate(list(embeddings.keys()))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Anatomy/Ontologies/human.owl\n",
      "../Anatomy/Ontologies/mouse.owl\n"
     ]
    }
   ],
   "source": [
    "ontologies_in_alignment = [[\"../Anatomy/Ontologies/mouse.owl\", \"../Anatomy/Ontologies/human.owl\"]]\n",
    "rootpath_dict_new = {}\n",
    "limit = 3\n",
    "def path_to_root(elem, ont_mappings, curr = [], rootpath=[]):\n",
    "    sys.stdout.flush()\n",
    "    curr.append(elem)\n",
    "    if elem not in ont_mappings or not ont_mappings[elem]:\n",
    "        rootpath.append(curr)\n",
    "        return\n",
    "    for node in ont_mappings[elem]:\n",
    "        curr_orig = deepcopy(curr)\n",
    "        _ = path_to_root(node, ont_mappings, curr, rootpath)\n",
    "        curr = curr_orig\n",
    "    return rootpath\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    global rootpath_dict_new\n",
    "    ont_obj = Ontology(ont)\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c,d) in triples]\n",
    "    neighbours_dict = {elem: [[] for i in range(4)] for elem in list(set(flatten(entities)))}\n",
    "    print (ont)\n",
    "    for (e1, e2, p, d) in triples:\n",
    "        if e1==e2:\n",
    "            continue\n",
    "        if d == \"Object Property\":\n",
    "            neighbours_dict[e1][2].append([e2])\n",
    "            neighbours_dict[e2][2].append([e1])\n",
    "        elif d == \"Datatype Property\":\n",
    "            neighbours_dict[e1][3].append([e2])\n",
    "            neighbours_dict[e2][3].append([e1])\n",
    "        elif d == \"Subclass\":\n",
    "            neighbours_dict[e2][1].append([e1])\n",
    "        else:\n",
    "            print (\"Error wrong value of d: \", d)\n",
    "    \n",
    "    rootpath_dict = ont_obj.parents_dict\n",
    "    rootpath_dict_new = {}\n",
    "    for elem in rootpath_dict:\n",
    "#         print (\"Done for \", elem)\n",
    "        rootpath_dict_new[elem] = path_to_root(elem, rootpath_dict, [], [])\n",
    "    ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    \n",
    "    for entity in neighbours_dict:\n",
    "        if entity in rootpath_dict_new and len(rootpath_dict_new[entity]) > 0:\n",
    "            neighbours_dict[entity][0].extend(rootpath_dict_new[entity])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "#     prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "#     neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "#     for e1, e2, p in prop_triples:\n",
    "#         neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "#     neighbours_dict = {elem: [key[:1] + sorted(list(set(key[1:]))) for key in neighbours_dict[elem]]\n",
    "#                        for elem in neighbours_dict}\n",
    "#     neighbours_dict = {el: neighbours_dict[el][:23] for el in neighbours_dict if len( neighbours_dict[el]) > 2}\n",
    "#     ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "    neighbours_dict = {ont + \"#\" + el: [OrderedSet([tuple([ont + \"#\" + node for node in path]) for path in nbr_type])\n",
    "                                        for nbr_type in neighbours_dict[el]] \n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: [[list(path) for path in nbr_type] for nbr_type in neighbours_dict[el]]\n",
    "                       for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "neighbours_dicts = {}\n",
    "for ont in list(set(flatten(ontologies_in_alignment))):\n",
    "    neighbours_dicts = {**neighbours_dicts, **get_one_hop_neighbours(ont)}\n",
    "max_types = np.max([len([nbr_type for nbr_type in elem if nbr_type]) for elem in neighbours_dicts.values()])\n",
    "max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "# neighbours_dicts_lenpadded = {elem: [[path + [\"<UNK>\" for i in range(max_pathlen -len(path))] for path in nbr_type]\n",
    "#                                 for nbr_type in neighbours_dicts[elem]] for elem in neighbours_dicts}\n",
    "# neighbours_dicts_pathpadded = {elem: [nbr_type + [[\"<UNK>\" for j in range(max_pathlen)] for i in range(max_paths - len(nbr_type))]\n",
    "#                                 for k,nbr_type in enumerate(neighbours_dicts_lenpadded[elem])] for elem in neighbours_dicts_lenpadded}\n",
    "# neighbours_dicts_pathpadded = {elem: np.array(neighbours_dicts_pathpadded[elem]) for elem in neighbours_dicts_pathpadded}\n",
    "\n",
    "data_items = data.items()\n",
    "data_shuffled_t = [elem for elem in data_items if elem[1]]\n",
    "data_shuffled_f = [elem for elem in data_items if not elem[1]]\n",
    "np.random.shuffle(data_shuffled_f)\n",
    "data_shuffled_f = data_shuffled_f[:150000-len(data_shuffled_t)]\n",
    "data = data_shuffled_t + data_shuffled_f\n",
    "data = OrderedDict(data)\n",
    "\n",
    "aml_data_items = aml_data.items()\n",
    "aml_data_shuffled_t = [elem for elem in aml_data_items if elem[1]]\n",
    "aml_data_shuffled_f = [elem for elem in aml_data_items if not elem[1]]\n",
    "np.random.shuffle(aml_data_shuffled_f)\n",
    "aml_data_shuffled_f = aml_data_shuffled_f[:150000-len(aml_data_shuffled_t)]\n",
    "aml_data = aml_data_shuffled_t + aml_data_shuffled_f\n",
    "aml_data = OrderedDict(aml_data)\n",
    "\n",
    "ontologies_in_alignment_rev = [[el.split(\"/\")[-1].split(\".\")[0] for el in ont] for ont in ontologies_in_alignment]\n",
    "# f = open(\"Input/data_anatomy_oaei.pkl\", \"wb\")\n",
    "# pickle.dump([data, aml_data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment_rev, max_types, max_paths, max_pathlen], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[2276, 4584, 5856, 2483, 3045]], [[5226, 5702]], [[705]], [[]]],\n",
       " [[[1774, 4125, 3664, 1866, 95, 3670, 3045],\n",
       "   [1774, 4125, 3664, 1866, 95, 5638, 3045],\n",
       "   [1774, 4125, 3664, 1866, 504, 3670, 3045],\n",
       "   [1774, 4125, 5856, 2483, 3045]],\n",
       "  [[]],\n",
       "  [[2748]],\n",
       "  [[]]]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Sequence\n",
    "def embedify(seq, emb_indexer):\n",
    "    for item in seq:\n",
    "        if isinstance(item, list):\n",
    "            yield list(embedify(item, emb_indexer))\n",
    "        else:\n",
    "            yield emb_indexer[item]\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return list(embedify([neighbours_dicts[elem] for elem in elem_tuple], emb_indexer))\n",
    "\n",
    "generate_data(('human#Intercostal_Muscle', 'human#Circular_Ligament_of_the_Tooth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 151, 13]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = pickle.load(open(\"Input/data_anatomy_oaei.pkl\", \"rb\"))\n",
    "inputs[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Anatomy/Ontologies/human.owl\n",
      "../Anatomy/Ontologies/mouse.owl\n"
     ]
    }
   ],
   "source": [
    "ontologies_in_alignment = [[\"../Anatomy/Ontologies/mouse.owl\", \"../Anatomy/Ontologies/human.owl\"]]\n",
    "rootpath_dict_new = {}\n",
    "limit = 3\n",
    "def path_to_root(elem, ont_mappings, curr = [], rootpath=[]):\n",
    "    sys.stdout.flush()\n",
    "    curr.append(elem)\n",
    "    if elem not in ont_mappings or not ont_mappings[elem]:\n",
    "        rootpath.append(curr)\n",
    "        return\n",
    "    for node in ont_mappings[elem]:\n",
    "        curr_orig = deepcopy(curr)\n",
    "        _ = path_to_root(node, ont_mappings, curr, rootpath)\n",
    "        curr = curr_orig\n",
    "    return rootpath\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    global rootpath_dict_new\n",
    "    ont_obj = Ontology(ont)\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c,d) in triples]\n",
    "    neighbours_dict = {elem: [[] for i in range(4)] for elem in list(set(flatten(entities)))}\n",
    "    print (ont)\n",
    "    for (e1, e2, p, d) in triples:\n",
    "        if e1==e2:\n",
    "            continue\n",
    "        if d == \"Object Property\":\n",
    "            neighbours_dict[e1][2].append(e2)\n",
    "            neighbours_dict[e2][2].append(e1)\n",
    "        elif d == \"Datatype Property\":\n",
    "            neighbours_dict[e1][3].append(e2)\n",
    "            neighbours_dict[e2][3].append(e1)\n",
    "        elif d == \"Subclass\":\n",
    "            neighbours_dict[e2][1].append(e1)\n",
    "        else:\n",
    "            print (\"Error wrong value of d: \", d)\n",
    "    \n",
    "    rootpath_dict = ont_obj.parents_dict\n",
    "    rootpath_dict_new = {}\n",
    "    for elem in rootpath_dict:\n",
    "        rootpath_dict_new[elem] = path_to_root(elem, rootpath_dict, [], [])\n",
    "    ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    \n",
    "    for entity in neighbours_dict:\n",
    "        neighbours_dict[entity][1] = [neighbours_dict[entity][1]]\n",
    "        neighbours_dict[entity][2] = [neighbours_dict[entity][2]]\n",
    "        neighbours_dict[entity][3] = [neighbours_dict[entity][3]]\n",
    "        if entity in rootpath_dict_new and len(rootpath_dict_new[entity]) > 0:\n",
    "            neighbours_dict[entity][0].extend(rootpath_dict_new[entity])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "#     prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "#     neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "#     for e1, e2, p in prop_triples:\n",
    "#         neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "#     neighbours_dict = {elem: [key[:1] + sorted(list(set(key[1:]))) for key in neighbours_dict[elem]]\n",
    "#                        for elem in neighbours_dict}\n",
    "#     neighbours_dict = {el: neighbours_dict[el][:23] for el in neighbours_dict if len( neighbours_dict[el]) > 2}\n",
    "#     ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "    neighbours_dict = {ont + \"#\" + el: [[tuple([ont + \"#\" + node for node in path]) for path in nbr_type]\n",
    "                                        for nbr_type in neighbours_dict[el]] \n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: [[list(path) for path in nbr_type] for nbr_type in neighbours_dict[el]]\n",
    "                       for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "neighbours_dicts = {}\n",
    "for ont in list(set(flatten(ontologies_in_alignment))):\n",
    "    neighbours_dicts = {**neighbours_dicts, **get_one_hop_neighbours(ont)}\n",
    "max_types = np.max([len([nbr_type for nbr_type in elem if nbr_type]) for elem in neighbours_dicts.values()])\n",
    "max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "# neighbours_dicts_lenpadded = {elem: [[path + [\"<UNK>\" for i in range(max_pathlen -len(path))] for path in nbr_type]\n",
    "#                                 for nbr_type in neighbours_dicts[elem]] for elem in neighbours_dicts}\n",
    "# neighbours_dicts_pathpadded = {elem: [nbr_type + [[\"<UNK>\" for j in range(max_pathlen)] for i in range(max_paths - len(nbr_type))]\n",
    "#                                 for k,nbr_type in enumerate(neighbours_dicts_lenpadded[elem])] for elem in neighbours_dicts_lenpadded}\n",
    "# neighbours_dicts_pathpadded = {elem: np.array(neighbours_dicts_pathpadded[elem]) for elem in neighbours_dicts_pathpadded}\n",
    "\n",
    "data_items = data.items()\n",
    "data_shuffled_t = [elem for elem in data_items if elem[1]]\n",
    "data_shuffled_f = [elem for elem in data_items if not elem[1]]\n",
    "np.random.shuffle(data_shuffled_f)\n",
    "data_shuffled_f = data_shuffled_f[:150000-len(data_shuffled_t)]\n",
    "data = data_shuffled_t + data_shuffled_f\n",
    "data = OrderedDict(data)\n",
    "\n",
    "aml_data_items = aml_data.items()\n",
    "aml_data_shuffled_t = [elem for elem in aml_data_items if elem[1]]\n",
    "aml_data_shuffled_f = [elem for elem in aml_data_items if not elem[1]]\n",
    "np.random.shuffle(aml_data_shuffled_f)\n",
    "aml_data_shuffled_f = aml_data_shuffled_f[:150000-len(aml_data_shuffled_t)]\n",
    "aml_data = aml_data_shuffled_t + aml_data_shuffled_f\n",
    "aml_data = OrderedDict(aml_data)\n",
    "\n",
    "ontologies_in_alignment_rev = [[el.split(\"/\")[-1].split(\".\")[0] for el in ont] for ont in ontologies_in_alignment]\n",
    "# f = open(\"Input/data_anatomy_oaei_bagofnbrs.pkl\", \"wb\")\n",
    "# pickle.dump([data, aml_data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment_rev, max_types, max_paths, max_pathlen], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Input/data_anatomy_oaei.pkl\", \"rb\")\n",
    "data, aml_data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment, max_types, max_paths, max_pathlen = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "limit = 22\n",
    "x = list(range(limit))\n",
    "up_limit = 20\n",
    "plt.plot(x, [el if el < up_limit else up_limit for el in a_idx[:limit]], 'r') # plotting t, a separately \n",
    "plt.plot(x, [el if el < up_limit else up_limit for el in b_idx[:limit]], 'b') # plotting t, b separately \n",
    "plt.plot(x, [el if el < up_limit else up_limit for el in c_idx[:limit]], 'g') # plotting t, c separately \n",
    "plt.legend([\"rootpaths\", \"children\", \"object property\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 13571),\n",
       " (1, 2541),\n",
       " (2, 1303),\n",
       " (3, 1143),\n",
       " (4, 1405),\n",
       " (5, 1752),\n",
       " (6, 1842),\n",
       " (7, 1573),\n",
       " (8, 985),\n",
       " (9, 378),\n",
       " (10, 153),\n",
       " (11, 51),\n",
       " (12, 36),\n",
       " (13, 14),\n",
       " (14, 9),\n",
       " (15, 5),\n",
       " (16, 3),\n",
       " (17, 6),\n",
       " (18, 3),\n",
       " (19, 5),\n",
       " (20, 2),\n",
       " (21, 2),\n",
       " (22, 3),\n",
       " (23, 3),\n",
       " (24, 2),\n",
       " (25, 3),\n",
       " (26, 2),\n",
       " (29, 3),\n",
       " (30, 1),\n",
       " (31, 2),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 2),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (46, 1),\n",
       " (48, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (72, 1),\n",
       " (75, 1),\n",
       " (82, 1),\n",
       " (94, 1),\n",
       " (106, 1),\n",
       " (123, 1),\n",
       " (127, 1),\n",
       " (129, 1),\n",
       " (151, 1)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()])).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1051),\n",
       " (1, 15451),\n",
       " (2, 641),\n",
       " (3, 531),\n",
       " (4, 191),\n",
       " (5, 19),\n",
       " (6, 130),\n",
       " (7, 26),\n",
       " (8, 39),\n",
       " (9, 8),\n",
       " (10, 3),\n",
       " (11, 1),\n",
       " (12, 4),\n",
       " (15, 3),\n",
       " (16, 5),\n",
       " (19, 1),\n",
       " (21, 1)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[(i,sum(x)) for i,x in enumerate(zip(*[a_idx, b_idx, c_idx])) if sum(x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_command = 'grep \"Final Results\" Results/Output*'\n",
    "process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mouse-human.rdf']\n",
      "1.0 0.9013157894736842 0.9480968858131489 0.9194630872483223 0.9785714285714288\n",
      "Final Results: [1.         0.90131579 0.94809689 0.91946309 0.97857143]\n"
     ]
    }
   ],
   "source": [
    "# AML test\n",
    "train_data, test_data = pickle.load(open(\"data_anatomy_bert.pkl\", \"rb\"))[:2]\n",
    "ontologies_in_alignment = [[\"../Anatomy/Ontologies/mouse.owl\", \"../Anatomy/Ontologies/human.owl\"]]\n",
    "test_data_t = [key for key in test_data if test_data[key]]\n",
    "results = []\n",
    "# all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "# for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "#     test_onto = all_ont_pairs[i:i+3]\n",
    "for ont_pair in ontologies_in_alignment:\n",
    "    a, b, c = ont_pair[0], ont_pair[1], ont_pair[0].split(\"/\")[-1].split(\".\")[0] + \"-\" + ont_pair[1].split(\"/\")[-1].split(\".\")[0]\n",
    "    java_command = \"java -jar AML_v3.1/AgreementMakerLight.jar -s \" + a + \" -t \" + b + \" -o AML-test-results/\" + c + \".rdf -a\"\n",
    "    process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "print (os.listdir(\"AML-test-results/\"))\n",
    "pred_aml = load_alignments(\"AML-test-results/\")\n",
    "\n",
    "pred_aml_filt = []\n",
    "\n",
    "for (ont1_name, ont2_name) in ontologies_in_alignment:\n",
    "    ont1 = Ontology(ont1_name)\n",
    "    ont2 = Ontology(ont2_name)\n",
    "    for elem in pred_aml:\n",
    "        pre1, pre2 = elem[0].split(\"#\")[0].split(\".\")[0].split(\"/\")[-1], elem[1].split(\"#\")[0].split(\".\")[0].split(\"/\")[-1]\n",
    "        elem1, elem2 = elem[0].split(\"#\")[-1], elem[1].split(\"#\")[-1]\n",
    "        pred_aml_filt.append(( pre1 + \"#\" + ont1.mapping_dict[elem1], pre2 + \"#\" + ont2.mapping_dict[elem2]))\n",
    "\n",
    "\n",
    "\n",
    "# pred_aml_filt = [tuple([el.split(\"/\")[-1] for el in key]) for key in pred_aml_filt]\n",
    "tp = [elem for elem in pred_aml_filt if elem in test_data and test_data[elem]]\n",
    "fn = [key for key in test_data_t if key not in set(pred_aml_filt)]\n",
    "fp = [elem for elem in pred_aml_filt if elem in test_data and not test_data[elem]]\n",
    "\n",
    "\n",
    "tp_len, fn_len, fp_len = len(tp), len(fn), len(fp)\n",
    "precision = tp_len/(tp_len+fp_len)\n",
    "recall = tp_len/(tp_len+fn_len)\n",
    "f1score = 2 * precision * recall / (precision + recall)\n",
    "f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "print (precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "results.append(metrics)\n",
    "\n",
    "_ = [os.remove(f) for f in glob.glob('AML-test-results/*')]\n",
    "    \n",
    "print (\"Final Results:\", np.mean(results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pathlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[elem for elem in pred_aml_filt if \"#\" not in elem[0] or \"#\" not in elem[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data_unhas.pkl\", \"wb\")\n",
    "pickle.dump([data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:int(sys.argv[1])] for el in neighbours_dicts[ont]\n",
    "       if count_non_unk(neighbours_dicts[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9066182, 9066182)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(all_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_t = [key for key in train_data if train_data[key]]\n",
    "excluded_aml = [key for key in test_data_t + train_data_t if key not in set(pred_aml_filt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://montanaflynn-spellcheck.p.rapidapi.com/check/\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"montanaflynn-spellcheck.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"9965b01207msh06291e57d6f2c55p1a6a16jsn0fb016da4a62\"\n",
    "    }\n",
    "\n",
    "# inp_spellchecked = []\n",
    "for concept in inp[731:]:\n",
    "    querystring = {\"text\": concept}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring).json()\n",
    "    if response[\"suggestion\"] != concept:\n",
    "        resolved = str(concept)\n",
    "        for word in response[\"corrections\"]:\n",
    "            if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "                resolved = resolved.replace(word, response[\"corrections\"][word][0])\n",
    "        \n",
    "        inp_spellchecked.append(resolved)\n",
    "        print (concept, resolved)\n",
    "    else:\n",
    "        inp_spellchecked.append(concept)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querystring = {\"text\": \"technically Organised By\"}\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment = pickle.load(open(\"data_anatomy_sent2vec.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn_spellchecked, fp_spellchecked = [dict(el) for el in pickle.load(open(\"test_v2.pkl\", \"rb\"))]\n",
    "# fn_baseline, fp_baseline = [dict(el) for el in pickle.load(open(\"test_best.pkl\", \"rb\"))]\n",
    "# fn_unhas, fp_unhas = [dict(el) for el in pickle.load(open(\"test_unhas.pkl\", \"rb\"))]\n",
    "# fn_resolved, fp_resolved = [dict(el) for el in pickle.load(open(\"test_resolved.pkl\", \"rb\"))]\n",
    "\n",
    "fn_dict, fp_dict = {}, {}\n",
    "def create_comparison_file(file, idx):\n",
    "    fn, fp = [dict(el) for el in pickle.load(open(file, \"rb\"))]\n",
    "    \n",
    "    for key in fn:\n",
    "        if key in fn_dict:\n",
    "            fn_dict[key][idx] = fn[key]\n",
    "        else:\n",
    "            fn_dict[key] = [\"N/A\" for i in range(2)]\n",
    "            fn_dict[key][idx] = fn[key]\n",
    "    \n",
    "    for key in fp:\n",
    "        if key in fp_dict:\n",
    "            fp_dict[key][idx] = fp[key]\n",
    "        else:\n",
    "            fp_dict[key] = [\"N/A\" for i in range(2)]\n",
    "            fp_dict[key][idx] = fp[key]\n",
    "    \n",
    "\n",
    "create_comparison_file(\"test_best.pkl\", 0)\n",
    "create_comparison_file(\"test_unhas.pkl\", 1)\n",
    "\n",
    "open(\"fn - comparison.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(el) for el in flatten(el)]) for el in fn_dict.items()]))\n",
    "open(\"fp - comparison.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(el) for el in flatten(el)]) for el in fp_dict.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_spellchecked, fp_spellchecked = [dict(el) for el in pickle.load(open(\"test_anatomy_use28_1.pkl\", \"rb\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('mouse#cerebellar layer',\n",
       "  'human#Cortical_Cell_Layer_of_the_Cerebellum'): 0.8012549559910933,\n",
       " ('mouse#vertebra dorsal arch',\n",
       "  'human#Arch_of_the_Vertebra'): 0.8190866110128275,\n",
       " ('mouse#iliopsoas', 'human#Iliopsoas_Muscle'): 0.7605249355878596,\n",
       " ('mouse#retina inner nuclear layer',\n",
       "  'human#Inner_Nuclear_Layer'): 0.8656842872069336,\n",
       " ('mouse#stomach greater curvature',\n",
       "  'human#Greater_Curvature'): 0.7226314051381755,\n",
       " ('mouse#corpora quadrigemina',\n",
       "  'human#Quadrigeminal_Body'): 0.45310238770181666,\n",
       " ('mouse#palatine gland', 'human#Palatine_Salivary_Gland'): 0.8566996196072939,\n",
       " ('mouse#lunate', 'human#Lunate_Bone'): 0.7091920887814754,\n",
       " ('mouse#crypt of lieberkuhn',\n",
       "  'human#Crypts_of_the_Lieberkuhn'): 0.7354369438482792,\n",
       " ('mouse#kidney interstitium',\n",
       "  'human#Renal_Interstitial_Tissue'): 0.8072747064853303,\n",
       " ('mouse#arrector pili smooth muscle',\n",
       "  'human#Erector_Muscle_of_the_Hair'): 0.7417348746968164,\n",
       " ('mouse#tooth substance', 'human#Tooth_Tissue'): 0.8003664647008165,\n",
       " ('mouse#afferent arteriole',\n",
       "  'human#Renal_Afferent_Vessel'): 0.7334917549480728,\n",
       " ('mouse#intermediate cuneiform',\n",
       "  'human#Middle_Cuneiform_Bone_of_the_Foot'): 0.7514291669175871,\n",
       " ('mouse#retina photoreceptor layer',\n",
       "  'human#Layer_of_the_Rods_and_Cones'): 0.6665336785623138,\n",
       " ('mouse#nasal cavity olfactory epithelium',\n",
       "  'human#Olfactory_Epithelium'): 0.7310510102006187,\n",
       " ('mouse#geniculate thalamic group',\n",
       "  'human#Geniculate_Body'): 0.36947392316745536,\n",
       " ('mouse#urinary bladder serosa',\n",
       "  'human#Bladder_Serosal_Surface'): 0.814007375391673,\n",
       " ('mouse#dermis papillary layer',\n",
       "  'human#Stratum_Papillare'): 0.7489151194882265,\n",
       " ('mouse#salivary duct',\n",
       "  'human#Duct_Salivary_Gland_System'): 0.8427961679736394,\n",
       " ('mouse#frontal cortex', 'human#Frontal_Lobe'): 0.7367125796917866,\n",
       " ('mouse#brain arachnoid matter',\n",
       "  'human#Cerebral_Arachnoid_Membrane'): 0.4281161093956011,\n",
       " ('mouse#digastric', 'human#Digastric_Muscle'): 0.7584581741008128,\n",
       " ('mouse#neural retinal epithelium',\n",
       "  'human#Neural_Retina'): 0.6919949276913518,\n",
       " ('mouse#prostate gland smooth muscle',\n",
       "  'human#Prostatic_Muscular_Tissue'): 0.7534558809390691,\n",
       " ('mouse#temporalis', 'human#Temporal_Muscle'): 0.23099265457762855,\n",
       " ('mouse#external naris', 'human#Nostril'): -0.030257228011588054}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_spellchecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_dict = {}\n",
    "final_dict = {}\n",
    "\n",
    "for mapping in all_mappings:\n",
    "    mapping = tuple([el.split(\"#\")[1] for el in mapping])\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[0])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\"_\")])\n",
    "        if is_abb.group() in abbreviation:\n",
    "            \n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[1].split(\"_\")[start:end])\n",
    "            print (\"left\", mapping, abbreviation, fullform)\n",
    "            \n",
    "            rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[1].split(\"_\")[:start] + mapping[1].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[1])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\"_\")])\n",
    "        \n",
    "        if is_abb.group() in abbreviation:\n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[0].split(\"_\")[start:end])\n",
    "            print (\"right\", mapping, abbreviation, fullform)\n",
    "\n",
    "            rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[0].split(\"_\")[:start] + mapping[0].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n",
    "\n",
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
    "filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n",
    "inp_resolved = []\n",
    "for concept in inp:\n",
    "    for key in filtered_dict:\n",
    "        concept = concept.replace(key, filtered_dict[key])\n",
    "    inp_resolved.append(concept)\n",
    "inp_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([key for key in data_train if data_train[key]]), len([key for key in data_test if data_test[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_case_handled = []\n",
    "for concept in inp:\n",
    "    final_list = []\n",
    "    for word in concept.split(\" \"):\n",
    "        if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "            final_list.append(word.lower())\n",
    "        else:\n",
    "            final_list.append(word)\n",
    "    case_resolved = \" \".join(final_list)\n",
    "    inp_case_handled.append(case_resolved)\n",
    "    \n",
    "inp_case_handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ontology(\"conference_ontologies/conference.owl\").triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import XLNetTokenizer, XLNetModel\n",
    "# import torch\n",
    "# import scipy\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"fastigial nucleus\", add_special_tokens=True)).unsqueeze(0)\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0].mean(1)\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"femur\", add_special_tokens=True)).unsqueeze(0) \n",
    "\n",
    "outputs1 = model(input_ids)\n",
    "last_hidden_states1 = outputs1[0].mean(1)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "cos(last_hidden_states, last_hidden_states1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35638532042503357"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = \"/home/vlead/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\n",
    "# model = sent2vec.Sent2vecModel()\n",
    "# try:\n",
    "#     model.load_model(model_path)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "# print('model successfully loaded')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_sentence(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_huggingface_embeddings(words):\n",
    "    return model.embed_sentences([preprocess_sentence(elem) for elem in words])\n",
    "\n",
    "\n",
    "cos_sim(*extractUSEEmbeddings([\"adrenal gland zona reticularis\", \"Reticularis Zone\"])), cos_sim(*extract_huggingface_embeddings([\"adrenal gland zona reticularis\", \"Reticularis Zone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 17.165284897635363 17.700582340359688\n"
     ]
    }
   ],
   "source": [
    "use, bio2vec = 0, 0\n",
    "final_list = []\n",
    "for pair in fn_spellchecked:\n",
    "    pair_proc = [\" \".join(parse(el.split(\"#\")[1])).lower() for el in pair]\n",
    "    use_score = cos_sim(embeddings[pair[0]], embeddings[pair[1]])\n",
    "    bio2vec_score = cos_sim(*extract_huggingface_embeddings(list(pair_proc)))\n",
    "    use += use_score\n",
    "    bio2vec += bio2vec_score\n",
    "    final_list.append([pair[0], pair[1], fn_spellchecked[pair], use_score, bio2vec_score, pair in excluded_aml, \"\"])\n",
    "open(\"results_anatomy_use_fn.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(col) for col in entry]) for entry in final_list]))\n",
    "print (\"Total\", use, bio2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 12.82707794533874 11.156346499919891\n"
     ]
    }
   ],
   "source": [
    "use, bio2vec = 0, 0\n",
    "final_list = []\n",
    "for pair in fp_spellchecked:\n",
    "    pair_proc = [\" \".join(parse(el.split(\"#\")[1])).lower() for el in pair]\n",
    "    use_score = cos_sim(embeddings[pair[0]], embeddings[pair[1]])\n",
    "    bio2vec_score = cos_sim(*extract_huggingface_embeddings(list(pair_proc)))\n",
    "    use += use_score\n",
    "    bio2vec += bio2vec_score\n",
    "    final_list.append([pair[0], pair[1], fp_spellchecked[pair], use_score, bio2vec_score, pair in excluded_aml, \"\"])\n",
    "open(\"results_anatomy_use_fp.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(col) for col in entry]) for entry in final_list]))\n",
    "print (\"Total\", use, bio2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
