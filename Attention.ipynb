{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Number of neighbours: -f\n",
      "Number of entities: 122893\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [7168 x 22], m2: [1 x 1] at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bdb96f9e8152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mtarg_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-bdb96f9e8152>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [7168 x 22], m2: [1 x 1] at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(\"data.pkl\", \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, all_ont_pairs = pickle.load(f)\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.mapping_dict = self.construct_mapping_dict()\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()        \n",
    "    \n",
    "    def construct_mapping_dict(self):\n",
    "        mapping_dict = {self.extract_ID(el, False): self.get_child_node(el, \"rdfs:label\")[0].firstChild.nodeValue for el in self.root.getElementsByTagName(\"owl:Class\") if self.get_child_node(el, \"rdfs:label\")}\n",
    "        return mapping_dict\n",
    "        \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(b,a,c) for (a,b,c) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, include_inv=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode, \"subclass_of\"))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    restriction = el.getElementsByTagName(\"owl:Restriction\")\n",
    "                    if not restriction:\n",
    "                        continue\n",
    "                    prop = self.get_child_node(restriction[0], \"owl:onProperty\")\n",
    "                    some_vals = self.get_child_node(restriction[0], \"owl:someValuesFrom\")\n",
    "                    \n",
    "                    if not prop or not some_vals:\n",
    "                        continue\n",
    "#                     print(self.extract_ID(el), \"**\", self.extract_ID(some_vals[0]), \"**\", self.extract_ID(prop[0]))\n",
    "                    subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop[0])))\n",
    "                else:\n",
    "                    if self.extract_ID(level1_class[0]):\n",
    "                        subclass_pairs.append((level1_class[0], el.parentNode, \"subclass_of\"))\n",
    "                    else:\n",
    "                        level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                        subclass_pairs.extend([(elem, el.parentNode, \"subclass_of\") for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        subclasses = [(self.extract_ID(a), self.extract_ID(b), c) for (a,b,c) in self.subclasses]\n",
    "        return [el for el in subclasses if el[0] and el[1] and el[2] and el[0]!=\"Thing\" and el[1]!=\"Thing\"]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element, check_coded = True):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        element_id = element_id.split(\"#\")[-1]\n",
    "        if len(list(filter(str.isdigit, element_id))) >= 3 and \"_\" in element_id and check_coded:\n",
    "            return self.mapping_dict[element_id]\n",
    "        return element_id\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_pos))\n",
    "        num_batches = int(ceil(len(inputs_pos)/batch_size))\n",
    "        batch_size_f = int(ceil(len(inputs_neg)/num_batches))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "            \n",
    "            inp = inputs.transpose(1,0,2)\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inp[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inp[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "            \n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            print (step, threshold, exception)\n",
    "            threshold += step \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        if optimum_metrics[2] != -1000:\n",
    "            all_metrics.append((opt_threshold, optimum_metrics))\n",
    "    return all_results\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "def normalize(inp):\n",
    "    return inp/torch.norm(inp, dim=-1)[:, None]\n",
    "context = None\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(1024, 300)\n",
    "        n = int(23)\n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(n-1) for i in range(n-1)]))\n",
    " \n",
    "    def forward(self, inputs):\n",
    "        results = []\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            node = x.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = x.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            att_weights = torch.bmm(neighbours, node.permute(0, 2, 1)).squeeze()\n",
    "            att_weights = masked_softmax(att_weights).unsqueeze(-1)\n",
    "            context = torch.matmul(self.v, att_weights * neighbours)\n",
    "\n",
    "            x = torch.cat((node.reshape(-1, self.embedding_dim), context.reshape(-1, self.embedding_dim)), dim=1)\n",
    "            x = self.output(x)\n",
    "            results.append(x)\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    ont_obj = Ontology(\"conference_ontologies/\" + ont + \".owl\")\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c) in triples]\n",
    "    neighbours_dict = {elem: [elem] for elem in list(set(flatten(entities)))}\n",
    "    for e1, e2 in entities:\n",
    "        neighbours_dict[e1].append(e2)\n",
    "        neighbours_dict[e2].append(e1)\n",
    "    \n",
    "    prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "    neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "    for e1, e2, p in prop_triples:\n",
    "        neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "    neighbours_dict = {el: neighbours_dict[el][:1] + sorted(list(set(neighbours_dict[el][1:])))\n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: neighbours_dict[el][:23] for el in neighbours_dict if len( neighbours_dict[el]) > 2}\n",
    "    neighbours_dict = {ont + \"#\" + el: [ont + \"#\" + e for e in neighbours_dict[el]] for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    op = np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets = [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "neighbours_dicts = {ont: get_one_hop_neighbours(ont) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "max_neighbours = np.max(flatten([[len(el[e]) for e in el] for el in neighbours_dicts.values()]))\n",
    "neighbours_lens = {ont: {key: len(neighbours_dicts[ont][key]) for key in neighbours_dicts[ont]}\n",
    "                   for ont in neighbours_dicts}\n",
    "neighbours_dicts = {ont: {key: neighbours_dicts[ont][key] + [\"<UNK>\" for i in range(max_neighbours -len(neighbours_dicts[ont][key]))]\n",
    "              for key in neighbours_dicts[ont]} for ont in neighbours_dicts}\n",
    "\n",
    "print(\"Number of neighbours: \" + str(sys.argv[1]))\n",
    "\n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    \n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    np.random.shuffle(train_data_t)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 50\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "        batch_size = min(batch_size, len(inputs_pos))\n",
    "        num_batches = int(ceil(len(inputs_pos)/batch_size))\n",
    "        batch_size_f = int(ceil(len(inputs_neg)/num_batches))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "            \n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inp_elems)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%10 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), \"/u/vivek98/attention.pt\")\n",
    "    \n",
    "    test_data_t = [key for key in test_data if test_data[key]]\n",
    "    test_data_f = [key for key in test_data if not test_data[key]]\n",
    "    \n",
    "    res = greedy_matching()\n",
    "    f1 = open(\"test_results.pkl\", \"wb\")\n",
    "    pickle.dump(res, f1)\n",
    "print (\"Final Results: \" + str(np.mean([el[1] for el in all_metrics], axis=0)))\n",
    "print (\"Best threshold: \" + str(all_metrics[np.argmax([el[1][2] for el in all_metrics])][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
