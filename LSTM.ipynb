{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"main.py\", \"7\", \"1\", \"data_path.pkl\", \"test_sample.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 122893\n",
      "Training size: 109284 Testing size: 13609\n",
      "Epoch: 0 Idx: 0 Loss: 0.19854786540561395\n",
      "Epoch: 0 Idx: 1000 Loss: 0.030676091880086714\n",
      "Epoch: 0 Idx: 2000 Loss: 0.029171343105992353\n",
      "Epoch: 0 Idx: 3000 Loss: 0.017704499908914206\n",
      "Epoch: 0 Idx: 4000 Loss: 0.01671864911086087\n",
      "Epoch: 0 Idx: 5000 Loss: 0.03543994034171454\n",
      "Epoch: 0 Idx: 6000 Loss: 0.01215168470953693\n",
      "Epoch: 0 Idx: 7000 Loss: 0.07749535322263937\n",
      "Epoch: 0 Idx: 8000 Loss: 0.01375119223244763\n",
      "Epoch: 0 Idx: 9000 Loss: 0.011168357302602013\n",
      "Epoch: 0 Idx: 10000 Loss: 0.01985825307481081\n",
      "Epoch: 0 Idx: 11000 Loss: 0.01836500510383604\n",
      "Epoch: 1 Idx: 0 Loss: 0.021306217898562414\n",
      "Epoch: 1 Idx: 1000 Loss: 0.018781362873355197\n",
      "Epoch: 1 Idx: 2000 Loss: 0.007590011350681556\n",
      "Epoch: 1 Idx: 3000 Loss: 0.03218291556630628\n",
      "Epoch: 1 Idx: 4000 Loss: 0.029352202185310046\n",
      "Epoch: 1 Idx: 5000 Loss: 0.0049155123950800255\n",
      "Epoch: 1 Idx: 6000 Loss: 0.044291209383165143\n",
      "Epoch: 1 Idx: 7000 Loss: 0.010811684427355667\n",
      "Epoch: 1 Idx: 8000 Loss: 0.008603193424018217\n",
      "Epoch: 1 Idx: 9000 Loss: 0.02137808604505335\n",
      "Epoch: 1 Idx: 10000 Loss: 0.016220518407453227\n",
      "Epoch: 1 Idx: 11000 Loss: 0.01981950726581852\n",
      "Epoch: 2 Idx: 0 Loss: 0.027400788952606447\n",
      "Epoch: 2 Idx: 1000 Loss: 0.020406023331552614\n",
      "Epoch: 2 Idx: 2000 Loss: 0.027749167318772444\n",
      "Epoch: 2 Idx: 3000 Loss: 0.008519030356722709\n",
      "Epoch: 2 Idx: 4000 Loss: 0.03972198389946601\n",
      "Epoch: 2 Idx: 5000 Loss: 0.019377862633303475\n",
      "Epoch: 2 Idx: 6000 Loss: 0.007449512550464249\n",
      "Epoch: 2 Idx: 7000 Loss: 0.037955896805000375\n",
      "Epoch: 2 Idx: 8000 Loss: 0.060967006658356415\n",
      "Epoch: 2 Idx: 9000 Loss: 0.008287204832923212\n",
      "Epoch: 2 Idx: 10000 Loss: 0.006505418328126719\n",
      "Epoch: 2 Idx: 11000 Loss: 0.049709369438291226\n",
      "Epoch: 3 Idx: 0 Loss: 0.002915428355142424\n",
      "Epoch: 3 Idx: 1000 Loss: 0.01233717138404965\n",
      "Epoch: 3 Idx: 2000 Loss: 0.0035284471726882193\n",
      "Epoch: 3 Idx: 3000 Loss: 0.003906024394029984\n",
      "Epoch: 3 Idx: 4000 Loss: 0.005302655660359566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-ff121be2f83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_elems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[3], \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment = pickle.load(f)\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_pos))\n",
    "        num_batches = int(ceil(len(inputs_pos)/batch_size))\n",
    "        batch_size_f = int(ceil(len(inputs_neg)/num_batches))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "            \n",
    "            inputs = inputs.transpose(1,0,2)\n",
    "            inputs_elem = inputs.copy()\n",
    "            \n",
    "            nonzero_elems = np.count_nonzero(inputs, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            inputs = np.stack((inputs[0][[indices[0]]], inputs[1][[indices[1]]]), axis=0)\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inputs_elem.shape[1])], [d2[k] for k in range(inputs_elem.shape[1])]))\n",
    "            \n",
    "            rev_indices = torch.LongTensor(rev_indices.T).to(device)\n",
    "            inp_elems = torch.LongTensor(inputs.transpose(1,0,2)).to(device)\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy().T).to(device)\n",
    "\n",
    "            outputs = model(inp_elems, seq_lens, rev_indices)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inputs_elem[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inputs_elem[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        opt_fn, opt_fp = [], []\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [(key, all_results[key][0]) for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "                opt_fn = fn_list\n",
    "                opt_fp = fp_list\n",
    "            \n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            print (step, threshold, exception)\n",
    "            threshold += step \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        all_fn.extend(opt_fn)\n",
    "        all_fp.extend(opt_fp)\n",
    "        if optimum_metrics[2] != -1000:\n",
    "            all_metrics.append((opt_threshold, optimum_metrics))\n",
    "    return all_results\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, num_directions):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim, \n",
    "                            self.hidden_dim, \n",
    "                            self.num_layers, \n",
    "                            bidirectional=True if self.num_directions == 2 else False,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.bilstm_flatten = nn.Linear(2, 1)\n",
    "        self.output = nn.Linear(self.embedding_dim + self.hidden_dim, 300)\n",
    " \n",
    "    def forward(self, inputs, seq_lens, rev_indices):\n",
    "        results = []\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "        seq_lens, rev_indices = seq_lens.T, rev_indices.T\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            node = x.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = x.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            packed_inp = pack_padded_sequence(neighbours, seq_lens[i].cpu().numpy(), batch_first=True)\n",
    "            op, (ht, ct) = self.lstm(packed_inp)\n",
    "            context = ht[2*(self.num_layers-1):].permute(1,2,0)\n",
    "            context = self.bilstm_flatten(context[rev_indices[i],:,:])\n",
    "            context = context.reshape(-1, self.hidden_dim * self.num_layers)\n",
    "            \n",
    "            x = torch.cat((node.reshape(-1, self.embedding_dim), context), dim=1)\n",
    "            x = self.output(x)\n",
    "            results.append(x)\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "        return x\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    op = np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets = [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "# print(\"Number of neighbours: \" + str(sys.argv[1]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:int(sys.argv[1])] for el in neighbours_dicts[ont]\n",
    "       if count_non_unk(neighbours_dicts[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts}\n",
    "\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:1]\n",
    "                       + [elem for elem in neighbours_dicts[ont][el][1:] if elem!=\"<UNK>\"][::-1]\n",
    "                       + [elem for elem in neighbours_dicts[ont][el][1:] if elem==\"<UNK>\"]\n",
    "                       for el in neighbours_dicts[ont]} \n",
    "                     for ont in neighbours_dicts}\n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i in list(range(0, len(ontologies_in_alignment)-1, 3)):\n",
    "    \n",
    "    test_onto = ontologies_in_alignment[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Testing size:\", len(test_data))\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 50\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork(512, 1, 2).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            inp = inputs.transpose(1,0,2)\n",
    "            nonzero_elems = np.count_nonzero(inp, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = seq_lens - 1\n",
    "\n",
    "            inp_elems = np.stack((inp[0][[indices[0]]], inp[1][[indices[1]]]), axis=0).transpose(1,0,2)\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inp.shape[1])], \n",
    "                                    [d2[k] for k in range(inp.shape[1])]))\n",
    "\n",
    "            rev_indices = torch.LongTensor(rev_indices.T).to(device)\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy().T).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inp_elems, seq_lens, rev_indices)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%1000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    test_data_t = [key for key in test_data if test_data[key]]\n",
    "    test_data_f = [key for key in test_data if not test_data[key]]\n",
    "    \n",
    "    greedy_matching()\n",
    "f1 = open(sys.argv[4], \"wb\")\n",
    "pickle.dump([all_fn, all_fp], f1)\n",
    "print (\"Final Results: \" + str(np.mean([el[1] for el in all_metrics], axis=0)))\n",
    "print (\"Best threshold: \" + str(all_metrics[np.argmax([el[1][2] for el in all_metrics])][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_indexer[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 2, 2, 1, 1, 1, 1, 1, 1],\n",
       "       [3, 2, 2, 2, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[484, 140,   0,   0,   0,   0,   0],\n",
       "        [ 24,  34,   0,   0,   0,   0,   0],\n",
       "        [812,  34,   0,   0,   0,   0,   0],\n",
       "        [ 74, 356,   0,   0,   0,   0,   0],\n",
       "        [ 39, 186,   0,   0,   0,   0,   0],\n",
       "        [  9, 211, 245,   0,   0,   0,   0],\n",
       "        [ 50,  35,  80, 707,   0,   0,   0],\n",
       "        [686,  26,   0,   0,   0,   0,   0],\n",
       "        [487, 728,  29,   0,   0,   0,   0],\n",
       "        [390, 795, 765,   0,   0,   0,   0]],\n",
       "\n",
       "       [[590, 477, 473,   0,   0,   0,   0],\n",
       "        [369, 728,   0,   0,   0,   0,   0],\n",
       "        [ 29, 728,   0,   0,   0,   0,   0],\n",
       "        [715,  34,   0,   0,   0,   0,   0],\n",
       "        [193, 511,   0,   0,   0,   0,   0],\n",
       "        [ 65, 102,   0,   0,   0,   0,   0],\n",
       "        [ 99, 276, 455, 343,   0,   0,   0],\n",
       "        [ 92, 833,   0,   0,   0,   0,   0],\n",
       "        [662, 276, 455,   0,   0,   0,   0],\n",
       "        [648, 477, 473,   0,   0,   0,   0]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'self,\\xc2\\xa0hidden_dim, num_layers, num_directions'\n"
     ]
    }
   ],
   "source": [
    "print (\"self, hidden_dim, num_layers, num_directions\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cmt#email', 'conference#has_an_email'),\n",
       " ('conference#has_a_track-workshop-tutorial_topic', 'confOf#hasTopic'),\n",
       " ('conference#has_the_first_name', 'confOf#hasFirstName'),\n",
       " ('cmt#hasSubjectArea', 'confOf#dealsWith'),\n",
       " ('cmt#email', 'confOf#hasEmail'),\n",
       " ('conference#has_an_email', 'sigkdd#E-mail'),\n",
       " ('cmt#email', 'sigkdd#E-mail'),\n",
       " ('conference#has_an_email', 'confOf#hasEmail'),\n",
       " ('cmt#title', 'confOf#hasTitle'),\n",
       " ('conference#has_the_last_name', 'confOf#hasSurname'),\n",
       " ('conference#contributes', 'iasted#write')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([tuple(a) for a in final]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
