{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle, sys\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"reference-alignment/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "        \n",
    "reference_alignments = load_alignments(alignment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(b,a,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, include_inv=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  834\n"
     ]
    }
   ],
   "source": [
    "# Extracting USE embeddings\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0).lower() for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples())))\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "\n",
    "inp = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+\")\n",
    "X = vectorizer.fit_transform(inp)\n",
    "word2idx_tfidf = {word: i for (i, word)  in enumerate(vectorizer.get_feature_names())}\n",
    "entity2idx_tfidf = {word.split(\"#\")[1]: i for (i, word)  in enumerate(extracted_elems)}\n",
    "\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "inp = [\"<UNK>\"] + inp\n",
    "extracted_elems = [\"<UNK>\"] + extracted_elems\n",
    "\n",
    "embeds = extractUSEEmbeddings(inp)\n",
    "embeddings = dict(zip(extracted_elems, embeds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type storage\n",
    "\n",
    "types_dict = {}\n",
    "\n",
    "def get_tfidf_score(word, phrase):\n",
    "    return np.sum([X[entity2idx_tfidf[phrase]][:,word2idx_tfidf[word]][0,0] for word in parse(phrase)])\n",
    "    \n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    \n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "\n",
    "    for entity in entities:\n",
    "        types_dict[entity] = {\"type\": \"entity\"}\n",
    "    for prop in props:\n",
    "        types_dict[prop] = {\"type\": \"property\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    all_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in mappings])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "\n",
    "data = {}\n",
    "for mapping in all_mappings:\n",
    "    if mapping in gt_mappings:\n",
    "        data[(mapping[0], mapping[1])] = True\n",
    "    else:\n",
    "        data[(mapping[0], mapping[1])] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics\n",
    "    all_results = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        batch_size = min(batch_size, len(test_data_t))\n",
    "        num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "        batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "            \n",
    "            pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "            neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])     \n",
    "            targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "            \n",
    "            indices = np.random.permutation(inputs.shape[0])\n",
    "            inputs, targets = inputs[indices].transpose(1,0,2), targets[indices]\n",
    "            inputs = inputs.transpose(1,0,2)\n",
    "            inputs_elem = inputs.copy()\n",
    "            \n",
    "            nonzero_elems = np.count_nonzero(inputs, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            inputs = np.stack((inputs[0][[indices[0]]], inputs[1][[indices[1]]]), axis=0)\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inputs_elem.shape[1])], [d2[k] for k in range(inputs_elem.shape[1])]))\n",
    "\n",
    "            rev_indices = torch.LongTensor(rev_indices)\n",
    "            inputs = torch.LongTensor(inputs)\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy())\n",
    "            targets = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inputs, seq_lens, rev_indices)\n",
    "            #             outputs /= torch.sum(outputs, dim=1).view(-1, 1)\n",
    "# #             write ((\"Outputs Finally: \", str([str(s) for s in outputs])))\n",
    "#             outputs = [(1-el[1].item()) for el in outputs]\n",
    "\n",
    "#             return\n",
    "#             print (\"2\", inputs)\n",
    "#             print (\"3\", seq_lens)\n",
    "#             print (\"4\", rev_indices)\n",
    "            \n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "#             print (inputs)\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inputs_elem[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inputs_elem[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        all_results = OrderedDict(sorted(all_results.items(), key=lambda x: x[0], reverse=True))\n",
    "        filtered_results = dict()\n",
    "        \n",
    "        entities_to_assign = set([el[0] for el in list(all_results.keys())])\n",
    "        for pair in all_results:\n",
    "            if pair[0] in entities_to_assign:\n",
    "                filtered_results[pair] = all_results[pair]\n",
    "                entities_to_assign.remove(pair[0])\n",
    "                \n",
    "        entities_to_assign = set([el[1] for el in list(all_results.keys())])\n",
    "        for pair in all_results:\n",
    "            if pair[1] in entities_to_assign:\n",
    "                filtered_results[pair] = all_results[pair]\n",
    "                entities_to_assign.remove(pair[1])        \n",
    "\n",
    "        filtered_results = OrderedDict(sorted(filtered_results.items(), key=lambda x: x[1][0], reverse=True))\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.01\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.01\n",
    "        for j,threshold in enumerate(np.arange(low_threshold, high_threshold, 0.01)):\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "        \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        all_metrics.append((opt_threshold, optimum_metrics))\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cmt#acceptPaper', 'edas#AcceptRating', 'iasted#Author', 'edas#MealEvent', 'confOf#Administrator', 'cmt#SubjectArea', 'edas#ReviewRating', 'cmt#setMaxPapers', 'conference#Publisher', 'iasted#is_used_by', 'iasted#need', 'iasted#Speaker_lecture', 'cmt#addProgramCommitteeMember', 'sigkdd#Abstract', 'confOf#Paper', 'confOf#Member', 'confOf#hasFirstName', 'cmt#rejectPaper', 'confOf#hasPhone', 'edas#string', 'iasted#Departure_tax', 'conference#has_an_organizing_committee', 'cmt#Meta-Review', 'iasted#Listener', 'conference#has_a_commtitee', 'iasted#prepare', 'confOf#Workshop', 'cmt#readPaper', 'edas#DiningPlace', 'cmt#ConferenceChair', 'cmt#acceptsHardcopySubmissions', 'conference#is_a_date_of_camera_ready_paper_submission', 'edas#isReviewHistoryOf', 'edas#hasCostCurrency', 'ekaw#University', 'iasted#Form', 'edas#ReviewForm', 'confOf#maxChoice', 'ekaw#Individual_Presentation', 'sigkdd#Best_Applications_Paper_Award', 'conference#Call_for_paper', 'iasted#Presenter_house', 'confOf#hasCountry', 'edas#endDate', 'sigkdd#End_of_conference', 'confOf#hasKeyword', 'iasted#Session_room', 'confOf#Company', 'confOf#hasSurname', 'confOf#location', 'edas#ConferenceEvent', 'edas#ComputerNetworksManagementTopic', 'sigkdd#Program_Committee', 'iasted#go_through', 'iasted#Lecture', 'ekaw#organises', 'sigkdd#Bronze_Supporter', 'ekaw#PC_Member', 'iasted#Money', 'edas#MedicineTopic', 'confOf#hasTopic', 'sigkdd#Name_of_conference', 'conference#is_part_of_conference_volumes', 'sigkdd#Start_of_conference', 'sigkdd#dateTime', 'edas#hasFirstName', 'sigkdd#award', 'ekaw#Demo_Session', 'ekaw#OC_Member', 'iasted#Technical_commitee', 'conference#Program_committee', 'sigkdd#Author_of_paper', 'cmt#PaperFullVersion', 'confOf#writes', 'iasted#Submission', 'conference#Conference_part', 'sigkdd#E-mail', 'edas#SocialEvent', 'ekaw#Scientific_Event', 'ekaw#Evaluated_Paper', 'iasted#Social_program', 'iasted#Worker_lecturer', 'edas#ComputerNetworksTopic', 'edas#isTopicOf', 'iasted#Activity_before_conference', 'sigkdd#can_stay_in', 'cmt#hasBid', 'conference#Written_contribution', 'conference#Invited_talk', 'conference#date', 'iasted#sign', 'confOf#Topic', 'iasted#Camera_ready_manuscript_deadline', 'edas#Sponsorship', 'conference#gives_presentations', 'confOf#hasPostalCode', 'ekaw#Conference_Proceedings', 'iasted#Trip_city', 'edas#WelcomeTalk', 'cmt#readByReviewer', 'cmt#Thing', 'cmt#Review', 'confOf#employedBy', 'cmt#Document', 'conference#was_a_committee_of', 'iasted#Fee_for_extra_trip', 'sigkdd#Program_Committee_member', 'ekaw#Industrial_Session', 'conference#reviews', 'cmt#Person', 'edas#hasCall', 'iasted#is_present', 'iasted#Conference_restaurant', 'edas#isReviewedBy', 'edas#Paper', 'edas#PersonalHistory', 'edas#ConferenceSession', 'conference#Tutorial', 'iasted#Sponsor_company_house', 'ekaw#Conference', 'ekaw#Research_Institute', 'confOf#Submission_event', 'ekaw#listsEvent', 'ekaw#Student', 'iasted#Introduction_of_speaker', 'conference#has_a_track-workshop-tutorial_topic', 'sigkdd#Deadline_Abstract_Submission', 'ekaw#OC_Chair', 'confOf#Social_event', 'iasted#is_paid_by', 'edas#TalkEvent', 'edas#Conference', 'ekaw#Presenter', 'cmt#endReview', 'iasted#is_made_from', 'cmt#Conference', 'edas#hasCity', 'sigkdd#holded_by', 'ekaw#Tutorial_Chair', 'ekaw#Contributed_Talk', 'iasted#Session_chair', 'conference#is_a_topis_of_conference_parts', 'ekaw#Conference_Paper', 'conference#Conference_contribution', 'ekaw#hasPart', 'ekaw#technicallyOrganises', 'cmt#ExternalReviewer', 'ekaw#Demo_Paper', 'sigkdd#string', 'cmt#hasDecision', 'edas#startDate', 'edas#OperatingTopicsystems', 'cmt#co-writePaper', 'conference#Review_preference', 'conference#Conference_www', 'iasted#Simulating', 'ekaw#subclass_of', 'edas#MobileComputingTopic', 'iasted#Author_book_proceedings_included', 'edas#PersonalReviewHistory', 'edas#Place', 'ekaw#Organisation', 'confOf#reviewes', 'sigkdd#Document', 'sigkdd#Registration_SIGMOD_Member', 'cmt#email', 'conference#Camera_ready_contribution', 'edas#isMenuOf', 'ekaw#Abstract', 'conference#has_a_program_committee', 'edas#TwoLevelConference', 'confOf#string', 'cmt#logoURL', 'cmt#readByMeta-Reviewer', 'conference#Committee_member', 'sigkdd#Gold_Supporter', 'sigkdd#Best_Research_Paper_Award', 'iasted#Conference_hotel', 'cmt#User', 'iasted#Hotel_presenter', 'edas#AcceptedPaper', 'iasted#give', 'conference#Paper', 'edas#GovernmentOrganization', 'conference#has_a_name', 'ekaw#eventOnList', 'iasted#Research', 'cmt#assignExternalReviewer', 'iasted#Sponsor', 'edas#hasPhone', 'cmt#ProgramCommitteeChair', 'cmt#enableVirtualMeeting', 'conference#Organization', 'sigkdd#Registration_SIGKDD_Member', 'ekaw#locationOf', 'iasted#Book_proceeding', 'conference#Organizing_committee', 'confOf#Student', 'ekaw#Submitted_Paper', 'ekaw#reviewOfPaper', 'conference#Invited_speaker', 'iasted#Nonmember_registration_fee', 'conference#has_a_review_reference_or_expertise', 'iasted#is_dated_on', 'conference#Active_conference_participant', 'edas#RadioCommunicationsTopic', 'conference#invites_co-reviewers', 'ekaw#publisherOf', 'ekaw#referencedIn', 'ekaw#SC_Member', 'edas#paperDueOn', 'iasted#Technic_activity', 'edas#CoffeeBreak', 'edas#PersonalPublicationHistory', 'conference#has_a_topic_or_a_submission_contribution', 'cmt#paperAssignmentFinalizedBy', 'cmt#memberOfConference', 'iasted#is_used_for', 'conference#has_a_steering_committee', 'cmt#writePaper', 'sigkdd#ACM_SIGKDD', 'conference#Conference_contributor', 'iasted#Tax', 'edas#Country', 'iasted#Single_hotel_room', 'sigkdd#Registration_fee', 'conference#Co-chair', 'edas#attendeeAt', 'cmt#hasSubjectArea', 'cmt#hasCo-author', 'iasted#IASTED_non_member', 'conference#Conference_applicant', 'edas#hasMenu', 'ekaw#Conference_Trip', 'conference#was_a_track-workshop_chair_of', 'iasted#is_occupied_by', 'cmt#assignedTo', 'edas#IndustryOrganization', 'confOf#Administrative_event', 'edas#hasPostalCode', 'edas#ComputerNetworksEnterpriseTopic', 'cmt#finalizePaperAssignment', 'iasted#obtain', 'iasted#Registation_deadline', 'cmt#hasConflictOfInterest', 'edas#ComputerNetworksSwitchingTopic', 'iasted#Worker_non_speaker', 'conference#has_authors', 'ekaw#writtenBy', 'edas#Organization', 'cmt#adjustedBy', 'cmt#AssociatedChair', 'edas#hasSubmissionInstructions', 'ekaw#Programme_Brochure', 'conference#was_an_organizing_committee_of', 'edas#CommunicationTheoryTopic', 'edas#hasCostAmount', 'ekaw#reviewerOfPaper', 'sigkdd#Webmaster', 'conference#Conference_fees', 'confOf#Reviewing_results_event', 'ekaw#Proceedings_Publisher', 'ekaw#Industrial_Paper', 'conference#Conference_document', 'conference#Thing', 'confOf#hasEmail', 'ekaw#references', 'edas#forEvent', 'confOf#Registration_of_participants_event', 'iasted#Submissions_deadline', 'iasted#is_given_to', 'ekaw#Conference_Session', 'edas#PerformanceTopic', 'sigkdd#Currency', 'edas#BreakEvent', 'ekaw#paperPresentedAs', 'edas#CallForReviews', 'edas#hasReviewHistory', 'iasted#is_writen_by', 'iasted#Car', 'confOf#hasHomePage', 'sigkdd#Deadline', 'edas#ParallelAndDistributedComputingTopic', 'sigkdd#Conference_hall', 'conference#has_members', 'sigkdd#Silver_Supporter', 'sigkdd#Date', 'iasted#Registration_form', 'conference#Registeered_applicant', 'conference#has_an_abstract', 'conference#issues', 'iasted#Conference_state', 'ekaw#Accepted_Paper', 'edas#AcademiaOrganization', 'cmt#unsignedLong', 'confOf#Event', 'edas#ComputerNetworksSensorTopic', 'edas#Presenter', 'confOf#writtenBy', 'ekaw#volumeContainsPaper', 'cmt#int', 'iasted#is_situated_in', 'confOf#City', 'edas#RejectRating', 'edas#CommunicationsTopic', 'ekaw#Agency_Staff_Member', 'conference#has_gender', 'iasted#Item', 'conference#Paid_applicant', 'cmt#Preference', 'conference#was_a_steering_committee_of', 'conference#int', 'confOf#positiveInteger', 'iasted#Memeber_registration_fee', 'iasted#Main_office', 'iasted#Session', 'cmt#submitPaper', 'confOf#Organization', 'edas#CryptographyTopic', 'confOf#Conference', 'iasted#Deadline_hotel_reservation', 'iasted#Author_attendee_cd_registration_fee', 'iasted#is_sent_after', 'iasted#Deadline_for_notification_of_acceptance', 'edas#ComputerNetworksOpticalTopic', 'iasted#Presentation', 'confOf#Scholar', 'iasted#Refusing_manuscript', 'edas#PendingPaper', 'ekaw#paperInVolume', 'edas#Reviewer', 'ekaw#Session', 'iasted#Plenary_lecture_speaker', 'iasted#is_present_in', 'cmt#virtualMeetingEnabledBy', 'ekaw#Early-Registered_Participant', 'conference#Topic', 'cmt#string', 'conference#Poster', 'edas#isWrittenBy', 'sigkdd#Deadline_Paper_Submission', 'iasted#One_day_presenter', 'conference#has_a_review_expertise', 'ekaw#Event', 'confOf#hasFax', 'sigkdd#Paper', 'edas#MealBreak', 'iasted#Viza', 'cmt#hardcopyMailingManifestsPrintedBy', 'iasted#is_given_by', 'cmt#assignedByReviewer', 'sigkdd#Author_of_paper_student', 'iasted#Speaker', 'iasted#Computer', 'edas#TPCMember', 'iasted#Student_non_speaker', 'iasted#is_paid_with', 'conference#has_a_submitted_contribution', 'ekaw#Session_Chair', 'iasted#Initial_manuscipt', 'edas#PowerlineTransmissionTopic', 'cmt#siteURL', 'conference#has_the_last_name', 'edas#ClosingTalk', 'conference#belong_to_a_conference_volume', 'sigkdd#Program_Chair', 'cmt#paperID', 'edas#Attendee', 'edas#Review', 'edas#MeetingRoomPlace', 'iasted#is_needed_for', 'cmt#Decision', 'iasted#is_paid_in', 'confOf#hasCity', 'iasted#Student_lecturer', 'edas#MicroelectronicsTopic', 'ekaw#organisedBy', 'cmt#reviewsPerPaper', 'sigkdd#Organizator', 'ekaw#Neutral_Review', 'iasted#Record_of_attendance', 'iasted#Invitation_letter', 'iasted#Person', 'sigkdd#Committee', 'edas#registrationDueOn', 'confOf#boolean', 'sigkdd#awarded_by', 'ekaw#Conference_Banquet', 'iasted#Registration', 'iasted#Conference_city', 'confOf#Member_PC', 'cmt#rejectedBy', 'sigkdd#notification_until', 'edas#ComputerNetworksAapplicationsTopic', 'confOf#subclass_of', 'ekaw#Poster_Session', 'conference#Submitted_contribution', 'edas#ActivePaper', 'cmt#Co-author', 'cmt#AuthorNotReviewer', 'iasted#Welcome_address', 'edas#belongsToEvent', 'edas#relatedToEvent', 'iasted#Delegate', 'confOf#starts_on', 'iasted#PowerPoint_presentation', 'iasted#Author_attendee_book_registration_fee', 'ekaw#updatedVersionOf', 'conference#is_a_full_paper_submission_date', 'conference#Conference', 'iasted#Student_registration_fee', 'cmt#writtenBy', 'sigkdd#Best_Student_Paper_Supporter', 'conference#Call_for_participation', 'edas#hasLocation', 'ekaw#Organising_Agency', 'edas#NumericalReviewQuestion', 'conference#has_a_committee_co-chair', 'confOf#contactEmail', 'sigkdd#Registration_Student', 'edas#SlideSet', 'conference#Organizer', 'confOf#Thing', 'sigkdd#Platinum_Supporter', 'edas#AcademicEvent', 'iasted#Sponsor_state', 'iasted#Final_manuscript', 'iasted#done_till', 'iasted#Review', 'iasted#is_visited_by', 'conference#Passive_conference_participant', 'edas#CallForPapers', 'ekaw#presentationOfPaper', 'confOf#Author', 'iasted#is_equipped_by', 'iasted#Taxi', 'iasted#Bank_transfer', 'edas#hasSubmissionDeadline', 'sigkdd#Conference', 'ekaw#hasEvent', 'iasted#dateTime', 'edas#TestOnlyTopic', 'ekaw#Review', 'iasted#Conference_activity', 'ekaw#Social_Event', 'edas#AccpetIfRoomRating', 'cmt#printHardcopyMailingManifests', 'confOf#Assistant', 'iasted#Accepting_manuscript', 'ekaw#hasReviewer', 'iasted#Presenter_city', 'conference#Chair', 'conference#was_a_committee_chair_of', 'confOf#expertOn', 'conference#has_a_review', 'cmt#PaperAbstract', 'ekaw#Workshop_Chair', 'conference#Contribution_co-author', 'iasted#is_sent_by', 'confOf#hasTitle', 'edas#ConferenceChair', 'edas#ReviewQuestion', 'ekaw#Track', 'cmt#detailsEnteredBy', 'iasted#Conference_hall', 'ekaw#coversTopic', 'cmt#enterConferenceDetails', 'conference#Accepted_contribution', 'conference#Contribution_1th-author', 'edas#hasRelatedPaper', 'cmt#ProgramCommittee', 'iasted#Van', 'iasted#Thing', 'cmt#Paper', 'sigkdd#Invited_Speaker', 'cmt#runPaperAssignmentTools', 'conference#Late_paid_applicant', 'iasted#Publication', 'iasted#Tip', 'cmt#Reviewer', 'ekaw#Regular_Session', 'confOf#hasVAT', 'edas#ConferenceDinner', 'confOf#remark', 'confOf#Person', 'sigkdd#Name_of_sponsor', 'edas#SecurityTopic', 'ekaw#Workshop_Session', 'iasted#Activity_after_conference', 'iasted#Deadline', 'ekaw#Late-Registered_Participant', 'iasted#is_sent_before', 'cmt#acceptedBy', 'iasted#date', 'ekaw#hasUpdatedVersion', 'ekaw#technicallyOrganisedBy', 'sigkdd#Organizing_Committee_member', 'cmt#Chairman', 'edas#TextualReviewQuestion', 'confOf#has_short_title', 'confOf#Short_paper', 'sigkdd#Best_Paper_Awards_Committee', 'edas#relatedToPaper', 'ekaw#Multi-author_Volume', 'iasted#Modelling', 'edas#ConferenceVenuePlace', 'iasted#Hotel_room', 'iasted#Transport_vehicle', 'conference#Conference_participant', 'ekaw#partOf', 'confOf#Banquet', 'iasted#Dinner_banquet', 'iasted#Payment_document', 'edas#ComputerNetworksMeasurementsTopic', 'iasted#write', 'edas#hasRelatedDocument', 'edas#hasStartDateTime', 'edas#Programme', 'conference#Steering_committee', 'edas#hasLastName', 'cmt#reviewCriteriaEnteredBy', 'conference#Review_expertise', 'ekaw#Proceedings', 'edas#TravelGrant', 'conference#Person', 'conference#is_an_abstract_submission_date', 'iasted#Shuttle_bus', 'conference#has_parts', 'iasted#Card', 'sigkdd#Person', 'confOf#Country', 'edas#WirelessCommunicationsTopic', 'edas#MultimediaTopic', 'sigkdd#City_of_conference', 'iasted#Video_presentation', 'cmt#hasAuthor', 'sigkdd#Fee', 'iasted#Author_information_form', 'conference#invited_by', 'edas#hasEndDateTime', 'iasted#Transparency', 'edas#hasAttendee', 'conference#has_a_publisher', 'iasted#occupy', 'iasted#Author_cd_proceedings_included', 'sigkdd#obtain', 'iasted#Tutorial', 'confOf#Reception', 'edas#Person', 'iasted#Departure', 'ekaw#Poster_Paper', 'sigkdd#Price', 'iasted#Coctail_reception', 'iasted#speak_in', 'edas#int', 'conference#is_given_by', 'conference#was_a_program_committee_of', 'sigkdd#Listener', 'sigkdd#searched_by', 'confOf#studyAt', 'ekaw#Person', 'ekaw#hasReview', 'ekaw#Rejected_Paper', 'iasted#Time', 'ekaw#Negative_Review', 'conference#is_a_starting_date', 'confOf#University', 'conference#subclass_of', 'edas#hasStreet', 'edas#hasEmail', 'sigkdd#Sponzor_fee', 'edas#WeekRejectRating', 'iasted#is_paid_for', 'ekaw#Paper', 'ekaw#PC_Chair', 'conference#Committee', 'iasted#is_held_in', 'edas#MealMenu', 'confOf#defaultChoice', 'edas#SatelliteAndSpaceCommunicationsTopic', 'edas#hasTopic', 'edas#initiates', 'conference#Conference_volume', 'iasted#Brief_introduction_for_Session_chair', 'sigkdd#Sponzor', 'ekaw#Workshop_Paper', 'conference#is_the_1th_part_of', 'conference#has_a_date_of_issue', 'edas#Reception', 'ekaw#Location', 'iasted#Mailing_list', 'iasted#Presenter_university', 'edas#AntennasTopic', 'ekaw#Camera_Ready_Paper', 'sigkdd#Deadline_Author_notification', 'ekaw#heldIn', 'iasted#Video_cassette_player', 'sigkdd#Author', 'iasted#Cheque', 'conference#Important_dates', 'iasted#Place', 'conference#has_an_ISBN', 'iasted#Conference_days', 'conference#contributes', 'edas#isProviderOf', 'sigkdd#Award', 'sigkdd#Name', 'ekaw#Positive_Review', 'conference#has_a_committee_chair', 'edas#FreeTimeBreak', 'conference#Extended_abstract', 'sigkdd#search', 'cmt#subclass_of', 'iasted#Full_day_tour', 'iasted#Sponsor_city', 'edas#PaperPresentation', 'iasted#Activity', 'cmt#startReviewerBidding', 'conference#has_a_degree', 'conference#Regular_contribution', 'ekaw#Paper_Author', 'edas#Topic', 'edas#isInitiatedBy', 'edas#Call', 'cmt#Meta-Reviewer', 'conference#has_a_location', 'conference#belongs_to_a_review_reference', 'confOf#Camera_Ready_event', 'edas#CADTopic', 'conference#is_submitted_at', 'ekaw#Research_Topic', 'iasted#Nonauthor_registration_fee', 'conference#has_workshops', 'sigkdd#General_Chair', 'sigkdd#pay', 'ekaw#Web_Site', 'edas#RatedPapers', 'conference#Presentation', 'iasted#One_conference_day', 'confOf#Science_Worker', 'confOf#Volunteer', 'ekaw#Tutorial_Abstract', 'conference#Information_for_participants', 'cmt#Rejection', 'conference#has_the_first_name', 'sigkdd#Best_Student_Paper_Award', 'conference#Rejected_contribution', 'confOf#Tutorial', 'iasted#Hotel_registration_form', 'iasted#Fee', 'ekaw#authorOf', 'cmt#anyURI', 'iasted#Introduction', 'cmt#maxPapers', 'sigkdd#Organizing_Committee', 'sigkdd#Hotel', 'iasted#Document', 'conference#is_a_date_of_acceptance_announcement', 'conference#Reviewer', 'ekaw#topicCoveredBy', 'ekaw#Invited_Speaker', 'edas#dateTime', 'iasted#Non_speaker', 'confOf#dealsWith', 'iasted#Sponzorship', 'sigkdd#Main_office', 'iasted#Conference_building', 'conference#Conference_announcement', 'confOf#ends_on', 'iasted#Plenary_lecture', 'cmt#addedBy', 'cmt#assignedByAdministrator', 'conference#has_tutorials', 'conference#has_a_track-workshop-tutorial_chair', 'cmt#name', 'confOf#Participant', 'edas#isLocationOf', 'cmt#adjustBid', 'iasted#City', 'edas#SingleLevelConference', 'sigkdd#Exhibitor', 'conference#Reviewed_contribution', 'ekaw#Demo_Chair', 'iasted#is_connected_with', 'sigkdd#submit_until', 'edas#hasRating', 'ekaw#Document', 'ekaw#Academic_Institution', 'sigkdd#Registration_Non-Member', 'iasted#LCD_projector', 'confOf#minChoice', 'sigkdd#hold', 'edas#isReviewing', 'edas#manuscriptDueOn', 'conference#has_a_volume', 'edas#NonAcademicEvent', 'edas#SessionChair', 'edas#ComputerNetworksSecurityTopic', 'confOf#has_title', 'iasted#State', 'iasted#Currency', 'iasted#is_signed_by', 'cmt#Author', 'confOf#hasStreet', 'iasted#Tutorial_speaker', 'ekaw#Invited_Talk', 'edas#hasCountry', 'cmt#assignReviewer', 'conference#was_a_committe_co-chair_of', 'edas#Excursion', 'cmt#hasConferenceMember', 'sigkdd#designed_by', 'iasted#subclass_of', 'ekaw#Assigned_Paper', 'edas#CallForManuscripts', 'iasted#Renting', 'sigkdd#payed_by', 'ekaw#Invited_Talk_Abstract', 'edas#hasProgramme', 'ekaw#Workshop', 'confOf#earlyRegistration', 'conference#string', 'confOf#Trip', 'edas#providedBy', 'conference#Conference_proceedings', 'sigkdd#submit', 'iasted#Credit_card', 'iasted#Double_hotel_room', 'iasted#IASTED_member', 'conference#is_an_ending_date', 'cmt#writeReview', 'sigkdd#presentation', 'edas#ContactInformation', 'conference#Track', 'sigkdd#Place', 'edas#hasName', 'edas#isMemberOf', 'conference#Early_paid_applicant', 'confOf#Regular', 'conference#has_an_expertise', 'cmt#memberOfProgramCommittee', 'iasted#is_held_after', 'edas#OrganizationalMeeting', 'cmt#paperAssignmentToolsRunBy', 'cmt#Administrator', 'conference#belongs_to_reviewers', 'confOf#abstract', 'edas#NGO', 'sigkdd#Speaker', 'sigkdd#subclass_of', 'conference#has_been_assigned_a_review_reference', 'cmt#title', 'confOf#hasAdministrativeEvent', 'confOf#Reviewing_event', 'edas#Author', 'ekaw#Conference_Participant', 'edas#Document', 'iasted#Audiovisual_equipment', 'iasted#Registration_fee', 'conference#Track-workshop_chair', 'iasted#int', 'iasted#Time_zone', 'cmt#hasProgramCommitteeMember', 'conference#Workshop', 'sigkdd#design', 'edas#ComputerArchitectureTopic', 'conference#has_a_URL', 'cmt#Bid', 'ekaw#Tutorial', 'ekaw#Regular_Paper', 'conference#has_tracks', 'edas#WithdrawnPaper', 'edas#AccommodationPlace', 'confOf#Working_event', 'cmt#markConflictOfInterest', 'cmt#enterReviewCriteria', 'iasted#pay', 'conference#Regular_author', 'ekaw#partOfEvent', 'ekaw#inverse_of_partOf_7', 'conference#has_an_email', 'edas#RejectedPaper', 'conference#was_a_member_of', 'edas#subclass_of', 'conference#Abstract', 'iasted#Coffee_break', 'sigkdd#Nation', 'edas#hasBiography', 'iasted#has_amount_of', 'confOf#Poster', 'edas#hasMember', 'ekaw#reviewWrittenBy', 'iasted#Conference_Hiker', 'edas#SignalProcessingTopic', 'edas#PublishedPaper', 'iasted#Presenter_state', 'iasted#Conference_airport', 'iasted#Reviewer', 'ekaw#Flyer', 'iasted#Cd_proceening', 'sigkdd#Thing', 'conference#has_important_dates', 'iasted#Value_added_tax', 'iasted#send', 'cmt#reviewerBiddingStartedBy', 'conference#has_contributions', 'sigkdd#Review', 'sigkdd#presentationed_by', 'edas#relatesTo', 'cmt#boolean', 'edas#Workshop', 'sigkdd#int', 'conference#Review', 'ekaw#scientificallyOrganises', 'iasted#Lecturer', 'cmt#hasBeenAssigned', 'iasted#Receiving_manuscript', 'iasted#Overhead_projector', 'iasted#Hotel_fee', 'ekaw#scientificallyOrganisedBy', 'iasted#is_held_before', 'confOf#follows', 'confOf#Contribution', 'ekaw#Possible_Reviewer', 'iasted#Building', 'cmt#ProgramCommitteeMember', 'confOf#Chair_PC', 'confOf#parallel_with', 'iasted#is_designed_for', 'cmt#date', 'cmt#ConferenceMember', 'iasted#Trip_day', 'cmt#Acceptance', 'iasted#is_prepared_by'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test, inp_test1, inp_test2 = None, None, None\n",
    "\n",
    "def write(elem):\n",
    "    f = open(\"Logs\", \"a+\")\n",
    "    if type(elem) == list or type(elem) == tuple:\n",
    "        string = str(\"\\n\".join([str(s) for s in elem]))\n",
    "    else:\n",
    "        string = str(elem)\n",
    "    f.write(\"\\n\"+string)\n",
    "    f.close()\n",
    "    \n",
    "inputs3, results3 = None, None\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.name_embedding = nn.Embedding(len(embeddings), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.layer1 = nn.Bilinear(self.hidden_dim, self.hidden_dim, 2)\n",
    "\n",
    "    def forward(self, inputs, seq_lens, rev_indices):\n",
    "        results = []\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "#             print (\"Embeddings\", x)\n",
    "            packed_inp = pack_padded_sequence(x, seq_lens[i].numpy(), batch_first=True)\n",
    "            op, (ht, ct) = self.lstm(x)\n",
    "            x = ht[2*(self.num_layers-1):].permute(1,0,2)\n",
    "            x = x[rev_indices[i],:,:]\n",
    "            results.append(x.reshape(-1, self.hidden_dim))\n",
    "        global inputs3, results3\n",
    "        results3 = results\n",
    "        inputs3 = inputs\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 122893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:429: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Idx: 0 Loss: 0.8681666936900662\n",
      "Epoch: 0 Idx: 10 Loss: 0.08333187536137159\n",
      "Epoch: 0 Idx: 20 Loss: 0.05669524778706547\n",
      "Epoch: 0 Idx: 30 Loss: 0.039384559004603435\n",
      "Epoch: 1 Idx: 0 Loss: 0.03662774375376728\n",
      "Epoch: 1 Idx: 10 Loss: 0.029273020941462394\n",
      "Epoch: 1 Idx: 20 Loss: 0.025473396930190902\n",
      "Epoch: 1 Idx: 30 Loss: 0.02115712242836873\n",
      "Epoch: 2 Idx: 0 Loss: 0.020624645732379966\n",
      "Epoch: 2 Idx: 10 Loss: 0.017814939296293585\n",
      "Epoch: 2 Idx: 20 Loss: 0.015476079151114182\n",
      "Epoch: 2 Idx: 30 Loss: 0.014443816850652007\n",
      "Epoch: 3 Idx: 0 Loss: 0.01391308938343195\n",
      "Epoch: 3 Idx: 10 Loss: 0.013251527192293347\n",
      "Epoch: 3 Idx: 20 Loss: 0.011875817320637128\n",
      "Epoch: 3 Idx: 30 Loss: 0.01059461895018708\n",
      "Epoch: 4 Idx: 0 Loss: 0.010398094188091407\n",
      "Epoch: 4 Idx: 10 Loss: 0.008951651317140086\n",
      "Epoch: 4 Idx: 20 Loss: 0.1513814348565302\n",
      "Epoch: 4 Idx: 30 Loss: 0.032158178464164335\n",
      "Epoch: 5 Idx: 0 Loss: 0.026726888528289774\n",
      "Epoch: 5 Idx: 10 Loss: 0.018704312813226164\n",
      "Epoch: 5 Idx: 20 Loss: 0.01694940351722562\n",
      "Epoch: 5 Idx: 30 Loss: 0.01479717793439893\n",
      "Epoch: 6 Idx: 0 Loss: 0.013432854666729041\n",
      "Epoch: 6 Idx: 10 Loss: 0.011841702791959948\n",
      "Epoch: 6 Idx: 20 Loss: 0.010961665044875992\n",
      "Epoch: 6 Idx: 30 Loss: 0.009736951425052754\n",
      "Epoch: 7 Idx: 0 Loss: 0.009465906290191678\n",
      "Epoch: 7 Idx: 10 Loss: 0.010401880501347409\n",
      "Epoch: 7 Idx: 20 Loss: 0.008908238487055487\n",
      "Epoch: 7 Idx: 30 Loss: 0.008566222995570544\n",
      "Epoch: 8 Idx: 0 Loss: 0.00891250810942792\n",
      "Epoch: 8 Idx: 10 Loss: 0.008910196565417073\n",
      "Epoch: 8 Idx: 20 Loss: 0.007906404352769073\n",
      "Epoch: 8 Idx: 30 Loss: 0.007910084220347204\n",
      "Epoch: 9 Idx: 0 Loss: 0.008886451312765291\n",
      "Epoch: 9 Idx: 10 Loss: 0.029306467709901796\n",
      "Epoch: 9 Idx: 20 Loss: 0.01758505499468865\n",
      "Epoch: 9 Idx: 30 Loss: 0.013101900926547243\n",
      "Epoch: 10 Idx: 0 Loss: 0.01151616176601483\n",
      "Epoch: 10 Idx: 10 Loss: 0.011535916959352153\n",
      "Epoch: 10 Idx: 20 Loss: 0.00923357140653027\n",
      "Epoch: 10 Idx: 30 Loss: 0.008292269651647404\n",
      "Epoch: 11 Idx: 0 Loss: 0.0077613249471631925\n",
      "Epoch: 11 Idx: 10 Loss: 0.007120350376874581\n",
      "Epoch: 11 Idx: 20 Loss: 0.007225231299493364\n",
      "Epoch: 11 Idx: 30 Loss: 0.00799595712965415\n",
      "Epoch: 12 Idx: 0 Loss: 0.006701327210625983\n",
      "Epoch: 12 Idx: 10 Loss: 0.007362117262842094\n",
      "Epoch: 12 Idx: 20 Loss: 0.0071267709899236345\n",
      "Epoch: 12 Idx: 30 Loss: 0.006207589851788533\n",
      "Epoch: 13 Idx: 0 Loss: 0.007415753906028155\n",
      "Epoch: 13 Idx: 10 Loss: 0.0064324994840494745\n",
      "Epoch: 13 Idx: 20 Loss: 0.006741548689245084\n",
      "Epoch: 13 Idx: 30 Loss: 0.007941014400505368\n",
      "Epoch: 14 Idx: 0 Loss: 0.017571797852005258\n",
      "Epoch: 14 Idx: 10 Loss: 0.06221579999362052\n",
      "Epoch: 14 Idx: 20 Loss: 0.039754692303433176\n",
      "Epoch: 14 Idx: 30 Loss: 0.034985953666936256\n",
      "Epoch: 15 Idx: 0 Loss: 0.029829021578307587\n",
      "Epoch: 15 Idx: 10 Loss: 0.027345053249863804\n",
      "Epoch: 15 Idx: 20 Loss: 0.023523695915027867\n",
      "Epoch: 15 Idx: 30 Loss: 0.019641317433443526\n",
      "Epoch: 16 Idx: 0 Loss: 0.01954015772984004\n",
      "Epoch: 16 Idx: 10 Loss: 0.016921400044723568\n",
      "Epoch: 16 Idx: 20 Loss: 0.016019017065257553\n",
      "Epoch: 16 Idx: 30 Loss: 0.014463879701744299\n",
      "Epoch: 17 Idx: 0 Loss: 0.013813642642743205\n",
      "Epoch: 17 Idx: 10 Loss: 0.011393797978554636\n",
      "Epoch: 17 Idx: 20 Loss: 0.011098065455831776\n",
      "Epoch: 17 Idx: 30 Loss: 0.011046022755177284\n",
      "Epoch: 18 Idx: 0 Loss: 0.010230215751277815\n",
      "Epoch: 18 Idx: 10 Loss: 0.00898873757328402\n",
      "Epoch: 18 Idx: 20 Loss: 0.00827482550555374\n",
      "Epoch: 18 Idx: 30 Loss: 0.007953522833038731\n",
      "Epoch: 19 Idx: 0 Loss: 0.0073519032785741055\n",
      "Epoch: 19 Idx: 10 Loss: 0.007425827693963418\n",
      "Epoch: 19 Idx: 20 Loss: 0.007393767824003154\n",
      "Epoch: 19 Idx: 30 Loss: 0.008302911040436923\n",
      "Epoch: 20 Idx: 0 Loss: 0.006844603604275901\n",
      "Epoch: 20 Idx: 10 Loss: 0.006806783665083156\n",
      "Epoch: 20 Idx: 20 Loss: 0.007049407131123291\n",
      "Epoch: 20 Idx: 30 Loss: 0.007905929250084332\n",
      "Epoch: 21 Idx: 0 Loss: 0.006686436989756173\n",
      "Epoch: 21 Idx: 10 Loss: 0.007019035542257221\n",
      "Epoch: 21 Idx: 20 Loss: 0.06547713043609645\n",
      "Epoch: 21 Idx: 30 Loss: 0.04268801793354522\n",
      "Epoch: 22 Idx: 0 Loss: 0.03560565931954506\n",
      "Epoch: 22 Idx: 10 Loss: 0.02942294071543491\n",
      "Epoch: 22 Idx: 20 Loss: 0.025241019904208246\n",
      "Epoch: 22 Idx: 30 Loss: 0.019668526434632516\n",
      "Epoch: 23 Idx: 0 Loss: 0.019888572915791793\n",
      "Epoch: 23 Idx: 10 Loss: 0.015163271711397294\n",
      "Epoch: 23 Idx: 20 Loss: 0.0146281782887491\n",
      "Epoch: 23 Idx: 30 Loss: 0.013529430794405044\n",
      "Epoch: 24 Idx: 0 Loss: 0.01350732510566852\n",
      "Epoch: 24 Idx: 10 Loss: 0.01124077801271577\n",
      "Epoch: 24 Idx: 20 Loss: 0.010477959634105216\n",
      "Epoch: 24 Idx: 30 Loss: 0.00995664278409715\n",
      "Epoch: 25 Idx: 0 Loss: 0.009192306328096355\n",
      "Epoch: 25 Idx: 10 Loss: 0.00848883416663198\n",
      "Epoch: 25 Idx: 20 Loss: 0.009920030274300957\n",
      "Epoch: 25 Idx: 30 Loss: 0.00821145065296688\n",
      "Epoch: 26 Idx: 0 Loss: 0.0071821076405213765\n",
      "Epoch: 26 Idx: 10 Loss: 0.007605484752583213\n",
      "Epoch: 26 Idx: 20 Loss: 0.0076758108434898135\n",
      "Epoch: 26 Idx: 30 Loss: 0.008167416188906903\n",
      "Epoch: 27 Idx: 0 Loss: 0.007042649608061105\n",
      "Epoch: 27 Idx: 10 Loss: 0.006628182991622415\n",
      "Epoch: 27 Idx: 20 Loss: 0.006925106946638814\n",
      "Epoch: 27 Idx: 30 Loss: 0.00695328650089415\n",
      "Epoch: 28 Idx: 0 Loss: 0.006022060579109077\n",
      "Epoch: 28 Idx: 10 Loss: 0.006732295882884012\n",
      "Epoch: 28 Idx: 20 Loss: 0.007514731468464286\n",
      "Epoch: 28 Idx: 30 Loss: 0.05749304224811691\n",
      "Epoch: 29 Idx: 0 Loss: 0.04955269353927022\n",
      "Epoch: 29 Idx: 10 Loss: 0.0323884943281719\n",
      "Epoch: 29 Idx: 20 Loss: 0.026392154572071475\n",
      "Epoch: 29 Idx: 30 Loss: 0.0209292730919887\n",
      "Epoch: 30 Idx: 0 Loss: 0.022219981885589116\n",
      "Epoch: 30 Idx: 10 Loss: 0.01931719027392825\n",
      "Epoch: 30 Idx: 20 Loss: 0.015699596010450213\n",
      "Epoch: 30 Idx: 30 Loss: 0.014155995565327687\n",
      "Epoch: 31 Idx: 0 Loss: 0.01276902728074927\n",
      "Epoch: 31 Idx: 10 Loss: 0.011740430289002108\n",
      "Epoch: 31 Idx: 20 Loss: 0.011046887874508311\n",
      "Epoch: 31 Idx: 30 Loss: 0.009242958470597999\n",
      "Epoch: 32 Idx: 0 Loss: 0.009847220962822274\n",
      "Epoch: 32 Idx: 10 Loss: 0.008360685241751098\n",
      "Epoch: 32 Idx: 20 Loss: 0.008537161985330086\n",
      "Epoch: 32 Idx: 30 Loss: 0.0077755154104849015\n",
      "Epoch: 33 Idx: 0 Loss: 0.007909002677642605\n",
      "Epoch: 33 Idx: 10 Loss: 0.006938637500278602\n",
      "Epoch: 33 Idx: 20 Loss: 0.0070380690087268934\n",
      "Epoch: 33 Idx: 30 Loss: 0.007069796139565896\n",
      "Epoch: 34 Idx: 0 Loss: 0.006413157368437715\n",
      "Epoch: 34 Idx: 10 Loss: 0.007233974274221065\n",
      "Epoch: 34 Idx: 20 Loss: 0.006909748259397898\n",
      "Epoch: 34 Idx: 30 Loss: 0.006487956378841593\n",
      "Epoch: 35 Idx: 0 Loss: 0.005643624466465676\n",
      "Epoch: 35 Idx: 10 Loss: 0.006360982714894593\n",
      "Epoch: 35 Idx: 20 Loss: 0.00675375508417187\n",
      "Epoch: 35 Idx: 30 Loss: 0.00724677492471489\n",
      "Epoch: 36 Idx: 0 Loss: 0.006028526096553021\n",
      "Epoch: 36 Idx: 10 Loss: 0.010633938430365779\n",
      "Epoch: 36 Idx: 20 Loss: 0.06467238131239503\n",
      "Epoch: 36 Idx: 30 Loss: 0.0402531537678576\n",
      "Epoch: 37 Idx: 0 Loss: 0.03766436064728214\n",
      "Epoch: 37 Idx: 10 Loss: 0.03206655437886655\n",
      "Epoch: 37 Idx: 20 Loss: 0.0260305088169854\n",
      "Epoch: 37 Idx: 30 Loss: 0.022571415084852168\n",
      "Epoch: 38 Idx: 0 Loss: 0.022784597119271645\n",
      "Epoch: 38 Idx: 10 Loss: 0.01844715281451082\n",
      "Epoch: 38 Idx: 20 Loss: 0.01759221342233756\n",
      "Epoch: 38 Idx: 30 Loss: 0.014439183633425416\n",
      "Epoch: 39 Idx: 0 Loss: 0.01402651744527566\n",
      "Epoch: 39 Idx: 10 Loss: 0.01197602199485594\n",
      "Epoch: 39 Idx: 20 Loss: 0.012062763786981541\n",
      "Epoch: 39 Idx: 30 Loss: 0.01083111971550884\n",
      "Epoch: 40 Idx: 0 Loss: 0.010631616765528136\n",
      "Epoch: 40 Idx: 10 Loss: 0.00928147500551178\n",
      "Epoch: 40 Idx: 20 Loss: 0.01037838530574039\n",
      "Epoch: 40 Idx: 30 Loss: 0.008017240467579708\n",
      "Epoch: 41 Idx: 0 Loss: 0.009411044775261347\n",
      "Epoch: 41 Idx: 10 Loss: 0.008435747089538002\n",
      "Epoch: 41 Idx: 20 Loss: 0.007443222841016875\n",
      "Epoch: 41 Idx: 30 Loss: 0.0076780703957327456\n",
      "Epoch: 42 Idx: 0 Loss: 0.007258471783059245\n",
      "Epoch: 42 Idx: 10 Loss: 0.006784163328366436\n",
      "Epoch: 42 Idx: 20 Loss: 0.00663237309503962\n",
      "Epoch: 42 Idx: 30 Loss: 0.006429025030560164\n",
      "Epoch: 43 Idx: 0 Loss: 0.006590034329169357\n",
      "Epoch: 43 Idx: 10 Loss: 0.006991680645946923\n",
      "Epoch: 43 Idx: 20 Loss: 0.005968019593377834\n",
      "Epoch: 43 Idx: 30 Loss: 0.007327784668865431\n",
      "Epoch: 44 Idx: 0 Loss: 0.0071723988634270685\n",
      "Epoch: 44 Idx: 10 Loss: 0.1313899663333349\n",
      "Epoch: 44 Idx: 20 Loss: 0.04542827516319264\n",
      "Epoch: 44 Idx: 30 Loss: 0.031485900969816086\n",
      "Epoch: 45 Idx: 0 Loss: 0.030179649626554228\n",
      "Epoch: 45 Idx: 10 Loss: 0.024451775493515536\n",
      "Epoch: 45 Idx: 20 Loss: 0.020389581284834493\n",
      "Epoch: 45 Idx: 30 Loss: 0.017960897517583563\n",
      "Epoch: 46 Idx: 0 Loss: 0.017418241189922017\n",
      "Epoch: 46 Idx: 10 Loss: 0.015625345087591946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 Idx: 20 Loss: 0.013372233686659234\n",
      "Epoch: 46 Idx: 30 Loss: 0.013314174427708098\n",
      "Epoch: 47 Idx: 0 Loss: 0.01126990435381286\n",
      "Epoch: 47 Idx: 10 Loss: 0.0123494708838174\n",
      "Epoch: 47 Idx: 20 Loss: 0.00980619119248796\n",
      "Epoch: 47 Idx: 30 Loss: 0.009210375379435214\n",
      "Epoch: 48 Idx: 0 Loss: 0.009351143826573795\n",
      "Epoch: 48 Idx: 10 Loss: 0.008852426413148255\n",
      "Epoch: 48 Idx: 20 Loss: 0.0078395257759159\n",
      "Epoch: 48 Idx: 30 Loss: 0.007545893111298204\n",
      "Epoch: 49 Idx: 0 Loss: 0.007800406635555696\n",
      "Epoch: 49 Idx: 10 Loss: 0.007302276357194918\n",
      "Epoch: 49 Idx: 20 Loss: 0.007498419202923148\n",
      "Epoch: 49 Idx: 30 Loss: 0.006701721014740097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:184: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  0.9 0.55 0.24444444444444444 0.3384615384615384 0.27499999999999997 0.44\n",
      "Threshold:  0.91 0.5789473684210527 0.24444444444444444 0.34375 0.27638190954773867 0.45454545454545453\n",
      "Threshold:  0.92 0.6 0.2 0.3 0.23076923076923078 0.42857142857142866\n",
      "Threshold:  0.93 0.875 0.15555555555555556 0.26415094339622647 0.18617021276595747 0.4545454545454546\n",
      "Threshold:  0.9400000000000001 1.0 0.13333333333333333 0.23529411764705882 0.16129032258064513 0.43478260869565216\n",
      "Threshold:  0.9500000000000001 1.0 0.13333333333333333 0.23529411764705882 0.16129032258064513 0.43478260869565216\n",
      "Threshold:  0.9600000000000001 1.0 0.08888888888888889 0.163265306122449 0.10869565217391305 0.3278688524590164\n",
      "Threshold:  0.9700000000000001 1.0 0.08888888888888889 0.163265306122449 0.10869565217391305 0.3278688524590164\n",
      "Threshold:  0.9800000000000001 1.0 0.08888888888888889 0.163265306122449 0.10869565217391305 0.3278688524590164\n",
      "Threshold:  0.9900000000000001 1.0 0.08888888888888889 0.163265306122449 0.10869565217391305 0.3278688524590164\n",
      "division by zero\n",
      "division by zero\n",
      "Precision: 0.5789473684210527 Recall: 0.24444444444444444 F1-Score: 0.34375 F2-Score: 0.27638190954773867 F0.5-Score: 0.45454545454545453\n",
      "Epoch: 0 Idx: 0 Loss: 0.8840381205315415\n",
      "Epoch: 0 Idx: 10 Loss: 0.08188658597325836\n",
      "Epoch: 0 Idx: 20 Loss: 0.05754754230343088\n",
      "Epoch: 0 Idx: 30 Loss: 0.04411866977870526\n",
      "Epoch: 1 Idx: 0 Loss: 0.037061676975094486\n",
      "Epoch: 1 Idx: 10 Loss: 0.030158944550455816\n",
      "Epoch: 1 Idx: 20 Loss: 0.022504162473388746\n",
      "Epoch: 1 Idx: 30 Loss: 0.02161904558863299\n",
      "Epoch: 2 Idx: 0 Loss: 0.021286159537143207\n",
      "Epoch: 2 Idx: 10 Loss: 0.016718498211476494\n",
      "Epoch: 2 Idx: 20 Loss: 0.01524904726298348\n",
      "Epoch: 2 Idx: 30 Loss: 0.013962492716166196\n",
      "Epoch: 3 Idx: 0 Loss: 0.014560077219149425\n",
      "Epoch: 3 Idx: 10 Loss: 0.012433387170949295\n",
      "Epoch: 3 Idx: 20 Loss: 0.011428677690109232\n",
      "Epoch: 3 Idx: 30 Loss: 0.011761101885796302\n",
      "Epoch: 4 Idx: 0 Loss: 0.010207902414181647\n",
      "Epoch: 4 Idx: 10 Loss: 0.00919666599587735\n",
      "Epoch: 4 Idx: 20 Loss: 0.009348917094816098\n",
      "Epoch: 4 Idx: 30 Loss: 0.056498004318017686\n",
      "Epoch: 5 Idx: 0 Loss: 0.04109044811703096\n",
      "Epoch: 5 Idx: 10 Loss: 0.02658706953986955\n",
      "Epoch: 5 Idx: 20 Loss: 0.02094164184551575\n",
      "Epoch: 5 Idx: 30 Loss: 0.01865518186727983\n",
      "Epoch: 6 Idx: 0 Loss: 0.017030729563547342\n",
      "Epoch: 6 Idx: 10 Loss: 0.013907862632695455\n",
      "Epoch: 6 Idx: 20 Loss: 0.012636924920635891\n",
      "Epoch: 6 Idx: 30 Loss: 0.011435622693265623\n",
      "Epoch: 7 Idx: 0 Loss: 0.010520180494586662\n",
      "Epoch: 7 Idx: 10 Loss: 0.011049288642534348\n",
      "Epoch: 7 Idx: 20 Loss: 0.010804966947510352\n",
      "Epoch: 7 Idx: 30 Loss: 0.00997725634990208\n",
      "Epoch: 8 Idx: 0 Loss: 0.009954561960467368\n",
      "Epoch: 8 Idx: 10 Loss: 0.009390602100864323\n",
      "Epoch: 8 Idx: 20 Loss: 0.009592487859686869\n",
      "Epoch: 8 Idx: 30 Loss: 0.008726594814371984\n",
      "Epoch: 9 Idx: 0 Loss: 0.00833566543408081\n",
      "Epoch: 9 Idx: 10 Loss: 0.00783086934857871\n",
      "Epoch: 9 Idx: 20 Loss: 0.11083196501229595\n",
      "Epoch: 9 Idx: 30 Loss: 0.0289525362952057\n",
      "Epoch: 10 Idx: 0 Loss: 0.025393095842500032\n",
      "Epoch: 10 Idx: 10 Loss: 0.018443262142625497\n",
      "Epoch: 10 Idx: 20 Loss: 0.013782262758844616\n",
      "Epoch: 10 Idx: 30 Loss: 0.012104911437547538\n",
      "Epoch: 11 Idx: 0 Loss: 0.010961384163153245\n",
      "Epoch: 11 Idx: 10 Loss: 0.009483606649550277\n",
      "Epoch: 11 Idx: 20 Loss: 0.009514460294916467\n",
      "Epoch: 11 Idx: 30 Loss: 0.009666981637367839\n",
      "Epoch: 12 Idx: 0 Loss: 0.007905607333487835\n",
      "Epoch: 12 Idx: 10 Loss: 0.007898266611418472\n",
      "Epoch: 12 Idx: 20 Loss: 0.008540972465717849\n",
      "Epoch: 12 Idx: 30 Loss: 0.007143769233842892\n",
      "Epoch: 13 Idx: 0 Loss: 0.007934233098079963\n",
      "Epoch: 13 Idx: 10 Loss: 0.007391573442497468\n",
      "Epoch: 13 Idx: 20 Loss: 0.007365191359001049\n",
      "Epoch: 13 Idx: 30 Loss: 0.007799011959678837\n",
      "Epoch: 14 Idx: 0 Loss: 0.007275428611242229\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-c29934458982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_elems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;31m#             break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(\"data.pkl\", \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings  = pickle.load(f)\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "ind_test, inp_test1, inp_test2 = None, None, None\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(b,a,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, include_inv=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n",
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics\n",
    "    all_results = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        batch_size = min(batch_size, len(test_data_t))\n",
    "        num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "        batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "            neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])            \n",
    "            targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "            \n",
    "            indices = np.random.permutation(inputs.shape[0])\n",
    "            inputs, targets = inputs[indices].transpose(1,0,2), targets[indices]\n",
    "            inputs_elem = inputs.copy()\n",
    "            \n",
    "            nonzero_elems = np.count_nonzero(inputs, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            inputs = np.stack((inputs[0][[indices[0]]], inputs[1][[indices[1]]]), axis=0)\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inputs_elem.shape[1])], [d2[k] for k in range(inputs_elem.shape[1])]))\n",
    "            \n",
    "            rev_indices = torch.LongTensor(rev_indices.T)\n",
    "            inputs = torch.LongTensor(inputs.transpose(1,0,2))\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy().T)\n",
    "            targets = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inputs, seq_lens, rev_indices)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            #outputs /= torch.sum(outputs, dim=1).view(-1, 1)\n",
    "            #outputs = [(1-el[1].item()) for el in outputs]\n",
    "\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "#             print (inputs)\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inputs_elem[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inputs_elem[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        #all_results = OrderedDict(sorted(all_results.items(), key=lambda x: x[0], reverse=True))\n",
    "        #filtered_results = dict()\n",
    "        \n",
    "        #entities_to_assign = set([el[0] for el in list(all_results.keys())])\n",
    "        #for pair in all_results:\n",
    "        #    if pair[0] in entities_to_assign:\n",
    "        #        filtered_results[pair] = all_results[pair]\n",
    "        #        entities_to_assign.remove(pair[0])\n",
    "                \n",
    "        #entities_to_assign = set([el[1] for el in list(all_results.keys())])\n",
    "        #for pair in all_results:\n",
    "        #    if pair[1] in entities_to_assign:\n",
    "        #        filtered_results[pair] = all_results[pair]\n",
    "        #        entities_to_assign.remove(pair[1])        \n",
    "\n",
    "        #filtered_results = OrderedDict(sorted(filtered_results.items(), key=lambda x: x[1][0], reverse=True))\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.01\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.01\n",
    "        low_threshold, high_threshold = 0.9, 1.02\n",
    "        for j,threshold in enumerate(np.arange(low_threshold, high_threshold, 0.01)):\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "        \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        all_metrics.append((opt_threshold, optimum_metrics))\n",
    "    return all_results\n",
    "\n",
    "def write(elem):\n",
    "    f = open(\"Logs\", \"a+\")\n",
    "    if type(elem) == list or type(elem) == tuple:\n",
    "        string = str(\"\\n\".join([str(s) for s in elem]))\n",
    "    else:\n",
    "        string = str(elem)\n",
    "    f.write(\"\\n\"+string)\n",
    "    f.close()\n",
    "    \n",
    "inputs3, results3 = None, None\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(embeddings), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, batch_first=True)\n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.bilinear = nn.Bilinear(self.hidden_dim, self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, inputs, seq_lens, rev_indices):\n",
    "        results = []\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "        seq_lens, rev_indices = seq_lens.T, rev_indices.T\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            self.lstm.flatten_parameters()\n",
    "            packed_inp = pack_padded_sequence(x, seq_lens[i].cpu().numpy(), batch_first=True)\n",
    "            op, (ht, ct) = self.lstm(x)\n",
    "            x = ht[2*(self.num_layers-1):].permute(1,0,2)\n",
    "            x = x[rev_indices[i],:,:]\n",
    "            results.append(x.reshape(-1, self.num_directions*self.hidden_dim))\n",
    "        #global inputs3, results3\n",
    "        #results3 = results\n",
    "        #inputs3 = inputs\n",
    "        #x = self.layer1(results[0], results[1])\n",
    "        #x = F.log_softmax(x)\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    ont_obj = Ontology(\"conference_ontologies/\" + ont + \".owl\")\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c) in triples]\n",
    "    neighbours_dict = {elem: [elem] for elem in list(set(flatten(entities)))}\n",
    "    for e1, e2 in entities:\n",
    "        neighbours_dict[e1].append(e2)\n",
    "        neighbours_dict[e2].append(e1)\n",
    "    \n",
    "    prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "    neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "    for e1, e2, p in prop_triples:\n",
    "        neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "        if elem not in neighbours_dict:\n",
    "            neighbours_dict[elem] = [elem]\n",
    "\n",
    "    neighbours_dict = {el: neighbours_dict[el][:1] + sorted(list(set(neighbours_dict[el][1:])))\n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: neighbours_dict[el][:10] for el in neighbours_dict}\n",
    "    neighbours_dict = {ont + \"#\" + el: [ont + \"#\" + e for e in neighbours_dict[el]] for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    op = np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs = np.array([generate_data(elem) for elem in list(elems)])\n",
    "    targets = np.array([target for i in range(len(elems))])\n",
    "    return inputs, targets\n",
    "\n",
    "neighbours_dicts = {ont: get_one_hop_neighbours(ont) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "max_neighbours = np.max(flatten([[len(el[e]) for e in el] for el in neighbours_dicts.values()]))\n",
    "neighbours_lens = {ont: {key: len(neighbours_dicts[ont][key]) for key in neighbours_dicts[ont]}\n",
    "                   for ont in neighbours_dicts}\n",
    "neighbours_dicts = {ont: {key: neighbours_dicts[ont][key] + [\"<UNK>\" for i in range(max_neighbours -len(neighbours_dicts[ont][key]))]\n",
    "              for key in neighbours_dicts[ont]} for ont in neighbours_dicts}\n",
    "\n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    \n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if data[key]]\n",
    "    train_data_f = [key for key in train_data if not data[key]]\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "#     np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 50\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 8\n",
    "    dropout = 0.3\n",
    "    batch_size = min(batch_size, len(train_data_t))\n",
    "    num_batches = int(ceil(len(train_data_t)/batch_size))\n",
    "    batch_size_f = int(ceil(len(train_data_f)/num_batches))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = nn.DataParallel(SiameseNetwork(512, 250, 1)).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "\n",
    "#        indices = np.random.permutation(len(inputs_pos) + len(inputs_neg))\n",
    "        \n",
    "#        inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "#        targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "\n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            \n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "            \n",
    "            inp = inputs.transpose(1,0,2)\n",
    "            nonzero_elems = np.count_nonzero(inp, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            inp_elems = np.stack((inp[0][[indices[0]]], inp[1][[indices[1]]]), axis=0).transpose(1,0,2)\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inp_elems)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inp.shape[1])], \n",
    "                                    [d2[k] for k in range(inp.shape[1])]))\n",
    "\n",
    "            rev_indices = torch.LongTensor(rev_indices.T)\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy().T)\n",
    "            outputs = model(inp_elems, seq_lens, rev_indices)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "#             break\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%10 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    test_data_t = [key for key in test_data if data[key]]\n",
    "    test_data_f = [key for key in test_data if not data[key]]\n",
    "    \n",
    "    res = greedy_matching()\n",
    "\n",
    "print (\"Final Results: \", np.mean([el[1] for el in all_metrics], axis=0))\n",
    "print (\"Best threshold: \", all_metrics[np.argmax([el[1][2] for el in all_metrics])][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = inputs.transpose(1,0,2)\n",
    "nonzero_elems = np.count_nonzero(inp, axis=-1)\n",
    "indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "inp_elems = np.stack((inp[0][[indices[0]]], inp[1][[indices[1]]]), axis=0).transpose(1,0,2)\n",
    "seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[273, 1058, 1248, 2429, 2508, 2998], []]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[i for i, e in enumerate(el) if e==0] for el in nonzero_elems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Conference_part', 0),\n",
       " ('Workshop', 1.0),\n",
       " ('string', 1.0),\n",
       " ('Tutorial', 1.0),\n",
       " ('Topic', 1.0),\n",
       " ('Track', 1.0),\n",
       " ('Track-workshop_chair', 1.0),\n",
       " ('Conference_volume', 1.0),\n",
       " ('Program_committee', 2.0),\n",
       " ('Person', 2.0),\n",
       " ('Conference', 2.0),\n",
       " ('Organizing_committee', 2.0),\n",
       " ('Steering_committee', 2.0),\n",
       " ('Important_dates', 2.0),\n",
       " ('Review_preference', 2.0),\n",
       " ('Conference_www', 2.0),\n",
       " ('Conference_contribution', 2.0),\n",
       " ('Committee', 2.0),\n",
       " ('Conference_proceedings', 2.0),\n",
       " ('Publisher', 2.0),\n",
       " ('Committee_member', 3.0),\n",
       " ('Submitted_contribution', 3.0),\n",
       " ('Written_contribution', 3.0),\n",
       " ('date', 3.0),\n",
       " ('Conference_contributor', 3.0),\n",
       " ('Poster', 3.0),\n",
       " ('int', 3.0),\n",
       " ('Co-chair', 3.0),\n",
       " ('Conference_document', 3.0),\n",
       " ('Conference_participant', 3.0),\n",
       " ('Chair', 3.0),\n",
       " ('Conference_applicant', 3.0),\n",
       " ('Reviewer', 3.0),\n",
       " ('Presentation', 3.0),\n",
       " ('Thing', 3.0),\n",
       " ('Review_expertise', 4.0),\n",
       " ('Invited_speaker', 4.0),\n",
       " ('Reviewed_contribution', 4.0),\n",
       " ('Invited_talk', 4.0),\n",
       " ('Registeered_applicant', 4.0),\n",
       " ('Passive_conference_participant', 4.0),\n",
       " ('Conference_announcement', 4.0),\n",
       " ('Active_conference_participant', 4.0),\n",
       " ('Call_for_paper', 4.0),\n",
       " ('Information_for_participants', 4.0),\n",
       " ('Abstract', 4.0),\n",
       " ('Regular_contribution', 4.0),\n",
       " ('Organizer', 4.0),\n",
       " ('Regular_author', 4.0),\n",
       " ('Call_for_participation', 4.0),\n",
       " ('Review', 4.0),\n",
       " ('Contribution_co-author', 5.0),\n",
       " ('Accepted_contribution', 5.0),\n",
       " ('Paper', 5.0),\n",
       " ('Rejected_contribution', 5.0),\n",
       " ('Organization', 5.0),\n",
       " ('Contribution_1th-author', 5.0),\n",
       " ('Extended_abstract', 5.0),\n",
       " ('Paid_applicant', 5.0),\n",
       " ('Early_paid_applicant', 6.0),\n",
       " ('Late_paid_applicant', 6.0),\n",
       " ('Camera_ready_contribution', 6.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def djikstra(n, adj_matrix, start_node): \n",
    "\n",
    "    distances = [sys.maxsize] * n \n",
    "    path = [False] * n\n",
    "    \n",
    "    distances[start_node] = 0\n",
    "    \n",
    "    for node in range(n): \n",
    "        \n",
    "        distances_dict = {elem: i for (i,elem) in enumerate(distances) if not path[i]}\n",
    "        closest_node = distances_dict[min(list(distances_dict.keys()))]\n",
    "\n",
    "        path[closest_node] = True\n",
    " \n",
    "        for curr_node in range(n): \n",
    "            if adj_matrix[closest_node][curr_node] > 0 and not path[curr_node] and \\\n",
    "            distances[curr_node] > distances[closest_node] + adj_matrix[closest_node][curr_node]: \n",
    "                distances[curr_node] = distances[closest_node] + adj_matrix[closest_node][curr_node]\n",
    "\n",
    "    return distances\n",
    "\n",
    "all_triples = Ontology(\"conference_ontologies/conference.owl\").get_triples()\n",
    "\n",
    "entities = {entity:i for i,entity in enumerate(list(set(flatten([(el[0], el[1]) for el in all_triples]))))}\n",
    "entities_inv = {entities[entity]:entity for entity in entities}\n",
    "\n",
    "adj_mat = np.zeros((len(entities), len(entities)))\n",
    "\n",
    "for (a,b,_) in all_triples:\n",
    "    adj_mat[entities[a]][entities[b]] = 1\n",
    "    adj_mat[entities[b]][entities[a]] = 1\n",
    "\n",
    "src = entities[\"Conference_part\"]\n",
    "sorted([(entities_inv[i], entity) for i,entity in enumerate(djikstra(len(entities), adj_mat, src))], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Workshop',\n",
       " 'string',\n",
       " 'Tutorial',\n",
       " 'Topic',\n",
       " 'Track',\n",
       " 'Track-workshop_chair',\n",
       " 'Conference_volume']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics\n",
    "all_results = OrderedDict()\n",
    "with torch.no_grad():\n",
    "    all_pred = []\n",
    "    batch_size = min(batch_size, len(test_data_t))\n",
    "    num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "    batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "\n",
    "    np.random.shuffle(test_data_t)\n",
    "    np.random.shuffle(test_data_f)\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "        batch_start_f = batch_idx * batch_size_f\n",
    "        batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "        pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "        neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])     \n",
    "        targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[270],\n",
       "        [354]],\n",
       "\n",
       "       [[693],\n",
       "        [166]],\n",
       "\n",
       "       [[ 48],\n",
       "        [433]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[582],\n",
       "        [ 82]],\n",
       "\n",
       "       [[434],\n",
       "        [150]],\n",
       "\n",
       "       [[612],\n",
       "        [746]]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings], open(\"data.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OrderedDict(embeddings.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9505597933298291,\n",
       " 0.9500549453989662,\n",
       " 0.9355142329346077,\n",
       " 0.9252969578090027,\n",
       " 0.924161773234341,\n",
       " 0.9161917929525936,\n",
       " 0.9145560906065195,\n",
       " 0.8607537499975881,\n",
       " 0.8595574122184512,\n",
       " 0.840830050230915,\n",
       " 0.838817190186913,\n",
       " 0.8381331004237735,\n",
       " 0.8291336802251279,\n",
       " 0.8228901352304357,\n",
       " 0.8157427755323794,\n",
       " 0.8069762065144923,\n",
       " 0.7977771476109715,\n",
       " 0.7594279307306226,\n",
       " 0.7533139414827859,\n",
       " 0.7478530008395881,\n",
       " 0.7134300596593071,\n",
       " 0.6391785502563588,\n",
       " 0.6257840836565325,\n",
       " 0.6073872489885819,\n",
       " 0.5860778090195681,\n",
       " 0.5650955207825324,\n",
       " 0.5507956373847711,\n",
       " 0.5393309269257885,\n",
       " 0.5257189219718555,\n",
       " 0.5153706107987324,\n",
       " 0.510112028539558,\n",
       " 0.49499912680970426,\n",
       " 0.49266799684424933,\n",
       " 0.44016891096565636,\n",
       " 0.43322190471028676,\n",
       " 0.3771392341675334,\n",
       " 0.30175813053199474,\n",
       " 0.2908530352691452,\n",
       " 0.2852969074169313,\n",
       " 0.2412825846927616,\n",
       " 0.11172718329336721]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([res[el][0] for el in res if res[el][1]], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
