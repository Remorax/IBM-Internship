{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.argv = [\"main\", \"../Input/data_demarcated_bagofnbrs.pkl\", 26, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of nodes in a path: 26\n",
      "Number of entities: 122893\n",
      "Training size: 109284 Testing size: 3734\n",
      "Epoch: 0 Idx: 0 Loss: 0.08766887560906116\n",
      "Len (direct inputs):  1251\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  1690\n",
      "Training size: 106829 Testing size: 2364\n",
      "Epoch: 0 Idx: 0 Loss: 0.3745607071743461\n",
      "Len (direct inputs):  2856\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  992\n",
      "Training size: 111630 Testing size: 3871\n",
      "Epoch: 0 Idx: 0 Loss: 0.14526162510938062\n",
      "Len (direct inputs):  1687\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  1127\n",
      "Training size: 98364 Testing size: 15620\n",
      "Epoch: 0 Idx: 0 Loss: 0.18861624013452657\n",
      "Len (direct inputs):  3541\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  1820\n",
      "Training size: 95580 Testing size: 11474\n",
      "Epoch: 0 Idx: 0 Loss: 0.2474082355423351\n",
      "Len (direct inputs):  3211\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  1400\n",
      "Training size: 107758 Testing size: 1969\n",
      "Epoch: 0 Idx: 0 Loss: 0.22961013361492225\n",
      "Len (direct inputs):  3066\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  905\n",
      "Training size: 107913 Testing size: 5952\n",
      "Epoch: 0 Idx: 0 Loss: 0.21952926412535353\n",
      "Len (direct inputs):  2166\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  2088\n",
      "Performance for [('cmt', 'ekaw')] is : (0.5555555555555556, 0.45454545454545453, 0.5, 0.4716981132075471, 0.5319148936170213)\n",
      "Performance for [('cmt', 'sigkdd')] is : (0.8181818181818182, 0.75, 0.7826086956521738, 0.7627118644067796, 0.8035714285714286)\n",
      "Performance for [('conference', 'sigkdd')] is : (0.6666666666666666, 0.5333333333333333, 0.5925925925925926, 0.5555555555555556, 0.6349206349206349)\n",
      "Performance for [('edas', 'iasted')] is : (0.8, 0.42105263157894735, 0.5517241379310345, 0.4651162790697674, 0.6779661016949153)\n",
      "Performance for [('ekaw', 'iasted')] is : (1.0, 0.6, 0.7499999999999999, 0.6521739130434783, 0.8823529411764706)\n",
      "Performance for [('cmt', 'confOf')] is : (0.7272727272727273, 0.5, 0.5925925925925926, 0.5333333333333333, 0.6666666666666666)\n",
      "Performance for [('cmt', 'iasted')] is : (0.8, 1.0, 0.888888888888889, 0.9523809523809523, 0.8333333333333334)\n",
      "Final Results: [0.76681097 0.60841877 0.6654867  0.62756714 0.71867514]\n",
      "Threshold:  0.9153553257332794\n"
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle, operator\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[1], \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, features_dict, ontologies_in_alignment = pickle.load(f)\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "def test():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()    \n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "        nodes_all = np.array(nodes_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            nodes = nodes_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "    return (test_onto, all_results)\n",
    "\n",
    "def optimize_threshold():\n",
    "    global batch_size, val_data_t, val_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(val_data_t)\n",
    "        np.random.shuffle(val_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(val_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(val_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        nodes_all = np.array(nodes_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            nodes = nodes_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [(key, all_results[key][0]) for key in val_data_t if key not in set(res) and not is_valid(val_onto, key)]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            # print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "            if threshold in threshold_results:\n",
    "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
    "            else:\n",
    "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
    "\n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            threshold += step \n",
    "\n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_onto, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        fn_list = [(key, all_results[key][0]) for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.features_arr = np.array(list(features_dict.values()))\n",
    "        self.n_neighbours = self.features_arr.shape[1]\n",
    "        self.max_paths = self.features_arr.shape[2]\n",
    "        self.max_pathlen = self.features_arr.shape[3]\n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
    "        \n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(self.max_pathlen) for i in range(self.max_pathlen)]))\n",
    " \n",
    "    def forward(self, nodes, features):\n",
    "        '''\n",
    "        Arguments:\n",
    "            - nodes: batch_size * 2\n",
    "            - features: batch_size * 2 * 4 * max_paths * max_pathlen\n",
    "        '''\n",
    "        results = []\n",
    "        nodes = nodes.permute(1,0) # 2 * batch_size\n",
    "        features = features.permute(1,0,2,3,4) # 2 * batch_size * 4 * max_paths * max_pathlen\n",
    "        for i in range(2):\n",
    "            node_emb = self.name_embedding(nodes[i]) # batch_size * 512\n",
    "            feature_emb = self.name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512\n",
    "            \n",
    "            feature_emb_reshaped = feature_emb.permute(0,4,1,2,3).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_paths * self.max_pathlen)\n",
    "            path_weights = torch.bmm(node_emb[:, None, :], feature_emb_reshaped)\n",
    "            path_weights = path_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_paths, self.max_pathlen)\n",
    "            path_weights = torch.sum(path_weights, dim=-1)\n",
    "            best_path_indices = torch.max(path_weights, dim=-1)[1][(..., ) + (None, ) * 3]\n",
    "            best_path_indices = best_path_indices.expand(-1, -1, -1, self.max_pathlen,  self.embedding_dim)\n",
    "            best_path = torch.gather(feature_emb, 2, best_path_indices).squeeze(2) # batch_size * 4 * max_pathlen * 512\n",
    "            # Another way: \n",
    "            # path_weights = masked_softmax(path_weights)\n",
    "            # best_path = torch.sum(path_weights[:, :, :, None, None] * feature_emb, dim=2)\n",
    "\n",
    "            best_path_reshaped = best_path.permute(0,3,1,2).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_pathlen)\n",
    "            node_weights = torch.bmm(node_emb.unsqueeze(1), best_path_reshaped) # batch_size * 4 * max_pathlen\n",
    "            node_weights = masked_softmax(node_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_pathlen)) # batch_size * 4 * max_pathlen\n",
    "            attended_path = node_weights.unsqueeze(-1) * best_path # batch_size * 4 * max_pathlen * 512\n",
    "\n",
    "            distance_weighted_path = torch.sum((self.v[None,None,:,None] * attended_path), dim=2) # batch_size * 4 * 512\n",
    "            context_emb = distance_weighted_path.squeeze(1)\n",
    "\n",
    "            contextual_node_emb = torch.cat((node_emb, context_emb), dim=1)\n",
    "            output_node_emb = self.output(contextual_node_emb)\n",
    "            results.append(output_node_emb)\n",
    "        sim = self.cosine_sim_layer(results[0], results[1])\n",
    "        return sim\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    return np.vectorize(embedify)(elem_tuple)\n",
    "\n",
    "def embedify(elem):\n",
    "    return emb_indexer[elem]\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.vectorize(embedify)([features_dict[elem] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets, nodes = [], [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            nodes.append(generate_data_neighbourless(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return np.array(inputs), np.array(nodes), np.array(targets)\n",
    "\n",
    "print(\"Max number of nodes in a path: \" + str(sys.argv[2]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "type_path = int(sys.argv[3])\n",
    "features_dict = {elem: features_dict[elem][type_path:type_path+1,:,:int(sys.argv[2])] for elem in features_dict}\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "for i in list(range(0, len(ontologies_in_alignment), 3)):\n",
    "    \n",
    "    test_onto = ontologies_in_alignment[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "\n",
    "    val_onto = test_onto[:2]\n",
    "    test_onto = test_onto[2:]\n",
    "    val_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in val_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Testing size:\", len(test_data))\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 1\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 32\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all][:10]\n",
    "        targets_all = np.array(targets_all)[indices_all][:10]\n",
    "        nodes_all = np.array(nodes_all)[indices_all][:10]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            nodes = nodes_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    val_data_t = [key for key in val_data if val_data[key]]\n",
    "    val_data_f = [key for key in val_data if not val_data[key]]\n",
    "    \n",
    "    optimize_threshold()\n",
    "\n",
    "    test_data_t = [key for key in test_data if test_data[key]]\n",
    "    test_data_f = [key for key in test_data if not test_data[key]]\n",
    "\n",
    "    final_results.append(test())\n",
    "    sys.stdout.flush()\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for [('confOf', 'sigkdd')] is : (0.6666666666666666, 0.8571428571428571, 0.75, 0.8108108108108107, 0.6976744186046512)\n",
      "Performance for [('iasted', 'sigkdd')] is : (0.5714285714285714, 0.8, 0.6666666666666666, 0.7407407407407408, 0.6060606060606061)\n",
      "Performance for [('cmt', 'ekaw')] is : (0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454)\n",
      "Performance for [('confOf', 'iasted')] is : (1.0, 0.8888888888888888, 0.9411764705882353, 0.9090909090909091, 0.9756097560975611)\n",
      "Performance for [('conference', 'edas')] is : (0.7222222222222222, 0.7647058823529411, 0.7428571428571428, 0.7558139534883721, 0.7303370786516854)\n",
      "Performance for [('cmt', 'sigkdd')] is : (0.6923076923076923, 0.75, 0.7199999999999999, 0.7377049180327869, 0.703125)\n",
      "Performance for [('ekaw', 'sigkdd')] is : (0.7142857142857143, 0.9090909090909091, 0.8, 0.8620689655172413, 0.746268656716418)\n",
      "Performance for [('conference', 'confOf')] is : (0.9090909090909091, 0.6666666666666666, 0.7692307692307692, 0.704225352112676, 0.8474576271186439)\n",
      "Performance for [('conference', 'sigkdd')] is : (0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333)\n",
      "Performance for [('confOf', 'edas')] is : (0.6190476190476191, 0.6842105263157895, 0.6500000000000001, 0.6701030927835052, 0.6310679611650486)\n",
      "Performance for [('cmt', 'conference')] is : (0.8181818181818182, 0.6, 0.6923076923076923, 0.6338028169014084, 0.7627118644067796)\n",
      "Performance for [('edas', 'iasted')] is : (0.6875, 0.5789473684210527, 0.6285714285714286, 0.5978260869565217, 0.6626506024096386)\n",
      "Performance for [('conference', 'iasted')] is : (0.6666666666666666, 0.42857142857142855, 0.5217391304347826, 0.4615384615384615, 0.6)\n",
      "Performance for [('edas', 'sigkdd')] is : (0.6153846153846154, 0.5333333333333333, 0.5714285714285715, 0.5479452054794521, 0.5970149253731344)\n",
      "Performance for [('ekaw', 'iasted')] is : (1.0, 0.9, 0.9473684210526316, 0.9183673469387754, 0.9782608695652175)\n",
      "Performance for [('cmt', 'edas')] is : (0.6666666666666666, 0.6153846153846154, 0.64, 0.625, 0.6557377049180327)\n",
      "Performance for [('edas', 'ekaw')] is : (0.75, 0.6521739130434783, 0.6976744186046512, 0.6696428571428571, 0.7281553398058253)\n",
      "Performance for [('cmt', 'confOf')] is : (0.75, 0.5625, 0.6428571428571429, 0.5921052631578947, 0.703125)\n",
      "Performance for [('confOf', 'ekaw')] is : (0.875, 0.7, 0.7777777777777777, 0.7291666666666666, 0.8333333333333334)\n",
      "Performance for [('conference', 'ekaw')] is : (0.7, 0.56, 0.6222222222222222, 0.5833333333333334, 0.6666666666666666)\n",
      "Performance for [('cmt', 'iasted')] is : (0.8, 1.0, 0.888888888888889, 0.9523809523809523, 0.8333333333333334)\n",
      "Final Results: [0.73824938 0.70144782 0.71188355 0.70383122 0.72558946]\n",
      "Threshold:  0.897\n",
      "Performance for [('confOf', 'sigkdd')] is : (0.6666666666666666, 0.8571428571428571, 0.75, 0.8108108108108107, 0.6976744186046512)\n",
      "Performance for [('iasted', 'sigkdd')] is : (0.5714285714285714, 0.8, 0.6666666666666666, 0.7407407407407408, 0.6060606060606061)\n",
      "Performance for [('cmt', 'ekaw')] is : (0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454, 0.5454545454545454)\n",
      "Performance for [('confOf', 'iasted')] is : (1.0, 0.7777777777777778, 0.8750000000000001, 0.813953488372093, 0.945945945945946)\n",
      "Performance for [('conference', 'edas')] is : (0.7368421052631579, 0.8235294117647058, 0.7777777777777778, 0.8045977011494253, 0.7526881720430109)\n",
      "Performance for [('cmt', 'sigkdd')] is : (0.7142857142857143, 0.8333333333333334, 0.7692307692307692, 0.8064516129032258, 0.7352941176470589)\n",
      "Performance for [('ekaw', 'sigkdd')] is : (0.7142857142857143, 0.9090909090909091, 0.8, 0.8620689655172413, 0.746268656716418)\n",
      "Performance for [('conference', 'confOf')] is : (0.9090909090909091, 0.6666666666666666, 0.7692307692307692, 0.704225352112676, 0.8474576271186439)\n",
      "Performance for [('conference', 'sigkdd')] is : (0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333, 0.7333333333333333)\n",
      "Performance for [('confOf', 'edas')] is : (0.6190476190476191, 0.6842105263157895, 0.6500000000000001, 0.6701030927835052, 0.6310679611650486)\n",
      "Performance for [('cmt', 'conference')] is : (0.8, 0.5333333333333333, 0.64, 0.5714285714285714, 0.7272727272727272)\n",
      "Performance for [('edas', 'iasted')] is : (0.625, 0.5263157894736842, 0.5714285714285714, 0.5434782608695652, 0.6024096385542168)\n",
      "Performance for [('conference', 'iasted')] is : (0.6, 0.42857142857142855, 0.5, 0.45454545454545453, 0.5555555555555556)\n",
      "Performance for [('edas', 'sigkdd')] is : (0.6153846153846154, 0.5333333333333333, 0.5714285714285715, 0.5479452054794521, 0.5970149253731344)\n",
      "Performance for [('ekaw', 'iasted')] is : (1.0, 1.0, 1.0, 1.0, 1.0)\n",
      "Performance for [('cmt', 'edas')] is : (0.6666666666666666, 0.6153846153846154, 0.64, 0.625, 0.6557377049180327)\n",
      "Performance for [('edas', 'ekaw')] is : (0.7368421052631579, 0.6086956521739131, 0.6666666666666666, 0.6306306306306306, 0.707070707070707)\n",
      "Performance for [('cmt', 'confOf')] is : (0.75, 0.5625, 0.6428571428571429, 0.5921052631578947, 0.703125)\n",
      "Performance for [('confOf', 'ekaw')] is : (0.8947368421052632, 0.85, 0.8717948717948718, 0.8585858585858586, 0.8854166666666667)\n",
      "Performance for [('conference', 'ekaw')] is : (0.7142857142857143, 0.6, 0.6521739130434783, 0.6198347107438016, 0.6880733944954128)\n",
      "Performance for [('cmt', 'iasted')] is : (0.8, 1.0, 0.888888888888889, 0.9523809523809523, 0.8333333333333334)\n",
      "Final Results: [0.7339691  0.70898445 0.71342536 0.70893688 0.72363119]\n",
      "Threshold:  0.895\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pickle, sys\n",
    "\n",
    "sys.argv = [\"main\", \"Output/test2_4.pkl\"]\n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_onto, test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in set(res)]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "for file_pair in list(zip(sorted(glob.glob(\"Output/*part1*\")), sorted(glob.glob(\"Output/*part2*\")))):\n",
    "    final_results1, threshold_results1 = pickle.load(open(file_pair[0], \"rb\"))\n",
    "    final_results2, threshold_results2 = pickle.load(open(file_pair[1], \"rb\"))\n",
    "    final_results = final_results1 + final_results2 \n",
    "    threshold_results = {}\n",
    "    for thresh in threshold_results1:\n",
    "        threshold_results[thresh] = threshold_results1[thresh]\n",
    "    for thresh in threshold_results2:\n",
    "        if thresh in threshold_results:\n",
    "            threshold_results[thresh].extend(threshold_results2[thresh])\n",
    "        else:\n",
    "            threshold_results[thresh] = threshold_results2[thresh]\n",
    "    threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "    threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "    all_metrics, all_fn, all_fp = calculate_performance()\n",
    "\n",
    "    f = open(sys.argv[1], \"wb\")\n",
    "    pickle.dump([all_fn, all_fp], f)\n",
    "    f.close()\n",
    "\n",
    "    print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "    print (\"Threshold: \", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Output/conf_weighted6_6_part1.pkl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_onto, test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in set(res)]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "\n",
    "\n",
    "final_results1, threshold_results1 = pickle.load(open(\"Output/conf6_2_part1.pkl\", \"rb\"))\n",
    "final_results2, threshold_results2 = pickle.load(open(\"Output/conf6_2_part2.pkl\", \"rb\"))\n",
    "threshold_results = {}\n",
    "for key in threshold_results1:\n",
    "    threshold_results[key] = threshold_results1[key]\n",
    "    if key in threshold_results2:\n",
    "        threshold_results[key].extend(threshold_results2[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "final_results = final_results1 + final_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for [('confOf', 'sigkdd')] is : (0.8333333333333334, 0.7142857142857143, 0.7692307692307692, 0.7352941176470589, 0.8064516129032258)\n",
      "Performance for [('iasted', 'sigkdd')] is : (0.8, 0.8, 0.8000000000000002, 0.8, 0.8)\n",
      "Performance for [('cmt', 'ekaw')] is : (0.625, 0.45454545454545453, 0.5263157894736842, 0.4807692307692307, 0.5813953488372092)\n",
      "Performance for [('conference', 'iasted')] is : (0.6666666666666666, 0.2857142857142857, 0.4, 0.3225806451612903, 0.5263157894736842)\n",
      "Performance for [('edas', 'sigkdd')] is : (0.7, 0.4666666666666667, 0.56, 0.5, 0.6363636363636365)\n",
      "Performance for [('ekaw', 'iasted')] is : (1.0, 0.6, 0.7499999999999999, 0.6521739130434783, 0.8823529411764706)\n",
      "Final Results: [0.77083333 0.55353535 0.63425776 0.58180298 0.70547989]\n",
      "Threshold:  0.934\n"
     ]
    }
   ],
   "source": [
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3240"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickle.load(open(\"Input/data_lebensmittel.pkl\", \"rb\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('german_datasets_copy/webdirectory/dmoz.owl', 'german_datasets_copy/webdirectory/google.owl')\n",
      "('german_datasets_copy/webdirectory/dmoz.owl', 'german_datasets_copy/webdirectory/web.owl')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b5d1427ed3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                      \u001b[0;34m\" file:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data/Vivek/IBM/IBM-Internship/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/ false\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_command\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mpred_logmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/logmap2_mappings.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgt_mappings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arr = [('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/google.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/web.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl')]\n",
    "\n",
    "for ont_pair in arr:\n",
    "    print (ont_pair)\n",
    "    a, b, c = ont_pair[0], ont_pair[1], ont_pair[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"-\" + ont_pair[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    !rm -rf $c\n",
    "    os.mkdir(c)\n",
    "    java_command = \"java -jar logmap-matcher/target/logmap-matcher-4.0.jar MATCHER file:\" +  os.path.abspath(a) + \\\n",
    "                     \" file:\" + os.path.abspath(b) + \" \" + \"/data/Vivek/IBM/IBM-Internship/\" + c + \"/ false\"\n",
    "    process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "pred_logmap = [[el.split(\"/\")[-1] for el in l.split(\"\\t\")[:-1]] for l in open(os.path.abspath(c + \"/\") + \"/logmap2_mappings.tsv\",  \"r\").read().split(\"\\n\")[:-1] if not l.startswith(\"Optional\")]\n",
    "gt_mappings = []\n",
    "for elem in pred_logmap:\n",
    "    gt_mappings.append(tuple([el.split(\"#\")[0].replace(\".v2\", \"\").rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"#\" + el.split(\"#\")[1] for el in elem]))\n",
    "data_orig = pickle.load(open(\"Input/data_webdir.pkl\", \"rb\"))[0]\n",
    "data_logmap = {}\n",
    "for key in data_orig:\n",
    "    data_logmap[key] = False\n",
    "s = set(list(data_logmap.keys()))\n",
    "gt_mappings = [tuple(pair) for pair in gt_mappings]\n",
    "for mapping in gt_mappings:\n",
    "    \n",
    "    if mapping in s:\n",
    "        data_logmap[mapping] = True\n",
    "    else:\n",
    "        mapping = tuple([el.replace(\",-\", \"_\") for el in mapping])\n",
    "        if mapping in s:\n",
    "            data_logmap[mapping] = True\n",
    "        else:\n",
    "            print (mapping)\n",
    "all_metrics = []\n",
    "def return_test_data(data, i):\n",
    "    data_t = {elem: data[elem] for elem in data if data[elem]}\n",
    "    data_f = {elem: data[elem] for elem in data if not data[elem]}\n",
    "\n",
    "    data_t_items = list(data_t.keys())\n",
    "    data_f_items = list(data_f.keys())\n",
    "\n",
    "    test_data_t = data_t_items[int((0.2*i)*len(data_t)):int((0.2*i + 0.2)*len(data_t))]\n",
    "    test_data_f = data_f_items[int((0.2*i)*len(data_f)):int((0.2*i + 0.2)*len(data_f))]\n",
    "    \n",
    "    test_data = {}\n",
    "    for elem in test_data_t:\n",
    "        test_data[elem] = True\n",
    "    for elem in test_data_f:\n",
    "        test_data[elem] = False\n",
    "    return test_data\n",
    "\n",
    "for i in range(5):\n",
    "    test_gt = return_test_data(data_orig, i)\n",
    "    test_logmap = {elem: data_logmap[elem] for elem in test_gt}\n",
    "    tp = len([elem for elem in test_gt if test_gt[elem] and test_logmap[elem]])\n",
    "    fp = len([elem for elem in test_logmap if not test_gt[elem] and test_logmap[elem]])\n",
    "    fn = len([elem for elem in test_logmap if test_gt[elem] and not test_logmap[elem]])\n",
    "    \n",
    "    try:\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "        f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        continue\n",
    "    all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    \n",
    "np.mean(all_metrics, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
