{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"reference-alignment/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "        \n",
    "reference_alignments = load_alignments(alignment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(a,b,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True):\n",
    "        if union_flag == 0:\n",
    "            return self.triples\n",
    "        else:\n",
    "            return self.parse_triples(union_flag = 1, subclass_of = False)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  834\n"
     ]
    }
   ],
   "source": [
    "# Extracting USE embeddings\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0).lower() for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples(subclass_of=False))))\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "\n",
    "inp = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+\")\n",
    "X = vectorizer.fit_transform(inp)\n",
    "word2idx_tfidf = {word: i for (i, word)  in enumerate(vectorizer.get_feature_names())}\n",
    "entity2idx_tfidf = {word.split(\"#\")[1]: i for (i, word)  in enumerate(extracted_elems)}\n",
    "\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "embeds = extractUSEEmbeddings(inp)\n",
    "embeddings = dict(zip(extracted_elems, embeds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type storage\n",
    "\n",
    "types_dict = {}\n",
    "\n",
    "def get_tfidf_score(word, phrase):\n",
    "    return np.sum([X[entity2idx_tfidf[phrase]][:,word2idx_tfidf[word]][0,0] for word in parse(phrase)])\n",
    "    \n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    \n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "\n",
    "    for entity in entities:\n",
    "        types_dict[entity] = {\"type\": \"entity\"}\n",
    "    for prop in props:\n",
    "        types_dict[prop] = {\"type\": \"property\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    all_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in mappings])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "\n",
    "data = {}\n",
    "for mapping in all_mappings:\n",
    "    if mapping in gt_mappings:\n",
    "        data[(mapping[0], mapping[1])] = True\n",
    "    else:\n",
    "        data[(mapping[0], mapping[1])] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics\n",
    "    all_results = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        batch_size = min(batch_size, len(test_data_t))\n",
    "        num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "        batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "        \n",
    "        print (\"F batch size: \", batch_size_f)\n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "            \n",
    "            pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "            neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])\n",
    "            print (\"Inputs len: \", len(inputs))\n",
    "            targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "\n",
    "            indices = np.random.permutation(inputs.shape[0])\n",
    "            inputs, targets = inputs[indices], targets[indices]\n",
    "            inputs = torch.LongTensor(list(zip(*inputs)))\n",
    "            targets = torch.LongTensor(targets)\n",
    "\n",
    "            outputs = model(inputs, 1)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "#             print ([(el, targets[i]) for i,el in enumerate(outputs) if targets[i]])\n",
    "            print (\"Targets: \", targets)\n",
    "#             write ((\"Outputs initially: \", str([str(s) for s in outputs])))\n",
    "#             outputs /= torch.sum(outputs, dim=1).view(-1, 1)\n",
    "#             write ((\"Outputs Finally: \", str([str(s) for s in outputs])))\n",
    "#             outputs = [el[1].item() for el in outputs]\n",
    "            \n",
    "            \n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inputs.numpy()[0][idx]]\n",
    "                ent2 = emb_indexer_inv[inputs.numpy()[1][idx]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "#         all_results = OrderedDict(sorted(all_results.items(), key=lambda x: x[0], reverse=True))\n",
    "#         filtered_results = dict()\n",
    "#         entities_to_assign = set([el[0] for el in list(all_results.keys())])\n",
    "#         for pair in all_results:\n",
    "#             if pair[0] in entities_to_assign:\n",
    "#                 filtered_results[pair] = all_results[pair]\n",
    "#                 entities_to_assign.remove(pair[0])\n",
    "#         filtered_results = OrderedDict(sorted(filtered_results.items(), key=lambda x: x[1][0], reverse=True))\n",
    "#         \n",
    "        min_val = np.min([el[0] for el in list(all_results.values())])\n",
    "#         max_val = np.max([el[0] for el in list(all_results.values())])\n",
    "#         normalized_results = {}\n",
    "#         for key,val in all_results.items():\n",
    "#             tmp = (np.array(val[0]) - min_val) / (max_val - min_val)\n",
    "#             normalized_results[key] = (tmp[1], val[1])\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.01\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.01\n",
    "        print (\"Low:\", low_threshold, \"High:\", high_threshold)\n",
    "        for j,threshold in enumerate(np.arange(low_threshold, high_threshold, 0.01)):\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "        \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        all_metrics.append((opt_threshold, optimum_metrics))\n",
    "        \n",
    "    print (\"Final Results: \", np.mean([el[1] for el in all_metrics], axis=0))\n",
    "    print (\"Best threshold: \", all_metrics[np.argmax([el[1][2] for el in all_metrics])][0])\n",
    "    return all_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(elem):\n",
    "    f = open(\"Logs\", \"a+\")\n",
    "    if type(elem) == list or type(elem) == tuple:\n",
    "        string = str(\"\\n\".join([str(s) for s in elem]))\n",
    "    else:\n",
    "        string = str(elem)\n",
    "    f.write(\"\\n\"+string)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    \n",
    "        self.name_embedding = nn.Embedding(len(embeddings), 512)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.require_grad = False\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.layer1 = nn.Bilinear(512, 512, 2)\n",
    "        self.layer2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, inputs, epoch):\n",
    "        results = []\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            x = self.layer2(x)\n",
    "            x = F.dropout(x, p=0.3)\n",
    "            results.append(x)\n",
    "#             print (\"Embeddings\", x)\n",
    "#         x = results[0] @ results[1].T\n",
    "#         print (x)\n",
    "#         x = torch.mean(x, axis=1)\n",
    "#         if epoch == num_epochs -1:\n",
    "#             print (\"zipped:\", list(zip(results[0].detach().numpy(), results[1].detach().numpy())))\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "#         x = self.layer1(results[0], results[1])\n",
    "#         print (x.shape)\n",
    "#         x = F.log_softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "name 'test_onto' is parameter and global (<ipython-input-134-ac37d05ced15>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-134-ac37d05ced15>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    def generate_data(elem_tuple):\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m name 'test_onto' is parameter and global\n"
     ]
    }
   ],
   "source": [
    "emb_indexer = {word: i for i, word in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: word for i, word in enumerate(list(embeddings.keys()))}\n",
    "emb_vals = list(embeddings.values())\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs = np.array([generate_data(elem) for elem in list(elems)])\n",
    "    targets = np.array([target for i in range(len(elems))])\n",
    "    return inputs, targets\n",
    "    \n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "# data = OrderedDict(sorted(data.items(), key=lambda x: x[0]))\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    \n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if data[key]]\n",
    "    train_data_f = [key for key in train_data if not data[key]]\n",
    "    train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 200\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    batch_size = min(batch_size, len(train_data_t))\n",
    "    num_batches = int(ceil(len(train_data_t)/batch_size))\n",
    "    batch_size_f = int(ceil(len(train_data_f)/num_batches))\n",
    "    \n",
    "    model = SiameseNetwork()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        \n",
    "#         indices = np.random.permutation(len(inputs_pos) + len(inputs_neg))\n",
    "        \n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "        \n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "        \n",
    "        \n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "#             print (inputs.shape)\n",
    "            inp_elems = torch.LongTensor(inputs.T)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "#             print (targ_elems)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inp_elems, epoch)\n",
    "#             print (outputs)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%10 == 0:\n",
    "#                 print (\"Outupts: \", list(zip(outputs.detach().numpy(), targ_elems.detach().numpy())))\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    test_data_t = [key for key in test_data if data[key]]\n",
    "    test_data_f = [key for key in test_data if not data[key]]\n",
    "    \n",
    "    res = greedy_matching()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs len:  4182\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "batch_start = batch_idx * batch_size\n",
    "batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "batch_start_f = batch_idx * batch_size_f\n",
    "batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "optimizer.zero_grad()\n",
    "\n",
    "inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])\n",
    "print (\"Inputs len: \", len(inputs))\n",
    "targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "\n",
    "indices = np.random.permutation(inputs.shape[0])\n",
    "inputs, targets = inputs[indices], targets[indices]\n",
    "inputs = torch.LongTensor(list(zip(*inputs)))\n",
    "targets = torch.LongTensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.unsqueeze(torch.LongTensor(generate_data(('conference#Review_expertise', 'edas#MeetingRoomPlace'))), 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = min(batch_size, len(test_data_t))\n",
    "num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "batch_size_f = int(ceil(len(test_data_f)/num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4143, 29262)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([el for el in res if res[el][0] > 0.99]), len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conference#Information_for_participants', 'ekaw#Programme_Brochure'),\n",
       " ('conference#Person', 'ekaw#Person'),\n",
       " ('conference#Tutorial', 'ekaw#Tutorial'),\n",
       " ('conference#Review', 'ekaw#Review'),\n",
       " ('conference#has_a_review', 'ekaw#hasReview'),\n",
       " ('conference#Workshop', 'ekaw#Workshop'),\n",
       " ('conference#Late_paid_applicant', 'ekaw#Late-Registered_Participant'),\n",
       " ('conference#Early_paid_applicant', 'ekaw#Early-Registered_Participant'),\n",
       " ('conference#Organization', 'ekaw#Organisation'),\n",
       " ('conference#Track-workshop_chair', 'ekaw#Workshop_Chair'),\n",
       " ('conference#Abstract', 'ekaw#Abstract'),\n",
       " ('conference#Conference_proceedings', 'ekaw#Conference_Proceedings'),\n",
       " ('conference#Conference_volume', 'ekaw#Conference'),\n",
       " ('conference#Rejected_contribution', 'ekaw#Rejected_Paper'),\n",
       " ('conference#Poster', 'ekaw#Poster_Paper'),\n",
       " ('conference#Track', 'ekaw#Track'),\n",
       " ('conference#Topic', 'ekaw#Research_Topic'),\n",
       " ('conference#Conference_www', 'ekaw#Web_Site'),\n",
       " ('conference#Invited_speaker', 'ekaw#Invited_Speaker'),\n",
       " ('conference#contributes', 'ekaw#authorOf'),\n",
       " ('conference#Accepted_contribution', 'ekaw#Accepted_Paper'),\n",
       " ('conference#Conference_document', 'ekaw#Document'),\n",
       " ('conference#Reviewed_contribution', 'ekaw#Evaluated_Paper'),\n",
       " ('conference#Submitted_contribution', 'ekaw#Submitted_Paper'),\n",
       " ('conference#Regular_author', 'ekaw#Paper_Author'),\n",
       " ('confOf#Tutorial', 'ekaw#Tutorial'),\n",
       " ('confOf#Poster', 'ekaw#Poster_Paper'),\n",
       " ('confOf#Social_event', 'ekaw#Social_Event'),\n",
       " ('confOf#Person', 'ekaw#Person'),\n",
       " ('confOf#Working_event', 'ekaw#Scientific_Event'),\n",
       " ('confOf#Conference', 'ekaw#Conference'),\n",
       " ('confOf#Author', 'ekaw#Paper_Author'),\n",
       " ('confOf#Banquet', 'ekaw#Conference_Banquet'),\n",
       " ('confOf#Workshop', 'ekaw#Workshop'),\n",
       " ('confOf#Topic', 'ekaw#Research_Topic'),\n",
       " ('confOf#Contribution', 'ekaw#Paper'),\n",
       " ('confOf#Participant', 'ekaw#Conference_Participant'),\n",
       " ('confOf#Chair_PC', 'ekaw#PC_Chair'),\n",
       " ('confOf#Organization', 'ekaw#Organisation'),\n",
       " ('confOf#Student', 'ekaw#Student'),\n",
       " ('confOf#University', 'ekaw#University'),\n",
       " ('confOf#Trip', 'ekaw#Conference_Trip'),\n",
       " ('confOf#Member_PC', 'ekaw#PC_Member'),\n",
       " ('confOf#Scholar', 'ekaw#Student'),\n",
       " ('confOf#Event', 'ekaw#Event'),\n",
       " ('ekaw#Conference', 'sigkdd#Conference'),\n",
       " ('ekaw#Person', 'sigkdd#Person'),\n",
       " ('ekaw#Paper', 'sigkdd#Paper'),\n",
       " ('ekaw#Review', 'sigkdd#Review'),\n",
       " ('ekaw#Invited_Speaker', 'sigkdd#Invited_Speaker'),\n",
       " ('ekaw#OC_Member', 'sigkdd#Organizing_Committee_member'),\n",
       " ('ekaw#Abstract', 'sigkdd#Abstract'),\n",
       " ('ekaw#PC_Chair', 'sigkdd#Program_Chair'),\n",
       " ('ekaw#Paper_Author', 'sigkdd#Author'),\n",
       " ('ekaw#Document', 'sigkdd#Document'),\n",
       " ('ekaw#Location', 'sigkdd#Place'),\n",
       " ('edas#Place', 'sigkdd#Place'),\n",
       " ('edas#hasCostAmount', 'sigkdd#Price'),\n",
       " ('edas#Person', 'sigkdd#Person'),\n",
       " ('edas#hasName', 'sigkdd#Name_of_conference'),\n",
       " ('edas#ConferenceVenuePlace', 'sigkdd#Conference_hall'),\n",
       " ('edas#Author', 'sigkdd#Author'),\n",
       " ('edas#AccommodationPlace', 'sigkdd#Hotel'),\n",
       " ('edas#startDate', 'sigkdd#Start_of_conference'),\n",
       " ('edas#ConferenceChair', 'sigkdd#General_Chair'),\n",
       " ('edas#Conference', 'sigkdd#Conference'),\n",
       " ('edas#endDate', 'sigkdd#End_of_conference'),\n",
       " ('edas#Review', 'sigkdd#Review'),\n",
       " ('edas#Document', 'sigkdd#Document'),\n",
       " ('edas#Paper', 'sigkdd#Paper'),\n",
       " ('edas#Attendee', 'sigkdd#Listener'),\n",
       " ('confOf#Person', 'sigkdd#Person'),\n",
       " ('confOf#Member_PC', 'sigkdd#Program_Committee_member'),\n",
       " ('confOf#hasEmail', 'sigkdd#E-mail'),\n",
       " ('confOf#Author', 'sigkdd#Author'),\n",
       " ('confOf#Conference', 'sigkdd#Conference'),\n",
       " ('confOf#Chair_PC', 'sigkdd#Program_Chair'),\n",
       " ('confOf#Paper', 'sigkdd#Paper'),\n",
       " ('iasted#Place', 'sigkdd#Place'),\n",
       " ('iasted#Review', 'sigkdd#Review'),\n",
       " ('iasted#Student_registration_fee', 'sigkdd#Registration_Student'),\n",
       " ('iasted#Fee', 'sigkdd#Fee'),\n",
       " ('iasted#Registration_fee', 'sigkdd#Registration_fee'),\n",
       " ('iasted#Sponsor', 'sigkdd#Sponzor'),\n",
       " ('iasted#Deadline_for_notification_of_acceptance',\n",
       "  'sigkdd#Deadline_Author_notification'),\n",
       " ('iasted#Nonmember_registration_fee', 'sigkdd#Registration_Non-Member'),\n",
       " ('iasted#Author', 'sigkdd#Author'),\n",
       " ('iasted#Listener', 'sigkdd#Listener'),\n",
       " ('iasted#Main_office', 'sigkdd#Main_office'),\n",
       " ('iasted#Conference_hall', 'sigkdd#Conference_hall'),\n",
       " ('iasted#Person', 'sigkdd#Person'),\n",
       " ('iasted#Deadline', 'sigkdd#Deadline'),\n",
       " ('iasted#Speaker', 'sigkdd#Speaker'),\n",
       " ('confOf#Event', 'iasted#Activity'),\n",
       " ('confOf#Author', 'iasted#Author'),\n",
       " ('confOf#Person', 'iasted#Person'),\n",
       " ('confOf#Banquet', 'iasted#Dinner_banquet'),\n",
       " ('confOf#Administrative_event', 'iasted#Activity_before_conference'),\n",
       " ('confOf#Reception', 'iasted#Coctail_reception'),\n",
       " ('confOf#City', 'iasted#City'),\n",
       " ('confOf#Tutorial', 'iasted#Tutorial'),\n",
       " ('confOf#Country', 'iasted#State'),\n",
       " ('conference#Passive_conference_participant', 'iasted#Listener'),\n",
       " ('conference#Active_conference_participant', 'iasted#Speaker'),\n",
       " ('conference#Reviewer', 'iasted#Reviewer'),\n",
       " ('conference#Person', 'iasted#Person'),\n",
       " ('conference#Regular_author', 'iasted#Author'),\n",
       " ('conference#Conference_fees', 'iasted#Fee'),\n",
       " ('conference#Tutorial', 'iasted#Tutorial'),\n",
       " ('conference#contributes', 'iasted#write'),\n",
       " ('conference#Review', 'iasted#Review'),\n",
       " ('conference#Conference_document', 'iasted#Document'),\n",
       " ('conference#Conference_part', 'iasted#Conference_activity'),\n",
       " ('conference#Submitted_contribution', 'iasted#Submission'),\n",
       " ('conference#Camera_ready_contribution', 'iasted#Final_manuscript'),\n",
       " ('conference#Conference_proceedings', 'iasted#Publication'),\n",
       " ('confOf#Trip', 'edas#Excursion'),\n",
       " ('confOf#Social_event', 'edas#SocialEvent'),\n",
       " ('confOf#reviewes', 'edas#isReviewing'),\n",
       " ('confOf#Organization', 'edas#Organization'),\n",
       " ('confOf#writtenBy', 'edas#isWrittenBy'),\n",
       " ('confOf#Working_event', 'edas#AcademicEvent'),\n",
       " ('confOf#Reception', 'edas#Reception'),\n",
       " ('confOf#hasSurname', 'edas#hasLastName'),\n",
       " ('confOf#Workshop', 'edas#Workshop'),\n",
       " ('confOf#Author', 'edas#Author'),\n",
       " ('confOf#hasFirstName', 'edas#hasFirstName'),\n",
       " ('confOf#Event', 'edas#ConferenceEvent'),\n",
       " ('confOf#Topic', 'edas#Topic'),\n",
       " ('confOf#Country', 'edas#Country'),\n",
       " ('confOf#Participant', 'edas#Attendee'),\n",
       " ('confOf#Person', 'edas#Person'),\n",
       " ('confOf#Member_PC', 'edas#TPCMember'),\n",
       " ('confOf#Paper', 'edas#Paper'),\n",
       " ('confOf#writes', 'edas#hasRelatedPaper'),\n",
       " ('edas#Place', 'iasted#Place'),\n",
       " ('edas#SessionChair', 'iasted#Session_chair'),\n",
       " ('edas#Author', 'iasted#Author'),\n",
       " ('edas#Document', 'iasted#Document'),\n",
       " ('edas#Country', 'iasted#State'),\n",
       " ('edas#WelcomeTalk', 'iasted#Welcome_address'),\n",
       " ('edas#Sponsorship', 'iasted#Sponzorship'),\n",
       " ('edas#Reviewer', 'iasted#Reviewer'),\n",
       " ('edas#Attendee', 'iasted#Delegate'),\n",
       " ('edas#ConferenceDinner', 'iasted#Dinner_banquet'),\n",
       " ('edas#SocialEvent', 'iasted#Social_program'),\n",
       " ('edas#CoffeeBreak', 'iasted#Coffee_break'),\n",
       " ('edas#Review', 'iasted#Review'),\n",
       " ('edas#Reception', 'iasted#Coctail_reception'),\n",
       " ('edas#ConferenceEvent', 'iasted#Conference_activity'),\n",
       " ('edas#SlideSet', 'iasted#Transparency'),\n",
       " ('edas#Paper', 'iasted#Submission'),\n",
       " ('edas#ConferenceVenuePlace', 'iasted#Conference_building'),\n",
       " ('edas#DiningPlace', 'iasted#Conference_restaurant'),\n",
       " ('conference#Person', 'edas#Person'),\n",
       " ('conference#Conference_participant', 'edas#Attendee'),\n",
       " ('conference#Organization', 'edas#Organization'),\n",
       " ('conference#Reviewer', 'edas#Reviewer'),\n",
       " ('conference#has_the_first_name', 'edas#hasFirstName'),\n",
       " ('conference#Conference_part', 'edas#ConferenceEvent'),\n",
       " ('conference#Workshop', 'edas#Workshop'),\n",
       " ('conference#Conference_document', 'edas#Document'),\n",
       " ('conference#Paper', 'edas#Paper'),\n",
       " ('conference#has_a_review_expertise', 'edas#hasRating'),\n",
       " ('conference#has_the_last_name', 'edas#hasLastName'),\n",
       " ('conference#Review', 'edas#Review'),\n",
       " ('conference#Conference_volume', 'edas#Conference'),\n",
       " ('conference#Rejected_contribution', 'edas#RejectedPaper'),\n",
       " ('conference#Topic', 'edas#Topic'),\n",
       " ('conference#Accepted_contribution', 'edas#AcceptedPaper'),\n",
       " ('conference#Regular_author', 'edas#Author'),\n",
       " ('cmt#Document', 'ekaw#Document'),\n",
       " ('cmt#ConferenceMember', 'ekaw#Conference_Participant'),\n",
       " ('cmt#Author', 'ekaw#Paper_Author'),\n",
       " ('cmt#writtenBy', 'ekaw#reviewWrittenBy'),\n",
       " ('cmt#hasBeenAssigned', 'ekaw#reviewerOfPaper'),\n",
       " ('cmt#Person', 'ekaw#Person'),\n",
       " ('cmt#Conference', 'ekaw#Conference'),\n",
       " ('cmt#assignedTo', 'ekaw#hasReviewer'),\n",
       " ('cmt#Review', 'ekaw#Review'),\n",
       " ('cmt#Paper', 'ekaw#Paper'),\n",
       " ('cmt#PaperFullVersion', 'ekaw#Regular_Paper'),\n",
       " ('cmt#ProgramCommitteeChair', 'confOf#Chair_PC'),\n",
       " ('cmt#writePaper', 'confOf#writes'),\n",
       " ('cmt#Author', 'confOf#Author'),\n",
       " ('cmt#ConferenceMember', 'confOf#Member'),\n",
       " ('cmt#Administrator', 'confOf#Administrator'),\n",
       " ('cmt#title', 'confOf#hasTitle'),\n",
       " ('cmt#SubjectArea', 'confOf#Topic'),\n",
       " ('cmt#PaperFullVersion', 'confOf#Paper'),\n",
       " ('cmt#hasBeenAssigned', 'confOf#reviewes'),\n",
       " ('cmt#hasAuthor', 'confOf#writtenBy'),\n",
       " ('cmt#Conference', 'confOf#Conference'),\n",
       " ('cmt#ProgramCommitteeMember', 'confOf#Member_PC'),\n",
       " ('cmt#hasSubjectArea', 'confOf#dealsWith'),\n",
       " ('cmt#Person', 'confOf#Person'),\n",
       " ('cmt#Paper', 'confOf#Contribution'),\n",
       " ('cmt#email', 'confOf#hasEmail'),\n",
       " ('cmt#Person', 'edas#Person'),\n",
       " ('cmt#Conference', 'edas#Conference'),\n",
       " ('cmt#Author', 'edas#Author'),\n",
       " ('cmt#Reviewer', 'edas#Reviewer'),\n",
       " ('cmt#hasConferenceMember', 'edas#hasMember'),\n",
       " ('cmt#memberOfConference', 'edas#isMemberOf'),\n",
       " ('cmt#ConferenceChair', 'edas#ConferenceChair'),\n",
       " ('cmt#Review', 'edas#Review'),\n",
       " ('cmt#Document', 'edas#Document'),\n",
       " ('cmt#Paper', 'edas#Paper'),\n",
       " ('cmt#hasAuthor', 'edas#isWrittenBy'),\n",
       " ('cmt#hasBeenAssigned', 'edas#isReviewing'),\n",
       " ('cmt#assignedTo', 'edas#isReviewedBy'),\n",
       " ('conference#Abstract', 'sigkdd#Abstract'),\n",
       " ('conference#Invited_speaker', 'sigkdd#Invited_Speaker'),\n",
       " ('conference#Regular_author', 'sigkdd#Author'),\n",
       " ('conference#Review', 'sigkdd#Review'),\n",
       " ('conference#Program_committee', 'sigkdd#Program_Committee'),\n",
       " ('conference#Conference_volume', 'sigkdd#Conference'),\n",
       " ('conference#Conference_fees', 'sigkdd#Fee'),\n",
       " ('conference#has_an_email', 'sigkdd#E-mail'),\n",
       " ('conference#Paper', 'sigkdd#Paper'),\n",
       " ('conference#Organizing_committee', 'sigkdd#Organizing_Committee'),\n",
       " ('conference#Person', 'sigkdd#Person'),\n",
       " ('conference#Committee', 'sigkdd#Committee'),\n",
       " ('conference#is_given_by', 'sigkdd#presentationed_by'),\n",
       " ('conference#gives_presentations', 'sigkdd#presentation'),\n",
       " ('conference#Conference_document', 'sigkdd#Document'),\n",
       " ('cmt#Conference', 'sigkdd#Conference'),\n",
       " ('cmt#Paper', 'sigkdd#Paper'),\n",
       " ('cmt#ProgramCommitteeMember', 'sigkdd#Program_Committee_member'),\n",
       " ('cmt#Document', 'sigkdd#Document'),\n",
       " ('cmt#ConferenceChair', 'sigkdd#General_Chair'),\n",
       " ('cmt#email', 'sigkdd#E-mail'),\n",
       " ('cmt#Review', 'sigkdd#Review'),\n",
       " ('cmt#ProgramCommittee', 'sigkdd#Program_Committee'),\n",
       " ('cmt#ProgramCommitteeChair', 'sigkdd#Program_Chair'),\n",
       " ('cmt#Author', 'sigkdd#Author'),\n",
       " ('cmt#submitPaper', 'sigkdd#submit'),\n",
       " ('cmt#Person', 'sigkdd#Person'),\n",
       " ('conference#Conference_participant', 'confOf#Participant'),\n",
       " ('conference#has_an_email', 'confOf#hasEmail'),\n",
       " ('conference#Poster', 'confOf#Poster'),\n",
       " ('conference#Organization', 'confOf#Organization'),\n",
       " ('conference#Topic', 'confOf#Topic'),\n",
       " ('conference#Workshop', 'confOf#Workshop'),\n",
       " ('conference#Paper', 'confOf#Paper'),\n",
       " ('conference#Person', 'confOf#Person'),\n",
       " ('conference#Conference_contribution', 'confOf#Contribution'),\n",
       " ('conference#Tutorial', 'confOf#Tutorial'),\n",
       " ('conference#Conference_volume', 'confOf#Conference'),\n",
       " ('conference#has_a_track-workshop-tutorial_topic', 'confOf#hasTopic'),\n",
       " ('conference#Regular_author', 'confOf#Author'),\n",
       " ('conference#has_the_last_name', 'confOf#hasSurname'),\n",
       " ('conference#has_the_first_name', 'confOf#hasFirstName'),\n",
       " ('edas#ConferenceDinner', 'ekaw#Conference_Banquet'),\n",
       " ('edas#AcademicEvent', 'ekaw#Scientific_Event'),\n",
       " ('edas#AcceptedPaper', 'ekaw#Accepted_Paper'),\n",
       " ('edas#isReviewedBy', 'ekaw#hasReviewer'),\n",
       " ('edas#Place', 'ekaw#Location'),\n",
       " ('edas#AcademiaOrganization', 'ekaw#Academic_Institution'),\n",
       " ('edas#SocialEvent', 'ekaw#Social_Event'),\n",
       " ('edas#isReviewing', 'ekaw#reviewerOfPaper'),\n",
       " ('edas#Organization', 'ekaw#Organisation'),\n",
       " ('edas#Author', 'ekaw#Paper_Author'),\n",
       " ('edas#isLocationOf', 'ekaw#locationOf'),\n",
       " ('edas#Topic', 'ekaw#Research_Topic'),\n",
       " ('edas#Document', 'ekaw#Document'),\n",
       " ('edas#RejectedPaper', 'ekaw#Rejected_Paper'),\n",
       " ('edas#ConferenceEvent', 'ekaw#Event'),\n",
       " ('edas#SessionChair', 'ekaw#Session_Chair'),\n",
       " ('edas#Person', 'ekaw#Person'),\n",
       " ('edas#Programme', 'ekaw#Programme_Brochure'),\n",
       " ('edas#Review', 'ekaw#Review'),\n",
       " ('edas#Workshop', 'ekaw#Workshop'),\n",
       " ('edas#Paper', 'ekaw#Paper'),\n",
       " ('edas#Attendee', 'ekaw#Conference_Participant'),\n",
       " ('edas#hasLocation', 'ekaw#heldIn'),\n",
       " ('cmt#Conference', 'conference#Conference_volume'),\n",
       " ('cmt#Preference', 'conference#Review_preference'),\n",
       " ('cmt#Author', 'conference#Regular_author'),\n",
       " ('cmt#Person', 'conference#Person'),\n",
       " ('cmt#email', 'conference#has_an_email'),\n",
       " ('cmt#Co-author', 'conference#Contribution_co-author'),\n",
       " ('cmt#PaperAbstract', 'conference#Abstract'),\n",
       " ('cmt#Document', 'conference#Conference_document'),\n",
       " ('cmt#Review', 'conference#Review'),\n",
       " ('cmt#Conference', 'conference#Conference'),\n",
       " ('cmt#ProgramCommittee', 'conference#Program_committee'),\n",
       " ('cmt#Chairman', 'conference#Chair'),\n",
       " ('cmt#SubjectArea', 'conference#Topic'),\n",
       " ('cmt#assignedByReviewer', 'conference#invited_by'),\n",
       " ('cmt#assignExternalReviewer', 'conference#invites_co-reviewers'),\n",
       " ('cmt#Author', 'iasted#Author'),\n",
       " ('cmt#Review', 'iasted#Review'),\n",
       " ('cmt#Person', 'iasted#Person'),\n",
       " ('cmt#Reviewer', 'iasted#Reviewer'),\n",
       " ('ekaw#Location', 'iasted#Place'),\n",
       " ('ekaw#Document', 'iasted#Document'),\n",
       " ('ekaw#Event', 'iasted#Activity'),\n",
       " ('ekaw#Person', 'iasted#Person'),\n",
       " ('ekaw#Paper_Author', 'iasted#Author'),\n",
       " ('ekaw#Session', 'iasted#Session'),\n",
       " ('ekaw#Review', 'iasted#Review'),\n",
       " ('ekaw#Conference_Banquet', 'iasted#Dinner_banquet'),\n",
       " ('ekaw#Tutorial', 'iasted#Tutorial'),\n",
       " ('ekaw#Session_Chair', 'iasted#Session_chair')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el for el in gt_mappings if el not in test_data_t and el\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
