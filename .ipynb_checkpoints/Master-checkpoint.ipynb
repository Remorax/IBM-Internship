{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"reference-alignment/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "        \n",
    "reference_alignments = load_alignments(alignment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(a,b,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True):\n",
    "        if union_flag == 0:\n",
    "            return self.triples\n",
    "        else:\n",
    "            return self.parse_triples(union_flag = 1, subclass_of = False)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  834\n"
     ]
    }
   ],
   "source": [
    "# Extracting USE embeddings\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0).lower() for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples(subclass_of=False))))\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "\n",
    "inp = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+\")\n",
    "X = vectorizer.fit_transform(inp)\n",
    "word2idx_tfidf = {word: i for (i, word)  in enumerate(vectorizer.get_feature_names())}\n",
    "entity2idx_tfidf = {word.split(\"#\")[1]: i for (i, word)  in enumerate(extracted_elems)}\n",
    "\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "embeds = extractUSEEmbeddings(inp)\n",
    "embeddings = dict(zip(extracted_elems, embeds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type storage\n",
    "\n",
    "types_dict = {}\n",
    "\n",
    "def get_tfidf_score(word, phrase):\n",
    "    return np.sum([X[entity2idx_tfidf[phrase]][:,word2idx_tfidf[word]][0,0] for word in parse(phrase)])\n",
    "    \n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    \n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "\n",
    "    for entity in entities:\n",
    "        types_dict[entity] = {\"type\": \"entity\"}\n",
    "    for prop in props:\n",
    "        types_dict[prop] = {\"type\": \"property\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    all_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in mappings])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "\n",
    "data = {}\n",
    "for mapping in all_mappings:\n",
    "    if mapping in gt_mappings:\n",
    "        data[(mapping[0], mapping[1])] = True\n",
    "    else:\n",
    "        data[(mapping[0], mapping[1])] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics\n",
    "    all_results = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        batch_size = min(batch_size, len(test_data_t))\n",
    "        num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "        batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "        \n",
    "        print (\"F batch size: \", batch_size_f)\n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "            \n",
    "            pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "            neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])\n",
    "            print (\"Inputs len: \", len(inputs))\n",
    "            targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "\n",
    "            indices = np.random.permutation(inputs.shape[0])\n",
    "            inputs, targets = inputs[indices], targets[indices]\n",
    "            inputs = torch.LongTensor(list(zip(*inputs)))\n",
    "            targets = torch.LongTensor(targets)\n",
    "\n",
    "            outputs = model(inputs, 1)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "#             print ([(el, targets[i]) for i,el in enumerate(outputs) if targets[i]])\n",
    "            print (\"Targets: \", targets)\n",
    "#             write ((\"Outputs initially: \", str([str(s) for s in outputs])))\n",
    "#             outputs /= torch.sum(outputs, dim=1).view(-1, 1)\n",
    "#             write ((\"Outputs Finally: \", str([str(s) for s in outputs])))\n",
    "#             outputs = [el[1].item() for el in outputs]\n",
    "            \n",
    "            \n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inputs.numpy()[0][idx]]\n",
    "                ent2 = emb_indexer_inv[inputs.numpy()[1][idx]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "#         all_results = OrderedDict(sorted(all_results.items(), key=lambda x: x[0], reverse=True))\n",
    "#         filtered_results = dict()\n",
    "#         entities_to_assign = set([el[0] for el in list(all_results.keys())])\n",
    "#         for pair in all_results:\n",
    "#             if pair[0] in entities_to_assign:\n",
    "#                 filtered_results[pair] = all_results[pair]\n",
    "#                 entities_to_assign.remove(pair[0])\n",
    "#         filtered_results = OrderedDict(sorted(filtered_results.items(), key=lambda x: x[1][0], reverse=True))\n",
    "#         min_val = np.min([el[0] for el in list(all_results.values())])\n",
    "#         max_val = np.max([el[0] for el in list(all_results.values())])\n",
    "#         normalized_results = {}\n",
    "#         for key,val in all_results.items():\n",
    "#             tmp = (np.array(val[0]) - min_val) / (max_val - min_val)\n",
    "#             normalized_results[key] = (tmp[1], val[1])\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.01\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.01\n",
    "        print (\"Low:\", low_threshold, \"High:\", high_threshold)\n",
    "        for j,threshold in enumerate(np.arange(low_threshold, high_threshold, 0.01)):\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "        \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        all_metrics.append((opt_threshold, optimum_metrics))\n",
    "        \n",
    "    print (\"Final Results: \", np.mean([el[1] for el in all_metrics], axis=0))\n",
    "    print (\"Best threshold: \", all_metrics[np.argmax([el[1][2] for el in all_metrics])][0])\n",
    "    return all_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(elem):\n",
    "    f = open(\"Logs\", \"a+\")\n",
    "    if type(elem) == list or type(elem) == tuple:\n",
    "        string = str(\"\\n\".join([str(s) for s in elem]))\n",
    "    else:\n",
    "        string = str(elem)\n",
    "    f.write(\"\\n\"+string)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()    \n",
    "        self.name_embedding = nn.Embedding(len(embeddings), 512)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.require_grad = False\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.layer1 = nn.Bilinear(512, 512, 2)\n",
    "        self.layer2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, inputs, epoch):\n",
    "        results = []\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            x = self.layer2(x)\n",
    "            x = F.dropout(x, p=0.3)\n",
    "            results.append(x)\n",
    "#             print (\"Embeddings\", x)\n",
    "#         x = results[0] @ results[1].T\n",
    "#         print (x)\n",
    "#         x = torch.mean(x, axis=1)\n",
    "#         if epoch == num_epochs -1:\n",
    "#             print (\"zipped:\", list(zip(results[0].detach().numpy(), results[1].detach().numpy())))\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "#         x = self.layer1(results[0], results[1])\n",
    "#         print (x.shape)\n",
    "#         x = F.log_softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 122893\n",
      "Epoch: 0 Idx: 0 Loss: 0.13782654056655427\n",
      "Epoch: 0 Idx: 10 Loss: 0.13823447076052534\n",
      "Epoch: 0 Idx: 20 Loss: 0.10972667795453983\n",
      "Epoch: 1 Idx: 0 Loss: 0.14026473153276323\n",
      "Epoch: 1 Idx: 10 Loss: 0.12220597524303503\n",
      "Epoch: 1 Idx: 20 Loss: 0.06296249453623394\n",
      "Epoch: 2 Idx: 0 Loss: 0.07424661612317371\n",
      "Epoch: 2 Idx: 10 Loss: 0.13444311208609866\n",
      "Epoch: 2 Idx: 20 Loss: 0.08264135279965754\n",
      "Epoch: 3 Idx: 0 Loss: 0.09091659435289848\n",
      "Epoch: 3 Idx: 10 Loss: 0.13729659789922788\n",
      "Epoch: 3 Idx: 20 Loss: 0.051596235881551364\n",
      "Epoch: 4 Idx: 0 Loss: 0.11545552720024066\n",
      "Epoch: 4 Idx: 10 Loss: 0.08313278015188755\n",
      "Epoch: 4 Idx: 20 Loss: 0.06707026054125645\n",
      "Epoch: 5 Idx: 0 Loss: 0.06507038700933429\n",
      "Epoch: 5 Idx: 10 Loss: 0.07844332152888647\n",
      "Epoch: 5 Idx: 20 Loss: 0.10293178949548676\n",
      "Epoch: 6 Idx: 0 Loss: 0.07460529504571069\n",
      "Epoch: 6 Idx: 10 Loss: 0.07285317603345318\n",
      "Epoch: 6 Idx: 20 Loss: 0.10396858345042655\n",
      "Epoch: 7 Idx: 0 Loss: 0.05328237620808503\n",
      "Epoch: 7 Idx: 10 Loss: 0.043298450731167074\n",
      "Epoch: 7 Idx: 20 Loss: 0.07234823457358722\n",
      "Epoch: 8 Idx: 0 Loss: 0.08310549915280285\n",
      "Epoch: 8 Idx: 10 Loss: 0.0797551202542773\n",
      "Epoch: 8 Idx: 20 Loss: 0.07495223297501942\n",
      "Epoch: 9 Idx: 0 Loss: 0.06164773030743652\n",
      "Epoch: 9 Idx: 10 Loss: 0.06873002058473525\n",
      "Epoch: 9 Idx: 20 Loss: 0.10193427750947197\n",
      "Epoch: 10 Idx: 0 Loss: 0.07694774028043781\n",
      "Epoch: 10 Idx: 10 Loss: 0.08533947791521561\n",
      "Epoch: 10 Idx: 20 Loss: 0.12962672828141972\n",
      "Epoch: 11 Idx: 0 Loss: 0.09256186994464333\n",
      "Epoch: 11 Idx: 10 Loss: 0.09527925873006562\n",
      "Epoch: 11 Idx: 20 Loss: 0.0598563019026741\n",
      "Epoch: 12 Idx: 0 Loss: 0.061827282781301285\n",
      "Epoch: 12 Idx: 10 Loss: 0.1176527205833487\n",
      "Epoch: 12 Idx: 20 Loss: 0.0751742384375189\n",
      "Epoch: 13 Idx: 0 Loss: 0.07942769002692428\n",
      "Epoch: 13 Idx: 10 Loss: 0.05998813541117289\n",
      "Epoch: 13 Idx: 20 Loss: 0.06045068236705944\n",
      "Epoch: 14 Idx: 0 Loss: 0.04304365611608636\n",
      "Epoch: 14 Idx: 10 Loss: 0.11907389183384691\n",
      "Epoch: 14 Idx: 20 Loss: 0.08507883612852245\n",
      "Epoch: 15 Idx: 0 Loss: 0.06293219987824046\n",
      "Epoch: 15 Idx: 10 Loss: 0.0752641923342557\n",
      "Epoch: 15 Idx: 20 Loss: 0.07702988931902952\n",
      "Epoch: 16 Idx: 0 Loss: 0.0631622724467773\n",
      "Epoch: 16 Idx: 10 Loss: 0.08388370815981058\n",
      "Epoch: 16 Idx: 20 Loss: 0.07475947111955479\n",
      "Epoch: 17 Idx: 0 Loss: 0.09208508857732178\n",
      "Epoch: 17 Idx: 10 Loss: 0.08243647680962359\n",
      "Epoch: 17 Idx: 20 Loss: 0.04966463709933787\n",
      "Epoch: 18 Idx: 0 Loss: 0.033637688165518986\n",
      "Epoch: 18 Idx: 10 Loss: 0.06174380067941437\n",
      "Epoch: 18 Idx: 20 Loss: 0.06905909363338752\n",
      "Epoch: 19 Idx: 0 Loss: 0.15643987001104048\n",
      "Epoch: 19 Idx: 10 Loss: 0.04033126735556996\n",
      "Epoch: 19 Idx: 20 Loss: 0.05149982105336429\n",
      "Epoch: 20 Idx: 0 Loss: 0.0601751632356479\n",
      "Epoch: 20 Idx: 10 Loss: 0.07545084299349938\n",
      "Epoch: 20 Idx: 20 Loss: 0.06908008749844485\n",
      "Epoch: 21 Idx: 0 Loss: 0.08086652861538367\n",
      "Epoch: 21 Idx: 10 Loss: 0.05559674285111264\n",
      "Epoch: 21 Idx: 20 Loss: 0.0826534873333966\n",
      "Epoch: 22 Idx: 0 Loss: 0.13202652617514\n",
      "Epoch: 22 Idx: 10 Loss: 0.08042258281706008\n",
      "Epoch: 22 Idx: 20 Loss: 0.06490483146314095\n",
      "Epoch: 23 Idx: 0 Loss: 0.10649641414389366\n",
      "Epoch: 23 Idx: 10 Loss: 0.09174663125478566\n",
      "Epoch: 23 Idx: 20 Loss: 0.06443752248891603\n",
      "Epoch: 24 Idx: 0 Loss: 0.07219536272846691\n",
      "Epoch: 24 Idx: 10 Loss: 0.08798454086825976\n",
      "Epoch: 24 Idx: 20 Loss: 0.11417631080284249\n",
      "Epoch: 25 Idx: 0 Loss: 0.06576885074625981\n",
      "Epoch: 25 Idx: 10 Loss: 0.058652232274208085\n",
      "Epoch: 25 Idx: 20 Loss: 0.06777422488154058\n",
      "Epoch: 26 Idx: 0 Loss: 0.05091091237857169\n",
      "Epoch: 26 Idx: 10 Loss: 0.08247699988324544\n",
      "Epoch: 26 Idx: 20 Loss: 0.051671036613656976\n",
      "Epoch: 27 Idx: 0 Loss: 0.12308560336549386\n",
      "Epoch: 27 Idx: 10 Loss: 0.05186730282237186\n",
      "Epoch: 27 Idx: 20 Loss: 0.06363705819425933\n",
      "Epoch: 28 Idx: 0 Loss: 0.07237783365791625\n",
      "Epoch: 28 Idx: 10 Loss: 0.08072402580884182\n",
      "Epoch: 28 Idx: 20 Loss: 0.05240636191136975\n",
      "Epoch: 29 Idx: 0 Loss: 0.06604797467750749\n",
      "Epoch: 29 Idx: 10 Loss: 0.05669437786851121\n",
      "Epoch: 29 Idx: 20 Loss: 0.05072617316325158\n",
      "Epoch: 30 Idx: 0 Loss: 0.041026982542166524\n",
      "Epoch: 30 Idx: 10 Loss: 0.11410727420236311\n",
      "Epoch: 30 Idx: 20 Loss: 0.07856322262455842\n",
      "Epoch: 31 Idx: 0 Loss: 0.07274740364945712\n",
      "Epoch: 31 Idx: 10 Loss: 0.06005439064892616\n",
      "Epoch: 31 Idx: 20 Loss: 0.09309988192924248\n",
      "Epoch: 32 Idx: 0 Loss: 0.08425260047335813\n",
      "Epoch: 32 Idx: 10 Loss: 0.055262289328858724\n",
      "Epoch: 32 Idx: 20 Loss: 0.0456488729367234\n",
      "Epoch: 33 Idx: 0 Loss: 0.04468857976143058\n",
      "Epoch: 33 Idx: 10 Loss: 0.07093809000398318\n",
      "Epoch: 33 Idx: 20 Loss: 0.048920226410821964\n",
      "Epoch: 34 Idx: 0 Loss: 0.11220405299777605\n",
      "Epoch: 34 Idx: 10 Loss: 0.08556959531605779\n",
      "Epoch: 34 Idx: 20 Loss: 0.04142815170542257\n",
      "Epoch: 35 Idx: 0 Loss: 0.06837397903404274\n",
      "Epoch: 35 Idx: 10 Loss: 0.07297950105050209\n",
      "Epoch: 35 Idx: 20 Loss: 0.07562674394994498\n",
      "Epoch: 36 Idx: 0 Loss: 0.0843984957492368\n",
      "Epoch: 36 Idx: 10 Loss: 0.07036056638081054\n",
      "Epoch: 36 Idx: 20 Loss: 0.07130255086745127\n",
      "Epoch: 37 Idx: 0 Loss: 0.08439281377665817\n",
      "Epoch: 37 Idx: 10 Loss: 0.06851272920557364\n",
      "Epoch: 37 Idx: 20 Loss: 0.04191869653555601\n",
      "Epoch: 38 Idx: 0 Loss: 0.037480321549529266\n",
      "Epoch: 38 Idx: 10 Loss: 0.06838767154241648\n",
      "Epoch: 38 Idx: 20 Loss: 0.09692802537307127\n",
      "Epoch: 39 Idx: 0 Loss: 0.048269855341540116\n",
      "Epoch: 39 Idx: 10 Loss: 0.05208379713595679\n",
      "Epoch: 39 Idx: 20 Loss: 0.07067752643381628\n",
      "Epoch: 40 Idx: 0 Loss: 0.11897503094322807\n",
      "Epoch: 40 Idx: 10 Loss: 0.09799065308921605\n",
      "Epoch: 40 Idx: 20 Loss: 0.08025573775515475\n",
      "Epoch: 41 Idx: 0 Loss: 0.10927038898331057\n",
      "Epoch: 41 Idx: 10 Loss: 0.03552616503592358\n",
      "Epoch: 41 Idx: 20 Loss: 0.0642118492478527\n",
      "Epoch: 42 Idx: 0 Loss: 0.062057500674339613\n",
      "Epoch: 42 Idx: 10 Loss: 0.11101657824306102\n",
      "Epoch: 42 Idx: 20 Loss: 0.052590416614155436\n",
      "Epoch: 43 Idx: 0 Loss: 0.05701280196306953\n",
      "Epoch: 43 Idx: 10 Loss: 0.05401991347468952\n",
      "Epoch: 43 Idx: 20 Loss: 0.07257491409072352\n",
      "Epoch: 44 Idx: 0 Loss: 0.03941007711796042\n",
      "Epoch: 44 Idx: 10 Loss: 0.08100350255717025\n",
      "Epoch: 44 Idx: 20 Loss: 0.10461192888872317\n",
      "Epoch: 45 Idx: 0 Loss: 0.07996487213370933\n",
      "Epoch: 45 Idx: 10 Loss: 0.06022054551142715\n",
      "Epoch: 45 Idx: 20 Loss: 0.05605888996323294\n",
      "Epoch: 46 Idx: 0 Loss: 0.0993534496191942\n",
      "Epoch: 46 Idx: 10 Loss: 0.05598064060344354\n",
      "Epoch: 46 Idx: 20 Loss: 0.055537916588406844\n",
      "Epoch: 47 Idx: 0 Loss: 0.09809651636939144\n",
      "Epoch: 47 Idx: 10 Loss: 0.05759270941630762\n",
      "Epoch: 47 Idx: 20 Loss: 0.04965375030466593\n",
      "Epoch: 48 Idx: 0 Loss: 0.0447721691583606\n",
      "Epoch: 48 Idx: 10 Loss: 0.06706444969478506\n",
      "Epoch: 48 Idx: 20 Loss: 0.09175726399078246\n",
      "Epoch: 49 Idx: 0 Loss: 0.05566354993541717\n",
      "Epoch: 49 Idx: 10 Loss: 0.0940028630931404\n",
      "Epoch: 49 Idx: 20 Loss: 0.06806386378365045\n",
      "Epoch: 50 Idx: 0 Loss: 0.07740927662627847\n",
      "Epoch: 50 Idx: 10 Loss: 0.062315663097178196\n",
      "Epoch: 50 Idx: 20 Loss: 0.03335594819474305\n",
      "Epoch: 51 Idx: 0 Loss: 0.10903708455033942\n",
      "Epoch: 51 Idx: 10 Loss: 0.05946086597742456\n",
      "Epoch: 51 Idx: 20 Loss: 0.0792141223345489\n",
      "Epoch: 52 Idx: 0 Loss: 0.09281110312760846\n",
      "Epoch: 52 Idx: 10 Loss: 0.09811769100402336\n",
      "Epoch: 52 Idx: 20 Loss: 0.06977838599325385\n",
      "Epoch: 53 Idx: 0 Loss: 0.03910934478320816\n",
      "Epoch: 53 Idx: 10 Loss: 0.10611519922636832\n",
      "Epoch: 53 Idx: 20 Loss: 0.06078332530099212\n",
      "Epoch: 54 Idx: 0 Loss: 0.047722516706691094\n",
      "Epoch: 54 Idx: 10 Loss: 0.04606975174599104\n",
      "Epoch: 54 Idx: 20 Loss: 0.04335635098425586\n",
      "Epoch: 55 Idx: 0 Loss: 0.06195187676698392\n",
      "Epoch: 55 Idx: 10 Loss: 0.07506312147881057\n",
      "Epoch: 55 Idx: 20 Loss: 0.10508278033076908\n",
      "Epoch: 56 Idx: 0 Loss: 0.08347108743260064\n",
      "Epoch: 56 Idx: 10 Loss: 0.046610754833696436\n",
      "Epoch: 56 Idx: 20 Loss: 0.06713909152401944\n",
      "Epoch: 57 Idx: 0 Loss: 0.09408334212786246\n",
      "Epoch: 57 Idx: 10 Loss: 0.07822151542606523\n",
      "Epoch: 57 Idx: 20 Loss: 0.0666008390476154\n",
      "Epoch: 58 Idx: 0 Loss: 0.08183348570037496\n",
      "Epoch: 58 Idx: 10 Loss: 0.07650922021154816\n",
      "Epoch: 58 Idx: 20 Loss: 0.10037852532013455\n",
      "Epoch: 59 Idx: 0 Loss: 0.1458058268601191\n",
      "Epoch: 59 Idx: 10 Loss: 0.10732753566714415\n",
      "Epoch: 59 Idx: 20 Loss: 0.04738910275420663\n",
      "Epoch: 60 Idx: 0 Loss: 0.10651097321242264\n",
      "Epoch: 60 Idx: 10 Loss: 0.11060670721214219\n",
      "Epoch: 60 Idx: 20 Loss: 0.07395413555511345\n",
      "Epoch: 61 Idx: 0 Loss: 0.07013081683447121\n",
      "Epoch: 61 Idx: 10 Loss: 0.0828951271277822\n",
      "Epoch: 61 Idx: 20 Loss: 0.058185276610179214\n",
      "Epoch: 62 Idx: 0 Loss: 0.06156120517423992\n",
      "Epoch: 62 Idx: 10 Loss: 0.08470323031192668\n",
      "Epoch: 62 Idx: 20 Loss: 0.06756017557777869\n",
      "Epoch: 63 Idx: 0 Loss: 0.04143412711177079\n",
      "Epoch: 63 Idx: 10 Loss: 0.08244979984631023\n",
      "Epoch: 63 Idx: 20 Loss: 0.0630870382989859\n",
      "Epoch: 64 Idx: 0 Loss: 0.11793450248969131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 Idx: 10 Loss: 0.04106987108786425\n",
      "Epoch: 64 Idx: 20 Loss: 0.05203094635402398\n",
      "Epoch: 65 Idx: 0 Loss: 0.02824754470150212\n",
      "Epoch: 65 Idx: 10 Loss: 0.06617772585479319\n",
      "Epoch: 65 Idx: 20 Loss: 0.06500667169554701\n",
      "Epoch: 66 Idx: 0 Loss: 0.0659482351626178\n",
      "Epoch: 66 Idx: 10 Loss: 0.047471373176006\n",
      "Epoch: 66 Idx: 20 Loss: 0.046540279990178776\n",
      "Epoch: 67 Idx: 0 Loss: 0.06465544981753629\n",
      "Epoch: 67 Idx: 10 Loss: 0.05255734703068188\n",
      "Epoch: 67 Idx: 20 Loss: 0.07096457276613266\n",
      "Epoch: 68 Idx: 0 Loss: 0.04597616548653322\n",
      "Epoch: 68 Idx: 10 Loss: 0.08142003201446159\n",
      "Epoch: 68 Idx: 20 Loss: 0.04571177281118993\n",
      "Epoch: 69 Idx: 0 Loss: 0.10902060866090549\n",
      "Epoch: 69 Idx: 10 Loss: 0.09846910231909922\n",
      "Epoch: 69 Idx: 20 Loss: 0.07029956222840866\n",
      "Epoch: 70 Idx: 0 Loss: 0.09316659601909442\n",
      "Epoch: 70 Idx: 10 Loss: 0.1074940811032302\n",
      "Epoch: 70 Idx: 20 Loss: 0.0734464576491579\n",
      "Epoch: 71 Idx: 0 Loss: 0.06712483341456701\n",
      "Epoch: 71 Idx: 10 Loss: 0.11259943045296772\n",
      "Epoch: 71 Idx: 20 Loss: 0.07759437763086638\n",
      "Epoch: 72 Idx: 0 Loss: 0.08211656871283692\n",
      "Epoch: 72 Idx: 10 Loss: 0.0405223617405029\n",
      "Epoch: 72 Idx: 20 Loss: 0.07669365387873733\n",
      "Epoch: 73 Idx: 0 Loss: 0.0811018056664577\n",
      "Epoch: 73 Idx: 10 Loss: 0.06349405273000343\n",
      "Epoch: 73 Idx: 20 Loss: 0.06607002689336597\n",
      "Epoch: 74 Idx: 0 Loss: 0.04711373516595353\n",
      "Epoch: 74 Idx: 10 Loss: 0.0443344415646514\n",
      "Epoch: 74 Idx: 20 Loss: 0.05081640246607627\n",
      "Epoch: 75 Idx: 0 Loss: 0.08522099151779977\n",
      "Epoch: 75 Idx: 10 Loss: 0.06197291968780444\n",
      "Epoch: 75 Idx: 20 Loss: 0.07221198121131808\n",
      "Epoch: 76 Idx: 0 Loss: 0.11227088072073214\n",
      "Epoch: 76 Idx: 10 Loss: 0.11925117719944704\n",
      "Epoch: 76 Idx: 20 Loss: 0.08791475542532945\n",
      "Epoch: 77 Idx: 0 Loss: 0.051800583794067115\n",
      "Epoch: 77 Idx: 10 Loss: 0.10538024882896631\n",
      "Epoch: 77 Idx: 20 Loss: 0.05212797730893816\n",
      "Epoch: 78 Idx: 0 Loss: 0.031158646427742957\n",
      "Epoch: 78 Idx: 10 Loss: 0.054012063552225445\n",
      "Epoch: 78 Idx: 20 Loss: 0.05727593670363036\n",
      "Epoch: 79 Idx: 0 Loss: 0.09124546647035403\n",
      "Epoch: 79 Idx: 10 Loss: 0.05104256796093252\n",
      "Epoch: 79 Idx: 20 Loss: 0.11688182187542699\n",
      "Epoch: 80 Idx: 0 Loss: 0.08183659733623165\n",
      "Epoch: 80 Idx: 10 Loss: 0.07493823351670356\n",
      "Epoch: 80 Idx: 20 Loss: 0.1155600893344481\n",
      "Epoch: 81 Idx: 0 Loss: 0.042909252144665386\n",
      "Epoch: 81 Idx: 10 Loss: 0.07833014923630675\n",
      "Epoch: 81 Idx: 20 Loss: 0.09923551590620172\n",
      "Epoch: 82 Idx: 0 Loss: 0.04232991485884413\n",
      "Epoch: 82 Idx: 10 Loss: 0.08009996322236639\n",
      "Epoch: 82 Idx: 20 Loss: 0.051194098698083904\n",
      "Epoch: 83 Idx: 0 Loss: 0.07749284498537498\n",
      "Epoch: 83 Idx: 10 Loss: 0.04494206998388724\n",
      "Epoch: 83 Idx: 20 Loss: 0.10003138568059682\n",
      "Epoch: 84 Idx: 0 Loss: 0.04501533489648956\n",
      "Epoch: 84 Idx: 10 Loss: 0.094416244758148\n",
      "Epoch: 84 Idx: 20 Loss: 0.05265442490954453\n",
      "Epoch: 85 Idx: 0 Loss: 0.09099021367177293\n",
      "Epoch: 85 Idx: 10 Loss: 0.06271411735832978\n",
      "Epoch: 85 Idx: 20 Loss: 0.05338235141363704\n",
      "Epoch: 86 Idx: 0 Loss: 0.050815158949703065\n",
      "Epoch: 86 Idx: 10 Loss: 0.06788378026997258\n",
      "Epoch: 86 Idx: 20 Loss: 0.05103762200756086\n",
      "Epoch: 87 Idx: 0 Loss: 0.06784055253057708\n",
      "Epoch: 87 Idx: 10 Loss: 0.038950372303670176\n",
      "Epoch: 87 Idx: 20 Loss: 0.03604696617100098\n",
      "Epoch: 88 Idx: 0 Loss: 0.0779890296834112\n",
      "Epoch: 88 Idx: 10 Loss: 0.06545323430815544\n",
      "Epoch: 88 Idx: 20 Loss: 0.08009672143116406\n",
      "Epoch: 89 Idx: 0 Loss: 0.12656882080123838\n",
      "Epoch: 89 Idx: 10 Loss: 0.060028487836112907\n",
      "Epoch: 89 Idx: 20 Loss: 0.05799192089649378\n",
      "Epoch: 90 Idx: 0 Loss: 0.049582157806856944\n",
      "Epoch: 90 Idx: 10 Loss: 0.05879871046660935\n",
      "Epoch: 90 Idx: 20 Loss: 0.11377200682985693\n",
      "Epoch: 91 Idx: 0 Loss: 0.07614041910940476\n",
      "Epoch: 91 Idx: 10 Loss: 0.09683147376604408\n",
      "Epoch: 91 Idx: 20 Loss: 0.04104879175224674\n",
      "Epoch: 92 Idx: 0 Loss: 0.11137856091723417\n",
      "Epoch: 92 Idx: 10 Loss: 0.07812084144461694\n",
      "Epoch: 92 Idx: 20 Loss: 0.05537086346916907\n",
      "Epoch: 93 Idx: 0 Loss: 0.09472833602157103\n",
      "Epoch: 93 Idx: 10 Loss: 0.0846181201391826\n",
      "Epoch: 93 Idx: 20 Loss: 0.033850252519882267\n",
      "Epoch: 94 Idx: 0 Loss: 0.129186554466105\n",
      "Epoch: 94 Idx: 10 Loss: 0.040046671894324044\n",
      "Epoch: 94 Idx: 20 Loss: 0.0940735544945454\n",
      "Epoch: 95 Idx: 0 Loss: 0.03855223421425947\n",
      "Epoch: 95 Idx: 10 Loss: 0.0649182647177222\n",
      "Epoch: 95 Idx: 20 Loss: 0.09774632812387711\n",
      "Epoch: 96 Idx: 0 Loss: 0.09960156611473198\n",
      "Epoch: 96 Idx: 10 Loss: 0.10393907722360755\n",
      "Epoch: 96 Idx: 20 Loss: 0.06526539782955225\n",
      "Epoch: 97 Idx: 0 Loss: 0.08605362105291213\n",
      "Epoch: 97 Idx: 10 Loss: 0.08439235656409952\n",
      "Epoch: 97 Idx: 20 Loss: 0.0528329798627307\n",
      "Epoch: 98 Idx: 0 Loss: 0.11507600209344264\n",
      "Epoch: 98 Idx: 10 Loss: 0.06186506894130559\n",
      "Epoch: 98 Idx: 20 Loss: 0.08538941075419425\n",
      "Epoch: 99 Idx: 0 Loss: 0.04231295347179946\n",
      "Epoch: 99 Idx: 10 Loss: 0.07724019942805285\n",
      "Epoch: 99 Idx: 20 Loss: 0.04046507445240919\n",
      "Epoch: 100 Idx: 0 Loss: 0.10498076214804138\n",
      "Epoch: 100 Idx: 10 Loss: 0.05185694747043908\n",
      "Epoch: 100 Idx: 20 Loss: 0.09768565154659807\n",
      "Epoch: 101 Idx: 0 Loss: 0.0801863265209643\n",
      "Epoch: 101 Idx: 10 Loss: 0.10913544832969686\n",
      "Epoch: 101 Idx: 20 Loss: 0.10303291209741583\n",
      "Epoch: 102 Idx: 0 Loss: 0.0627678407405512\n",
      "Epoch: 102 Idx: 10 Loss: 0.03847558747890483\n",
      "Epoch: 102 Idx: 20 Loss: 0.11673790606304435\n",
      "Epoch: 103 Idx: 0 Loss: 0.06404592713446147\n",
      "Epoch: 103 Idx: 10 Loss: 0.06561390271762811\n",
      "Epoch: 103 Idx: 20 Loss: 0.10328891136821716\n",
      "Epoch: 104 Idx: 0 Loss: 0.07207868045326116\n",
      "Epoch: 104 Idx: 10 Loss: 0.047299454858381274\n",
      "Epoch: 104 Idx: 20 Loss: 0.07131292382558214\n",
      "Epoch: 105 Idx: 0 Loss: 0.04271594098425739\n",
      "Epoch: 105 Idx: 10 Loss: 0.06869336267605824\n",
      "Epoch: 105 Idx: 20 Loss: 0.12239276252629913\n",
      "Epoch: 106 Idx: 0 Loss: 0.05836911499115314\n",
      "Epoch: 106 Idx: 10 Loss: 0.08003915927499121\n",
      "Epoch: 106 Idx: 20 Loss: 0.054410931055771515\n",
      "Epoch: 107 Idx: 0 Loss: 0.08004773017831729\n",
      "Epoch: 107 Idx: 10 Loss: 0.05564479893618164\n",
      "Epoch: 107 Idx: 20 Loss: 0.0572911244072065\n",
      "Epoch: 108 Idx: 0 Loss: 0.061629892487639074\n",
      "Epoch: 108 Idx: 10 Loss: 0.07766655364751839\n",
      "Epoch: 108 Idx: 20 Loss: 0.02353086515865541\n",
      "Epoch: 109 Idx: 0 Loss: 0.10257790627446861\n",
      "Epoch: 109 Idx: 10 Loss: 0.09831252466596258\n",
      "Epoch: 109 Idx: 20 Loss: 0.042379819429722305\n",
      "Epoch: 110 Idx: 0 Loss: 0.038750800893303544\n",
      "Epoch: 110 Idx: 10 Loss: 0.03843058188446508\n",
      "Epoch: 110 Idx: 20 Loss: 0.05631111187301922\n",
      "Epoch: 111 Idx: 0 Loss: 0.08345260779356646\n",
      "Epoch: 111 Idx: 10 Loss: 0.08990837518445993\n",
      "Epoch: 111 Idx: 20 Loss: 0.054790598176876405\n",
      "Epoch: 112 Idx: 0 Loss: 0.13369097726679155\n",
      "Epoch: 112 Idx: 10 Loss: 0.06223280597431828\n",
      "Epoch: 112 Idx: 20 Loss: 0.09329635176565354\n",
      "Epoch: 113 Idx: 0 Loss: 0.05091232313198416\n",
      "Epoch: 113 Idx: 10 Loss: 0.07460177090093487\n",
      "Epoch: 113 Idx: 20 Loss: 0.0853104751366045\n",
      "Epoch: 114 Idx: 0 Loss: 0.06165074858476484\n",
      "Epoch: 114 Idx: 10 Loss: 0.0625602877410662\n",
      "Epoch: 114 Idx: 20 Loss: 0.060665564258535996\n",
      "Epoch: 115 Idx: 0 Loss: 0.1126293015935302\n",
      "Epoch: 115 Idx: 10 Loss: 0.09598399622654931\n",
      "Epoch: 115 Idx: 20 Loss: 0.15010357978757308\n",
      "Epoch: 116 Idx: 0 Loss: 0.0548418131471474\n",
      "Epoch: 116 Idx: 10 Loss: 0.05255181256530898\n",
      "Epoch: 116 Idx: 20 Loss: 0.04315859782344528\n",
      "Epoch: 117 Idx: 0 Loss: 0.039874892361002216\n",
      "Epoch: 117 Idx: 10 Loss: 0.08977711635794348\n",
      "Epoch: 117 Idx: 20 Loss: 0.058635879821717043\n",
      "Epoch: 118 Idx: 0 Loss: 0.06330733099617088\n",
      "Epoch: 118 Idx: 10 Loss: 0.06668300351669898\n",
      "Epoch: 118 Idx: 20 Loss: 0.1240452658074285\n",
      "Epoch: 119 Idx: 0 Loss: 0.0638580714193199\n",
      "Epoch: 119 Idx: 10 Loss: 0.08078135034842558\n",
      "Epoch: 119 Idx: 20 Loss: 0.08600090718938716\n",
      "Epoch: 120 Idx: 0 Loss: 0.08438732605105505\n",
      "Epoch: 120 Idx: 10 Loss: 0.07393140340832358\n",
      "Epoch: 120 Idx: 20 Loss: 0.06986966454618529\n",
      "Epoch: 121 Idx: 0 Loss: 0.04340065516108129\n",
      "Epoch: 121 Idx: 10 Loss: 0.07027769738015979\n",
      "Epoch: 121 Idx: 20 Loss: 0.10602909941283559\n",
      "Epoch: 122 Idx: 0 Loss: 0.04673979416865107\n",
      "Epoch: 122 Idx: 10 Loss: 0.06010610503892612\n",
      "Epoch: 122 Idx: 20 Loss: 0.07844455762310579\n",
      "Epoch: 123 Idx: 0 Loss: 0.11341979447794266\n",
      "Epoch: 123 Idx: 10 Loss: 0.050657613841550764\n",
      "Epoch: 123 Idx: 20 Loss: 0.10561006425096786\n",
      "Epoch: 124 Idx: 0 Loss: 0.12318150359655952\n",
      "Epoch: 124 Idx: 10 Loss: 0.07973577879141543\n",
      "Epoch: 124 Idx: 20 Loss: 0.06812380634873595\n",
      "Epoch: 125 Idx: 0 Loss: 0.06465898253015792\n",
      "Epoch: 125 Idx: 10 Loss: 0.06969582274964761\n",
      "Epoch: 125 Idx: 20 Loss: 0.0691233295904851\n",
      "Epoch: 126 Idx: 0 Loss: 0.07094974674046414\n",
      "Epoch: 126 Idx: 10 Loss: 0.07290159561075857\n",
      "Epoch: 126 Idx: 20 Loss: 0.06554209459480746\n",
      "Epoch: 127 Idx: 0 Loss: 0.07818344452921926\n",
      "Epoch: 127 Idx: 10 Loss: 0.1445944728586173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127 Idx: 20 Loss: 0.07697671588587927\n",
      "Epoch: 128 Idx: 0 Loss: 0.08838601369154478\n",
      "Epoch: 128 Idx: 10 Loss: 0.0851390262286156\n",
      "Epoch: 128 Idx: 20 Loss: 0.10125827491040704\n",
      "Epoch: 129 Idx: 0 Loss: 0.09477972143102045\n",
      "Epoch: 129 Idx: 10 Loss: 0.08348792950220235\n",
      "Epoch: 129 Idx: 20 Loss: 0.08812744077346023\n",
      "Epoch: 130 Idx: 0 Loss: 0.055590009592306575\n",
      "Epoch: 130 Idx: 10 Loss: 0.05304962994363601\n",
      "Epoch: 130 Idx: 20 Loss: 0.08134597775956229\n",
      "Epoch: 131 Idx: 0 Loss: 0.11371585539595024\n",
      "Epoch: 131 Idx: 10 Loss: 0.06000270635949688\n",
      "Epoch: 131 Idx: 20 Loss: 0.08670760794943339\n",
      "Epoch: 132 Idx: 0 Loss: 0.05259878260659332\n",
      "Epoch: 132 Idx: 10 Loss: 0.03666567228370399\n",
      "Epoch: 132 Idx: 20 Loss: 0.06784162231626135\n",
      "Epoch: 133 Idx: 0 Loss: 0.09117379553412377\n",
      "Epoch: 133 Idx: 10 Loss: 0.055385492594866956\n",
      "Epoch: 133 Idx: 20 Loss: 0.0465095191703773\n",
      "Epoch: 134 Idx: 0 Loss: 0.10251252792272325\n",
      "Epoch: 134 Idx: 10 Loss: 0.0635040555599786\n",
      "Epoch: 134 Idx: 20 Loss: 0.08132925855196757\n",
      "Epoch: 135 Idx: 0 Loss: 0.09650352148754268\n",
      "Epoch: 135 Idx: 10 Loss: 0.062367895545198906\n",
      "Epoch: 135 Idx: 20 Loss: 0.056475491443485196\n",
      "Epoch: 136 Idx: 0 Loss: 0.09716927152432189\n",
      "Epoch: 136 Idx: 10 Loss: 0.09214984185056599\n",
      "Epoch: 136 Idx: 20 Loss: 0.11704000560564123\n",
      "Epoch: 137 Idx: 0 Loss: 0.07820699231256402\n",
      "Epoch: 137 Idx: 10 Loss: 0.07271543683703306\n",
      "Epoch: 137 Idx: 20 Loss: 0.07638289776460624\n",
      "Epoch: 138 Idx: 0 Loss: 0.12134502888378147\n",
      "Epoch: 138 Idx: 10 Loss: 0.056063135326950474\n",
      "Epoch: 138 Idx: 20 Loss: 0.05361695246900242\n",
      "Epoch: 139 Idx: 0 Loss: 0.08081086708915204\n",
      "Epoch: 139 Idx: 10 Loss: 0.08880224430812136\n",
      "Epoch: 139 Idx: 20 Loss: 0.08979576711538859\n",
      "Epoch: 140 Idx: 0 Loss: 0.05954634059197483\n",
      "Epoch: 140 Idx: 10 Loss: 0.11556923076217848\n",
      "Epoch: 140 Idx: 20 Loss: 0.0575156619795887\n",
      "Epoch: 141 Idx: 0 Loss: 0.10019849618026966\n",
      "Epoch: 141 Idx: 10 Loss: 0.06025137585488072\n",
      "Epoch: 141 Idx: 20 Loss: 0.08678745644497216\n",
      "Epoch: 142 Idx: 0 Loss: 0.0652827171319548\n",
      "Epoch: 142 Idx: 10 Loss: 0.08436088418203071\n",
      "Epoch: 142 Idx: 20 Loss: 0.06965267209178214\n",
      "Epoch: 143 Idx: 0 Loss: 0.06135222658812021\n",
      "Epoch: 143 Idx: 10 Loss: 0.09017473090568197\n",
      "Epoch: 143 Idx: 20 Loss: 0.07597052942252439\n",
      "Epoch: 144 Idx: 0 Loss: 0.04389829070522782\n",
      "Epoch: 144 Idx: 10 Loss: 0.12766753815676787\n",
      "Epoch: 144 Idx: 20 Loss: 0.07013947065544965\n",
      "Epoch: 145 Idx: 0 Loss: 0.07059732986852403\n",
      "Epoch: 145 Idx: 10 Loss: 0.0474417182606849\n",
      "Epoch: 145 Idx: 20 Loss: 0.06973760602656459\n",
      "Epoch: 146 Idx: 0 Loss: 0.082016849553212\n",
      "Epoch: 146 Idx: 10 Loss: 0.10291567871918175\n",
      "Epoch: 146 Idx: 20 Loss: 0.08050728643442127\n",
      "Epoch: 147 Idx: 0 Loss: 0.03443513330013759\n",
      "Epoch: 147 Idx: 10 Loss: 0.039366088747022125\n",
      "Epoch: 147 Idx: 20 Loss: 0.046565481992524436\n",
      "Epoch: 148 Idx: 0 Loss: 0.08925479406829635\n",
      "Epoch: 148 Idx: 10 Loss: 0.06471357989331314\n",
      "Epoch: 148 Idx: 20 Loss: 0.06480604739737993\n",
      "Epoch: 149 Idx: 0 Loss: 0.08775293818368111\n",
      "Epoch: 149 Idx: 10 Loss: 0.03653292419222986\n",
      "Epoch: 149 Idx: 20 Loss: 0.03350766969050283\n",
      "Epoch: 150 Idx: 0 Loss: 0.04539348931771528\n",
      "Epoch: 150 Idx: 10 Loss: 0.07765279777367123\n",
      "Epoch: 150 Idx: 20 Loss: 0.06618893469147274\n",
      "Epoch: 151 Idx: 0 Loss: 0.09274643620799701\n",
      "Epoch: 151 Idx: 10 Loss: 0.0776762172273854\n",
      "Epoch: 151 Idx: 20 Loss: 0.1019840521172221\n",
      "Epoch: 152 Idx: 0 Loss: 0.07663684324525219\n",
      "Epoch: 152 Idx: 10 Loss: 0.05835154408267936\n",
      "Epoch: 152 Idx: 20 Loss: 0.12116893349698762\n",
      "Epoch: 153 Idx: 0 Loss: 0.0640963077538797\n",
      "Epoch: 153 Idx: 10 Loss: 0.08534706864154426\n",
      "Epoch: 153 Idx: 20 Loss: 0.06507904876745452\n",
      "Epoch: 154 Idx: 0 Loss: 0.042008165073387296\n",
      "Epoch: 154 Idx: 10 Loss: 0.05611119531632784\n",
      "Epoch: 154 Idx: 20 Loss: 0.06255329480510016\n",
      "Epoch: 155 Idx: 0 Loss: 0.08013668834341425\n",
      "Epoch: 155 Idx: 10 Loss: 0.08271044827221624\n",
      "Epoch: 155 Idx: 20 Loss: 0.04271642327876242\n",
      "Epoch: 156 Idx: 0 Loss: 0.021992636749196014\n",
      "Epoch: 156 Idx: 10 Loss: 0.049691307504744944\n",
      "Epoch: 156 Idx: 20 Loss: 0.08424074177103638\n",
      "Epoch: 157 Idx: 0 Loss: 0.04313191908399612\n",
      "Epoch: 157 Idx: 10 Loss: 0.0806267255502261\n",
      "Epoch: 157 Idx: 20 Loss: 0.05244646615855758\n",
      "Epoch: 158 Idx: 0 Loss: 0.07203338673764202\n",
      "Epoch: 158 Idx: 10 Loss: 0.0423938369570688\n",
      "Epoch: 158 Idx: 20 Loss: 0.1293321744029357\n",
      "Epoch: 159 Idx: 0 Loss: 0.07205786881263607\n",
      "Epoch: 159 Idx: 10 Loss: 0.11673098023362691\n",
      "Epoch: 159 Idx: 20 Loss: 0.04592750893880347\n",
      "Epoch: 160 Idx: 0 Loss: 0.08228121269648378\n",
      "Epoch: 160 Idx: 10 Loss: 0.059336365733910434\n",
      "Epoch: 160 Idx: 20 Loss: 0.059526321542378344\n",
      "Epoch: 161 Idx: 0 Loss: 0.047638610572970634\n",
      "Epoch: 161 Idx: 10 Loss: 0.07948312194339899\n",
      "Epoch: 161 Idx: 20 Loss: 0.05735815888585184\n",
      "Epoch: 162 Idx: 0 Loss: 0.11408865348577255\n",
      "Epoch: 162 Idx: 10 Loss: 0.06962884724112914\n",
      "Epoch: 162 Idx: 20 Loss: 0.06621314639549758\n",
      "Epoch: 163 Idx: 0 Loss: 0.06753870576976262\n",
      "Epoch: 163 Idx: 10 Loss: 0.05946115924060454\n",
      "Epoch: 163 Idx: 20 Loss: 0.08712389687231013\n",
      "Epoch: 164 Idx: 0 Loss: 0.0845885888351241\n",
      "Epoch: 164 Idx: 10 Loss: 0.05150419989740155\n",
      "Epoch: 164 Idx: 20 Loss: 0.06116586129574013\n",
      "Epoch: 165 Idx: 0 Loss: 0.06613578638945919\n",
      "Epoch: 165 Idx: 10 Loss: 0.049109577151550746\n",
      "Epoch: 165 Idx: 20 Loss: 0.06473488624829732\n",
      "Epoch: 166 Idx: 0 Loss: 0.0827965971279357\n",
      "Epoch: 166 Idx: 10 Loss: 0.08089202880010718\n",
      "Epoch: 166 Idx: 20 Loss: 0.09029066177477856\n",
      "Epoch: 167 Idx: 0 Loss: 0.10805633867251267\n",
      "Epoch: 167 Idx: 10 Loss: 0.05628913160983943\n",
      "Epoch: 167 Idx: 20 Loss: 0.10036786300041915\n",
      "Epoch: 168 Idx: 0 Loss: 0.06771228189574771\n",
      "Epoch: 168 Idx: 10 Loss: 0.05625702700630274\n",
      "Epoch: 168 Idx: 20 Loss: 0.11109178569415601\n",
      "Epoch: 169 Idx: 0 Loss: 0.0544956142045188\n",
      "Epoch: 169 Idx: 10 Loss: 0.07045314947264103\n",
      "Epoch: 169 Idx: 20 Loss: 0.05695058269366819\n",
      "Epoch: 170 Idx: 0 Loss: 0.08207839374756568\n",
      "Epoch: 170 Idx: 10 Loss: 0.0415829620217101\n",
      "Epoch: 170 Idx: 20 Loss: 0.04402794002329019\n",
      "Epoch: 171 Idx: 0 Loss: 0.03570114389811593\n",
      "Epoch: 171 Idx: 10 Loss: 0.09618893844027203\n",
      "Epoch: 171 Idx: 20 Loss: 0.04100291737402963\n",
      "Epoch: 172 Idx: 0 Loss: 0.1008765808084258\n",
      "Epoch: 172 Idx: 10 Loss: 0.0428816537031622\n",
      "Epoch: 172 Idx: 20 Loss: 0.05034365489077007\n",
      "Epoch: 173 Idx: 0 Loss: 0.06848563983530319\n",
      "Epoch: 173 Idx: 10 Loss: 0.048006174825526556\n",
      "Epoch: 173 Idx: 20 Loss: 0.059268087719396415\n",
      "Epoch: 174 Idx: 0 Loss: 0.140639197764058\n",
      "Epoch: 174 Idx: 10 Loss: 0.06530109648084446\n",
      "Epoch: 174 Idx: 20 Loss: 0.11210657339722092\n",
      "Epoch: 175 Idx: 0 Loss: 0.08817419294571605\n",
      "Epoch: 175 Idx: 10 Loss: 0.08914612033031669\n",
      "Epoch: 175 Idx: 20 Loss: 0.06898912875480584\n",
      "Epoch: 176 Idx: 0 Loss: 0.07170942617177631\n",
      "Epoch: 176 Idx: 10 Loss: 0.06360829389497671\n",
      "Epoch: 176 Idx: 20 Loss: 0.1224629703878197\n",
      "Epoch: 177 Idx: 0 Loss: 0.04355085184967467\n",
      "Epoch: 177 Idx: 10 Loss: 0.09007118807404188\n",
      "Epoch: 177 Idx: 20 Loss: 0.06317880985523047\n",
      "Epoch: 178 Idx: 0 Loss: 0.06037424547495126\n",
      "Epoch: 178 Idx: 10 Loss: 0.06404368643713912\n",
      "Epoch: 178 Idx: 20 Loss: 0.0924026687046143\n",
      "Epoch: 179 Idx: 0 Loss: 0.05842864313108139\n",
      "Epoch: 179 Idx: 10 Loss: 0.04434839132137121\n",
      "Epoch: 179 Idx: 20 Loss: 0.06849487956913905\n",
      "Epoch: 180 Idx: 0 Loss: 0.07537065596835027\n",
      "Epoch: 180 Idx: 10 Loss: 0.10558833932476663\n",
      "Epoch: 180 Idx: 20 Loss: 0.04942136414511135\n",
      "Epoch: 181 Idx: 0 Loss: 0.028574237225650674\n",
      "Epoch: 181 Idx: 10 Loss: 0.0787220140089264\n",
      "Epoch: 181 Idx: 20 Loss: 0.07994577863796853\n",
      "Epoch: 182 Idx: 0 Loss: 0.06896196725977417\n",
      "Epoch: 182 Idx: 10 Loss: 0.05841846929780795\n",
      "Epoch: 182 Idx: 20 Loss: 0.07962841146237004\n",
      "Epoch: 183 Idx: 0 Loss: 0.05353573510496464\n",
      "Epoch: 183 Idx: 10 Loss: 0.05362163355966637\n",
      "Epoch: 183 Idx: 20 Loss: 0.07421748331324746\n",
      "Epoch: 184 Idx: 0 Loss: 0.09206488820911532\n",
      "Epoch: 184 Idx: 10 Loss: 0.05831359724890195\n",
      "Epoch: 184 Idx: 20 Loss: 0.06946963874074555\n",
      "Epoch: 185 Idx: 0 Loss: 0.0987015219245607\n",
      "Epoch: 185 Idx: 10 Loss: 0.061090945814257155\n",
      "Epoch: 185 Idx: 20 Loss: 0.03782278457236457\n",
      "Epoch: 186 Idx: 0 Loss: 0.06842794881041606\n",
      "Epoch: 186 Idx: 10 Loss: 0.03125783347377657\n",
      "Epoch: 186 Idx: 20 Loss: 0.10382312732104368\n",
      "Epoch: 187 Idx: 0 Loss: 0.04013745131678109\n",
      "Epoch: 187 Idx: 10 Loss: 0.05847047992911899\n",
      "Epoch: 187 Idx: 20 Loss: 0.078937621460122\n",
      "Epoch: 188 Idx: 0 Loss: 0.08402807471743433\n",
      "Epoch: 188 Idx: 10 Loss: 0.08207543546580189\n",
      "Epoch: 188 Idx: 20 Loss: 0.040937031668782535\n",
      "Epoch: 189 Idx: 0 Loss: 0.05931771146045274\n",
      "Epoch: 189 Idx: 10 Loss: 0.0650806793997448\n",
      "Epoch: 189 Idx: 20 Loss: 0.09208271304349497\n",
      "Epoch: 190 Idx: 0 Loss: 0.07501896923559834\n",
      "Epoch: 190 Idx: 10 Loss: 0.04558666832505297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 Idx: 20 Loss: 0.07719947258818491\n",
      "Epoch: 191 Idx: 0 Loss: 0.04631192208204056\n",
      "Epoch: 191 Idx: 10 Loss: 0.06300285178425577\n",
      "Epoch: 191 Idx: 20 Loss: 0.07646562219396084\n",
      "Epoch: 192 Idx: 0 Loss: 0.05354063768609074\n",
      "Epoch: 192 Idx: 10 Loss: 0.0786774634689649\n",
      "Epoch: 192 Idx: 20 Loss: 0.0725362031997974\n",
      "Epoch: 193 Idx: 0 Loss: 0.09099787045244637\n",
      "Epoch: 193 Idx: 10 Loss: 0.05863975384944731\n",
      "Epoch: 193 Idx: 20 Loss: 0.04016271776775829\n",
      "Epoch: 194 Idx: 0 Loss: 0.04325191344406391\n",
      "Epoch: 194 Idx: 10 Loss: 0.050952899210359115\n",
      "Epoch: 194 Idx: 20 Loss: 0.05555850735932246\n",
      "Epoch: 195 Idx: 0 Loss: 0.04663522624011533\n",
      "Epoch: 195 Idx: 10 Loss: 0.054535017114657545\n",
      "Epoch: 195 Idx: 20 Loss: 0.06962296784418762\n",
      "Epoch: 196 Idx: 0 Loss: 0.06284468586554225\n",
      "Epoch: 196 Idx: 10 Loss: 0.08610784768339977\n",
      "Epoch: 196 Idx: 20 Loss: 0.04954195226362518\n",
      "Epoch: 197 Idx: 0 Loss: 0.07193964826267532\n",
      "Epoch: 197 Idx: 10 Loss: 0.06035164178475708\n",
      "Epoch: 197 Idx: 20 Loss: 0.03824767318962717\n",
      "Epoch: 198 Idx: 0 Loss: 0.06010307067159487\n",
      "Epoch: 198 Idx: 10 Loss: 0.06622850880106153\n",
      "Epoch: 198 Idx: 20 Loss: 0.10570462072067095\n",
      "Epoch: 199 Idx: 0 Loss: 0.06087273060217215\n",
      "Epoch: 199 Idx: 10 Loss: 0.05377571171202632\n",
      "Epoch: 199 Idx: 20 Loss: 0.06396193804238073\n",
      "F batch size:  4172\n",
      "Inputs len:  4182\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Inputs len:  4182\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Inputs len:  4182\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Inputs len:  4182\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Inputs len:  4182\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Inputs len:  4182\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 1, 0])\n",
      "Inputs len:  4170\n",
      "Targets:  tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Low: -0.5187763310904898 High: 1.0099999999999925\n",
      "Threshold:  -0.5187763310904898 0.0020846148588613217 1.0 0.0041605565596971655 0.01033688063444723 0.002604411274966057\n",
      "Threshold:  -0.5087763310904898 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.4987763310904898 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.4887763310904898 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.4787763310904898 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.4687763310904898 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.45877633109048976 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.44877633109048976 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.43877633109048975 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.42877633109048974 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.41877633109048973 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.4087763310904897 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.3987763310904897 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.3887763310904897 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.3787763310904897 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.3687763310904897 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.3587763310904897 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.34877633109048967 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.33877633109048966 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.32877633109048965 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.31877633109048964 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.30877633109048963 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.2987763310904896 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.2887763310904896 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.2787763310904896 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.2687763310904896 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.2587763310904896 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.24877633109048958 0.0020846861009534874 1.0 0.00416069845167451 0.010337230977800373 0.002604500234831988\n",
      "Threshold:  -0.23877633109048957 0.0020847573479152426 1.0 0.004160840353330378 0.010337581344902386 0.0026045892007753994\n",
      "Threshold:  -0.22877633109048956 0.0020847573479152426 1.0 0.004160840353330378 0.010337581344902386 0.0026045892007753994\n",
      "Threshold:  -0.21877633109048955 0.0020847573479152426 1.0 0.004160840353330378 0.010337581344902386 0.0026045892007753994\n",
      "Threshold:  -0.20877633109048954 0.0020848285997470866 1.0 0.004160982264665758 0.010337931735755687 0.0026046781727969122\n",
      "Threshold:  -0.19877633109048953 0.0020848285997470866 1.0 0.004160982264665758 0.010337931735755687 0.0026046781727969122\n",
      "Threshold:  -0.18877633109048952 0.0020848285997470866 1.0 0.004160982264665758 0.010337931735755687 0.0026046781727969122\n",
      "Threshold:  -0.17877633109048952 0.0020849711180230372 1.0 0.004161266116379016 0.010338632588725806 0.0026048561350767363\n",
      "Threshold:  -0.1687763310904895 0.0020849711180230372 1.0 0.004161266116379016 0.010338632588725806 0.0026048561350767363\n",
      "Threshold:  -0.1587763310904895 0.002085042384468143 1.0 0.004161408056758877 0.010338983050847456 0.002604945125336294\n",
      "Threshold:  -0.1487763310904895 0.0020851136557853357 1.0 0.004161550006822213 0.010339333536730057 0.002605034121676446\n",
      "Threshold:  -0.13877633109048948 0.0020851849319751146 1.0 0.004161691966570015 0.010339684046376026 0.0026051231240978166\n",
      "Threshold:  -0.12877633109048947 0.002085327498974429 1.0 0.004161975915122983 0.010340385136967724 0.0026053011471867016\n",
      "Threshold:  -0.11877633109048946 0.0020856126914660833 1.0 0.00416254392848613 0.010341787603417878 0.002605657266366518\n",
      "Threshold:  -0.10877633109048945 0.0020856126914660833 1.0 0.00416254392848613 0.010341787603417878 0.002605657266366518\n",
      "Threshold:  -0.09877633109048944 0.00208568400177796 1.0 0.004162685956052955 0.010342138279475095 0.0026057463113738693\n",
      "Threshold:  -0.08877633109048944 0.0020858266370319713 1.0 0.004162970040264792 0.010342839702940079 0.002605924419647816\n",
      "Threshold:  -0.07877633109048943 0.002085969291796327 1.0 0.004163254163254163 0.01034354122155526 0.0026061025522715812\n",
      "Threshold:  -0.06877633109048942 0.0020860406264961357 1.0 0.004163396239292905 0.010343892016550227 0.002606191627716207\n",
      "Threshold:  -0.05877633109048941 0.0020862546598720885 1.0 0.004163822525597269 0.01034494454431367 0.002606458890588547\n",
      "Threshold:  -0.0487763310904894 0.002086326014091251 1.0 0.004163964640431414 0.010345295434502409 0.0026065479903942296\n",
      "Threshold:  -0.03877633109048939 0.0020864687371733477 1.0 0.0041642488992046965 0.010345997286295794 0.0026067262082816968\n",
      "Threshold:  -0.028776331090489382 0.0020866114797838133 1.0 0.004164533196791262 0.010346699233326548 0.002606904450541467\n",
      "Threshold:  -0.018776331090489373 0.002087111232764225 1.0 0.00416552854411363 0.010349156798208409 0.0026075284904547363\n",
      "Threshold:  -0.008776331090489364 0.0020879685093274005 1.0 0.004167235961196885 0.010353372483791033 0.0026085989685343098\n",
      "Threshold:  0.0012236689095106446 0.0020906162176982657 1.0 0.004172509319744177 0.010366392495411597 0.0026119051493067747\n",
      "Threshold:  0.011223668909510653 0.0020913329676357654 1.0 0.004173936843545794 0.010369917040663675 0.0026128001507714185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  0.021223668909510662 0.002091763253549139 1.0 0.004174793826780276 0.010372032918452016 0.002613337446126691\n",
      "Threshold:  0.03122366890951067 0.002092696147380699 1.0 0.0041766518315645325 0.010376620283741025 0.002614502344479971\n",
      "Threshold:  0.04122366890951068 0.0020933424845573094 1.0 0.004177939111674258 0.010379798529812141 0.0026153094211162652\n",
      "Threshold:  0.05122366890951069 0.002094420600858369 1.0 0.00418008634276708 0.010385099935305937 0.002616655656694778\n",
      "Threshold:  0.0612236689095107 0.002095571816276753 1.0 0.00418237915666781 0.010390760739958437 0.0026180931697811958\n",
      "Threshold:  0.0712236689095107 0.0020968684472861022 1.0 0.0041849615806805705 0.01039713652633373 0.002619712261112304\n",
      "Threshold:  0.08122366890951072 0.0020985997866996937 1.0 0.004188409777533644 0.010405649756064277 0.0026218741672325904\n",
      "Threshold:  0.09122366890951072 0.002100478633655866 1.0 0.004192151742148306 0.010414888168004099 0.0026242202624220264\n",
      "Threshold:  0.10122366890951073 0.0021016365202411714 1.0 0.00419445781475624 0.010420581502613688 0.0026256661013593203\n",
      "Threshold:  0.11122366890951074 0.0021025057732740496 1.0 0.004196189034876522 0.010424855590115188 0.0026267515265301906\n",
      "Threshold:  0.12122366890951075 0.0021035933512656047 1.0 0.004198355070718194 0.01043020313248068 0.0026281095706272135\n",
      "Threshold:  0.13122366890951076 0.002105263157894737 1.0 0.004201680672268907 0.010438413361169102 0.0026301946344029457\n",
      "Threshold:  0.14122366890951077 0.0021072995474487855 1.0 0.004205736348593492 0.010448425884690487 0.002632737442705591\n",
      "Threshold:  0.15122366890951078 0.0021088294268132475 1.0 0.004208783247662745 0.010455947891669523 0.0026346477778257677\n",
      "Threshold:  0.1612236689095108 0.0021105805826586396 1.0 0.004212270828298174 0.01046455774377273 0.0026368344154440687\n",
      "Threshold:  0.1712236689095108 0.002111895859299266 1.0 0.004214890309207118 0.010471024443834112 0.0026384767770789904\n",
      "Threshold:  0.1812236689095108 0.0021142381810619715 1.0 0.004219555217376267 0.010482540555402804 0.0026414015883050863\n",
      "Threshold:  0.1912236689095108 0.002115851543531044 1.0 0.0042227683361600495 0.010490472587191305 0.0026434161603730256\n",
      "Threshold:  0.20122366890951082 0.0021173938699711897 1.0 0.004225839972289574 0.010498055278284516 0.002645342029714563\n",
      "Threshold:  0.21122366890951083 0.0021190856666435075 1.0 0.0042292092765278885 0.010506372717878059 0.0026474545375634742\n",
      "Threshold:  0.22122366890951084 0.0021218129326237433 1.0 0.004234640749739674 0.01051978063670541 0.0026508600085175167\n",
      "Threshold:  0.23122366890951085 0.002124991291019299 1.0 0.0042409705565404805 0.010535405872193437 0.0026548287417852636\n",
      "Threshold:  0.24122366890951086 0.002092487968194183 0.9836065573770492 0.004176091874021229 0.010374161421951725 0.002614219611875528\n",
      "Threshold:  0.25122366890951087 0.002096069868995633 0.9836065573770492 0.00418322526668061 0.010391769718383041 0.002618692225102784\n",
      "Threshold:  0.2612236689095109 0.002100619682806428 0.9836065573770492 0.004192286193404136 0.01041413545318846 0.002624373430843386\n",
      "Threshold:  0.2712236689095109 0.0021057064645188463 0.9836065573770492 0.004202416389423919 0.010439139814879254 0.0026307251155326785\n",
      "Threshold:  0.2812236689095109 0.0021114864864864866 0.9836065573770492 0.004213927028830285 0.0104675505931612 0.0026379424049241588\n",
      "Threshold:  0.2912236689095109 0.002083627631021331 0.9672131147540983 0.004158297212531276 0.010329131652661066 0.0026031325832781825\n",
      "Threshold:  0.3012236689095109 0.0020913827939456242 0.9672131147540983 0.0041737408036219585 0.010367246529608153 0.0026128160843186753\n",
      "Threshold:  0.3112236689095109 0.0020991212153556055 0.9672131147540983 0.00418915080942914 0.010405276709816233 0.0026224786423561415\n",
      "Threshold:  0.32122366890951093 0.002106691423266443 0.9672131147540983 0.004204225603021342 0.010442477876106197 0.0026319311237007633\n",
      "Threshold:  0.33122366890951094 0.002114922751550346 0.9672131147540983 0.004220616639244581 0.0104829252691802 0.00264220906591192\n",
      "Threshold:  0.34122366890951095 0.0020885095963415075 0.9508196721311475 0.004167864328830124 0.010351597358557917 0.0026092041927212204\n",
      "Threshold:  0.35122366890951096 0.0020617065142691793 0.9344262295081968 0.00411433521004764 0.01021835000537808 0.0025757123878209476\n",
      "Threshold:  0.36122366890951096 0.002070543790184896 0.9344262295081968 0.004131931859369337 0.010261765023584056 0.002586746780180982\n",
      "Threshold:  0.371223668909511 0.0020423793719683433 0.9180327868852459 0.0040756914119359534 0.010121823374182121 0.0025515550816953263\n",
      "Threshold:  0.381223668909511 0.0020505309410472354 0.9180327868852459 0.004091922107339886 0.010161863976192204 0.00256173319548769\n",
      "Threshold:  0.391223668909511 0.0020226537216828477 0.9016393442622951 0.0040362528895901365 0.010023327015599942 0.0025268999990811273\n",
      "Threshold:  0.401223668909511 0.0019948282231252307 0.8852459016393442 0.0039806862998046505 0.009885040638500403 0.002492131326090769\n",
      "Threshold:  0.411223668909511 0.0020048264340077966 0.8852459016393442 0.004000592680397096 0.009934140328930426 0.002504614985018692\n",
      "Threshold:  0.421223668909511 0.0019787194325182004 0.8688524590163934 0.003948446695969604 0.00980428428724703 0.0024719918657475208\n",
      "Threshold:  0.431223668909511 0.0019929307362562985 0.8688524590163934 0.003976739823672857 0.00987405916983382 0.0024897357122053427\n",
      "Threshold:  0.44122366890951104 0.0019696223627892885 0.8524590163934426 0.00393016400876729 0.009757928316757366 0.0024606066341740407\n",
      "Threshold:  0.45122366890951104 0.0019100737288459335 0.819672131147541 0.003811266102599284 0.00946217024336702 0.0023862020254082797\n",
      "Threshold:  0.46122366890951105 0.0018908698001080498 0.8032786885245902 0.0037728585178055827 0.009366159492315927 0.00236219713258193\n",
      "Threshold:  0.47122366890951106 0.00187207488299532 0.7868852459016393 0.003735263219329987 0.00927213722763097 0.002338702604730026\n",
      "Threshold:  0.48122366890951107 0.0018948365703458077 0.7868852459016393 0.003780569448273146 0.009383797309978105 0.002367120693566363\n",
      "Threshold:  0.4912236689095111 0.0019176221485358155 0.7868852459016393 0.0038259206121472977 0.009495548961424334 0.0023955681988321606\n",
      "Threshold:  0.5012236689095111 0.00186219739292365 0.7540983606557377 0.0037152202883334005 0.009219915016435501 0.002326310572575833\n",
      "Threshold:  0.5112236689095111 0.0018431292238378046 0.7377049180327869 0.0036770714168981858 0.00912445760168701 0.0023024733680580426\n",
      "Threshold:  0.5212236689095111 0.0018710240738430836 0.7377049180327869 0.003732581287325813 0.009261164848734307 0.002337298083415572\n",
      "Threshold:  0.5312236689095111 0.0018989745537409798 0.7377049180327869 0.0037881976597356677 0.009398103671525835 0.00237219158873578\n",
      "Threshold:  0.5412236689095111 0.0018840455596471697 0.7213114754098361 0.0037582746102925477 0.009322823968132894 0.002353520117248093\n",
      "Threshold:  0.5512236689095111 0.0019107173875282265 0.7213114754098361 0.0038113387327298716 0.009453420419388105 0.0023868160958198173\n",
      "Threshold:  0.5612236689095111 0.0018970309260157938 0.7049180327868853 0.003783878915874692 0.009384138623368687 0.002369694364536146\n",
      "Threshold:  0.5712236689095112 0.0018405458789728857 0.6721311475409836 0.003671039083135605 0.009103019538188277 0.002299108394549431\n",
      "Threshold:  0.5812236689095112 0.0018829797005603013 0.6721311475409836 0.0037554385161438063 0.00931056408393133 0.0023520772858175477\n",
      "Threshold:  0.5912236689095112 0.0019290486496659452 0.6721311475409836 0.0038470560638048323 0.009535770769373895 0.00240958190815379\n",
      "Threshold:  0.6012236689095112 0.0019294775939414403 0.6557377049180327 0.0038476337052712577 0.009535160905840287 0.002410074109778876\n",
      "Threshold:  0.6112236689095112 0.001886886141317841 0.6229508196721312 0.0037623762376237627 0.00932149340136388 0.0023568230025925053\n",
      "Threshold:  0.6212236689095112 0.0018950064020486555 0.6065573770491803 0.003778208924742163 0.009358085892053215 0.002366909328181574\n",
      "Threshold:  0.6312236689095112 0.0017964704639120786 0.5573770491803278 0.003581397798493706 0.008868022952529994 0.002243780109549264\n",
      "Threshold:  0.6412236689095112 0.001695935226215876 0.5081967213114754 0.0033805888767720824 0.008367974950062083 0.002118151878322424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  0.6512236689095112 0.001694245213757271 0.4918032786885246 0.0033768572714993244 0.008356080441201048 0.0021159841442254792\n",
      "Threshold:  0.6612236689095112 0.0017563374509689128 0.4918032786885246 0.0035001750087504373 0.008658008658008658 0.0021934634788330775\n",
      "Threshold:  0.6712236689095112 0.0017023346303501946 0.45901639344262296 0.0033920891634865833 0.008387251377905585 0.00212594718539778\n",
      "Threshold:  0.6812236689095112 0.0017042226850975193 0.4426229508196721 0.0033953722334004025 0.008391869211164294 0.0021282297857582017\n",
      "Threshold:  0.6912236689095113 0.001774681214670698 0.4426229508196721 0.00353518821603928 0.008733341958856257 0.002216130144294696\n",
      "Threshold:  0.7012236689095113 0.001860529217199559 0.4426229508196721 0.0037054827420572294 0.00914882081865004 0.0023232201552255247\n",
      "Threshold:  0.7112236689095113 0.0016803039158386909 0.3770491803278688 0.0033456978689359227 0.00825437840941717 0.0020980424351887328\n",
      "Threshold:  0.7212236689095113 0.0017794970986460348 0.3770491803278688 0.003542276297551209 0.00873262966056648 0.0022217499661907614\n",
      "Threshold:  0.7312236689095113 0.0018177311410394117 0.36065573770491804 0.003617231173955936 0.008909046731999676 0.002269304561302168\n",
      "Threshold:  0.7412236689095113 0.0017719500310091256 0.32786885245901637 0.003524850193866761 0.00867227473766369 0.002211948948218275\n",
      "Threshold:  0.7512236689095113 0.0018043684710351377 0.3114754098360656 0.003587952034746483 0.008817523668089846 0.0022521988573054224\n",
      "Threshold:  0.7612236689095113 0.0019352210226115298 0.3114754098360656 0.003846543172385869 0.009441462929835023 0.002415274705717845\n",
      "Threshold:  0.7712236689095113 0.001860566925686768 0.2786885245901639 0.0036964557512502716 0.009060867711331414 0.0023218334289382392\n",
      "Threshold:  0.7812236689095113 0.0016546507505023047 0.22950819672131148 0.003285613705702887 0.0080413555427915 0.0020645922430320013\n",
      "Threshold:  0.7912236689095113 0.0017992545945251252 0.22950819672131148 0.003570517725070135 0.008722741433021805 0.0022446689113355778\n",
      "Threshold:  0.8012236689095114 0.00197768046334228 0.22950819672131148 0.00392156862745098 0.009558923938276663 0.0024667864820100784\n",
      "Threshold:  0.8112236689095114 0.002037936980717981 0.21311475409836064 0.004037267080745342 0.009814283557300317 0.002541345740313563\n",
      "Threshold:  0.8212236689095114 0.002321014104624174 0.21311475409836064 0.0045920169551395265 0.011120615911035072 0.0028933897173380814\n",
      "Threshold:  0.8312236689095114 0.0026519787841697267 0.21311475409836064 0.005238766874874069 0.012631169840652934 0.003304692663582287\n",
      "Threshold:  0.8412236689095114 0.0025737014506317267 0.18032786885245902 0.005074971164936562 0.012173528109783092 0.003205688640205164\n",
      "Threshold:  0.8512236689095114 0.0021522733387140166 0.13114754098360656 0.004235044997353097 0.01009845998485231 0.002679348918212874\n",
      "Threshold:  0.8612236689095114 0.0024103645676408557 0.13114754098360656 0.004733727810650888 0.011226494527083918 0.0029991752268126266\n",
      "Threshold:  0.8712236689095114 0.002006688963210702 0.09836065573770492 0.003933136676499508 0.009276437847866418 0.0024956326428749683\n",
      "Threshold:  0.8812236689095114 0.0018740629685157421 0.08196721311475409 0.003664345914254306 0.008585164835164836 0.002329264884002609\n",
      "Threshold:  0.8912236689095114 0.0022192632046160675 0.08196721311475409 0.00432152117545376 0.010012014417300763 0.0027554281935412766\n",
      "Threshold:  0.9012236689095114 0.0022598870056497176 0.06557377049180328 0.0043691971600218465 0.009930486593843098 0.0028007281893292256\n",
      "Threshold:  0.9112236689095115 0.002271006813020439 0.04918032786885246 0.0043415340086830675 0.009584664536741215 0.002806361085126286\n",
      "Threshold:  0.9212236689095115 0.0030991735537190084 0.04918032786885246 0.005830903790087463 0.012376237623762377 0.0038138825324180023\n",
      "Threshold:  0.9312236689095115 0.0013736263736263737 0.01639344262295082 0.002534854245880862 0.0051440329218107 0.0016818028927009757\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "division by zero\n",
      "Precision: 0.0030991735537190084 Recall: 0.04918032786885246 F1-Score: 0.005830903790087463 F2-Score: 0.012376237623762377 F0.5-Score: 0.0038138825324180023\n",
      "Final Results:  [0.00309917 0.04918033 0.0058309  0.01237624 0.00381388]\n",
      "Best threshold:  0.9212236689095115\n"
     ]
    }
   ],
   "source": [
    "emb_indexer = {word: i for i, word in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: word for i, word in enumerate(list(embeddings.keys()))}\n",
    "emb_vals = list(embeddings.values())\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs = np.array([generate_data(elem) for elem in list(elems)])\n",
    "    targets = np.array([target for i in range(len(elems))])\n",
    "    return inputs, targets\n",
    "    \n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "# data = OrderedDict(sorted(data.items(), key=lambda x: x[0]))\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    \n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if data[key]]\n",
    "    train_data_f = [key for key in train_data if not data[key]]\n",
    "    train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 200\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    batch_size = min(batch_size, len(train_data_t))\n",
    "    num_batches = int(ceil(len(train_data_t)/batch_size))\n",
    "    batch_size_f = int(ceil(len(train_data_f)/num_batches))\n",
    "    \n",
    "    model = SiameseNetwork()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        \n",
    "#         indices = np.random.permutation(len(inputs_pos) + len(inputs_neg))\n",
    "        \n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "        \n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "        \n",
    "        \n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "#             print (inputs.shape)\n",
    "            inp_elems = torch.LongTensor(inputs.T)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "#             print (targ_elems)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inp_elems, epoch)\n",
    "#             print (outputs)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%10 == 0:\n",
    "#                 print (\"Outupts: \", list(zip(outputs.detach().numpy(), targ_elems.detach().numpy())))\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    test_data_t = [key for key in test_data if data[key]]\n",
    "    test_data_f = [key for key in test_data if not data[key]]\n",
    "    \n",
    "    res = greedy_matching()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs len:  4182\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "batch_start = batch_idx * batch_size\n",
    "batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "batch_start_f = batch_idx * batch_size_f\n",
    "batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "optimizer.zero_grad()\n",
    "\n",
    "inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])\n",
    "print (\"Inputs len: \", len(inputs))\n",
    "targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "\n",
    "indices = np.random.permutation(inputs.shape[0])\n",
    "inputs, targets = inputs[indices], targets[indices]\n",
    "inputs = torch.LongTensor(list(zip(*inputs)))\n",
    "targets = torch.LongTensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.unsqueeze(torch.LongTensor(generate_data(('conference#Review_expertise', 'edas#MeetingRoomPlace'))), 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = min(batch_size, len(test_data_t))\n",
    "num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "batch_size_f = int(ceil(len(test_data_f)/num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4143, 29262)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([el for el in res if res[el][0] > 0.99]), len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
