{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle, sys\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"reference-alignment/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "        \n",
    "reference_alignments = load_alignments(alignment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(b,a,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, include_inv=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  834\n"
     ]
    }
   ],
   "source": [
    "# Extracting USE embeddings\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0).lower() for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples())))\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "\n",
    "inp = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+\")\n",
    "X = vectorizer.fit_transform(inp)\n",
    "word2idx_tfidf = {word: i for (i, word)  in enumerate(vectorizer.get_feature_names())}\n",
    "entity2idx_tfidf = {word.split(\"#\")[1]: i for (i, word)  in enumerate(extracted_elems)}\n",
    "\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "inp = [\"<UNK>\"] + inp\n",
    "extracted_elems = [\"<UNK>\"] + extracted_elems\n",
    "\n",
    "embeds = extractUSEEmbeddings(inp)\n",
    "embeddings = dict(zip(extracted_elems, embeds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type storage\n",
    "\n",
    "types_dict = {}\n",
    "\n",
    "def get_tfidf_score(word, phrase):\n",
    "    return np.sum([X[entity2idx_tfidf[phrase]][:,word2idx_tfidf[word]][0,0] for word in parse(phrase)])\n",
    "    \n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    \n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "\n",
    "    for entity in entities:\n",
    "        types_dict[entity] = {\"type\": \"entity\"}\n",
    "    for prop in props:\n",
    "        types_dict[prop] = {\"type\": \"property\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    all_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in mappings])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "\n",
    "data = {}\n",
    "for mapping in all_mappings:\n",
    "    if mapping in gt_mappings:\n",
    "        data[(mapping[0], mapping[1])] = True\n",
    "    else:\n",
    "        data[(mapping[0], mapping[1])] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics\n",
    "    all_results = OrderedDict()\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        batch_size = min(batch_size, len(test_data_t))\n",
    "        num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "        batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "            \n",
    "            \n",
    "            \n",
    "            pos_elems = np.array(test_data_t)[batch_start:batch_end]\n",
    "            neg_elems = np.array(test_data_f)[batch_start_f:batch_end_f]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = np.array([generate_data(elem) for elem in list(pos_elems) + list(neg_elems)])\n",
    "            \n",
    "            targets = np.array([1 for i in range(len(pos_elems))] + [0 for i in range(len(neg_elems))])\n",
    "            \n",
    "            \n",
    "            indices = np.random.permutation(inputs.shape[0])\n",
    "            inputs, targets = inputs[indices].transpose(1,0,2), targets[indices]\n",
    "#             inputs = inputs.transpose(1,0,2)\n",
    "            inputs_elem = inputs.copy()\n",
    "            \n",
    "#             print (\"1\", inputs_elem)\n",
    "            \n",
    "            nonzero_elems = np.count_nonzero(inputs, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            inputs = np.stack((inputs[0][[indices[0]]], inputs[1][[indices[1]]]), axis=0)\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inputs_elem.shape[1])], [d2[k] for k in range(inputs_elem.shape[1])]))\n",
    "#             print (\"2\", rev_indices)\n",
    "#             print (\"3\", indices)\n",
    "            rev_indices = torch.LongTensor(rev_indices)\n",
    "            inputs = torch.LongTensor(inputs)\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy())\n",
    "            targets = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inputs, seq_lens, rev_indices)\n",
    "            outputs /= torch.sum(outputs, dim=1).view(-1, 1)\n",
    "            outputs = [(1-el[1].item()) for el in outputs]\n",
    "#             return\n",
    "#             print (\"2\", inputs)\n",
    "#             print (\"3\", seq_lens)\n",
    "#             print (\"4\", rev_indices)\n",
    "            \n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "#             print (inputs)\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inputs_elem[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inputs_elem[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        all_results = OrderedDict(sorted(all_results.items(), key=lambda x: x[0], reverse=True))\n",
    "        filtered_results = dict()\n",
    "        \n",
    "        entities_to_assign = set([el[0] for el in list(all_results.keys())])\n",
    "        for pair in all_results:\n",
    "            if pair[0] in entities_to_assign:\n",
    "                filtered_results[pair] = all_results[pair]\n",
    "                entities_to_assign.remove(pair[0])\n",
    "                \n",
    "        entities_to_assign = set([el[1] for el in list(all_results.keys())])\n",
    "        for pair in all_results:\n",
    "            if pair[1] in entities_to_assign:\n",
    "                filtered_results[pair] = all_results[pair]\n",
    "                entities_to_assign.remove(pair[1])        \n",
    "\n",
    "        filtered_results = OrderedDict(sorted(filtered_results.items(), key=lambda x: x[1][0], reverse=True))\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in filtered_results.values()]) - 0.01\n",
    "        high_threshold = np.max([el[0] for el in filtered_results.values()]) + 0.01\n",
    "        for j,threshold in enumerate(np.arange(low_threshold, high_threshold, 0.01)):\n",
    "            res = []\n",
    "            for i,key in enumerate(filtered_results):\n",
    "                if filtered_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not filtered_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if filtered_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "        \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        all_metrics.append((opt_threshold, optimum_metrics))\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test, inp_test1, inp_test2 = None, None, None\n",
    "\n",
    "def write(elem):\n",
    "    f = open(\"Logs\", \"a+\")\n",
    "    if type(elem) == list or type(elem) == tuple:\n",
    "        string = str(\"\\n\".join([str(s) for s in elem]))\n",
    "    else:\n",
    "        string = str(elem)\n",
    "    f.write(\"\\n\"+string)\n",
    "    f.close()\n",
    "    \n",
    "inputs3, results3 = None, None\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.name_embedding = nn.Embedding(len(embeddings), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.layer1 = nn.Bilinear(self.hidden_dim, self.hidden_dim, 2)\n",
    "\n",
    "    def forward(self, inputs, seq_lens, rev_indices):\n",
    "        results = []\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            packed_inp = pack_padded_sequence(x, seq_lens[i].numpy(), batch_first=True)\n",
    "            op, (ht, ct) = self.lstm(x)\n",
    "            x = ht[2*(self.num_layers-1):].permute(1,0,2)\n",
    "            x = x[rev_indices[i],:,:]\n",
    "            results.append(x.reshape(-1, self.hidden_dim))\n",
    "        global inputs3, results3\n",
    "        results3 = results\n",
    "        inputs3 = inputs\n",
    "        x = self.layer1(results[0], results[1])\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 122893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:110: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "/home/vlead/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Idx: 0 Loss: 0.7146655472955523\n",
      "Epoch: 0 Idx: 10 Loss: 0.1176082519284726\n",
      "Epoch: 0 Idx: 20 Loss: 0.012446852604034157\n",
      "Epoch: 0 Idx: 30 Loss: 2.2648136220874525e-09\n",
      "Epoch: 0 Idx: 40 Loss: 0.0\n",
      "Epoch: 0 Idx: 50 Loss: 4.026998913356752e-05\n",
      "Epoch: 0 Idx: 60 Loss: 0.09208088241738943\n",
      "Epoch: 0 Idx: 70 Loss: 0.006906993617233273\n",
      "Epoch: 0 Idx: 80 Loss: 0.0028398788165331277\n",
      "Epoch: 0 Idx: 90 Loss: 0.00021028501735365426\n",
      "Epoch: 0 Idx: 100 Loss: 9.812541301138478e-06\n",
      "Epoch: 0 Idx: 110 Loss: 9.967650840006475e-06\n",
      "Epoch: 0 Idx: 120 Loss: 0.00010050497313364199\n",
      "Epoch: 0 Idx: 130 Loss: 0.0002127990944003863\n",
      "Epoch: 0 Idx: 140 Loss: 0.00045763123444809186\n",
      "Epoch: 0 Idx: 150 Loss: 0.012428280929828835\n",
      "Epoch: 0 Idx: 160 Loss: 0.00019159721124777836\n",
      "Epoch: 0 Idx: 170 Loss: 1.6001921181494127e-05\n",
      "Epoch: 0 Idx: 180 Loss: 0.0001463721980203329\n",
      "Epoch: 0 Idx: 190 Loss: 0.00025384026913418274\n",
      "Epoch: 0 Idx: 200 Loss: 4.5792146578935e-05\n",
      "Epoch: 0 Idx: 210 Loss: 0.023425507891712192\n",
      "Epoch: 0 Idx: 220 Loss: 0.011083867753036483\n",
      "Epoch: 0 Idx: 230 Loss: 0.007524835635141983\n",
      "Epoch: 0 Idx: 240 Loss: 0.002284086451981621\n",
      "Epoch: 0 Idx: 250 Loss: 0.0019776809660928393\n",
      "Epoch: 0 Idx: 260 Loss: 2.97260200003568e-05\n",
      "Epoch: 0 Idx: 270 Loss: 6.327876751711163e-05\n",
      "Epoch: 0 Idx: 280 Loss: 0.004155662420512595\n",
      "Epoch: 0 Idx: 290 Loss: 0.03263938372234614\n",
      "Epoch: 0 Idx: 300 Loss: 0.0015388103896228952\n",
      "Epoch: 0 Idx: 310 Loss: 0.05619890152994238\n",
      "Epoch: 0 Idx: 320 Loss: 0.014495177204079415\n",
      "Epoch: 0 Idx: 330 Loss: 0.0006310180229897368\n",
      "Epoch: 0 Idx: 340 Loss: 0.00012964846323449612\n",
      "Epoch: 0 Idx: 350 Loss: 3.154703533232657e-05\n",
      "Epoch: 0 Idx: 360 Loss: 0.0002040201668915912\n",
      "Epoch: 0 Idx: 370 Loss: 0.00011308191878877156\n",
      "Epoch: 0 Idx: 380 Loss: 3.3518810683896025e-06\n",
      "Epoch: 0 Idx: 390 Loss: 0.0007241231802414464\n",
      "Epoch: 0 Idx: 400 Loss: 0.0007448396701227122\n",
      "Epoch: 0 Idx: 410 Loss: 0.003196088485226131\n",
      "Epoch: 0 Idx: 420 Loss: 0.02534689570116664\n",
      "Epoch: 0 Idx: 430 Loss: 0.004891916451538103\n",
      "Epoch: 0 Idx: 440 Loss: 0.0019978690979795613\n",
      "Epoch: 0 Idx: 450 Loss: 0.019403817331261102\n",
      "Epoch: 0 Idx: 460 Loss: 0.010017205569807697\n",
      "Epoch: 0 Idx: 470 Loss: 0.0009644422189788862\n",
      "Epoch: 0 Idx: 480 Loss: 0.00012354876004807118\n",
      "Epoch: 0 Idx: 490 Loss: 0.00111768695003324\n",
      "Epoch: 0 Idx: 500 Loss: 0.010725853201227327\n",
      "Epoch: 0 Idx: 510 Loss: 0.0018239008305193393\n",
      "Epoch: 0 Idx: 520 Loss: 0.000566090983886813\n",
      "Epoch: 0 Idx: 530 Loss: 5.017090875065835e-05\n",
      "Epoch: 0 Idx: 540 Loss: 1.0257922817608163\n",
      "Epoch: 0 Idx: 550 Loss: 0.03713299036868065\n",
      "Epoch: 0 Idx: 560 Loss: 0.007487149192751015\n",
      "Epoch: 0 Idx: 570 Loss: 0.005275955322303282\n",
      "Epoch: 0 Idx: 580 Loss: 0.004649768662351784\n",
      "Epoch: 0 Idx: 590 Loss: 0.009662875144651247\n",
      "Epoch: 0 Idx: 600 Loss: 0.0012019843424584187\n",
      "Epoch: 0 Idx: 610 Loss: 0.0022851487327980057\n",
      "Epoch: 0 Idx: 620 Loss: 0.03125830006487919\n",
      "Epoch: 0 Idx: 630 Loss: 0.0029212618395592555\n",
      "Epoch: 0 Idx: 640 Loss: 0.0750317086735671\n",
      "Epoch: 0 Idx: 650 Loss: 0.01657268765971431\n",
      "Epoch: 0 Idx: 660 Loss: 0.00011215161939357589\n",
      "Epoch: 0 Idx: 670 Loss: 9.406653058800723e-06\n",
      "Epoch: 0 Idx: 680 Loss: 6.682381022048649e-06\n",
      "Epoch: 0 Idx: 690 Loss: 3.4597449395630566e-05\n",
      "Epoch: 0 Idx: 700 Loss: 0.006545325888145123\n",
      "Epoch: 0 Idx: 710 Loss: 0.06332053197902252\n",
      "Epoch: 0 Idx: 720 Loss: 1.0082797517698029e-06\n",
      "Epoch: 0 Idx: 730 Loss: 0.036403551851109175\n",
      "Epoch: 0 Idx: 740 Loss: 0.011448612196118455\n",
      "Epoch: 0 Idx: 750 Loss: 0.005571355388336239\n",
      "Epoch: 0 Idx: 760 Loss: 0.0002834801411892707\n",
      "Epoch: 0 Idx: 770 Loss: 0.002113257616778695\n",
      "Epoch: 0 Idx: 780 Loss: 0.00013724801582081658\n",
      "Epoch: 0 Idx: 790 Loss: 0.00010229195708551147\n",
      "Epoch: 0 Idx: 800 Loss: 6.550912350544347e-05\n",
      "Epoch: 0 Idx: 810 Loss: 0.0003462393786825649\n",
      "Epoch: 0 Idx: 820 Loss: 0.003112923049952387\n",
      "Epoch: 0 Idx: 830 Loss: 0.01374791055222509\n",
      "Epoch: 0 Idx: 840 Loss: 0.004211265592559407\n",
      "Epoch: 0 Idx: 850 Loss: 0.0006826894045681265\n",
      "Epoch: 0 Idx: 860 Loss: 0.0002189482205699159\n",
      "Epoch: 0 Idx: 870 Loss: 0.0070487050207710235\n",
      "Epoch: 0 Idx: 880 Loss: 0.007058335016739465\n",
      "Epoch: 0 Idx: 890 Loss: 0.0041641968609045675\n",
      "Epoch: 0 Idx: 900 Loss: 0.005463823097884464\n",
      "Epoch: 0 Idx: 910 Loss: 0.030402185706618755\n",
      "Epoch: 0 Idx: 920 Loss: 0.0033878560889259355\n",
      "Epoch: 0 Idx: 930 Loss: 0.0005031257059660078\n",
      "Epoch: 0 Idx: 940 Loss: 0.00017660358940448535\n",
      "Epoch: 0 Idx: 950 Loss: 0.00010579404611043186\n",
      "Epoch: 0 Idx: 960 Loss: 0.0005988193445316135\n",
      "Epoch: 0 Idx: 970 Loss: 0.0029731939882697792\n",
      "Epoch: 0 Idx: 980 Loss: 0.011334586453651208\n",
      "Epoch: 0 Idx: 990 Loss: 0.002083985118409114\n",
      "Epoch: 0 Idx: 1000 Loss: 0.0005772232723987271\n",
      "Epoch: 0 Idx: 1010 Loss: 0.010994457745760416\n",
      "Epoch: 0 Idx: 1020 Loss: 0.014727046740314447\n",
      "Epoch: 0 Idx: 1030 Loss: 0.017621625474239087\n",
      "Epoch: 0 Idx: 1040 Loss: 0.008744227329447836\n",
      "Epoch: 0 Idx: 1050 Loss: 0.0007072166020511456\n",
      "Epoch: 0 Idx: 1060 Loss: 8.319350434674795e-05\n",
      "Epoch: 0 Idx: 1070 Loss: 6.615757424516849e-05\n",
      "Epoch: 0 Idx: 1080 Loss: 3.981693253303321e-05\n",
      "Epoch: 0 Idx: 1090 Loss: 6.074486026869596e-05\n",
      "Epoch: 0 Idx: 1100 Loss: 0.0005928780608229875\n",
      "Epoch: 0 Idx: 1110 Loss: 0.00014064912397250898\n",
      "Epoch: 0 Idx: 1120 Loss: 6.232753812392959e-05\n",
      "Epoch: 0 Idx: 1130 Loss: 0.00010414771092971212\n",
      "Epoch: 0 Idx: 1140 Loss: 7.184203907036956e-05\n",
      "Epoch: 0 Idx: 1150 Loss: 0.4364421054997596\n",
      "Epoch: 0 Idx: 1160 Loss: 0.003179096908603607\n",
      "Epoch: 0 Idx: 1170 Loss: 0.5950220878878456\n",
      "Epoch: 0 Idx: 1180 Loss: 0.010439589280258696\n",
      "Epoch: 0 Idx: 1190 Loss: 0.0017630499184042472\n",
      "Epoch: 0 Idx: 1200 Loss: 0.641471847306656\n",
      "Epoch: 0 Idx: 1210 Loss: 0.006695561971451517\n",
      "Epoch: 0 Idx: 1220 Loss: 0.0017636474091575448\n",
      "Epoch: 0 Idx: 1230 Loss: 0.0018068935871207597\n",
      "Epoch: 0 Idx: 1240 Loss: 0.0009451015997510347\n",
      "Epoch: 0 Idx: 1250 Loss: 0.00035278764684323384\n",
      "Epoch: 0 Idx: 1260 Loss: 0.01528870940384793\n",
      "Epoch: 0 Idx: 1270 Loss: 0.008462650787394702\n",
      "Epoch: 0 Idx: 1280 Loss: 0.005226023707490273\n",
      "Epoch: 0 Idx: 1290 Loss: 0.0008726919048499388\n",
      "Epoch: 0 Idx: 1300 Loss: 0.0008886752155914173\n",
      "Epoch: 0 Idx: 1310 Loss: 0.00018590125611472335\n",
      "Epoch: 0 Idx: 1320 Loss: 0.015392065038943567\n",
      "Epoch: 0 Idx: 1330 Loss: 0.003983278053721171\n",
      "Epoch: 0 Idx: 1340 Loss: 0.0014531912501970056\n",
      "Epoch: 0 Idx: 1350 Loss: 0.015396586916332836\n",
      "Epoch: 0 Idx: 1360 Loss: 0.012712086698632075\n",
      "Epoch: 0 Idx: 1370 Loss: 0.001271090291928036\n",
      "Epoch: 0 Idx: 1380 Loss: 9.358053081431917e-05\n",
      "Epoch: 0 Idx: 1390 Loss: 8.85656254045827e-05\n",
      "Epoch: 0 Idx: 1400 Loss: 0.00016864084431234921\n",
      "Epoch: 0 Idx: 1410 Loss: 0.00011239322455100669\n",
      "Epoch: 0 Idx: 1420 Loss: 0.00025028884649481965\n",
      "Epoch: 0 Idx: 1430 Loss: 0.00029685103138405204\n",
      "Epoch: 0 Idx: 1440 Loss: 0.00023529909698505468\n",
      "Epoch: 0 Idx: 1450 Loss: 0.008735068089015073\n",
      "Epoch: 0 Idx: 1460 Loss: 0.008248822491672208\n",
      "Epoch: 0 Idx: 1470 Loss: 0.0004901454333755995\n",
      "Epoch: 0 Idx: 1480 Loss: 0.006334365517602009\n",
      "Epoch: 0 Idx: 1490 Loss: 0.008432906978138215\n",
      "Epoch: 0 Idx: 1500 Loss: 0.0012535391952450865\n",
      "Epoch: 0 Idx: 1510 Loss: 0.0007185219909817804\n",
      "Epoch: 0 Idx: 1520 Loss: 0.03593097857157498\n",
      "Epoch: 0 Idx: 1530 Loss: 0.0009917166607849618\n",
      "Epoch: 0 Idx: 1540 Loss: 6.301110243964047e-06\n",
      "Epoch: 0 Idx: 1550 Loss: 3.032881219082459e-05\n",
      "Epoch: 0 Idx: 1560 Loss: 8.667016031642851e-06\n",
      "Epoch: 0 Idx: 1570 Loss: 0.039849809250574686\n",
      "Epoch: 0 Idx: 1580 Loss: 0.027018373469661322\n",
      "Epoch: 0 Idx: 1590 Loss: 0.0032102667491809836\n",
      "Epoch: 0 Idx: 1600 Loss: 0.6834054425503064\n",
      "Epoch: 0 Idx: 1610 Loss: 0.02489602124784419\n",
      "Epoch: 0 Idx: 1620 Loss: 0.013364505978622557\n",
      "Epoch: 0 Idx: 1630 Loss: 0.001553004805322948\n",
      "Epoch: 0 Idx: 1640 Loss: 0.011947471300500076\n",
      "Epoch: 0 Idx: 1650 Loss: 0.006535028269878983\n",
      "Epoch: 0 Idx: 1660 Loss: 0.000689706416201447\n",
      "Epoch: 0 Idx: 1670 Loss: 0.0037435410063372692\n",
      "Epoch: 0 Idx: 1680 Loss: 0.027195481596897435\n",
      "Epoch: 0 Idx: 1690 Loss: 0.01461259633249037\n",
      "Epoch: 0 Idx: 1700 Loss: 0.004524336383680717\n",
      "Epoch: 0 Idx: 1710 Loss: 0.0008233301731295145\n",
      "Epoch: 0 Idx: 1720 Loss: 0.0008904670785969803\n",
      "Epoch: 0 Idx: 1730 Loss: 0.00040052537269611116\n",
      "Epoch: 0 Idx: 1740 Loss: 0.00020793857662760925\n",
      "Epoch: 0 Idx: 1750 Loss: 0.00023582194892823164\n",
      "Epoch: 0 Idx: 1760 Loss: 0.00031569481933286944\n",
      "Epoch: 0 Idx: 1770 Loss: 0.0003745082899536564\n",
      "Epoch: 0 Idx: 1780 Loss: 0.005069695520992043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Idx: 1790 Loss: 0.008586280466752812\n",
      "Epoch: 0 Idx: 1800 Loss: 0.003308084061998533\n",
      "Epoch: 0 Idx: 1810 Loss: 0.00158450206464174\n",
      "Epoch: 0 Idx: 1820 Loss: 0.0006674748688541991\n",
      "Epoch: 0 Idx: 1830 Loss: 0.0003961000604873817\n",
      "Epoch: 0 Idx: 1840 Loss: 0.0030328816508178387\n",
      "Epoch: 0 Idx: 1850 Loss: 0.007619182212308248\n",
      "Epoch: 0 Idx: 1860 Loss: 0.0033552973589194126\n",
      "Epoch: 0 Idx: 1870 Loss: 0.002519587013723102\n",
      "Epoch: 0 Idx: 1880 Loss: 0.009344823747909157\n",
      "Epoch: 0 Idx: 1890 Loss: 0.011467174696818752\n",
      "Epoch: 0 Idx: 1900 Loss: 0.006057331593246162\n",
      "Epoch: 0 Idx: 1910 Loss: 0.016915778484015598\n",
      "Epoch: 0 Idx: 1920 Loss: 0.022020869325756698\n",
      "Epoch: 0 Idx: 1930 Loss: 0.010142741702197648\n",
      "Epoch: 0 Idx: 1940 Loss: 0.0036685256506756047\n",
      "Epoch: 0 Idx: 1950 Loss: 0.4582708536232004\n",
      "Epoch: 0 Idx: 1960 Loss: 0.00502980428649327\n",
      "Epoch: 0 Idx: 1970 Loss: 0.002568656294432631\n",
      "Epoch: 0 Idx: 1980 Loss: 0.0012267246514946315\n",
      "Epoch: 0 Idx: 1990 Loss: 0.0006578732903367267\n",
      "Epoch: 0 Idx: 2000 Loss: 0.0004241474734056713\n",
      "Epoch: 0 Idx: 2010 Loss: 0.0002727903995273376\n",
      "Epoch: 0 Idx: 2020 Loss: 0.0004261430167266227\n",
      "Epoch: 0 Idx: 2030 Loss: 0.0027401531540512274\n",
      "Epoch: 0 Idx: 2040 Loss: 0.008218072511566962\n",
      "Epoch: 0 Idx: 2050 Loss: 0.0043235841743384405\n",
      "Epoch: 0 Idx: 2060 Loss: 0.0009617879711644034\n",
      "Epoch: 0 Idx: 2070 Loss: 0.8538682363275892\n",
      "Epoch: 0 Idx: 2080 Loss: 0.03651749234100383\n",
      "Epoch: 0 Idx: 2090 Loss: 0.001917318514313301\n",
      "Epoch: 0 Idx: 2100 Loss: 0.0003736448765202927\n",
      "Epoch: 0 Idx: 2110 Loss: 0.018933002666337878\n",
      "Epoch: 0 Idx: 2120 Loss: 0.013249285085718921\n",
      "Epoch: 0 Idx: 2130 Loss: 0.006865793080393506\n",
      "Epoch: 0 Idx: 2140 Loss: 0.0024468714927339645\n",
      "Epoch: 0 Idx: 2150 Loss: 0.0005240869657199916\n",
      "Epoch: 0 Idx: 2160 Loss: 0.0005134126754253816\n",
      "Epoch: 0 Idx: 2170 Loss: 0.000282663885493362\n",
      "Epoch: 0 Idx: 2180 Loss: 0.005426466962548206\n",
      "Epoch: 0 Idx: 2190 Loss: 0.00909717539618325\n",
      "Epoch: 0 Idx: 2200 Loss: 0.005384217030620362\n",
      "Epoch: 0 Idx: 2210 Loss: 0.0012043834779726509\n",
      "Epoch: 0 Idx: 2220 Loss: 0.0006419047793123839\n",
      "Epoch: 0 Idx: 2230 Loss: 0.001933151398034821\n",
      "Epoch: 0 Idx: 2240 Loss: 0.016700751609828975\n",
      "Epoch: 0 Idx: 2250 Loss: 0.013099529046927424\n",
      "Epoch: 0 Idx: 2260 Loss: 0.0037065780565310145\n",
      "Epoch: 0 Idx: 2270 Loss: 0.00047521192947369936\n",
      "Epoch: 0 Idx: 2280 Loss: 0.006654181133087604\n",
      "Epoch: 0 Idx: 2290 Loss: 0.02216280434979195\n",
      "Epoch: 0 Idx: 2300 Loss: 0.009460460585458264\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emb_indexer = {word: i for i, word in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: word for i, word in enumerate(list(embeddings.keys()))}\n",
    "emb_vals = list(embeddings.values())\n",
    "\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    ont_obj = Ontology(\"conference_ontologies/\" + ont + \".owl\")\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c) in triples]\n",
    "    neighbours_dict = {elem: [elem] for elem in list(set(flatten(entities)))}\n",
    "    for e1, e2 in entities:\n",
    "        neighbours_dict[e1].append(e2)\n",
    "        neighbours_dict[e2].append(e1)\n",
    "    \n",
    "    prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "    neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "    for e1, e2, p in prop_triples:\n",
    "        neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "        if elem not in neighbours_dict:\n",
    "            neighbours_dict[elem] = [elem]\n",
    "\n",
    "    neighbours_dict = {el: neighbours_dict[el][:1] + sorted(list(set(neighbours_dict[el][1:])))\n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: neighbours_dict[el][:10] for el in neighbours_dict}\n",
    "    neighbours_dict = {ont + \"#\" + el: [ont + \"#\" + e for e in neighbours_dict[el]] for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    op = np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs = np.array([generate_data(elem) for elem in list(elems)])\n",
    "    targets = np.array([target for i in range(len(elems))])\n",
    "    return inputs, targets\n",
    "\n",
    "neighbours_dicts = {ont: get_one_hop_neighbours(ont) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "max_neighbours = np.max(flatten([[len(el[e]) for e in el] for el in neighbours_dicts.values()]))\n",
    "neighbours_lens = {ont: {key: len(neighbours_dicts[ont][key]) for key in neighbours_dicts[ont]}\n",
    "                   for ont in neighbours_dicts}\n",
    "neighbours_dicts = {ont: {key: neighbours_dicts[ont][key] + [\"<UNK>\" for i in range(max_neighbours -len(neighbours_dicts[ont][key]))]\n",
    "              for key in neighbours_dicts[ont]} for ont in neighbours_dicts}\n",
    "\n",
    "data_items = data.items()\n",
    "np.random.shuffle(list(data_items))\n",
    "data = OrderedDict(data_items)\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    \n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if data[key]]\n",
    "    train_data_f = [key for key in train_data if not data[key]]\n",
    "    train_data_f = train_data_f[:int(0.75*len(train_data_f))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "#     np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 10\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    batch_size = min(batch_size, len(train_data_t))\n",
    "    num_batches = int(ceil((len(train_data_t) + len(train_data_f))/batch_size))\n",
    "    \n",
    "    \n",
    "    model = SiameseNetwork(512, 512, 1)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        \n",
    "        indices = np.random.permutation(len(inputs_pos) + len(inputs_neg))\n",
    "        \n",
    "        inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "        targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "\n",
    "#         inputs = np.array(list(inputs_pos) + list(inputs_neg))\n",
    "#         targets = np.array(list(targets_pos) + list(targets_neg))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inp = inputs[batch_start:batch_end].transpose(1,0,2)\n",
    "            nonzero_elems = np.count_nonzero(inp, axis=-1)\n",
    "            indices = np.flip(np.argsort(nonzero_elems, axis=-1), axis=-1)\n",
    "            seq_lens = np.flip(np.sort(nonzero_elems, axis=-1), axis=-1)\n",
    "            inp_elems = np.stack((inp[0][[indices[0]]], inp[1][[indices[1]]]), axis=0)\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inp_elems)\n",
    "            targ_elems = torch.LongTensor(targets[batch_start:batch_end])\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            d1 = {elem:i for i,elem in enumerate(indices[0])}\n",
    "            d2 = {elem:i for i,elem in enumerate(indices[1])}\n",
    "            rev_indices = np.stack(([d1[k] for k in range(inp.shape[1])], \n",
    "                                    [d2[k] for k in range(inp.shape[1])]))\n",
    "\n",
    "            rev_indices = torch.LongTensor(rev_indices)\n",
    "            seq_lens = torch.LongTensor(seq_lens.copy())\n",
    "            outputs = model(inp_elems, seq_lens, rev_indices)\n",
    "            loss = F.nll_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "#             break\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%10 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    test_data_t = [key for key in test_data if data[key]]\n",
    "    test_data_f = [key for key in test_data if not data[key]]\n",
    "    \n",
    "    res = greedy_matching()\n",
    "    break\n",
    "# print (\"Final Results: \", np.mean([el[1] for el in all_metrics], axis=0))\n",
    "# print (\"Best threshold: \", all_metrics[np.argmax([el[1][2] for el in all_metrics])][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Conference_part', 0),\n",
       " ('Workshop', 1.0),\n",
       " ('string', 1.0),\n",
       " ('Tutorial', 1.0),\n",
       " ('Topic', 1.0),\n",
       " ('Track', 1.0),\n",
       " ('Track-workshop_chair', 1.0),\n",
       " ('Conference_volume', 1.0),\n",
       " ('Program_committee', 2.0),\n",
       " ('Person', 2.0),\n",
       " ('Conference', 2.0),\n",
       " ('Organizing_committee', 2.0),\n",
       " ('Steering_committee', 2.0),\n",
       " ('Important_dates', 2.0),\n",
       " ('Review_preference', 2.0),\n",
       " ('Conference_www', 2.0),\n",
       " ('Conference_contribution', 2.0),\n",
       " ('Committee', 2.0),\n",
       " ('Conference_proceedings', 2.0),\n",
       " ('Publisher', 2.0),\n",
       " ('Committee_member', 3.0),\n",
       " ('Submitted_contribution', 3.0),\n",
       " ('Written_contribution', 3.0),\n",
       " ('date', 3.0),\n",
       " ('Conference_contributor', 3.0),\n",
       " ('Poster', 3.0),\n",
       " ('int', 3.0),\n",
       " ('Co-chair', 3.0),\n",
       " ('Conference_document', 3.0),\n",
       " ('Conference_participant', 3.0),\n",
       " ('Chair', 3.0),\n",
       " ('Conference_applicant', 3.0),\n",
       " ('Reviewer', 3.0),\n",
       " ('Presentation', 3.0),\n",
       " ('Thing', 3.0),\n",
       " ('Review_expertise', 4.0),\n",
       " ('Invited_speaker', 4.0),\n",
       " ('Reviewed_contribution', 4.0),\n",
       " ('Invited_talk', 4.0),\n",
       " ('Registeered_applicant', 4.0),\n",
       " ('Passive_conference_participant', 4.0),\n",
       " ('Conference_announcement', 4.0),\n",
       " ('Active_conference_participant', 4.0),\n",
       " ('Call_for_paper', 4.0),\n",
       " ('Information_for_participants', 4.0),\n",
       " ('Abstract', 4.0),\n",
       " ('Regular_contribution', 4.0),\n",
       " ('Organizer', 4.0),\n",
       " ('Regular_author', 4.0),\n",
       " ('Call_for_participation', 4.0),\n",
       " ('Review', 4.0),\n",
       " ('Contribution_co-author', 5.0),\n",
       " ('Accepted_contribution', 5.0),\n",
       " ('Paper', 5.0),\n",
       " ('Rejected_contribution', 5.0),\n",
       " ('Organization', 5.0),\n",
       " ('Contribution_1th-author', 5.0),\n",
       " ('Extended_abstract', 5.0),\n",
       " ('Paid_applicant', 5.0),\n",
       " ('Early_paid_applicant', 6.0),\n",
       " ('Late_paid_applicant', 6.0),\n",
       " ('Camera_ready_contribution', 6.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def djikstra(n, adj_matrix, start_node): \n",
    "\n",
    "    distances = [sys.maxsize] * n \n",
    "    path = [False] * n\n",
    "    \n",
    "    distances[start_node] = 0\n",
    "    \n",
    "    for node in range(n): \n",
    "        \n",
    "        distances_dict = {elem: i for (i,elem) in enumerate(distances) if not path[i]}\n",
    "        closest_node = distances_dict[min(list(distances_dict.keys()))]\n",
    "\n",
    "        path[closest_node] = True\n",
    " \n",
    "        for curr_node in range(n): \n",
    "            if adj_matrix[closest_node][curr_node] > 0 and not path[curr_node] and \\\n",
    "            distances[curr_node] > distances[closest_node] + adj_matrix[closest_node][curr_node]: \n",
    "                distances[curr_node] = distances[closest_node] + adj_matrix[closest_node][curr_node]\n",
    "\n",
    "    return distances\n",
    "\n",
    "all_triples = Ontology(\"conference_ontologies/conference.owl\").get_triples()\n",
    "\n",
    "entities = {entity:i for i,entity in enumerate(list(set(flatten([(el[0], el[1]) for el in all_triples]))))}\n",
    "entities_inv = {entities[entity]:entity for entity in entities}\n",
    "\n",
    "adj_mat = np.zeros((len(entities), len(entities)))\n",
    "\n",
    "for (a,b,_) in all_triples:\n",
    "    adj_mat[entities[a]][entities[b]] = 1\n",
    "    adj_mat[entities[b]][entities[a]] = 1\n",
    "\n",
    "src = entities[\"Conference_part\"]\n",
    "sorted([(entities_inv[i], entity) for i,entity in enumerate(djikstra(len(entities), adj_mat, src))], key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Workshop',\n",
       " 'string',\n",
       " 'Tutorial',\n",
       " 'Topic',\n",
       " 'Track',\n",
       " 'Track-workshop_chair',\n",
       " 'Conference_volume']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Workshop', 1.0),\n",
       " ('string', 1.0),\n",
       " ('Tutorial', 1.0),\n",
       " ('Topic', 1.0),\n",
       " ('Track', 1.0),\n",
       " ('Conference_part', 0),\n",
       " ('Track-workshop_chair', 1.0),\n",
       " ('Conference_volume', 1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(entities_inv[i], entity) for i,entity in enumerate(djikstra(len(entities), adj_mat, src)) if entity < sys.maxsize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
