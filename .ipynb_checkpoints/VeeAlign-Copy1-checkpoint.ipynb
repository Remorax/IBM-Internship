{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"reference-alignment/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "        \n",
    "reference_alignments = load_alignments(alignment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(a,b,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True):\n",
    "        if union_flag == 0:\n",
    "            return self.triples\n",
    "        else:\n",
    "            return self.parse_triples(union_flag = 1, subclass_of = False)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  1228\n"
     ]
    }
   ],
   "source": [
    "# Extracting USE embeddings\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0).lower() for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples(subclass_of=False))))\n",
    "    extracted_elems.extend(entities + props + triples)\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "\n",
    "entities_parsed = [parse(word) for word in extracted_elems]\n",
    "inp = [\" \".join(e) for e in entities_parsed]\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+\")\n",
    "X = vectorizer.fit_transform(inp)\n",
    "word2idx_tfidf = {word: i for (i, word)  in enumerate(vectorizer.get_feature_names())}\n",
    "entity2idx_tfidf = {word: i for (i, word)  in enumerate(extracted_elems)}\n",
    "\n",
    "\n",
    "extracted_elems.extend(flatten(entities_parsed))\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "embeds = extractUSEEmbeddings([\" \".join(parse(word)) for word in extracted_elems])\n",
    "embeddings = dict(zip(extracted_elems, embeds))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature generation\n",
    "\n",
    "features_dict = {}\n",
    "\n",
    "def get_tfidf_score(word, phrase):\n",
    "    temp = X[entity2idx_tfidf[phrase]]\n",
    "    return temp[:,word2idx_tfidf[word]][0,0]\n",
    "\n",
    "def get_property_features(triple):\n",
    "    weight = np.sum([get_tfidf_score(word, triple[-1]) for word in parse(triple[-1])])\n",
    "    feats = []\n",
    "    feats.append(weight*embeddings[triple[-1]]) # Property name\n",
    "    feats.append([embeddings[el] for el in triple[0].split(\"###\")]) # Domain \n",
    "    feats.append([embeddings[el] for el in triple[1].split(\"###\")]) # Range\n",
    "    return feats\n",
    "    \n",
    "def get_entity_features(entity):\n",
    "    feats = []\n",
    "    weight = np.sum([get_tfidf_score(word, entity) for word in parse(entity)])\n",
    "    feats.append(weight * embeddings[entity]) # Entity name\n",
    "#     feats.append(np.sum([get_tfidf_score(word, entity) * embeddings[word] for word in parse(entity)]))\n",
    "    return feats\n",
    "    \n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    \n",
    "    triples = ont.get_triples(union_flag=1, subclass_of=False)\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "\n",
    "    for triple in triples:\n",
    "        features_dict[triple[-1]] = {\"type\": \"triple\", \"features\": get_property_features(triple)}\n",
    "    for entity in entities:\n",
    "        features_dict[entity] = {\"type\": \"entity\", \"features\": get_entity_features(entity)}\n",
    "    for prop in props:\n",
    "        if prop not in features_dict:\n",
    "            features_dict[prop] = {\"type\": \"property\", \"features\": get_entity_features(prop)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    all_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in mappings])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "\n",
    "def calc_sim_score(mapping):\n",
    "    features = embedify(mapping)\n",
    "    if features[0][\"type\"] != \"triple\" and features[1][\"type\"] != \"triple\":\n",
    "        return cos_sim(*tuple([el[\"features\"][0] for el in features]))\n",
    "    elif features[0][\"type\"] == \"triple\"  and features[1][\"type\"] == \"triple\":\n",
    "        sim_score = 0\n",
    "        names, domains, ranges = [], [], []\n",
    "        \n",
    "        for elem in [el[\"features\"] for el in features]:\n",
    "            names.append(elem[0])\n",
    "            domains.append(elem[1])\n",
    "            ranges.append(elem[2])\n",
    "        name_score = cos_sim(names[0], names[1])\n",
    "#         domain_score = np.mean([cos_sim(*elem) for elem in itertools.product(domains[0], domains[1])])\n",
    "#         ranges_score = np.mean([cos_sim(*elem) for elem in itertools.product(ranges[0], ranges[1])])\n",
    "#         return ((name_score + domain_score + ranges_score)/(len(domains[0]) + len(ranges[0]) +  ))\n",
    "#         return (name_score + domain_score + ranges_score)/3\n",
    "        return name_score\n",
    "    return cos_sim(*tuple([el[\"features\"][0] for el in features])) \n",
    "        \n",
    "        \n",
    "\n",
    "def embedify(mapping):\n",
    "    removed_hash = tuple([elem.split(\"#\")[1] for elem in mapping])\n",
    "    return (features_dict[removed_hash[0]], features_dict[removed_hash[1]])\n",
    "\n",
    "data = {}\n",
    "for mapping in all_mappings:\n",
    "    if mapping in gt_mappings:\n",
    "        data[(mapping[0], mapping[1])] = (calc_sim_score(mapping), \"T\")\n",
    "    else:\n",
    "        data[(mapping[0], mapping[1])] = (calc_sim_score(mapping), \"F\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.15 Time =  4.267692565917969e-05\n",
      "0.0038821550273907606 0.989010989010989 0.007733952049497294 0.019110714740731307 0.004847936395074497\n",
      "threshold = 0.16 Time =  3.3263349533081055\n",
      "0.004109088695440434 0.989010989010989 0.008184174231975873 0.020209580838323356 0.005131031337298798\n",
      "threshold = 0.17 Time =  6.33664083480835\n",
      "0.0043719740272357786 0.989010989010989 0.00870546509753345 0.0214800553708094 0.005458934658573964\n",
      "threshold = 0.18000000000000002 Time =  9.057249307632446\n"
     ]
    }
   ],
   "source": [
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "data = OrderedDict(sorted(data.items(),  key=lambda x:x[1][0], reverse=True))\n",
    "all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "results = []\n",
    "failed = []\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    opt_threshold, optimum_metrics = -1000, [-1000 for i in range(5)]\n",
    "    t = time.time()\n",
    "    for j,threshold in enumerate(np.arange(0.15, 1.0005, 0.01)):\n",
    "        print (\"threshold =\", threshold, \"Time = \", time.time()-t) \n",
    "        pred = []\n",
    "        for i,key in enumerate(train_data):\n",
    "            if train_data[key][0] > threshold:\n",
    "                pred.append(key)\n",
    "\n",
    "        tp = len([elem for elem in pred if train_data[elem][1] == \"T\"])\n",
    "        fn = len([key for key in gt_mappings if key not in set(pred) and is_valid(test_onto, key)])\n",
    "        fp = len([elem for elem in pred if train_data[elem][1] == \"F\"])\n",
    "\n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "        if f1score > optimum_metrics[2]:\n",
    "            optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "            opt_threshold = threshold\n",
    "    \n",
    "    threshold = opt_threshold\n",
    "    pred = []\n",
    "    for i,key in enumerate(test_data):\n",
    "        if test_data[key][0] > threshold:\n",
    "            pred.append(key)\n",
    "\n",
    "    curr = dict()\n",
    "    \n",
    "    curr[\"fn\"] = [key for key in gt_mappings if key not in set(pred) and not is_valid(test_onto, key)]\n",
    "    curr[\"fp\"] = [elem for elem in pred if test_data[elem][1] == \"F\"]\n",
    "    tp = len([elem for elem in pred if test_data[elem][1] == \"T\"])\n",
    "    fn = len(curr[\"fn\"])\n",
    "    fp = len(curr[\"fp\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "        f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        pass\n",
    "            \n",
    "    metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "    failed.append(curr)\n",
    "    results.append(metrics)\n",
    "\n",
    "print (\"Final Results:\", np.mean(results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "# idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "# features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "# labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "# build graph\n",
    "\n",
    "\n",
    "\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}\n",
    "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "# adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "# features = normalize_features(features)\n",
    "# adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "# idx_train = range(140)\n",
    "# idx_val = range(200, 500)\n",
    "# idx_test = range(500, 1500)\n",
    "\n",
    "# adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "# features = torch.FloatTensor(np.array(features.todense()))\n",
    "# labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "# idx_train = torch.LongTensor(idx_train)\n",
    "# idx_val = torch.LongTensor(idx_val)\n",
    "# idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, dropout=0.3, alpha=0.001, nheads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(embeddings), 512)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(embeddings))})\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        results = []\n",
    "        for i in range(2):            \n",
    "            x = F.dropout(data[i], self.dropout, training=self.training)\n",
    "            x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "            x = F.leaky_relu(F.dropout(x, self.dropout, training=self.training))\n",
    "            results.append(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Contribution', 'string'), ('Social_event###Working_event', 'string'), ('Working_event', 'Administrative_event'), ('Event', 'string'), ('Person###Organization', 'string'), ('Contribution', 'string'), ('Author', 'Contribution'), ('Participant', 'boolean'), ('Social_event###Working_event', 'string'), ('Organization###Person', 'Country'), ('Event', 'string'), ('Member_PC', 'Topic'), ('Person', 'string'), ('Contribution', 'string'), ('Contribution', 'Topic'), ('Person', 'string'), ('Administrative_event', 'Administrative_event'), ('Member_PC', 'Contribution'), ('Person###Organization', 'string'), ('Person', 'Organization'), ('Person', 'string'), ('Administrative_event', 'Administrative_event'), ('Person', 'string'), ('Contribution', 'string'), ('Contribution', 'string'), ('Person', 'string'), ('Contribution', 'Author'), ('Social_event###Working_event', 'string'), ('Working_event', 'Topic'), ('Social_event###Working_event', 'positiveInteger'), ('Scholar', 'University'), ('Organization###Person', 'City'), ('Person', 'string'), ('Social_event###Working_event', 'string'), ('Social_event###Working_event', 'positiveInteger'), ('Social_event###Working_event', 'positiveInteger')]\n",
      "[('Administrator', 'Paper'), ('Administrator', 'Conference'), ('ProgramCommitteeMember', 'Administrator'), ('Conference', 'boolean'), ('Paper', 'unsignedLong'), ('Conference', 'Administrator'), ('Review', 'Reviewer'), ('Administrator', 'Paper'), ('Conference', 'Administrator'), ('ProgramCommitteeMember', 'ProgramCommittee'), ('Administrator', 'Conference'), ('Paper', 'Bid'), ('Co-author', 'Paper'), ('Paper', 'Decision'), ('Paper', 'Administrator'), ('Paper', 'Author'), ('Administrator', 'ProgramCommitteeMember'), ('Conference', 'int'), ('Bid', 'Reviewer'), ('ExternalReviewer', 'Reviewer'), ('Person', 'Document'), ('ProgramCommitteeMember', 'int'), ('Administrator', 'Conference'), ('Conference', 'Administrator'), ('Reviewer', 'Paper'), ('Administrator', 'Reviewer'), ('Administrator', 'Conference'), ('ConferenceMember', 'Conference'), ('Conference', 'Administrator'), ('Person', 'string'), ('Paper', 'Reviewer'), ('Administrator', 'Conference'), ('Paper', 'Reviewer'), ('Conference###Person', 'string'), ('Reviewer', 'Administrator'), ('Administrator', 'ProgramCommitteeMember'), ('Conference', 'anyURI'), ('Conference', 'anyURI'), ('Conference', 'Administrator'), ('Reviewer', 'ExternalReviewer'), ('Reviewer', 'Bid'), ('Paper', 'Co-author'), ('Reviewer', 'Review'), ('Paper', 'Meta-Reviewer'), ('Administrator', 'Conference'), ('Paper', 'SubjectArea'), ('Paper', 'Administrator'), ('Conference', 'Administrator'), ('Reviewer###Chairman###Author', 'Paper'), ('ProgramCommittee', 'ProgramCommitteeMember'), ('Paper', 'string'), ('Administrator', 'Conference'), ('ProgramCommitteeChair', 'Review'), ('Conference', 'ConferenceMember'), ('Author', 'Paper'), ('Conference', 'date'), ('Author', 'Paper'), ('Reviewer', 'Paper'), ('Conference', 'Administrator')]\n",
      "[('Document', 'Deadline'), ('ACM_SIGKDD', 'Conference'), ('Author', 'Paper'), ('Deadline', 'dateTime'), ('Author', 'Award'), ('Registration_fee', 'int'), ('ACM_SIGKDD', 'Deadline'), ('Deadline', 'ACM_SIGKDD'), ('Author', 'Deadline_Author_notification'), ('Sponzor', 'string'), ('Conference', 'dateTime'), ('Person', 'string'), ('Conference', 'string'), ('Conference', 'ACM_SIGKDD'), ('Person', 'Place'), ('Person', 'string'), ('Sponzor', 'ACM_SIGKDD'), ('Conference', 'string'), ('Person', 'Registration_fee'), ('Document', 'Speaker'), ('Conference', 'dateTime'), ('Person', 'string'), ('Speaker', 'Document'), ('Registration_fee', 'string'), ('ACM_SIGKDD', 'Sponzor'), ('Registration_fee', 'Person')]\n",
      "[('Committee', 'Conference_volume'), ('Conference_part', 'Topic'), ('Conference_proceedings', 'Publisher'), ('Important_dates', 'date'), ('Conference_volume', 'string'), ('Review_expertise', 'Submitted_contribution'), ('Reviewer', 'Reviewer'), ('Committee_member', 'Committee'), ('Committee', 'Chair'), ('Person', 'string'), ('Important_dates', 'date'), ('Review', 'Reviewed_contribution'), ('Reviewer', 'Review_preference###Review_expertise'), ('Review_preference', 'int'), ('Topic', 'Review_preference'), ('Conference_document', 'date'), ('Conference_volume', 'Important_dates'), ('Person', 'string'), ('Organizing_committee', 'Conference_volume'), ('Conference_proceedings', 'string'), ('Conference_part', 'Conference_volume'), ('Steering_committee', 'Conference_volume'), ('Co-chair', 'Committee'), ('Submitted_contribution###Topic', 'Review_preference'), ('Person', 'string'), ('Committee', 'Committee_member'), ('Conference_proceedings###Publisher###Conference###Topic###Conference_part', 'string'), ('Chair', 'Committee'), ('Committee', 'Co-chair'), ('Conference_part', 'Track-workshop_chair'), ('Conference_proceedings', 'int'), ('Publisher', 'Conference_proceedings'), ('Conference_volume', 'Steering_committee'), ('Topic', 'Conference_part'), ('Conference_volume', 'Conference_contribution'), ('Conference_contribution', 'Conference_volume'), ('Conference_volume', 'Program_committee'), ('Important_dates', 'date'), ('Reviewed_contribution', 'Review'), ('Submitted_contribution', 'Review_expertise'), ('Active_conference_participant', 'Presentation'), ('Conference_volume', 'Tutorial'), ('Reviewer', 'Reviewer'), ('Conference_www', 'string'), ('Conference_volume', 'Track'), ('Important_dates', 'date'), ('Review_preference###Review_expertise', 'Reviewer'), ('Conference_volume', 'Conference_part'), ('Abstract', 'Paper###Poster###Presentation'), ('Conference_volume', 'Committee'), ('Track-workshop_chair', 'Conference_part'), ('Presentation###Paper###Poster', 'Abstract'), ('Conference_document', 'Person'), ('Important_dates', 'Conference_volume'), ('Presentation', 'Active_conference_participant'), ('Important_dates', 'date'), ('Conference_volume', 'Organizing_committee'), ('Important_dates', 'date'), ('Conference_volume', 'Workshop'), ('Person', 'Conference_document'), ('Review_preference', 'Submitted_contribution###Topic'), ('Program_committee', 'Conference_volume')]\n",
      "[('AcceptedPaper', 'PaperPresentation'), ('Person', 'string'), ('Conference###Paper', 'Topic'), ('PaperPresentation', 'AcceptedPaper'), ('ContactInformation', 'string'), ('ContactInformation', 'string'), ('Sponsorship', 'Organization'), ('Topic', 'Paper###Conference'), ('ContactInformation', 'string'), ('AcademicEvent', 'Call'), ('Call', 'SessionChair###TPCMember###ConferenceChair'), ('Sponsorship', 'string'), ('ConferenceEvent', 'Programme'), ('Reviewer###SessionChair###TPCMember###ConferenceChair', 'Paper'), ('Person', 'string'), ('Call', 'AcademicEvent'), ('Person', 'string'), ('PersonalReviewHistory', 'Reviewer'), ('ConferenceEvent', 'Person'), ('MealEvent', 'MealMenu'), ('Conference', 'dateTime'), ('Conference', 'dateTime'), ('Reviewer', 'PersonalReviewHistory'), ('AcademicEvent###Conference###Person###ConferenceSession', 'Document'), ('Call', 'dateTime'), ('Conference', 'dateTime'), ('Conference', 'string'), ('Conference', 'Person'), ('Sponsorship', 'int'), ('Place', 'ConferenceEvent'), ('Author', 'Paper'), ('Person', 'ConferenceEvent'), ('Organization', 'Sponsorship'), ('Conference', 'dateTime'), ('ContactInformation###Person', 'string'), ('Conference', 'Country'), ('Paper', 'Author'), ('Conference', 'dateTime'), ('ContactInformation', 'string'), ('ConferenceChair###SessionChair###TPCMember', 'Call'), ('ConferenceEvent', 'dateTime'), ('Call', 'string'), ('Paper', 'SessionChair###Reviewer###TPCMember###ConferenceChair'), ('ActivePaper', 'ReviewRating'), ('ConferenceEvent', 'Place'), ('MealMenu', 'MealEvent'), ('Document', 'AcademicEvent###Conference###Person###ConferenceSession'), ('ConferenceEvent', 'dateTime'), ('Programme', 'ConferenceEvent'), ('Person', 'Conference')]\n",
      "[('Activity###Money###Time', 'Person###Activity'), ('Item', 'Person'), ('Person', 'Item'), ('Activity', 'Time###Place'), ('Person', 'Item'), ('Item', 'Person'), ('Deadline', 'date'), ('Person', 'Money'), ('Item###Activity', 'Person'), ('Money', 'Activity###Item'), ('Person', 'Item'), ('Item', 'Time'), ('Person', 'Item'), ('Place', 'Item'), ('Activity###Person', 'Item###Place'), ('Item', 'Person'), ('Activity', 'dateTime'), ('Person', 'Building###State###City'), ('Building###State###City', 'Person'), ('Item', 'Activity'), ('Person', 'Item'), ('Item', 'Time'), ('Item', 'Item'), ('Activity', 'Time'), ('Person', 'Time###Place'), ('Item', 'Person'), ('Item', 'Person'), ('Activity', 'Time'), ('Item', 'Activity###Money'), ('Place###City###Building', 'State###Place###Building###City'), ('Person', 'Place###Time'), ('Item', 'Person'), ('Person', 'Item'), ('Item', 'Person'), ('Money', 'Item'), ('Person', 'Time'), ('Money', 'Building'), ('State###City###Building', 'Person'), ('Money', 'Person'), ('Person', 'Item###Activity'), ('Money', 'int')]\n",
      "[('Event', 'Programme_Brochure###Web_Site'), ('Person', 'Document'), ('Organisation###Person', 'Event'), ('Paper', 'Review'), ('Paper', 'Possible_Reviewer'), ('Review', 'Possible_Reviewer'), ('Event', 'Event'), ('Individual_Presentation', 'Abstract###Paper'), ('Event', 'Location'), ('Event', 'Event'), ('Abstract###Paper', 'Individual_Presentation'), ('Document', 'Person'), ('Review', 'Paper'), ('Organisation', 'Multi-author_Volume###Programme_Brochure###Web_Site###Flyer'), ('Research_Topic', 'Paper###Event'), ('Document', 'Document'), ('Programme_Brochure###Web_Site', 'Event'), ('Document', 'Document'), ('Paper###Event', 'Research_Topic'), ('Event', 'Organisation###Person'), ('Possible_Reviewer', 'Paper'), ('Location', 'Event')]\n"
     ]
    }
   ],
   "source": [
    "trips_all, entities_all = [], []\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    \n",
    "    triples = ont.get_triples(union_flag=1, subclass_of=False)\n",
    "    entities = ont.get_entities()\n",
    "    trips_all.extend(triples)\n",
    "    entities_all.extend(entities)\n",
    "    print ([el[:-1] for el in triples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    " a  = list(set(flatten(flatten([list(itertools.product(el[0].split(\"###\"), el[1].split(\"###\"))) for el in trips_all]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dateTime',\n",
       " 'string',\n",
       " 'int',\n",
       " 'date',\n",
       " 'boolean',\n",
       " 'anyURI',\n",
       " 'positiveInteger',\n",
       " 'unsignedLong']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el for el in a if el not in list(set(entities_all))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
