{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.argv = [\"main\", 6, \"Input/data_german_oaei.pkl\", \"test.pkl\", \"model.pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbours: 6\n",
      "Number of entities: 150000\n",
      "Training size: 127500 Val size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.30518708207151224\n",
      "Len (direct inputs):  524\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Training size: 127500 Val size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.29523230957740376\n",
      "Len (direct inputs):  514\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Training size: 127500 Val size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.3304262296540006\n",
      "Len (direct inputs):  515\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Training size: 127500 Val size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.34091159831885176\n",
      "Len (direct inputs):  528\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Training size: 127500 Val size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.3512884075053102\n",
      "Len (direct inputs):  526\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Training size: 127500 Val size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.3542009796763982\n",
      "Len (direct inputs):  517\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "float division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Epoch: 0 Idx: 0 Loss: 0.27430620325967603\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SiameseNetwork:\n\tsize mismatch for name_embedding.weight: copying a param with shape torch.Size([3291, 512]) from checkpoint, the shape in current model is torch.Size([830, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4b9b4f13f6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiameseNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_vals_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_dict_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0montologies_in_alignment_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 847\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SiameseNetwork:\n\tsize mismatch for name_embedding.weight: copying a param with shape torch.Size([3291, 512]) from checkpoint, the shape in current model is torch.Size([830, 512])."
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle, operator\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[2], \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, emb_vals, neighbours_dicts, ontologies_in_alignment_rev, data_conf, emb_indexer_conf, emb_indexer_inv_conf, emb_vals_conf, gt_mappings_conf, neighbours_dicts_pathpadded_conf, ontologies_in_alignment_conf = pickle.load(f)\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "def test():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv_conf, gt_mappings_conf, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()    \n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "        nodes_all = np.array(nodes_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            nodes = nodes_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "    return (test_data_t, all_results)\n",
    "\n",
    "def optimize_threshold():\n",
    "    global batch_size, val_data_t, val_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(val_data_t)\n",
    "        np.random.shuffle(val_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(val_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(val_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        nodes_all = np.array(nodes_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            nodes = nodes_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            s = set(res)\n",
    "            fn_list = [(key, all_results[key][0]) for key in val_data_t if key not in s]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            # print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "            if threshold in threshold_results:\n",
    "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
    "            else:\n",
    "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
    "\n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            threshold += step \n",
    "        \n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        s = set(res)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in s]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "    return (all_fn, all_fp, all_metrics)\n",
    "\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, emb_vals, features_dict, threshold=0.9):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.features_arr = np.array(list(features_dict.values()))\n",
    "        self.n_neighbours = self.features_arr.shape[1]\n",
    "        self.max_paths = self.features_arr.shape[2]\n",
    "        self.max_pathlen = self.features_arr.shape[3]\n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.threshold = nn.Parameter(torch.DoubleTensor([threshold]))\n",
    "        self.threshold.requires_grad = False\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
    "        \n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(self.max_pathlen) for i in range(self.max_pathlen)]))\n",
    "        self.w_rootpath = nn.Parameter(torch.DoubleTensor([0.25]))\n",
    "        self.w_children = nn.Parameter(torch.DoubleTensor([0.25]))\n",
    "        self.w_obj_neighbours = nn.Parameter(torch.DoubleTensor([0.25]))\n",
    " \n",
    "    def forward(self, nodes, features):\n",
    "        '''\n",
    "        Arguments:\n",
    "            - nodes: batch_size * 2\n",
    "            - features: batch_size * 2 * 4 * max_paths * max_pathlen\n",
    "        '''\n",
    "        results = []\n",
    "        nodes = nodes.permute(1,0) # 2 * batch_size\n",
    "        features = features.permute(1,0,2,3,4) # 2 * batch_size * 4 * max_paths * max_pathlen\n",
    "        for i in range(2):\n",
    "            node_emb = self.name_embedding(nodes[i]) # batch_size * 512\n",
    "            feature_emb = self.name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512\n",
    "            \n",
    "            feature_emb_reshaped = feature_emb.permute(0,4,1,2,3).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_paths * self.max_pathlen)\n",
    "            path_weights = torch.bmm(node_emb[:, None, :], feature_emb_reshaped)\n",
    "            path_weights = path_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_paths, self.max_pathlen)\n",
    "            path_weights = torch.sum(path_weights, dim=-1)\n",
    "            best_path_indices = torch.max(path_weights, dim=-1)[1][(..., ) + (None, ) * 3]\n",
    "            best_path_indices = best_path_indices.expand(-1, -1, -1, self.max_pathlen,  self.embedding_dim)\n",
    "            best_path = torch.gather(feature_emb, 2, best_path_indices).squeeze(2) # batch_size * 4 * max_pathlen * 512\n",
    "            # Another way: \n",
    "            # path_weights = masked_softmax(path_weights)\n",
    "            # best_path = torch.sum(path_weights[:, :, :, None, None] * feature_emb, dim=2)\n",
    "\n",
    "            best_path_reshaped = best_path.permute(0,3,1,2).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_pathlen)\n",
    "            node_weights = torch.bmm(node_emb.unsqueeze(1), best_path_reshaped) # batch_size * 4 * max_pathlen\n",
    "            node_weights = masked_softmax(node_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_pathlen)) # batch_size * 4 * max_pathlen\n",
    "            attended_path = node_weights.unsqueeze(-1) * best_path # batch_size * 4 * max_pathlen * 512\n",
    "\n",
    "            distance_weighted_path = torch.sum((self.v[None,None,:,None] * attended_path), dim=2) # batch_size * 4 * 512\n",
    "\n",
    "            self.w_data_neighbours = (1-self.w_rootpath-self.w_children-self.w_obj_neighbours)\n",
    "            context_emb = self.w_rootpath * distance_weighted_path[:,0,:] \\\n",
    "                        + self.w_children * distance_weighted_path[:,1,:] \\\n",
    "                        + self.w_obj_neighbours * distance_weighted_path[:,2,:] \\\n",
    "                        + self.w_data_neighbours * distance_weighted_path[:,3,:]\n",
    "\n",
    "            contextual_node_emb = torch.cat((node_emb, context_emb), dim=1)\n",
    "            output_node_emb = self.output(contextual_node_emb)\n",
    "            results.append(output_node_emb)\n",
    "        sim = self.cosine_sim_layer(results[0], results[1])\n",
    "        return sim\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    return np.vectorize(embedify)(elem_tuple)\n",
    "\n",
    "def embedify(elem):\n",
    "    return emb_indexer[elem]\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.vectorize(embedify)([features_dict[elem] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets, nodes = [], [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            nodes.append(generate_data_neighbourless(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return np.array(inputs), np.array(nodes), np.array(targets)\n",
    "\n",
    "print(\"Number of neighbours: \" + str(sys.argv[1]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "neighbours_dicts_lenpadded = {elem: [[path + [\"<UNK>\" for i in range(max_pathlen -len(path))] for path in nbr_type]\n",
    "                                for nbr_type in neighbours_dicts[elem]] for elem in neighbours_dicts}\n",
    "neighbours_dicts_pathpadded = {elem: [nbr_type + [[\"<UNK>\" for j in range(max_pathlen)] for i in range(max_paths - len(nbr_type))]\n",
    "                                for k,nbr_type in enumerate(neighbours_dicts_lenpadded[elem])] for elem in neighbours_dicts_lenpadded}\n",
    "features_dict = {elem: np.array(neighbours_dicts_pathpadded[elem]) for elem in neighbours_dicts_pathpadded}\n",
    "\n",
    "features_dict = {elem: features_dict[elem][:,:,:int(sys.argv[1])] for elem in features_dict}\n",
    "\n",
    "data_items = list(data.items())\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    val_data = dict(data_items[int((0.15*i)*len(data)):int((0.15*i + 0.15)*len(data))])\n",
    "    train_data = dict(data_items[:int(0.15*i*len(data))] + data_items[int(0.15*(i+1)*len(data)):])\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Val size:\", len(val_data))\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 1\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 32\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork(emb_vals, features_dict).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all][:100]\n",
    "        targets_all = np.array(targets_all)[indices_all][:100]\n",
    "        nodes_all = np.array(nodes_all)[indices_all][:100]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            nodes = nodes_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    val_data_t = [key for key in val_data if val_data[key]]\n",
    "    val_data_f = [key for key in val_data if not val_data[key]]\n",
    "    np.random.shuffle(val_data_f)\n",
    "    fval_len = len(val_data_f)\n",
    "    val_data_f = val_data_f[:int(0.3*fval_len)]\n",
    "    \n",
    "    optimize_threshold()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "train_data_t = [key for key in data if data[key]]\n",
    "train_data_f = [key for key in data if not data[key]]\n",
    "np.random.shuffle(train_data_f)\n",
    "train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "\n",
    "model = SiameseNetwork(emb_vals, features_dict).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "    inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "    inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "    targets_all = list(targets_pos) + list(targets_neg)\n",
    "    nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "    \n",
    "    indices_all = np.random.permutation(len(inputs_all))\n",
    "    inputs_all = np.array(inputs_all)[indices_all][:100]\n",
    "    targets_all = np.array(targets_all)[indices_all][:100]\n",
    "    nodes_all = np.array(nodes_all)[indices_all][:100]\n",
    "\n",
    "    batch_size = min(batch_size, len(inputs_all))\n",
    "    num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "        \n",
    "        inputs = inputs_all[batch_start: batch_end]\n",
    "        targets = targets_all[batch_start: batch_end]\n",
    "        nodes = nodes_all[batch_start: batch_end]\n",
    "        \n",
    "        inp_elems = torch.LongTensor(inputs).to(device)\n",
    "        targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "        node_elems = torch.LongTensor(nodes).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(node_elems, inp_elems)\n",
    "        loss = F.mse_loss(outputs, targ_elems)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx%5000 == 0:\n",
    "            print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.save(model.state_dict(), sys.argv[4])\n",
    "\n",
    "features_dict_conf = {elem: neighbours_dicts_pathpadded_conf[elem][:,:,:int(sys.argv[1])] for elem in neighbours_dicts_pathpadded_conf}\n",
    "\n",
    "model = SiameseNetwork(emb_vals_conf, features_dict_conf).to(device)\n",
    "model.load_state_dict(torch.load(sys.argv[4]), strict=False)\n",
    "\n",
    "threshold = model.threshold.data.cpu().numpy()[0]\n",
    "\n",
    "for i in list(range(0, len(ontologies_in_alignment_conf), 3)):\n",
    "    test_onto = ontologies_in_alignment_conf[i:i+3]\n",
    "    test_data = {elem: data_conf[elem] for elem in data_conf if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    test_data_t = [key for key in test_data if test_data[key]]\n",
    "    test_data_f = [key for key in test_data if not test_data[key]]\n",
    "\n",
    "    final_results.append(test())\n",
    "\n",
    "\n",
    "all_fn, all_fp, all_metrics = calculate_performance()\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)\n",
    "\n",
    "f1 = open(sys.argv[-1], \"wb\")\n",
    "pickle.dump([all_fn, all_fp], f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3291"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 165329\n"
     ]
    }
   ],
   "source": [
    "val_data_t = [key for key in val_data if val_data[key]]\n",
    "val_data_f = [key for key in val_data if not val_data[key]]\n",
    "print (len(val_data_t), len(val_data_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-55d46ddb1213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "list(set(inputs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(direct_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
