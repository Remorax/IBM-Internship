{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.argv = [\"main\", \"Input/data_lebensmittel.pkl\", 2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbours: 10\n",
      "Number of entities: 130000\n",
      "Training size: 110500 Testing size: 13000\n",
      "Epoch: 0 Idx: 0 Loss: 0.22921109024666714\n",
      "Len (direct inputs):  0\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  0\n",
      "Training size: 110500 Testing size: 13000\n",
      "Epoch: 0 Idx: 0 Loss: 0.2113765065101063\n",
      "Len (direct inputs):  0\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  0\n",
      "Training size: 110501 Testing size: 13000\n",
      "Epoch: 0 Idx: 0 Loss: 0.11066699154927566\n",
      "Len (direct inputs):  0\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  0\n",
      "Training size: 110499 Testing size: 13000\n",
      "Epoch: 0 Idx: 0 Loss: 0.06231932412136062\n",
      "Len (direct inputs):  0\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  0\n",
      "Training size: 110500 Testing size: 13000\n",
      "Epoch: 0 Idx: 0 Loss: 0.20850670316110773\n",
      "Len (direct inputs):  0\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  0\n",
      "Training size: 110501 Testing size: 13000\n",
      "Epoch: 0 Idx: 0 Loss: 0.08336651582283303\n",
      "Len (direct inputs):  0\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Len (direct inputs):  0\n",
      "Performance : (0.725, 0.3799126637554585, 0.498567335243553, 0.4198841698841699, 0.6135401974612129)\n",
      "Performance : (0.8921568627450981, 0.3922413793103448, 0.5449101796407185, 0.441747572815534, 0.7109375)\n",
      "Performance : (0.8913043478260869, 0.354978354978355, 0.5077399380804953, 0.4035433070866142, 0.6844741235392321)\n",
      "Performance : (0.569377990430622, 0.6071428571428571, 0.5876543209876544, 0.5991943605236656, 0.5765503875968992)\n",
      "Performance : (0.7524752475247525, 0.3671497584541063, 0.49350649350649345, 0.40904198062432723, 0.6219312602291325)\n",
      "Performance : (0.7450980392156863, 0.35348837209302325, 0.4794952681388012, 0.39501039501039503, 0.6099518459069021)\n",
      "Final Results: [0.76256875 0.40915223 0.51864559 0.44473696 0.63623089]\n",
      "Threshold:  0.8315654142259747\n"
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle, operator\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[3], \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment = pickle.load(f)\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "def test():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()    \n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            inp = inputs.transpose(1,0,2)\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inp[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inp[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "    return (test_data_t, all_results)\n",
    "\n",
    "def optimize_threshold():\n",
    "    global batch_size, val_data_t, val_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(val_data_t)\n",
    "        np.random.shuffle(val_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(val_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(val_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            inp = inputs.transpose(1,0,2)\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inp[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inp[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            s = set(res)\n",
    "            fn_list = [(key, all_results[key][0]) for key in val_data_t if key not in s]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            # print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "            if threshold in threshold_results:\n",
    "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
    "            else:\n",
    "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
    "\n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            threshold += step \n",
    "        \n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        s = set(res)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in s]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "    return (all_fn, all_fp, all_metrics)\n",
    "\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
    "        n = int(sys.argv[1])\n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(n-1) for i in range(n-1)]))\n",
    " \n",
    "    def forward(self, inputs):\n",
    "        results = []\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            node = x.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = x.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            att_weights = torch.bmm(neighbours, node.permute(0, 2, 1)).squeeze()\n",
    "            att_weights = masked_softmax(att_weights).unsqueeze(-1)\n",
    "            context = torch.matmul(self.v, att_weights * neighbours)\n",
    "\n",
    "            x = torch.cat((node.reshape(-1, self.embedding_dim), context.reshape(-1, self.embedding_dim)), dim=1)\n",
    "            x = self.output(x)\n",
    "            results.append(x)\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "        return x\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    op = np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets = [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "print(\"Number of neighbours: \" + str(sys.argv[1]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:int(sys.argv[1])] for el in neighbours_dicts[ont]\n",
    "       if count_non_unk(neighbours_dicts[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts}\n",
    "\n",
    "data_items = list(data.items())\n",
    "\n",
    "print (\"Number of entities:\", len(data))\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    test_data = dict(data_items[int(0.15*i*len(data)):int((0.15*i + 0.1)*len(data))])\n",
    "    val_data = dict(data_items[int((0.15*i + 0.1)*len(data)):int((0.15*i + 0.15)*len(data))])\n",
    "    train_data = dict(data_items[:int(0.15*i*len(data))] + data_items[int(0.15*(i+1)*len(data)):])\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Testing size:\", len(test_data))\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    train_data_f = train_data_f[:150000-len(train_data_t)]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 1\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all][:10]\n",
    "        targets_all = np.array(targets_all)[indices_all][:10]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inp_elems)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    val_data_t = [key for key in val_data if val_data[key]]\n",
    "    val_data_f = [key for key in val_data if not val_data[key]]\n",
    "    np.random.shuffle(val_data_f)\n",
    "    fval_len = len(val_data_f)\n",
    "    val_data_f = val_data_f[:int(0.3*fval_len)]\n",
    "    \n",
    "    optimize_threshold()\n",
    "\n",
    "    test_data_t = [key for key in test_data if test_data[key]]\n",
    "    test_data_f = [key for key in test_data if not test_data[key]]\n",
    "\n",
    "    final_results.append(test())\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "all_fn, all_fp, all_metrics = calculate_performance()\n",
    "\n",
    "f = open(sys.argv[4], \"wb\")\n",
    "pickle.dump([all_fn, all_fp], f)\n",
    "f.close()\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Supermarkt': 1,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Geschenke': 2,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Ahr': 3,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Kraeuter-Gewuerze': 4,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Honig': 5,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Pfalz': 6,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Nahe': 7,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke': 8,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Tee-Kaffee': 9,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Oesterreichisch': 10,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Home-Service': 11,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten': 12,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Franzoesisch': 13,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen_Whisky': 14,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Oekologischer-Weinbau': 15,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Gewuerze-und-Kraeuter': 16,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Italienisch': 17,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Rheingau': 18,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Tabakwaren': 19,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wasser': 20,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Zustellservice': 21,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Schweizer': 22,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Frankreich': 23,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Accessoires': 24,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Franzoesisch': 25,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel': 26,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Kaffee-und-Tee': 27,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Kleinstanbaugebiete': 28,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Mosel': 29,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland': 30,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Spanien': 31,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Kaffee-und-Tee_Tee': 32,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke': 33,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Saale-Unstrut': 34,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Bier_Heimbrauen': 35,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Bier': 36,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Deutsch': 37,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Schweiz': 38,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Fertigprodukte': 39,\n",
       " 'google_lebensmittel#Subclass': 40,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner': 41,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Kaffee-und-Tee_Kaffee': 42,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Suedafrikanisch': 43,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Naturkost': 44,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Cocktails': 45,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Hohenlohe': 46,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Spezialitaeten': 47,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Tiefkuehlkost': 48,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch': 49,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Weinpraesente': 50,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Weltweit': 51,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Ungarn': 52,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Delikatessen': 53,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen_Absinth': 54,\n",
       " 'google_lebensmittel#subclass_of': 55,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Oesterreich': 56,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Franken': 57,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Saar': 58,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Portugal': 59,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Griechisch': 60,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Tiefkuehlkost': 61,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Suesswaren': 62,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Kroatien': 63,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Franken': 64,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Rheingau': 65,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Baden': 66,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Oesterreichisch': 67,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Fisch': 68,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Rheinhessen': 69,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen': 70,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Wuerttemberg': 71,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Baden': 72,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Mittelrhein': 73,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Fleisch-Wurst': 74,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Vegetarisch_Vegan': 75,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Home-Service_Pizza': 76,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Mosel-Saar-Ruwer': 77,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Champagner': 78,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Spirituosen': 79,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Obst-Gemuese': 80,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Essig-und-Oel_Olivenoel': 81,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Headshops-Hanfprodukte': 82,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch': 83,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Bier': 84,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Essig-und-Oel': 85,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Kaese-Milchprodukte': 86,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Pfalz': 87,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein': 88,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Asiatisch': 89,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Fisch-und-Meeresfruechte': 90,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Spirituosen_Wodka': 91,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Internationale-Kueche': 92,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Vegetarisch': 93,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Bodensee': 94,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch_Spanisch': 95,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Biokost': 96,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops': 97,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Kaese': 98,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Backwaren_Weihnachtsgebaeck': 99,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Italienisch': 100,\n",
       " 'web_lebensmittel#subclass_of': 101,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Deutsch_Rheinhessen': 102,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Sekt': 103,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Europaeisch_Spanisch': 104,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Landwirtschaftliche-Erzeugnisse': 105,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen': 106,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Back-Suesswaren': 107,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Fleisch-und-Wurst': 108,\n",
       " 'web_lebensmittel#Subclass': 109,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken': 110,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Deutschland_Nahe': 111,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Regionale-Spezialitaeten_Europaeisch': 112,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Getraenke_Wein_Biowein': 113,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Wein_Sekt-Champagner_Italien': 114,\n",
       " 'google_lebensmittel#World_Deutsch_Online-Shops_Essen-und-Trinken_Backwaren': 115,\n",
       " 'web_lebensmittel#Verzeichnis_Einkaufen-Sparen_Nahrungs-Genussmittel_Getraenke_Erfrischungsgetraenke': 116}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_onto, test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in set(res)]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "\n",
    "\n",
    "final_results1, threshold_results1 = pickle.load(open(\"Output/conf6_2_part1.pkl\", \"rb\"))\n",
    "final_results2, threshold_results2 = pickle.load(open(\"Output/conf6_2_part2.pkl\", \"rb\"))\n",
    "threshold_results = {}\n",
    "for key in threshold_results1:\n",
    "    threshold_results[key] = threshold_results1[key]\n",
    "    if key in threshold_results2:\n",
    "        threshold_results[key].extend(threshold_results2[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "final_results = final_results1 + final_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for [('confOf', 'sigkdd')] is : (0.8333333333333334, 0.7142857142857143, 0.7692307692307692, 0.7352941176470589, 0.8064516129032258)\n",
      "Performance for [('iasted', 'sigkdd')] is : (0.8, 0.8, 0.8000000000000002, 0.8, 0.8)\n",
      "Performance for [('cmt', 'ekaw')] is : (0.625, 0.45454545454545453, 0.5263157894736842, 0.4807692307692307, 0.5813953488372092)\n",
      "Performance for [('conference', 'iasted')] is : (0.6666666666666666, 0.2857142857142857, 0.4, 0.3225806451612903, 0.5263157894736842)\n",
      "Performance for [('edas', 'sigkdd')] is : (0.7, 0.4666666666666667, 0.56, 0.5, 0.6363636363636365)\n",
      "Performance for [('ekaw', 'iasted')] is : (1.0, 0.6, 0.7499999999999999, 0.6521739130434783, 0.8823529411764706)\n",
      "Final Results: [0.77083333 0.55353535 0.63425776 0.58180298 0.70547989]\n",
      "Threshold:  0.934\n"
     ]
    }
   ],
   "source": [
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3240"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pickle.load(open(\"Input/data_lebensmittel.pkl\", \"rb\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('german_datasets_copy/webdirectory/dmoz.owl', 'german_datasets_copy/webdirectory/google.owl')\n",
      "('german_datasets_copy/webdirectory/dmoz.owl', 'german_datasets_copy/webdirectory/web.owl')\n"
     ]
    }
   ],
   "source": [
    "arr = [('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/google.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/web.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl')]\n",
    "\n",
    "for ont_pair in arr:\n",
    "    print (ont_pair)\n",
    "    a, b, c = ont_pair[0], ont_pair[1], ont_pair[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"-\" + ont_pair[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    !rm -rf $c\n",
    "    os.mkdir(c)\n",
    "    java_command = \"java -jar logmap-matcher/target/logmap-matcher-4.0.jar MATCHER file:\" +  os.path.abspath(a) + \\\n",
    "                     \" file:\" + os.path.abspath(b) + \" \" + \"/data/Vivek/IBM/IBM-Internship/\" + c + \"/ false\"\n",
    "    process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "pred_logmap = [[el.split(\"/\")[-1] for el in l.split(\"\\t\")[:-1]] for l in open(os.path.abspath(c + \"/\") + \"/logmap2_mappings.tsv\",  \"r\").read().split(\"\\n\")[:-1] if not l.startswith(\"Optional\")]\n",
    "gt_mappings = []\n",
    "for elem in pred_logmap:\n",
    "    gt_mappings.append(tuple([el.split(\"#\")[0].replace(\".v2\", \"\").rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"#\" + el.split(\"#\")[1] for el in elem]))\n",
    "data_orig = pickle.load(open(\"Input/data_webdir.pkl\", \"rb\"))[0]\n",
    "data_logmap = {}\n",
    "for key in data_orig:\n",
    "    data_logmap[key] = False\n",
    "s = set(list(data_logmap.keys()))\n",
    "gt_mappings = [tuple(pair) for pair in gt_mappings]\n",
    "for mapping in gt_mappings:\n",
    "    \n",
    "    if mapping in s:\n",
    "        data_logmap[mapping] = True\n",
    "    else:\n",
    "        mapping = tuple([el.replace(\",-\", \"_\") for el in mapping])\n",
    "        if mapping in s:\n",
    "            data_logmap[mapping] = True\n",
    "        else:\n",
    "            print (mapping)\n",
    "all_metrics = []\n",
    "def return_test_data(data, i):\n",
    "    data_t = {elem: data[elem] for elem in data if data[elem]}\n",
    "    data_f = {elem: data[elem] for elem in data if not data[elem]}\n",
    "\n",
    "    data_t_items = list(data_t.keys())\n",
    "    data_f_items = list(data_f.keys())\n",
    "\n",
    "    test_data_t = data_t_items[int((0.2*i)*len(data_t)):int((0.2*i + 0.2)*len(data_t))]\n",
    "    test_data_f = data_f_items[int((0.2*i)*len(data_f)):int((0.2*i + 0.2)*len(data_f))]\n",
    "    \n",
    "    test_data = {}\n",
    "    for elem in test_data_t:\n",
    "        test_data[elem] = True\n",
    "    for elem in test_data_f:\n",
    "        test_data[elem] = False\n",
    "    return test_data\n",
    "\n",
    "for i in range(5):\n",
    "    test_gt = return_test_data(data_orig, i)\n",
    "    test_logmap = {elem: data_logmap[elem] for elem in test_gt}\n",
    "    tp = len([elem for elem in test_gt if test_gt[elem] and test_logmap[elem]])\n",
    "    fp = len([elem for elem in test_logmap if not test_gt[elem] and test_logmap[elem]])\n",
    "    fn = len([elem for elem in test_logmap if test_gt[elem] and not test_logmap[elem]])\n",
    "    \n",
    "    try:\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "        f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        continue\n",
    "    all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    \n",
    "np.mean(all_metrics, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
