{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle, sys, glob\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "USE_folder = \"/home/vlead/USE\"\n",
    "alignment_folder = \"reference-alignment/\"\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    alignments = []\n",
    "    for f in os.listdir(folder):\n",
    "        doc = minidom.parse(folder + f)\n",
    "        ls = list(zip(doc.getElementsByTagName('entity1'), doc.getElementsByTagName('entity2')))\n",
    "        alignments.extend([(a.getAttribute('rdf:resource'), b.getAttribute('rdf:resource')) for (a,b) in ls])\n",
    "    return alignments\n",
    "        \n",
    "reference_alignments = load_alignments(alignment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(b,a,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, include_inv=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  834\n"
     ]
    }
   ],
   "source": [
    "# Extracting USE embeddings\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    try:\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "    except Exception as e:\n",
    "        !mkdir $USE_folder\n",
    "        !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\" | tar -zxvC $USE_folder\n",
    "        embed = hub.KerasLayer(USE_folder)\n",
    "        pass\n",
    "    word_embeddings = embed(words)\n",
    "    return word_embeddings.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0).lower() for m in matches]\n",
    "\n",
    "def parse(word):\n",
    "    return flatten([el.split(\"_\") for el in camel_case_split(word)])\n",
    "    \n",
    "\n",
    "extracted_elems = []\n",
    "\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(\"conference_ontologies/\" + ont_name + \".owl\")\n",
    "    entities = ont.get_entities()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples())))\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "inp = [\" \".join(parse(word.split(\"#\")[1])) for word in extracted_elems]\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "extracted_elems = [\"<UNK>\"] + extracted_elems\n",
    "\n",
    "embeds = np.array([np.zeros(512,)] + list(extractUSEEmbeddings(inp)))\n",
    "embeddings = dict(zip(extracted_elems, embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinatorial mapping generation\n",
    "\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(\"conference_ontologies/\" + l[0] + \".owl\")\n",
    "    ont2 = Ontology(\"conference_ontologies/\" + l[1] + \".owl\")\n",
    "    \n",
    "    ent1 = ont1.get_entities()\n",
    "    ent2 = ont2.get_entities()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "    \n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    all_mappings.extend([(l[0] + \"#\" + el[0], l[1] + \"#\" + el[1]) for el in mappings])\n",
    "    \n",
    "gt_mappings = [tuple([elem.split(\"/\")[-1] for elem in el]) for el in reference_alignments]\n",
    "\n",
    "data = {mapping: True if mapping in gt_mappings else False for mapping in all_mappings}\n",
    "\n",
    "emb_vals = list(embeddings.values())\n",
    "emb_indexer = {key: i for i, key in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: key for i, key in enumerate(list(embeddings.keys()))}\n",
    "\n",
    "f = open(\"data.pkl\", \"wb\")\n",
    "pickle.dump([data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, all_ont_pairs], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['confOf-sigkdd.rdf', 'iasted-sigkdd.rdf', 'cmt-ekaw.rdf']\n",
      "0.8275862068965517 0.7272727272727273 0.7741935483870968 0.7453416149068324 0.8053691275167786\n",
      "['confOf-iasted.rdf', 'conference-edas.rdf', 'cmt-sigkdd.rdf']\n",
      "0.8148148148148148 0.5789473684210527 0.6769230769230768 0.6145251396648045 0.7534246575342465\n",
      "['ekaw-sigkdd.rdf', 'conference-sigkdd.rdf', 'conference-confOf.rdf']\n",
      "0.78125 0.6097560975609756 0.684931506849315 0.6377551020408163 0.7396449704142012\n",
      "['confOf-edas.rdf', 'edas-iasted.rdf', 'cmt-conference.rdf']\n",
      "0.7941176470588235 0.5094339622641509 0.6206896551724137 0.548780487804878 0.7142857142857143\n",
      "['edas-sigkdd.rdf', 'conference-iasted.rdf', 'ekaw-iasted.rdf']\n",
      "0.7916666666666666 0.48717948717948717 0.6031746031746031 0.5277777777777778 0.7037037037037036\n",
      "['cmt-confOf.rdf', 'cmt-edas.rdf', 'edas-ekaw.rdf']\n",
      "0.8181818181818182 0.5192307692307693 0.6352941176470589 0.5601659751037344 0.733695652173913\n",
      "['conference-ekaw.rdf', 'confOf-ekaw.rdf', 'cmt-iasted.rdf']\n",
      "0.7954545454545454 0.7142857142857143 0.7526881720430106 0.7291666666666666 0.7777777777777778\n",
      "Final Results: [0.80329596 0.59230088 0.67827067 0.62335897 0.74684309]\n"
     ]
    }
   ],
   "source": [
    "# AML test\n",
    "def is_test(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) in test_onto\n",
    "\n",
    "results = []\n",
    "all_ont_pairs = list(set([tuple([el.split(\"#\")[0] for el in l]) for l in data.keys()]))\n",
    "for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    for ont_pair in test_onto:\n",
    "        a, b, c = ont_pair[0], ont_pair[1], ont_pair[0] + \"-\" + ont_pair[1]\n",
    "        java_command = \"java -jar AML_v3.1/AgreementMakerLight.jar -s conference_ontologies/\" + a + \".owl\" + \\\n",
    "                            \" -t conference_ontologies/\" + b + \".owl -o AML-test-results/\" + c + \".rdf -a\"\n",
    "        process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "    print (os.listdir(\"AML-test-results/\"))\n",
    "    pred_aml = load_alignments(\"AML-test-results/\")\n",
    "    pred_aml = [tuple([el.split(\"/\")[-1] for el in key]) for key in pred_aml]\n",
    "    tp = len([elem for elem in pred_aml if data[elem]])\n",
    "    fn = len([key for key in gt_mappings if key not in set(pred_aml) and is_test(test_onto, key)])\n",
    "    fp = len([elem for elem in pred_aml if not data[elem]])\n",
    "\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = 2 * precision * recall / (precision + recall)\n",
    "    f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "    f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    print (precision, recall, f1score, f2score, f0_5score)\n",
    "    \n",
    "    metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "    results.append(metrics)\n",
    "    \n",
    "    _ = [os.remove(f) for f in glob.glob('AML-test-results/*')]\n",
    "    \n",
    "print (\"Final Results:\", np.mean(results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6968d25bdadb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_indexer_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mappings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ont_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0montologies_in_alignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reference-alignment/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(\"data.pkl\", \"rb\")\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, all_ont_pairs = pickle.load(f)\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "ind_test, inp_test1, inp_test2 = None, None, None\n",
    "\n",
    "def write(statement):\n",
    "    op_file = open(\"Logs\", \"a+\")\n",
    "    op_file.write(\"\\n\" + str(statement) + \"\\n\")\n",
    "    op_file.close()\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()\n",
    "    \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self):\n",
    "        return [(b,a,\"subclass_of\") for (a,b) in self.get_subclasses()]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True):\n",
    "        obj_props = self.object_properties\n",
    "        data_props = self.data_properties\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop)) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop)))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples())\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, include_inv=True):\n",
    "        return self.parse_triples(union_flag, subclass_of)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    continue\n",
    "                if self.extract_ID(level1_class[0]):\n",
    "                    subclass_pairs.append((level1_class[0], el.parentNode))\n",
    "                else:\n",
    "                    level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "                    \n",
    "                    subclass_pairs.extend([(elem, el.parentNode) for elem in level2classes if self.extract_ID(elem)])\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self):\n",
    "        return [(self.extract_ID(a), self.extract_ID(b)) for (a,b) in self.subclasses]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        return element_id.split(\"#\")[-1]\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n",
    "\n",
    "gt_mappings, direct_inputs, direct_targets = [], [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "def greedy_matching():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, gt_mappings_filt, direct_inputs, direct_targets\n",
    "    all_results = OrderedDict()\n",
    "    gt_mappings_filt, direct_inputs, direct_targets = [], [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        batch_size = min(batch_size, len(test_data_t))\n",
    "        num_batches = int(ceil(len(test_data_t)/batch_size))\n",
    "        batch_size_f = int(ceil(len(test_data_f)/num_batches))\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        indices_pos = np.random.permutation(len(inputs_pos))\n",
    "        indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "        inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "        inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "\n",
    "        # gt_mappings_filt = [el for el in gt_mappings if el in test_data_t]\n",
    "        print (\"len(gt_mappings_filt)\", len(gt_mappings_filt), \"len(gt_mappings)\", len([el for el in gt_mappings if el in test_data_t]))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            batch_start_f = batch_idx * batch_size_f\n",
    "            batch_end_f = (batch_idx+1) * batch_size_f\n",
    "\n",
    "            inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "            targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "            \n",
    "            inp = inputs.transpose(1,0,2)\n",
    "            \n",
    "            nonzero_elems = np.count_nonzero(inp, axis=-1) - 1\n",
    "\n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            seq_lens = torch.LongTensor(nonzero_elems.T).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(inp_elems, seq_lens)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            #outputs /= torch.sum(outputs, dim=1).view(-1, 1)\n",
    "            #outputs = [(1-el[1].item()) for el in outputs]\n",
    "            \n",
    "\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "#             print (inputs)\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inp[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inp[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])    \n",
    "        #all_results = OrderedDict(sorted(all_results.items(), key=lambda x: x[0], reverse=True))\n",
    "        #filtered_results = dict()\n",
    "        \n",
    "        #entities_to_assign = set([el[0] for el in list(all_results.keys())])\n",
    "        #for pair in all_results:\n",
    "        #    if pair[0] in entities_to_assign:\n",
    "        #        filtered_results[pair] = all_results[pair]\n",
    "        #        entities_to_assign.remove(pair[0])\n",
    "                \n",
    "        #entities_to_assign = set([el[1] for el in list(all_results.keys())])\n",
    "        #for pair in all_results:\n",
    "        #    if pair[1] in entities_to_assign:\n",
    "        #        filtered_results[pair] = all_results[pair]\n",
    "        #        entities_to_assign.remove(pair[1])        \n",
    "\n",
    "        #filtered_results = OrderedDict(sorted(filtered_results.items(), key=lambda x: x[1][0], reverse=True))\n",
    "        \n",
    "        optimum_metrics, opt_threshold = [-1000 for i in range(5)], -1000\n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        #low_threshold, high_threshold = 0.9, 1.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [key for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "            fp_list = [elem for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [elem for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "\n",
    "            if f1score > optimum_metrics[2]:\n",
    "                optimum_metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "                opt_threshold = threshold\n",
    "            \n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            print (step, threshold, exception)\n",
    "            threshold += step \n",
    "        print (\"Precision: {} Recall: {} F1-Score: {} F2-Score: {} F0.5-Score: {}\".format(*optimum_metrics))\n",
    "        if optimum_metrics[2] != -1000:\n",
    "            all_metrics.append((opt_threshold, optimum_metrics))\n",
    "    return all_results\n",
    "\n",
    "def write(elem):\n",
    "    f = open(\"Logs\", \"a+\")\n",
    "    if type(elem) == list or type(elem) == tuple:\n",
    "        string = str(\"\\n\".join([str(s) for s in elem]))\n",
    "    else:\n",
    "        string = str(elem)\n",
    "    f.write(\"\\n\"+string)\n",
    "    f.close()\n",
    "    \n",
    "inputs3, results3 = None, None\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "def normalize(inp):\n",
    "    return inp/torch.norm(inp, dim=-1)[:, None]\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, self.num_layers, bidirectional=True, batch_first=True)\n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(1024, 300)\n",
    "        self.bilinear = nn.Bilinear(self.hidden_dim, self.hidden_dim, 1)\n",
    "        n = int(sys.argv[1])\n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(n-1) for i in range(n-1)]))\n",
    " \n",
    "    def forward(self, inputs, seq_lens):\n",
    "        results = []\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "        seq_lens = seq_lens.T\n",
    "        #print (\"input len: {} seq len: {}, rev len: {}\".format(inputs.shape, seq_lens.shape, rev_indices.shape))\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            \n",
    "            node = x.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = x.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            att_weights = torch.bmm(neighbours, node.permute(0, 2, 1)).squeeze()\n",
    "            #print (att_weights, masked_softmax(att_weights))\n",
    "            att_weights = masked_softmax(att_weights).unsqueeze(-1)\n",
    "            context = torch.mean(att_weights * neighbours, dim=1)\n",
    "            #context = torch.matmul(self.v, att_weights * neighbours)\n",
    "\n",
    "            x = torch.cat((node.reshape(-1, 512), context.reshape(-1, 512)), dim=1)\n",
    "            x = self.output(x)\n",
    "            results.append(x)\n",
    "        #global inputs3, results3\n",
    "        #results3 = results\n",
    "        #inputs3 = inputs\n",
    "        #x = self.layer1(results[0], results[1])\n",
    "        #x = F.log_softmax(x)\n",
    "        x = self.cosine_sim_layer(results[0], results[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    ont_obj = Ontology(\"conference_ontologies/\" + ont + \".owl\")\n",
    "    triples = ont_obj.get_triples()\n",
    "    entities = [(a,b) for (a,b,c) in triples]\n",
    "    neighbours_dict = {elem: [elem] for elem in list(set(flatten(entities)))}\n",
    "    for e1, e2 in entities:\n",
    "        neighbours_dict[e1].append(e2)\n",
    "        neighbours_dict[e2].append(e1)\n",
    "    \n",
    "    prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "    neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "    for e1, e2, p in prop_triples:\n",
    "        neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "    neighbours_dict = {el: neighbours_dict[el][:1] + sorted(list(set(neighbours_dict[el][1:])))\n",
    "                       for el in neighbours_dict}\n",
    "    print(len({key: neighbours_dict[key] for key in neighbours_dict if len(neighbours_dict[key])>1}))\n",
    "    neighbours_dict = {el: neighbours_dict[el][:int(sys.argv[1])] for el in neighbours_dict}\n",
    "    neighbours_dict = {ont + \"#\" + el: [ont + \"#\" + e for e in neighbours_dict[el]] for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    op = np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    op = np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets = [], []\n",
    "    global gt_mappings_filt, direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            targets.append(target)\n",
    "            if target==1:\n",
    "                gt_mappings_filt.append(elem)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    print (\"Filtered len: \", len(inputs), \"Original len:\", len(elems))\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "neighbours_dicts = {ont: get_one_hop_neighbours(ont) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "# max_neighbours = np.max(flatten([[len(el[e]) for e in el] for el in neighbours_dicts.values()]))\n",
    "# neighbours_lens = {ont: {key: len(neighbours_dicts[ont][key]) for key in neighbours_dicts[ont]}\n",
    "#                    for ont in neighbours_dicts}\n",
    "# neighbours_dicts = {ont: {key: neighbours_dicts[ont][key] + [\"<UNK>\" for i in range(max_neighbours -len(neighbours_dicts[ont][key]))]\n",
    "#               for key in neighbours_dicts[ont]} for ont in neighbours_dicts}\n",
    "\n",
    "# print(\"Number of neighbours: \" + sys.argv[1])\n",
    "\n",
    "# data_items = data.items()\n",
    "# np.random.shuffle(list(data_items))\n",
    "# data = OrderedDict(data_items)\n",
    "\n",
    "# print (\"Number of entities:\", len(data))\n",
    "\n",
    "# all_metrics = []\n",
    "\n",
    "# for i in list(range(0, len(all_ont_pairs), 3)):\n",
    "    \n",
    "#     test_onto = all_ont_pairs[i:i+3]\n",
    "    \n",
    "#     train_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "#     test_data = {elem: data[elem] for elem in data if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "#     torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "#     train_test_split = 0.9\n",
    "\n",
    "#     train_data_t = [key for key in train_data if data[key]]\n",
    "#     train_data_f = [key for key in train_data if not data[key]]\n",
    "#     #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "# #     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "# #     np.random.shuffle(train_data_f)\n",
    "    \n",
    "#     lr = 0.001\n",
    "#     num_epochs = 50\n",
    "#     weight_decay = 0.001\n",
    "#     batch_size = 10\n",
    "#     dropout = 0.3\n",
    "# #    batch_size = min(batch_size, len(train_data_t))\n",
    "# #    num_batches = int(ceil(len(train_data_t)/batch_size))\n",
    "# #    batch_size_f = int(ceil(len(train_data_f)/num_batches))\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     model = SiameseNetwork(512, 250, 1).to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     #print (\"Train\", len(train_data_t))\n",
    "#     for epoch in range(num_epochs):\n",
    "#         inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "#         inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "#         indices_pos = np.random.permutation(len(inputs_pos))\n",
    "#         indices_neg = np.random.permutation(len(inputs_neg))\n",
    "\n",
    "#         inputs_pos, targets_pos = inputs_pos[indices_pos], targets_pos[indices_pos]\n",
    "#         inputs_neg, targets_neg = inputs_neg[indices_neg], targets_neg[indices_neg]\n",
    "#         batch_size = min(batch_size, len(inputs_pos))\n",
    "#         num_batches = int(ceil(len(inputs_pos)/batch_size))\n",
    "#         batch_size_f = int(ceil(len(inputs_neg)/num_batches))\n",
    "# #        indices = np.random.permutation(len(inputs_pos) + len(inputs_neg))\n",
    "        \n",
    "# #        inputs = np.array(list(inputs_pos) + list(inputs_neg))[indices]\n",
    "# #        targets = np.array(list(targets_pos) + list(targets_neg))[indices]\n",
    "\n",
    "# #         inputs = np.array(list(inputs_pos) + list(inputs_neg))\n",
    "# #         targets = np.array(list(targets_pos) + list(targets_neg))\n",
    "# #        print (\"INputs pos\", inputs_pos.shape, \"Inputs neg\", inputs_neg.shape, num_batches)\n",
    "#         for batch_idx in range(num_batches):\n",
    "#             batch_start = batch_idx * batch_size\n",
    "#             batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "#             batch_start_f = batch_idx * batch_size_f\n",
    "#             batch_end_f = (batch_idx+1) * batch_size_f\n",
    "#  #           print (\"Start\", batch_start, batch_start_f, \"End\", batch_end, batch_end_f)\n",
    "            \n",
    "#             inputs = np.concatenate((inputs_pos[batch_start: batch_end], inputs_neg[batch_start_f: batch_end_f]))\n",
    "#             targets = np.concatenate((targets_pos[batch_start: batch_end], targets_neg[batch_start_f: batch_end_f]))\n",
    "            \n",
    "#             inp = inputs.transpose(1,0,2)\n",
    "#             nonzero_elems = np.count_nonzero(inp, axis=-1) - 1\n",
    "            \n",
    "#             inp_elems = torch.LongTensor(inputs).to(device)\n",
    "#             targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#   #          print (\"Inputs.shape\", inp_elems.shape)\n",
    "#             seq_lens = torch.LongTensor(nonzero_elems.T).to(device)\n",
    "#             outputs = model(inp_elems, seq_lens)\n",
    "#             loss = F.mse_loss(outputs, targ_elems)\n",
    "#             loss.backward()\n",
    "#             #break\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if batch_idx%10 == 0:\n",
    "#                 print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "#    #     sys.exit(1)\n",
    "#     model.eval()\n",
    "#     torch.save(model.state_dict(), \"/u/vivek98/attention.pt\")\n",
    "    \n",
    "#     test_data_t = [key for key in test_data if data[key]]\n",
    "#     test_data_f = [key for key in test_data if not data[key]]\n",
    "    \n",
    "#     res = greedy_matching()\n",
    "#     f1 = open(\"test_results.pkl\", \"wb\")\n",
    "#     pickle.dump(res, f1)\n",
    "# print (\"Final Results: \" + str(np.mean([el[1] for el in all_metrics], axis=0)))\n",
    "# print (\"Best threshold: \" + str(all_metrics[np.argmax([el[1][2] for el in all_metrics])][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
