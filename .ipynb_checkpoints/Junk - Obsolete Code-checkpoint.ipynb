{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from xml.dom import minidom\n",
    "\n",
    "all_ontologies = list(set([item + \".owl\" for sublist in [f.split(\".\")[0].split(\"-\") for  f in os.listdir(alignment_folder)] for item in sublist]))\n",
    "ontologies_folder = \"conference_ontologies/\"\n",
    "\n",
    "def parse_classes(ontology):\n",
    "    prefix = \"http://\" + ontology.split(\".\")[0] + \"#\"\n",
    "    doc = minidom.parse(ontology)\n",
    "    curr_classes = [el.getAttribute(\"rdf:ID\") for el in doc.getElementsByTagName('owl:Class')]\n",
    "    classes = [prefix + el for el in curr_classes if el]\n",
    "    return classes\n",
    "\n",
    "def parse_properties(ontology):\n",
    "    properties = []\n",
    "    for onto in ontologies:\n",
    "        prefix = \"http://\" + onto.split(\".\")[0] + \"#\"\n",
    "        doc = minidom.parse(folder + onto)\n",
    "        root = doc.documentElement\n",
    "        curr_properties = [el.getAttribute(\"rdf:ID\") for el in doc.getElementsByTagName('ObjectProperty')]\n",
    "        properties.extend([prefix + el for el in curr_properties if el])\n",
    "    return properties\n",
    "\n",
    "classes, properties = [], []\n",
    "for ontology in all_ontologies:\n",
    "    classes.extend(parse_classes(ontologies_folder + ontology))\n",
    "    properties.extend(parse_properties(ontologies_folder + ontology))\n",
    "properties = load_properties(ontologies_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hyphen(ontology, replaced_props):\n",
    "    onto_str = open(ontology).read()\n",
    "    for (a,b) in replaced_props:\n",
    "        onto_str = onto_str.replace(a,b)\n",
    "    ontology_temp = ontology.split(\".\")[0] + \"_temp.owl\"\n",
    "    open(ontology_temp, \"w+\").write(onto_str)\n",
    "    return ontology_temp\n",
    "\n",
    "replaced_props = [(str(prop).split(\".\")[1], str(prop).split(\".\")[1].replace(\"-\", \"_\")) for prop in list(onto.properties()) if \"-\" in str(prop)]\n",
    "replaced_ontology = replace_hyphen(ontology, replaced_props)\n",
    "replaced_onto = get_ontology(replaced_ontology).load()\n",
    "# os.remove(replaced_ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"http://\" + ontology.split(\".\")[0] + \"#\"\n",
    "doc = minidom.parse(ontology)\n",
    "root = doc.documentElement\n",
    "curr_properties = [el.getAttribute(\"rdf:ID\") for el in doc.getElementsByTagName('Class')]\n",
    "# properties.extend([prefix + el for el in curr_properties if el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [el for el in root.childNodes if type(el) == minidom.Element and el._get_tagName() == 'owl:ObjectProperty'] \n",
    "prop_ids = [prop.getAttribute('rdf:ID') for prop in props if prop.getAttribute('rdf:ID')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = minidom.parse(ontology)\n",
    "object_props = [el.getAttribute(\"rdf:ID\") for el in doc.getElementsByTagName('owl:ObjectProperty')]\n",
    "filt = [[e for e in el._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == \"rdf:type\"] for el in doc.getElementsByTagName('owl:FunctionalProperty')]\n",
    "[el[0].getAttribute(\"rdf:resource\") for el in filt if el]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear search with USE, no greedy\n",
    "\n",
    "opt_threshold, optimum_metrics = -1000, [-1000 for i in range(3)]\n",
    "t = time.time()\n",
    "for j,threshold in enumerate(np.arange(0.15, 1.0005, 0.01)):\n",
    "    print (\"threshold =\", threshold, \"Time = \", time.time()-t) \n",
    "    pred = [\"F\" for key in train_data]\n",
    "    mapped = []\n",
    "    for i,key in enumerate(train_data):\n",
    "        if train_data[key][0] > threshold:\n",
    "            pred[i] = \"T\"\n",
    "        mapped.extend(list(key))\n",
    "    gt = [l[1] for l in train_data.values()]\n",
    "    f1score = f1_score(gt, pred, pos_label=\"T\")\n",
    "    if f1score > optimum_metrics[-1]:\n",
    "        optimum_metrics = [precision_score(gt, pred, pos_label=\"T\"), recall_score(gt, pred, pos_label=\"T\"), f1score]\n",
    "        opt_threshold = threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = opt_threshold\n",
    "pred = [\"F\" for key in test_data]\n",
    "mapped = []\n",
    "for i,key in enumerate(test_data):\n",
    "    if test_data[key][0] > threshold:\n",
    "        pred[i] = \"T\"\n",
    "    mapped.extend(list(key))\n",
    "gt = [l[1] for l in test_data.values()]\n",
    "f1score = f1_score(gt, pred, pos_label=\"T\")\n",
    "metrics = [precision_score(gt, pred, pos_label=\"T\"), recall_score(gt, pred, pos_label=\"T\"), f1score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation for RotatE embeddings from OpenKE\n",
    "\n",
    "ontologies_in_alignment = [l.split(\".\")[0].split(\"-\") for l in os.listdir(\"reference-alignment/\")]\n",
    "\n",
    "def generate(ontology, concept_prefix, id_prefix=[0,0]):\n",
    "    ont = Ontology(ontology)\n",
    "    classes_dict = {concept_prefix+\"#\"+elem : str(id_prefix[0] + i) for i,elem in enumerate(ont.get_classes())} \n",
    "    props = ont.get_object_properties() + ont.get_data_properties() + [\"subclass_of\"]\n",
    "    prop_dict = {concept_prefix+\"#\"+elem : str(id_prefix[1] + i) for i,elem in enumerate(props)} \n",
    "    triplets = [\"\\t\".join((classes_dict[concept_prefix+\"#\"+el[0]], classes_dict[concept_prefix+\"#\"+el[1]], prop_dict[concept_prefix+\"#\"+el[2]]))\n",
    "                for el in ont.get_triples()]\n",
    "    return classes_dict, prop_dict, triplets\n",
    "\n",
    "    \n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = \"conference_ontologies/\" + l[0] + \".owl\"\n",
    "    ont2 = \"conference_ontologies/\" + l[1] + \".owl\"\n",
    "    \n",
    "    benchmark_dir = \"OpenKE/benchmarks/\" + l[0] + \"-\" + l[1]\n",
    "    if not os.path.isdir(benchmark_dir):\n",
    "        os.mkdir(benchmark_dir)\n",
    "    \n",
    "    c1, p1, t1 = generate(ont1, l[0])\n",
    "    c2, p2, t2 = generate(ont2, l[1], [len(c1), len(p1)])\n",
    "    \n",
    "    classes_dict = {**c1, **c2}\n",
    "    prop_dict = {**p1, **p2}\n",
    "    triplets = t1 + t2\n",
    "    \n",
    "    entity_str = str(len(classes_dict)) + \"\\n\" + \"\\n\".join([\"\\t\".join(elem) for elem in classes_dict.items()])\n",
    "    prop_str = str(len(prop_dict)) + \"\\n\" + \"\\n\".join([\"\\t\".join(elem) for elem in prop_dict.items()])\n",
    "    triplets_str = str(len(triplets)) + \"\\n\" + \"\\n\".join(triplets)\n",
    "\n",
    "    open(benchmark_dir + \"/entity2id.txt\", \"w+\").write(entity_str)\n",
    "    open(benchmark_dir + \"/relation2id.txt\", \"w+\").write(prop_str)\n",
    "    open(benchmark_dir + \"/train2id.txt\", \"w+\").write(triplets_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
