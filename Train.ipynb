{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "sys.argv = [\"main\", \"Input/data_anatomy_oaei.pkl\", 10, 10, \"test.pkl\", \"model.pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of nodes in a path: Input/data_anatomy_oaei.pkl\n",
      "Number of entities: 150000\n",
      "Training size: 127500 Validation size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.22264554505872475\n",
      "Epoch: 0 Idx: 5000 Loss: 0.01307642086863753\n",
      "Epoch: 1 Idx: 0 Loss: 0.01724470708984736\n",
      "Epoch: 1 Idx: 5000 Loss: 0.014226696446090498\n",
      "Epoch: 2 Idx: 0 Loss: 0.01490469357214902\n",
      "Epoch: 2 Idx: 5000 Loss: 0.04678227379703642\n",
      "Epoch: 3 Idx: 0 Loss: 0.011860856704795333\n",
      "Epoch: 3 Idx: 5000 Loss: 0.030335302134585868\n",
      "Epoch: 4 Idx: 0 Loss: 0.022233640356545903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ff500c70f1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle, operator\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys, random\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from collections import Sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[1], \"rb\")\n",
    "data, aml_data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment, max_types, max_paths, max_pathlen = pickle.load(f)\n",
    "max_paths = int(sys.argv[2])\n",
    "max_pathlen = int(sys.argv[3])\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "def test():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()    \n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "    return (test_data_t, all_results)\n",
    "\n",
    "def optimize_threshold():\n",
    "    global batch_size, val_data_t, val_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(val_data_t)\n",
    "        np.random.shuffle(val_data_f)\n",
    "\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(val_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(val_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets)\n",
    "\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[nodes[idx][0]]\n",
    "                ent2 = emb_indexer_inv[nodes[idx][1]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            s = set(res)\n",
    "            fn_list = [(key, all_results[key][0]) for key in val_data_t if key not in s]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            # print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "            if threshold in threshold_results:\n",
    "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
    "            else:\n",
    "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
    "\n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            threshold += step \n",
    "\n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics, all_fn, all_fp = [], [], []\n",
    "    for (test_data_t, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        s = set(res)\n",
    "        fn_list = [(key, all_results[key][0]) for key in test_data_t if key not in s]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_fn.extend(fn_list)\n",
    "        all_fp.extend(fp_list)\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics, all_fn, all_fp\n",
    "\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, emb_vals, threshold=0.9):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.n_neighbours = 4\n",
    "        self.max_paths = max_paths\n",
    "        self.max_pathlen = max_pathlen\n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.threshold = nn.Parameter(torch.DoubleTensor([threshold]))\n",
    "        self.threshold.requires_grad = False\n",
    "\n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
    "        \n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(self.max_pathlen) for i in range(self.max_pathlen)]))\n",
    "        self.w_rootpath = nn.Parameter(torch.DoubleTensor([0.33]))\n",
    "        self.w_children = nn.Parameter(torch.DoubleTensor([0.33]))\n",
    "        self.w_obj_neighbours = nn.Parameter(torch.DoubleTensor([0.33]))\n",
    " \n",
    "    def forward(self, nodes, features):\n",
    "        '''\n",
    "        Arguments:\n",
    "            - nodes: batch_size * 2\n",
    "            - features: batch_size * 2 * 4 * max_paths * max_pathlen\n",
    "        '''\n",
    "        results = []\n",
    "        nodes = nodes.permute(1,0) # 2 * batch_size\n",
    "        features = features.permute(1,0,2,3,4) # 2 * batch_size * 4 * max_paths * max_pathlen\n",
    "        for i in range(2):\n",
    "            node_emb = self.name_embedding(nodes[i]) # batch_size * 512\n",
    "            feature_emb = self.name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512\n",
    "            \n",
    "            feature_emb_reshaped = feature_emb.permute(0,4,1,2,3).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_paths * self.max_pathlen)\n",
    "            path_weights = torch.bmm(node_emb[:, None, :], feature_emb_reshaped)\n",
    "            path_weights = path_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_paths, self.max_pathlen)\n",
    "            path_weights = torch.sum(path_weights, dim=-1)\n",
    "            best_path_indices = torch.max(path_weights, dim=-1)[1][(..., ) + (None, ) * 3]\n",
    "            best_path_indices = best_path_indices.expand(-1, -1, -1, self.max_pathlen,  self.embedding_dim)\n",
    "            best_path = torch.gather(feature_emb, 2, best_path_indices).squeeze(2) # batch_size * 4 * max_pathlen * 512\n",
    "            # Another way: \n",
    "            # path_weights = masked_softmax(path_weights)\n",
    "            # best_path = torch.sum(path_weights[:, :, :, None, None] * feature_emb, dim=2)\n",
    "\n",
    "            best_path_reshaped = best_path.permute(0,3,1,2).reshape(-1, self.embedding_dim, self.n_neighbours * self.max_pathlen)\n",
    "            node_weights = torch.bmm(node_emb.unsqueeze(1), best_path_reshaped) # batch_size * 4 * max_pathlen\n",
    "            node_weights = masked_softmax(node_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_pathlen)) # batch_size * 4 * max_pathlen\n",
    "            attended_path = node_weights.unsqueeze(-1) * best_path # batch_size * 4 * max_pathlen * 512\n",
    "\n",
    "            distance_weighted_path = torch.sum((self.v[None,None,:,None] * attended_path), dim=2) # batch_size * 4 * 512\n",
    "\n",
    "            self.w_data_neighbours = (1-self.w_rootpath-self.w_children-self.w_obj_neighbours)\n",
    "            context_emb = self.w_rootpath * distance_weighted_path[:,0,:] \\\n",
    "                        + self.w_children * distance_weighted_path[:,1,:] \\\n",
    "                        + self.w_obj_neighbours * distance_weighted_path[:,2,:] \\\n",
    "                        + self.w_data_neighbours * distance_weighted_path[:,3,:]\n",
    "\n",
    "            contextual_node_emb = torch.cat((node_emb, context_emb), dim=1)\n",
    "            output_node_emb = self.output(contextual_node_emb)\n",
    "            results.append(output_node_emb)\n",
    "        sim = self.cosine_sim_layer(results[0], results[1])\n",
    "        return sim\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    return [emb_indexer[elem] for elem in elem_tuple]\n",
    "\n",
    "def embedify(seq, emb_indexer):\n",
    "    for item in seq:\n",
    "        if isinstance(item, list):\n",
    "            yield list(embedify(item, emb_indexer))\n",
    "        else:\n",
    "            yield emb_indexer[item]\n",
    "\n",
    "def generate_data(elem_tuple):\n",
    "    return list(embedify([neighbours_dicts[elem] for elem in elem_tuple], emb_indexer))\n",
    "\n",
    "def to_feature(inputs):\n",
    "    inputs_lenpadded = [[[[path[:max_pathlen] + [0 for i in range(max_pathlen -len(path[:max_pathlen]))]\n",
    "                                    for path in nbr_type[:max_paths]]\n",
    "                                for nbr_type in ent]\n",
    "                            for ent in elem]\n",
    "                        for elem in inputs]\n",
    "    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]\n",
    "                             for i in range(max_paths - len(nbr_type))]\n",
    "                            for nbr_type in ent] for ent in elem]\n",
    "                        for elem in inputs_lenpadded]\n",
    "    return inputs_pathpadded\n",
    "\n",
    "def generate_input(elems, target):\n",
    "    inputs, targets, nodes = [], [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem))\n",
    "            nodes.append(generate_data_neighbourless(elem))\n",
    "            targets.append(target)\n",
    "        except:\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return inputs, nodes, targets\n",
    "\n",
    "print(\"Max number of nodes in a path: \" + str(sys.argv[1]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "data_items = list(aml_data.items())\n",
    "np.random.shuffle(data_items)\n",
    "\n",
    "print (\"Number of entities:\", len(aml_data))\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "for i in range(6):\n",
    "    \n",
    "    val_data = dict(data_items[int((0.15*i)*len(aml_data)):int((0.15*i + 0.15)*len(aml_data))])\n",
    "    train_data = dict(data_items[:int(0.15*i*len(aml_data))] + data_items[int(0.15*(i+1)*len(aml_data)):])\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Validation size:\", len(val_data))\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 50\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 32\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork(emb_vals).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    val_data_t = [key for key in val_data if val_data[key]]\n",
    "    val_data_f = [key for key in val_data if not val_data[key]]\n",
    "    \n",
    "    optimize_threshold()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "train_data_t = [key for key in aml_data if aml_data[key]]\n",
    "train_data_f = [key for key in aml_data if not aml_data[key]]\n",
    "train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "print (threshold)\n",
    "#train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "np.random.shuffle(train_data_f)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "weight_decay = 0.001\n",
    "batch_size = 32\n",
    "dropout = 0.3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseNetwork(emb_vals, threshold).to(device)\n",
    "print (model.threshold)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "    inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "    inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "    targets_all = list(targets_pos) + list(targets_neg)\n",
    "    nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "    \n",
    "    all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "    all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "    inputs_all, targets_all, nodes_all = [list(elem) for elem in zip(*all_inp_shuffled)] \n",
    "\n",
    "    batch_size = min(batch_size, len(inputs_all))\n",
    "    num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "        inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "        targets = np.array(targets_all[batch_start: batch_end])\n",
    "        nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "\n",
    "        inp_elems = torch.LongTensor(inputs).to(device)\n",
    "        node_elems = torch.LongTensor(nodes).to(device)\n",
    "        targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(node_elems, inp_elems)\n",
    "\n",
    "        loss = F.mse_loss(outputs, targ_elems)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx%5000 == 0:\n",
    "            print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.save(model.state_dict(), sys.argv[5])\n",
    "\n",
    "model = SiameseNetwork(emb_vals).to(device)\n",
    "model.load_state_dict(torch.load(sys.argv[5]), strict=False)\n",
    "\n",
    "threshold = model.threshold.data.cpu().numpy()[0]\n",
    "\n",
    "test_data_t = [key for key in data if data[key]]\n",
    "test_data_f = [key for key in data if not data[key]]\n",
    "\n",
    "final_results.append(test())\n",
    "\n",
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)\n",
    "\n",
    "f1 = open(sys.argv[4], \"wb\")\n",
    "pickle.dump([all_fn, all_fp], f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 150000\n",
      "Training size: 127500 Validation size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.23268560164994873\n",
      "Epoch: 0 Idx: 5000 Loss: 0.022925660133716146\n",
      "Len (direct inputs):  105\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "Training size: 127500 Validation size: 22500\n",
      "Epoch: 0 Idx: 0 Loss: 0.1441918565703774\n",
      "Epoch: 0 Idx: 5000 Loss: 0.030422520338488808\n",
      "Len (direct inputs):  107\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "division by zero\n",
      "0.8525283135882556\n",
      "Parameter containing:\n",
      "tensor([0.8525])\n",
      "Epoch: 0 Idx: 0 Loss: 0.20414832311296116\n",
      "Epoch: 0 Idx: 5000 Loss: 0.01772343401667624\n",
      "Len (direct inputs):  642\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_onto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f2ca046ca85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mall_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Results: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c07e20351206>\u001b[0m in \u001b[0;36mcalculate_performance\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Performance for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_onto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0_5score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mall_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mall_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_onto' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print (\"Number of entities:\", len(aml_data))\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "for i in range(6):\n",
    "    \n",
    "    val_data = dict(data_items[int((0.15*i)*len(aml_data)):int((0.15*i + 0.15)*len(aml_data))])\n",
    "    train_data = dict(data_items[:int(0.15*i*len(aml_data))] + data_items[int(0.15*(i+1)*len(aml_data)):])\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Validation size:\", len(val_data))\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "    np.random.shuffle(train_data_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 50\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 32\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork(emb_vals).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "        \n",
    "        all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "        all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "        inputs_all, targets_all, nodes_all = list(zip(*all_inp_shuffled))\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "            targets = np.array(targets_all[batch_start: batch_end])\n",
    "            nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "            \n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            node_elems = torch.LongTensor(nodes).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(node_elems, inp_elems)\n",
    "\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    val_data_t = [key for key in val_data if val_data[key]]\n",
    "    val_data_f = [key for key in val_data if not val_data[key]]\n",
    "    \n",
    "    optimize_threshold()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "train_data_t = [key for key in aml_data if aml_data[key]]\n",
    "train_data_f = [key for key in aml_data if not aml_data[key]]\n",
    "train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "print (threshold)\n",
    "#train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "np.random.shuffle(train_data_f)\n",
    "\n",
    "lr = 0.001\n",
    "num_epochs = 50\n",
    "weight_decay = 0.001\n",
    "batch_size = 32\n",
    "dropout = 0.3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseNetwork(emb_vals, threshold).to(device)\n",
    "print (model.threshold)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    inputs_pos, nodes_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "    inputs_neg, nodes_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "    inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "    targets_all = list(targets_pos) + list(targets_neg)\n",
    "    nodes_all = list(nodes_pos) + list(nodes_neg)\n",
    "    \n",
    "    all_inp = list(zip(inputs_all, targets_all, nodes_all))\n",
    "    all_inp_shuffled = random.sample(all_inp, len(all_inp))\n",
    "    inputs_all, targets_all, nodes_all = [list(elem) for elem in zip(*all_inp_shuffled)] \n",
    "\n",
    "    batch_size = min(batch_size, len(inputs_all))\n",
    "    num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "        inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))\n",
    "        targets = np.array(targets_all[batch_start: batch_end])\n",
    "        nodes = np.array(nodes_all[batch_start: batch_end])\n",
    "\n",
    "        inp_elems = torch.LongTensor(inputs).to(device)\n",
    "        node_elems = torch.LongTensor(nodes).to(device)\n",
    "        targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(node_elems, inp_elems)\n",
    "\n",
    "        loss = F.mse_loss(outputs, targ_elems)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx%5000 == 0:\n",
    "            print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.save(model.state_dict(), sys.argv[5])\n",
    "\n",
    "model = SiameseNetwork(emb_vals).to(device)\n",
    "model.load_state_dict(torch.load(sys.argv[5]), strict=False)\n",
    "\n",
    "threshold = model.threshold.data.cpu().numpy()[0]\n",
    "\n",
    "test_data_t = [key for key in data if data[key]]\n",
    "test_data_f = [key for key in data if not data[key]]\n",
    "\n",
    "final_results.append(test())\n",
    "\n",
    "all_metrics, all_fn, all_fp = calculate_performance()\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)\n",
    "\n",
    "f1 = open(sys.argv[4], \"wb\")\n",
    "pickle.dump([all_fn, all_fp], f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[5749, 1957, 2721, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]],\n",
       "\n",
       "\n",
       "        [[[3839, 2379, 4710, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[2349, 1722,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[2573,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]],\n",
       "\n",
       "\n",
       "        [[[4257, 4402, 4054, ...,    0,    0,    0],\n",
       "          [4257, 4402, 4054, ...,    0,    0,    0],\n",
       "          [4257, 4402, 4054, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [4257, 4402, 2649, ..., 3045,    0,    0],\n",
       "          [4257, 4402, 2649, ..., 3045,    0,    0],\n",
       "          [4257, 5759, 1764, ..., 3045,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[2290,  101, 5379, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]],\n",
       "\n",
       "\n",
       "        [[[2587,   95, 3670, ...,    0,    0,    0],\n",
       "          [2587,   95, 5638, ...,    0,    0,    0],\n",
       "          [2587,  504, 3670, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[2861,    0,    0, ...,    0,    0,    0],\n",
       "          [1319,    0,    0, ...,    0,    0,    0],\n",
       "          [3832,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[1590,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[5731, 5409,  436, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[1352,    0,    0, ...,    0,    0,    0],\n",
       "          [4635,    0,    0, ...,    0,    0,    0],\n",
       "          [1800,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]],\n",
       "\n",
       "\n",
       "        [[[ 252, 5856, 2483, ...,    0,    0,    0],\n",
       "          [ 252, 4690, 5856, ...,    0,    0,    0],\n",
       "          [ 252, 4690, 5638, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[1465,    0,    0, ...,    0,    0,    0],\n",
       "          [3267,    0,    0, ...,    0,    0,    0],\n",
       "          [5706,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [3434,    0,    0, ...,    0,    0,    0],\n",
       "          [4533,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[3533,    0,    0, ...,    0,    0,    0],\n",
       "          [1613,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[5953,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]],\n",
       "\n",
       "\n",
       "        [[[3771, 3965, 2483, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[3100,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[2785, 5873, 1348, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]],\n",
       "\n",
       "\n",
       "        [[[2544, 3965, 2483, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]],\n",
       "\n",
       "         [[5719,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0],\n",
       "          [   0,    0,    0, ...,    0,    0,    0]]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debugg_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[5749, 1957, 2721, 1734, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       " [[[4931, 5662, 2627, 1393, 5043, 1965, 2375, 3059, 4994, 5915, 3670, 3045],\n",
       "   [4931, 5662, 2627, 1393, 5043, 1965, 2375, 3059, 4994, 5915, 5638, 3045],\n",
       "   [4931, 5662, 2627, 1393, 5043, 2647, 2375, 3059, 4994, 5915, 3670, 3045],\n",
       "   [4931, 5662, 2627, 1393, 5043, 2647, 2375, 3059, 4994, 5915, 5638, 3045],\n",
       "   [4931, 5662, 2627, 1393, 5344, 3059, 4994, 5915, 3670, 3045],\n",
       "   [4931, 5662, 2627, 1393, 5344, 3059, 4994, 5915, 5638, 3045],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debugg_false[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([list([0 for i in range(10)]) for j in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 165329\n"
     ]
    }
   ],
   "source": [
    "val_data_t = [key for key in val_data if val_data[key]]\n",
    "val_data_f = [key for key in val_data if not val_data[key]]\n",
    "print (len(val_data_t), len(val_data_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-55d46ddb1213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "list(set(inputs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(direct_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
