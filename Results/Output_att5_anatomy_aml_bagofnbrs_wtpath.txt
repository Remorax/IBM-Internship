Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1797887440920823
Epoch: 0 Idx: 5000 Loss: 0.02464447114973965
Epoch: 1 Idx: 0 Loss: 0.012527958397431847
Epoch: 1 Idx: 5000 Loss: 0.03360418079656759
Epoch: 2 Idx: 0 Loss: 0.031090745187104946
Epoch: 2 Idx: 5000 Loss: 0.009211543981106265
Epoch: 3 Idx: 0 Loss: 0.017648331151078386
Epoch: 3 Idx: 5000 Loss: 0.014356129318045925
Epoch: 4 Idx: 0 Loss: 0.015038951289427968
Epoch: 4 Idx: 5000 Loss: 0.01803988531955429
Epoch: 5 Idx: 0 Loss: 0.04180940561800746
Epoch: 5 Idx: 5000 Loss: 0.010909787753565357
Epoch: 6 Idx: 0 Loss: 0.03156381386116265
Epoch: 6 Idx: 5000 Loss: 0.025270517328119083
Epoch: 7 Idx: 0 Loss: 0.024343876216117194
Epoch: 7 Idx: 5000 Loss: 0.026963961518412627
Epoch: 8 Idx: 0 Loss: 0.01992941023537307
Epoch: 8 Idx: 5000 Loss: 0.013089287522087524
Epoch: 9 Idx: 0 Loss: 0.0252504264231809
Epoch: 9 Idx: 5000 Loss: 0.016654230697672898
Epoch: 10 Idx: 0 Loss: 0.01880596580434401
Epoch: 10 Idx: 5000 Loss: 0.01756938854203547
Epoch: 11 Idx: 0 Loss: 0.008713504913030145
Epoch: 11 Idx: 5000 Loss: 0.020630893046409565
Epoch: 12 Idx: 0 Loss: 0.04070633421710001
Epoch: 12 Idx: 5000 Loss: 0.026377840252146116
Epoch: 13 Idx: 0 Loss: 0.014530399041588517
Epoch: 13 Idx: 5000 Loss: 0.007748801534034908
Epoch: 14 Idx: 0 Loss: 0.02714671307113056
Epoch: 14 Idx: 5000 Loss: 0.008799474186016117
Epoch: 15 Idx: 0 Loss: 0.017320896688091968
Epoch: 15 Idx: 5000 Loss: 0.0077846691262386195
Epoch: 16 Idx: 0 Loss: 0.023966151021807103
Epoch: 16 Idx: 5000 Loss: 0.02142547479007963
Epoch: 17 Idx: 0 Loss: 0.020161054924100794
Epoch: 17 Idx: 5000 Loss: 0.03483430203667594
Epoch: 18 Idx: 0 Loss: 0.014266718683326147
Epoch: 18 Idx: 5000 Loss: 0.026271638485334082
Epoch: 19 Idx: 0 Loss: 0.03973507312735981
Epoch: 19 Idx: 5000 Loss: 0.018508656513060412
Epoch: 20 Idx: 0 Loss: 0.023177052892850464
Epoch: 20 Idx: 5000 Loss: 0.012319307749921835
Epoch: 21 Idx: 0 Loss: 0.016248985301554476
Epoch: 21 Idx: 5000 Loss: 0.017746871066677397
Epoch: 22 Idx: 0 Loss: 0.024983613735263963
Epoch: 22 Idx: 5000 Loss: 0.031173681273639023
Epoch: 23 Idx: 0 Loss: 0.0241815532612472
Epoch: 23 Idx: 5000 Loss: 0.007729224027859323
Epoch: 24 Idx: 0 Loss: 0.030751804658165828
Epoch: 24 Idx: 5000 Loss: 0.034091913325669396
Epoch: 25 Idx: 0 Loss: 0.020364274546393047
Epoch: 25 Idx: 5000 Loss: 0.02672242926244283
Epoch: 26 Idx: 0 Loss: 0.013093833015429568
Epoch: 26 Idx: 5000 Loss: 0.02855438097047954
Epoch: 27 Idx: 0 Loss: 0.015842693375332298
Epoch: 27 Idx: 5000 Loss: 0.03238822232763364
Epoch: 28 Idx: 0 Loss: 0.03222142028197825
Epoch: 28 Idx: 5000 Loss: 0.036556298366875586
Epoch: 29 Idx: 0 Loss: 0.020452123064446784
Epoch: 29 Idx: 5000 Loss: 0.02163549111396993
Epoch: 30 Idx: 0 Loss: 0.02210451724228601
Epoch: 30 Idx: 5000 Loss: 0.020323254279169554
Epoch: 31 Idx: 0 Loss: 0.022070641375158122
Epoch: 31 Idx: 5000 Loss: 0.03298648707182564
Epoch: 32 Idx: 0 Loss: 0.01664902334587471
Epoch: 32 Idx: 5000 Loss: 0.042646661777822295
Epoch: 33 Idx: 0 Loss: 0.025494711783503397
Epoch: 33 Idx: 5000 Loss: 0.020277377800516104
Epoch: 34 Idx: 0 Loss: 0.011759487307873528
Epoch: 34 Idx: 5000 Loss: 0.015808056726372057
Epoch: 35 Idx: 0 Loss: 0.0151078571116517
Epoch: 35 Idx: 5000 Loss: 0.011549252523719484
Epoch: 36 Idx: 0 Loss: 0.015571201911244923
Epoch: 36 Idx: 5000 Loss: 0.01921033968819892
Epoch: 37 Idx: 0 Loss: 0.03966837710512565
Epoch: 37 Idx: 5000 Loss: 0.01467724253824745
Epoch: 38 Idx: 0 Loss: 0.01953854861840488
Epoch: 38 Idx: 5000 Loss: 0.04311732279689602
Epoch: 39 Idx: 0 Loss: 0.024897232946149345
Epoch: 39 Idx: 5000 Loss: 0.02548059419109538
Epoch: 40 Idx: 0 Loss: 0.024104032757178637
Epoch: 40 Idx: 5000 Loss: 0.05246998621761414
Epoch: 41 Idx: 0 Loss: 0.013626379599155986
Epoch: 41 Idx: 5000 Loss: 0.02326736734041592
Epoch: 42 Idx: 0 Loss: 0.025411202648029423
Epoch: 42 Idx: 5000 Loss: 0.00946547193145773
Epoch: 43 Idx: 0 Loss: 0.013280971893077611
Epoch: 43 Idx: 5000 Loss: 0.022561367076802973
Epoch: 44 Idx: 0 Loss: 0.00979355515499378
Epoch: 44 Idx: 5000 Loss: 0.011252408808160856
Epoch: 45 Idx: 0 Loss: 0.05802760059674169
Epoch: 45 Idx: 5000 Loss: 0.017939727847878728
Epoch: 46 Idx: 0 Loss: 0.011254072946856401
Epoch: 46 Idx: 5000 Loss: 0.013029029520897129
Epoch: 47 Idx: 0 Loss: 0.01569351938748297
Epoch: 47 Idx: 5000 Loss: 0.018162094471076973
Epoch: 48 Idx: 0 Loss: 0.051244014415511754
Epoch: 48 Idx: 5000 Loss: 0.019137452808777055
Epoch: 49 Idx: 0 Loss: 0.03084475656813829
Epoch: 49 Idx: 5000 Loss: 0.013437104083918894
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
raining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17153091246060007
Epoch: 0 Idx: 5000 Loss: 0.015706644319573772
Epoch: 1 Idx: 0 Loss: 0.027725003698826745
Epoch: 1 Idx: 5000 Loss: 0.017115331771138595
Epoch: 2 Idx: 0 Loss: 0.016917906451312154
Epoch: 2 Idx: 5000 Loss: 0.026093633029685863
Epoch: 3 Idx: 0 Loss: 0.02423421283042155
Epoch: 3 Idx: 5000 Loss: 0.026337712583626625
Epoch: 4 Idx: 0 Loss: 0.01630569469927734
Epoch: 4 Idx: 5000 Loss: 0.021305647960477684
Epoch: 5 Idx: 0 Loss: 0.03858264320998938
Epoch: 5 Idx: 5000 Loss: 0.02756479457566791
Epoch: 6 Idx: 0 Loss: 0.027880812764220334
Epoch: 6 Idx: 5000 Loss: 0.03837626874517623
Epoch: 7 Idx: 0 Loss: 0.01799474106697074
Epoch: 7 Idx: 5000 Loss: 0.02079668966083023
Epoch: 8 Idx: 0 Loss: 0.012091653965352714
Epoch: 8 Idx: 5000 Loss: 0.026218895023914984
Epoch: 9 Idx: 0 Loss: 0.012052534062494366
Epoch: 9 Idx: 5000 Loss: 0.01831152259880091
Epoch: 10 Idx: 0 Loss: 0.016155347430921833
Epoch: 10 Idx: 5000 Loss: 0.025804160504840586
Epoch: 11 Idx: 0 Loss: 0.019794599839240618
Epoch: 11 Idx: 5000 Loss: 0.02129709513989126
Epoch: 12 Idx: 0 Loss: 0.028014702745921372
Epoch: 12 Idx: 5000 Loss: 0.02573112670847602
Epoch: 13 Idx: 0 Loss: 0.016700835729204833
Epoch: 13 Idx: 5000 Loss: 0.018262245493626043
Epoch: 14 Idx: 0 Loss: 0.05652545477087166
Epoch: 14 Idx: 5000 Loss: 0.008908959461262448
Epoch: 15 Idx: 0 Loss: 0.02590745005237555
Epoch: 15 Idx: 5000 Loss: 0.03258135642551713
Epoch: 16 Idx: 0 Loss: 0.012715100008319564
Epoch: 16 Idx: 5000 Loss: 0.03268735125930693
Epoch: 17 Idx: 0 Loss: 0.01616379316197019
Epoch: 17 Idx: 5000 Loss: 0.01223355433739719
Epoch: 18 Idx: 0 Loss: 0.010717943374493673
Epoch: 18 Idx: 5000 Loss: 0.029627592065011615
Epoch: 19 Idx: 0 Loss: 0.018611487129835385
Epoch: 19 Idx: 5000 Loss: 0.011577551096867996
Epoch: 20 Idx: 0 Loss: 0.022302956982314923
Epoch: 20 Idx: 5000 Loss: 0.02185868351755116
Epoch: 21 Idx: 0 Loss: 0.02816730329336161
Epoch: 21 Idx: 5000 Loss: 0.0410047046878471
Epoch: 22 Idx: 0 Loss: 0.0195070964112833
Epoch: 22 Idx: 5000 Loss: 0.030955549576883318
Epoch: 23 Idx: 0 Loss: 0.01070715027978757
Epoch: 23 Idx: 5000 Loss: 0.02885639200115771
Epoch: 24 Idx: 0 Loss: 0.018048899353885775
Epoch: 24 Idx: 5000 Loss: 0.02309189204374052
Epoch: 25 Idx: 0 Loss: 0.014378741852383116
Epoch: 25 Idx: 5000 Loss: 0.018066152200862308
Epoch: 26 Idx: 0 Loss: 0.019487248454841816
Epoch: 26 Idx: 5000 Loss: 0.018653648547881114
Epoch: 27 Idx: 0 Loss: 0.016677098849603958
Epoch: 27 Idx: 5000 Loss: 0.012267000558337564
Epoch: 28 Idx: 0 Loss: 0.027715659524470068
Epoch: 28 Idx: 5000 Loss: 0.022234254105055803
Epoch: 29 Idx: 0 Loss: 0.014761006032955078
Epoch: 29 Idx: 5000 Loss: 0.014616913759484085
Epoch: 30 Idx: 0 Loss: 0.03172561867543051
Epoch: 30 Idx: 5000 Loss: 0.023027713454024913
Epoch: 31 Idx: 0 Loss: 0.041121776453820355
Epoch: 31 Idx: 5000 Loss: 0.015272917782085943
Epoch: 32 Idx: 0 Loss: 0.020250148443698004
Epoch: 32 Idx: 5000 Loss: 0.0148564765027216
Epoch: 33 Idx: 0 Loss: 0.012394043700094632
Epoch: 33 Idx: 5000 Loss: 0.02894786664285947
Epoch: 34 Idx: 0 Loss: 0.014141368525465887
Epoch: 34 Idx: 5000 Loss: 0.017310739078469015
Epoch: 35 Idx: 0 Loss: 0.024611491796442654
Epoch: 35 Idx: 5000 Loss: 0.05014623057686753
Epoch: 36 Idx: 0 Loss: 0.016631443486218495
Epoch: 36 Idx: 5000 Loss: 0.026543009984121936
Epoch: 37 Idx: 0 Loss: 0.017797526763305687
Epoch: 37 Idx: 5000 Loss: 0.012299874994452287
Epoch: 38 Idx: 0 Loss: 0.01862024332769997
Epoch: 38 Idx: 5000 Loss: 0.024795322195486694
Epoch: 39 Idx: 0 Loss: 0.017741254176788422
Epoch: 39 Idx: 5000 Loss: 0.02356752753036831
Epoch: 40 Idx: 0 Loss: 0.012142010074666199
Epoch: 40 Idx: 5000 Loss: 0.019238265434491718
Epoch: 41 Idx: 0 Loss: 0.021916953205197737
Epoch: 41 Idx: 5000 Loss: 0.02557480338790343
Epoch: 42 Idx: 0 Loss: 0.03172184594330907
Epoch: 42 Idx: 5000 Loss: 0.022002939953338185
Epoch: 43 Idx: 0 Loss: 0.018580583106925244
Epoch: 43 Idx: 5000 Loss: 0.01346764918130258
Epoch: 44 Idx: 0 Loss: 0.013025962983123224
Epoch: 44 Idx: 5000 Loss: 0.037526508817995
Epoch: 45 Idx: 0 Loss: 0.01820977596892398
Epoch: 45 Idx: 5000 Loss: 0.03477884706272791
Epoch: 46 Idx: 0 Loss: 0.021894738039497354
Epoch: 46 Idx: 5000 Loss: 0.044540859632080085
Epoch: 47 Idx: 0 Loss: 0.020439211107205896
Epoch: 47 Idx: 5000 Loss: 0.02196591001072885
Epoch: 48 Idx: 0 Loss: 0.02290418179876043
Epoch: 48 Idx: 5000 Loss: 0.018399872131173868
Epoch: 49 Idx: 0 Loss: 0.022570693129399563
Epoch: 49 Idx: 5000 Loss: 0.027879538991185844
Len (direct inputs):  100
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17987299703263765
Epoch: 0 Idx: 5000 Loss: 0.031110680436638154
Epoch: 1 Idx: 0 Loss: 0.025631987704271282
Epoch: 1 Idx: 5000 Loss: 0.028271992907787834
Epoch: 2 Idx: 0 Loss: 0.02487835874349361
Epoch: 2 Idx: 5000 Loss: 0.016950689521789006
Epoch: 3 Idx: 0 Loss: 0.015044970651372034
Epoch: 3 Idx: 5000 Loss: 0.03250711052703753
Epoch: 4 Idx: 0 Loss: 0.017944944243384803
Epoch: 4 Idx: 5000 Loss: 0.026171229574816907
Epoch: 5 Idx: 0 Loss: 0.017460075781942364
Epoch: 5 Idx: 5000 Loss: 0.01904748636145111
Epoch: 6 Idx: 0 Loss: 0.02126030728681664
Epoch: 6 Idx: 5000 Loss: 0.013640005734285915
Epoch: 7 Idx: 0 Loss: 0.03181007983125586
Epoch: 7 Idx: 5000 Loss: 0.01976102066872365
Epoch: 8 Idx: 0 Loss: 0.02001758340372116
Epoch: 8 Idx: 5000 Loss: 0.024603367546329077
Epoch: 9 Idx: 0 Loss: 0.026730464041540315
Epoch: 9 Idx: 5000 Loss: 0.02403109302772705
Epoch: 10 Idx: 0 Loss: 0.022202811051252304
Epoch: 10 Idx: 5000 Loss: 0.024456622609351376
Epoch: 11 Idx: 0 Loss: 0.020087518695886326
Epoch: 11 Idx: 5000 Loss: 0.012123366863338269
Epoch: 12 Idx: 0 Loss: 0.04641432002989555
Epoch: 12 Idx: 5000 Loss: 0.024849098679560856
Epoch: 13 Idx: 0 Loss: 0.006387451357040363
Epoch: 13 Idx: 5000 Loss: 0.01554555368557385
Epoch: 14 Idx: 0 Loss: 0.01423058677635633
Epoch: 14 Idx: 5000 Loss: 0.03437777712752328
Epoch: 15 Idx: 0 Loss: 0.01963035909897672
Epoch: 15 Idx: 5000 Loss: 0.018212866389027076
Epoch: 16 Idx: 0 Loss: 0.024101755026499146
Epoch: 16 Idx: 5000 Loss: 0.011701525362162539
Epoch: 17 Idx: 0 Loss: 0.019014163980791858
Epoch: 17 Idx: 5000 Loss: 0.019527586941805463
Epoch: 18 Idx: 0 Loss: 0.01491568624050099
Epoch: 18 Idx: 5000 Loss: 0.02026500690892778
Epoch: 19 Idx: 0 Loss: 0.021122481597103625
Epoch: 19 Idx: 5000 Loss: 0.00784258913580985
Epoch: 20 Idx: 0 Loss: 0.023214360484027165
Epoch: 20 Idx: 5000 Loss: 0.04215694076021299
Epoch: 21 Idx: 0 Loss: 0.01848046317630765
Epoch: 21 Idx: 5000 Loss: 0.012820766154236336
Epoch: 22 Idx: 0 Loss: 0.022231541021987
Epoch: 22 Idx: 5000 Loss: 0.04150254064457204
Epoch: 23 Idx: 0 Loss: 0.0319782619142298
Epoch: 23 Idx: 5000 Loss: 0.012222287956989049
Epoch: 24 Idx: 0 Loss: 0.03065327241700873
Epoch: 24 Idx: 5000 Loss: 0.03314015439953191
Epoch: 25 Idx: 0 Loss: 0.0272886189701615
Epoch: 25 Idx: 5000 Loss: 0.018241562920991758
Epoch: 26 Idx: 0 Loss: 0.013362789435504981
Epoch: 26 Idx: 5000 Loss: 0.030422061775767447
Epoch: 27 Idx: 0 Loss: 0.030730984441844446
Epoch: 27 Idx: 5000 Loss: 0.01613189070960858
Epoch: 28 Idx: 0 Loss: 0.019700822883117888
Epoch: 28 Idx: 5000 Loss: 0.01938606811635046
Epoch: 29 Idx: 0 Loss: 0.018397238750066377
Epoch: 29 Idx: 5000 Loss: 0.006977841816303362
Epoch: 30 Idx: 0 Loss: 0.01750630589257647
Epoch: 30 Idx: 5000 Loss: 0.029114642880430988
Epoch: 31 Idx: 0 Loss: 0.015968495579042872
Epoch: 31 Idx: 5000 Loss: 0.023834423826270656
Epoch: 32 Idx: 0 Loss: 0.01960730730174506
Epoch: 32 Idx: 5000 Loss: 0.054431537350197234
Epoch: 33 Idx: 0 Loss: 0.016298860128100882
Epoch: 33 Idx: 5000 Loss: 0.0324682723110592
Epoch: 34 Idx: 0 Loss: 0.03338558357108026
Epoch: 34 Idx: 5000 Loss: 0.0379581417972291
Epoch: 35 Idx: 0 Loss: 0.011850610290118281
Epoch: 35 Idx: 5000 Loss: 0.012089204905196368
Epoch: 36 Idx: 0 Loss: 0.016297327599788886
Epoch: 36 Idx: 5000 Loss: 0.024427580595268387
Epoch: 37 Idx: 0 Loss: 0.023459444053525274
Epoch: 37 Idx: 5000 Loss: 0.017795140527530343
Epoch: 38 Idx: 0 Loss: 0.007343866189726928
Epoch: 38 Idx: 5000 Loss: 0.009957970154479096
Epoch: 39 Idx: 0 Loss: 0.015439479858085447
Epoch: 39 Idx: 5000 Loss: 0.021293660284712947
Epoch: 40 Idx: 0 Loss: 0.031456006810401185
Epoch: 40 Idx: 5000 Loss: 0.02495338714666876
Epoch: 41 Idx: 0 Loss: 0.011396597883157776
Epoch: 41 Idx: 5000 Loss: 0.012518627035882732
Epoch: 42 Idx: 0 Loss: 0.018231147951442126
Epoch: 42 Idx: 5000 Loss: 0.020124995905957413
Epoch: 43 Idx: 0 Loss: 0.018886206412358122
Epoch: 43 Idx: 5000 Loss: 0.01540457467376001
Epoch: 44 Idx: 0 Loss: 0.022758966264136003
Epoch: 44 Idx: 5000 Loss: 0.030673769071679808
Epoch: 45 Idx: 0 Loss: 0.04827089582151692
Epoch: 45 Idx: 5000 Loss: 0.03153875179477023
Epoch: 46 Idx: 0 Loss: 0.022838611782999636
Epoch: 46 Idx: 5000 Loss: 0.01958676921445318
Epoch: 47 Idx: 0 Loss: 0.0288201894765557
Epoch: 47 Idx: 5000 Loss: 0.040446941827323495
Epoch: 48 Idx: 0 Loss: 0.02205179584656071
Epoch: 48 Idx: 5000 Loss: 0.012872738460778197
Epoch: 49 Idx: 0 Loss: 0.01158225366057439
Epoch: 49 Idx: 5000 Loss: 0.030582486223424125
Len (direct inputs):  89
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
n by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18560432043968939
Epoch: 0 Idx: 5000 Loss: 0.04003992504620104
Epoch: 1 Idx: 0 Loss: 0.017257307533131563
Epoch: 1 Idx: 5000 Loss: 0.011948063253778726
Epoch: 2 Idx: 0 Loss: 0.022424037823217308
Epoch: 2 Idx: 5000 Loss: 0.017831289161401573
Epoch: 3 Idx: 0 Loss: 0.01647259418398117
Epoch: 3 Idx: 5000 Loss: 0.014365175474830329
Epoch: 4 Idx: 0 Loss: 0.012673708797689483
Epoch: 4 Idx: 5000 Loss: 0.016349442394999726
Epoch: 5 Idx: 0 Loss: 0.012612993989749202
Epoch: 5 Idx: 5000 Loss: 0.02734328855125097
Epoch: 6 Idx: 0 Loss: 0.02026273006065768
Epoch: 6 Idx: 5000 Loss: 0.007101418494236795
Epoch: 7 Idx: 0 Loss: 0.02688605123515327
Epoch: 7 Idx: 5000 Loss: 0.03981313967304598
Epoch: 8 Idx: 0 Loss: 0.019855766024322315
Epoch: 8 Idx: 5000 Loss: 0.018551072825846667
Epoch: 9 Idx: 0 Loss: 0.022635819938314573
Epoch: 9 Idx: 5000 Loss: 0.028854640496560315
Epoch: 10 Idx: 0 Loss: 0.015630834754424127
Epoch: 10 Idx: 5000 Loss: 0.028328123307752744
Epoch: 11 Idx: 0 Loss: 0.019329880739973167
Epoch: 11 Idx: 5000 Loss: 0.0389618001006284
Epoch: 12 Idx: 0 Loss: 0.009612860703380724
Epoch: 12 Idx: 5000 Loss: 0.017466396002404676
Epoch: 13 Idx: 0 Loss: 0.015328692505455481
Epoch: 13 Idx: 5000 Loss: 0.032342466350116394
Epoch: 14 Idx: 0 Loss: 0.0190140589451558
Epoch: 14 Idx: 5000 Loss: 0.03173548585167026
Epoch: 15 Idx: 0 Loss: 0.01935743667799944
Epoch: 15 Idx: 5000 Loss: 0.022264859115364563
Epoch: 16 Idx: 0 Loss: 0.033436136072787705
Epoch: 16 Idx: 5000 Loss: 0.025225399482925494
Epoch: 17 Idx: 0 Loss: 0.043546688734045366
Epoch: 17 Idx: 5000 Loss: 0.02022596184905257
Epoch: 18 Idx: 0 Loss: 0.004885080223198439
Epoch: 18 Idx: 5000 Loss: 0.03204289151765135
Epoch: 19 Idx: 0 Loss: 0.024307619452108725
Epoch: 19 Idx: 5000 Loss: 0.02508903829899572
Epoch: 20 Idx: 0 Loss: 0.028404866761807538
Epoch: 20 Idx: 5000 Loss: 0.021419426670522185
Epoch: 21 Idx: 0 Loss: 0.01459755565659946
Epoch: 21 Idx: 5000 Loss: 0.01888091073398495
Epoch: 22 Idx: 0 Loss: 0.04294243481925405
Epoch: 22 Idx: 5000 Loss: 0.020645404142401194
Epoch: 23 Idx: 0 Loss: 0.01852146436246951
Epoch: 23 Idx: 5000 Loss: 0.02540420927189195
Epoch: 24 Idx: 0 Loss: 0.017764263455682387
Epoch: 24 Idx: 5000 Loss: 0.021321038365693178
Epoch: 25 Idx: 0 Loss: 0.02798440567231035
Epoch: 25 Idx: 5000 Loss: 0.01816304113975505
Epoch: 26 Idx: 0 Loss: 0.026126331362096633
Epoch: 26 Idx: 5000 Loss: 0.019733926979368568
Epoch: 27 Idx: 0 Loss: 0.026780403415551823
Epoch: 27 Idx: 5000 Loss: 0.027501719408321298
Epoch: 28 Idx: 0 Loss: 0.024334602882422743
Epoch: 28 Idx: 5000 Loss: 0.01044152013945857
Epoch: 29 Idx: 0 Loss: 0.022723913983510802
Epoch: 29 Idx: 5000 Loss: 0.02995582008671847
Epoch: 30 Idx: 0 Loss: 0.016230369404082005
Epoch: 30 Idx: 5000 Loss: 0.016352585592534545
Epoch: 31 Idx: 0 Loss: 0.01183178684170473
Epoch: 31 Idx: 5000 Loss: 0.01570381029569643
Epoch: 32 Idx: 0 Loss: 0.021362339700084575
Epoch: 32 Idx: 5000 Loss: 0.015424032506218982
Epoch: 33 Idx: 0 Loss: 0.03495557573623581
Epoch: 33 Idx: 5000 Loss: 0.03682322725909474
Epoch: 34 Idx: 0 Loss: 0.042312489852084106
Epoch: 34 Idx: 5000 Loss: 0.02983789304483335
Epoch: 35 Idx: 0 Loss: 0.018993739669819404
Epoch: 35 Idx: 5000 Loss: 0.03157765543548332
Epoch: 36 Idx: 0 Loss: 0.013711906975716455
Epoch: 36 Idx: 5000 Loss: 0.013994957818041758
Epoch: 37 Idx: 0 Loss: 0.014490462864226844
Epoch: 37 Idx: 5000 Loss: 0.02298220314793848
Epoch: 38 Idx: 0 Loss: 0.030456380123676255
Epoch: 38 Idx: 5000 Loss: 0.012590513194979124
Epoch: 39 Idx: 0 Loss: 0.012425961425985753
Epoch: 39 Idx: 5000 Loss: 0.013054536098378762
Epoch: 40 Idx: 0 Loss: 0.018977992593468922
Epoch: 40 Idx: 5000 Loss: 0.022096249330647626
Epoch: 41 Idx: 0 Loss: 0.02012687264818217
Epoch: 41 Idx: 5000 Loss: 0.024183766590523043
Epoch: 42 Idx: 0 Loss: 0.02293476838191771
Epoch: 42 Idx: 5000 Loss: 0.015567703492537084
Epoch: 43 Idx: 0 Loss: 0.03132011784749473
Epoch: 43 Idx: 5000 Loss: 0.026622924735704323
Epoch: 44 Idx: 0 Loss: 0.024181725070060864
Epoch: 44 Idx: 5000 Loss: 0.018733604003414013
Epoch: 45 Idx: 0 Loss: 0.01483641986861713
Epoch: 45 Idx: 5000 Loss: 0.025365768055969698
Epoch: 46 Idx: 0 Loss: 0.025438570135431125
Epoch: 46 Idx: 5000 Loss: 0.02005151617759522
Epoch: 47 Idx: 0 Loss: 0.01812147562058069
Epoch: 47 Idx: 5000 Loss: 0.012561416128398914
Epoch: 48 Idx: 0 Loss: 0.016626580560773117
Epoch: 48 Idx: 5000 Loss: 0.017140488233208774
Epoch: 49 Idx: 0 Loss: 0.022667752809585873
Epoch: 49 Idx: 5000 Loss: 0.018150641928381818
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.285268848709466
Epoch: 0 Idx: 5000 Loss: 0.014844692559423838
Epoch: 1 Idx: 0 Loss: 0.026086526089328082
Epoch: 1 Idx: 5000 Loss: 0.019178787454554126
Epoch: 2 Idx: 0 Loss: 0.014520840750514071
Epoch: 2 Idx: 5000 Loss: 0.02018927461782409
Epoch: 3 Idx: 0 Loss: 0.027668751024292264
Epoch: 3 Idx: 5000 Loss: 0.023900808269888035
Epoch: 4 Idx: 0 Loss: 0.014711905630342588
Epoch: 4 Idx: 5000 Loss: 0.013980308634079106
Epoch: 5 Idx: 0 Loss: 0.023151577560058385
Epoch: 5 Idx: 5000 Loss: 0.022582198442597824
Epoch: 6 Idx: 0 Loss: 0.007880264840939028
Epoch: 6 Idx: 5000 Loss: 0.019112650039788705
Epoch: 7 Idx: 0 Loss: 0.02499586059872309
Epoch: 7 Idx: 5000 Loss: 0.03196606498436064
Epoch: 8 Idx: 0 Loss: 0.01552663979138445
Epoch: 8 Idx: 5000 Loss: 0.021912328012762886
Epoch: 9 Idx: 0 Loss: 0.023938116608898188
Epoch: 9 Idx: 5000 Loss: 0.016350149255732464
Epoch: 10 Idx: 0 Loss: 0.013754578354065086
Epoch: 10 Idx: 5000 Loss: 0.03861640613381774
Epoch: 11 Idx: 0 Loss: 0.03518682790524624
Epoch: 11 Idx: 5000 Loss: 0.01809620247652193
Epoch: 12 Idx: 0 Loss: 0.013106026008820945
Epoch: 12 Idx: 5000 Loss: 0.02324292507656356
Epoch: 13 Idx: 0 Loss: 0.015576750503553076
Epoch: 13 Idx: 5000 Loss: 0.025040733883372933
Epoch: 14 Idx: 0 Loss: 0.02090654406537886
Epoch: 14 Idx: 5000 Loss: 0.016331307029683485
Epoch: 15 Idx: 0 Loss: 0.014600133703715407
Epoch: 15 Idx: 5000 Loss: 0.022831340657106174
Epoch: 16 Idx: 0 Loss: 0.02109576680592544
Epoch: 16 Idx: 5000 Loss: 0.014119858056306483
Epoch: 17 Idx: 0 Loss: 0.014457579217997401
Epoch: 17 Idx: 5000 Loss: 0.042943864705392784
Epoch: 18 Idx: 0 Loss: 0.02169540340237432
Epoch: 18 Idx: 5000 Loss: 0.01805956721819492
Epoch: 19 Idx: 0 Loss: 0.01563176924230975
Epoch: 19 Idx: 5000 Loss: 0.02767312760241577
Epoch: 20 Idx: 0 Loss: 0.027629308621243635
Epoch: 20 Idx: 5000 Loss: 0.022528607687912802
Epoch: 21 Idx: 0 Loss: 0.017636377116717956
Epoch: 21 Idx: 5000 Loss: 0.028410763011975804
Epoch: 22 Idx: 0 Loss: 0.023983219683075808
Epoch: 22 Idx: 5000 Loss: 0.022251066960063313
Epoch: 23 Idx: 0 Loss: 0.018880184640267632
Epoch: 23 Idx: 5000 Loss: 0.02919040166715844
Epoch: 24 Idx: 0 Loss: 0.012185195269395727
Epoch: 24 Idx: 5000 Loss: 0.02893935166571951
Epoch: 25 Idx: 0 Loss: 0.029517913761475355
Epoch: 25 Idx: 5000 Loss: 0.03477754150138998
Epoch: 26 Idx: 0 Loss: 0.02259272692844358
Epoch: 26 Idx: 5000 Loss: 0.021931723926963916
Epoch: 27 Idx: 0 Loss: 0.01722606601675373
Epoch: 27 Idx: 5000 Loss: 0.045572040494785374
Epoch: 28 Idx: 0 Loss: 0.028832564554674187
Epoch: 28 Idx: 5000 Loss: 0.012793846331193896
Epoch: 29 Idx: 0 Loss: 0.029525082589621388
Epoch: 29 Idx: 5000 Loss: 0.013853340487821258
Epoch: 30 Idx: 0 Loss: 0.02189972551780132
Epoch: 30 Idx: 5000 Loss: 0.01502002625034451
Epoch: 31 Idx: 0 Loss: 0.01806684526372477
Epoch: 31 Idx: 5000 Loss: 0.012021091837738692
Epoch: 32 Idx: 0 Loss: 0.008350843723482462
Epoch: 32 Idx: 5000 Loss: 0.01146244194076335
Epoch: 33 Idx: 0 Loss: 0.02052446354571959
Epoch: 33 Idx: 5000 Loss: 0.0221885923519693
Epoch: 34 Idx: 0 Loss: 0.018461806689154828
Epoch: 34 Idx: 5000 Loss: 0.05128242907737967
Epoch: 35 Idx: 0 Loss: 0.028599579974900355
Epoch: 35 Idx: 5000 Loss: 0.023176500785924974
Epoch: 36 Idx: 0 Loss: 0.027091640267257702
Epoch: 36 Idx: 5000 Loss: 0.03150602881250561
Epoch: 37 Idx: 0 Loss: 0.01906339922083191
Epoch: 37 Idx: 5000 Loss: 0.016147534892186944
Epoch: 38 Idx: 0 Loss: 0.026314778364919928
Epoch: 38 Idx: 5000 Loss: 0.027792300386668085
Epoch: 39 Idx: 0 Loss: 0.020150494548098884
Epoch: 39 Idx: 5000 Loss: 0.037564945920129
Epoch: 40 Idx: 0 Loss: 0.02210206995440723
Epoch: 40 Idx: 5000 Loss: 0.020936923627441956
Epoch: 41 Idx: 0 Loss: 0.020645767245425474
Epoch: 41 Idx: 5000 Loss: 0.015348972877666298
Epoch: 42 Idx: 0 Loss: 0.01574109592420558
Epoch: 42 Idx: 5000 Loss: 0.029889053263348057
Epoch: 43 Idx: 0 Loss: 0.022783160143140548
Epoch: 43 Idx: 5000 Loss: 0.013375098245236937
Epoch: 44 Idx: 0 Loss: 0.016656501638353173
Epoch: 44 Idx: 5000 Loss: 0.013518420010206873
Epoch: 45 Idx: 0 Loss: 0.023248909620041853
Epoch: 45 Idx: 5000 Loss: 0.014599553665277681
Epoch: 46 Idx: 0 Loss: 0.01685979293137036
Epoch: 46 Idx: 5000 Loss: 0.0384879974198203
Epoch: 47 Idx: 0 Loss: 0.009662127637156839
Epoch: 47 Idx: 5000 Loss: 0.028255882059335036
Epoch: 48 Idx: 0 Loss: 0.020721883223009243
Epoch: 48 Idx: 5000 Loss: 0.04027057548579738
Epoch: 49 Idx: 0 Loss: 0.02473671571037031
Epoch: 49 Idx: 5000 Loss: 0.03419332564111723
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19623238032351242
Epoch: 0 Idx: 5000 Loss: 0.01934629066140743
Epoch: 1 Idx: 0 Loss: 0.033428894702975026
Epoch: 1 Idx: 5000 Loss: 0.024312708635861373
Epoch: 2 Idx: 0 Loss: 0.01673805363312046
Epoch: 2 Idx: 5000 Loss: 0.013827579486648683
Epoch: 3 Idx: 0 Loss: 0.017087009999932363
Epoch: 3 Idx: 5000 Loss: 0.02144481192735729
Epoch: 4 Idx: 0 Loss: 0.015271005402998984
Epoch: 4 Idx: 5000 Loss: 0.020051846449662646
Epoch: 5 Idx: 0 Loss: 0.02542748529690233
Epoch: 5 Idx: 5000 Loss: 0.013884277508913585
Epoch: 6 Idx: 0 Loss: 0.018973162536388866
Epoch: 6 Idx: 5000 Loss: 0.018825758586637216
Epoch: 7 Idx: 0 Loss: 0.032128452421634604
Epoch: 7 Idx: 5000 Loss: 0.035694417578813475
Epoch: 8 Idx: 0 Loss: 0.022839002891642708
Epoch: 8 Idx: 5000 Loss: 0.020389678885760217
Epoch: 9 Idx: 0 Loss: 0.01970408959036788
Epoch: 9 Idx: 5000 Loss: 0.0443806174627519
Epoch: 10 Idx: 0 Loss: 0.018273147806329874
Epoch: 10 Idx: 5000 Loss: 0.01544614011478678
Epoch: 11 Idx: 0 Loss: 0.02719435812254231
Epoch: 11 Idx: 5000 Loss: 0.011024293356463842
Epoch: 12 Idx: 0 Loss: 0.013452472048471206
Epoch: 12 Idx: 5000 Loss: 0.036298754363978045
Epoch: 13 Idx: 0 Loss: 0.03686361549385317
Epoch: 13 Idx: 5000 Loss: 0.012692913712185647
Epoch: 14 Idx: 0 Loss: 0.022125208035196166
Epoch: 14 Idx: 5000 Loss: 0.015350658143593654
Epoch: 15 Idx: 0 Loss: 0.02310924978062861
Epoch: 15 Idx: 5000 Loss: 0.021579614069471578
Epoch: 16 Idx: 0 Loss: 0.03822390797444043
Epoch: 16 Idx: 5000 Loss: 0.03469538039766669
Epoch: 17 Idx: 0 Loss: 0.040092227451798836
Epoch: 17 Idx: 5000 Loss: 0.017539446022968433
Epoch: 18 Idx: 0 Loss: 0.021315474145079255
Epoch: 18 Idx: 5000 Loss: 0.02395275076368902
Epoch: 19 Idx: 0 Loss: 0.018204878314487256
Epoch: 19 Idx: 5000 Loss: 0.03594890537680517
Epoch: 20 Idx: 0 Loss: 0.028782909718455957
Epoch: 20 Idx: 5000 Loss: 0.02462798461283749
Epoch: 21 Idx: 0 Loss: 0.02114460917530285
Epoch: 21 Idx: 5000 Loss: 0.014227011923614166
Epoch: 22 Idx: 0 Loss: 0.01498718012838747
Epoch: 22 Idx: 5000 Loss: 0.04272824473519543
Epoch: 23 Idx: 0 Loss: 0.01332109180511671
Epoch: 23 Idx: 5000 Loss: 0.015369390296639456
Epoch: 24 Idx: 0 Loss: 0.026273121813530594
Epoch: 24 Idx: 5000 Loss: 0.017853441036966844
Epoch: 25 Idx: 0 Loss: 0.01875218758396078
Epoch: 25 Idx: 5000 Loss: 0.011280557883434333
Epoch: 26 Idx: 0 Loss: 0.012549752144183683
Epoch: 26 Idx: 5000 Loss: 0.01759219709913152
Epoch: 27 Idx: 0 Loss: 0.024231836619950868
Epoch: 27 Idx: 5000 Loss: 0.021733399077769375
Epoch: 28 Idx: 0 Loss: 0.04354212853389557
Epoch: 28 Idx: 5000 Loss: 0.04674705077136134
Epoch: 29 Idx: 0 Loss: 0.0071058308843738646
Epoch: 29 Idx: 5000 Loss: 0.019973441431587198
Epoch: 30 Idx: 0 Loss: 0.010340315350734031
Epoch: 30 Idx: 5000 Loss: 0.020070205459603127
Epoch: 31 Idx: 0 Loss: 0.022939911057450198
Epoch: 31 Idx: 5000 Loss: 0.01641776233488572
Epoch: 32 Idx: 0 Loss: 0.03872747196035454
Epoch: 32 Idx: 5000 Loss: 0.01993598428709495
Epoch: 33 Idx: 0 Loss: 0.009554728327255213
Epoch: 33 Idx: 5000 Loss: 0.014358745289307594
Epoch: 34 Idx: 0 Loss: 0.02032778314929482
Epoch: 34 Idx: 5000 Loss: 0.009492454057628514
Epoch: 35 Idx: 0 Loss: 0.02283852083910557
Epoch: 35 Idx: 5000 Loss: 0.03203387915973004
Epoch: 36 Idx: 0 Loss: 0.02031056782333211
Epoch: 36 Idx: 5000 Loss: 0.016705456326229627
Epoch: 37 Idx: 0 Loss: 0.02477149242690539
Epoch: 37 Idx: 5000 Loss: 0.023794328997975775
Epoch: 38 Idx: 0 Loss: 0.02052028763755028
Epoch: 38 Idx: 5000 Loss: 0.012553870895826475
Epoch: 39 Idx: 0 Loss: 0.01797479564908882
Epoch: 39 Idx: 5000 Loss: 0.03381731043540643
Epoch: 40 Idx: 0 Loss: 0.02201924953486159
Epoch: 40 Idx: 5000 Loss: 0.020143705152840682
Epoch: 41 Idx: 0 Loss: 0.04411638147450901
Epoch: 41 Idx: 5000 Loss: 0.020834197136631184
Epoch: 42 Idx: 0 Loss: 0.01692040287514735
Epoch: 42 Idx: 5000 Loss: 0.012073153618842678
Epoch: 43 Idx: 0 Loss: 0.015045609787502079
Epoch: 43 Idx: 5000 Loss: 0.015194643702597547
Epoch: 44 Idx: 0 Loss: 0.019122711533823528
Epoch: 44 Idx: 5000 Loss: 0.013188585991669478
Epoch: 45 Idx: 0 Loss: 0.019294493642339955
Epoch: 45 Idx: 5000 Loss: 0.020008738812759733
Epoch: 46 Idx: 0 Loss: 0.021131603042960072
Epoch: 46 Idx: 5000 Loss: 0.036720935419988596
Epoch: 47 Idx: 0 Loss: 0.023036445301134596
Epoch: 47 Idx: 5000 Loss: 0.024029316377763787
Epoch: 48 Idx: 0 Loss: 0.03324882055539259
Epoch: 48 Idx: 5000 Loss: 0.03375928166852621
Epoch: 49 Idx: 0 Loss: 0.01560376466214502
Epoch: 49 Idx: 5000 Loss: 0.021226074254359087
Len (direct inputs):  95
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
0.8226260769475269
Parameter containing:
tensor([0.8226], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.2169572984268172
Epoch: 0 Idx: 5000 Loss: 0.01192099026405369
Epoch: 1 Idx: 0 Loss: 0.018978049282588432
Epoch: 1 Idx: 5000 Loss: 0.02277142675861628
Epoch: 2 Idx: 0 Loss: 0.029039956675396474
Epoch: 2 Idx: 5000 Loss: 0.02016040461962843
Epoch: 3 Idx: 0 Loss: 0.02721100149822324
Epoch: 3 Idx: 5000 Loss: 0.017535729754721
Epoch: 4 Idx: 0 Loss: 0.01721359122765576
Epoch: 4 Idx: 5000 Loss: 0.02430903333699258
Epoch: 5 Idx: 0 Loss: 0.024103647587858705
Epoch: 5 Idx: 5000 Loss: 0.02460995465128704
Epoch: 6 Idx: 0 Loss: 0.013408608592640522
Epoch: 6 Idx: 5000 Loss: 0.012780750772984876
Epoch: 7 Idx: 0 Loss: 0.022585128973968558
Epoch: 7 Idx: 5000 Loss: 0.023438500061023028
Epoch: 8 Idx: 0 Loss: 0.03457610384425304
Epoch: 8 Idx: 5000 Loss: 0.02273417217298144
Epoch: 9 Idx: 0 Loss: 0.019989777097752296
Epoch: 9 Idx: 5000 Loss: 0.012563047287882882
Epoch: 10 Idx: 0 Loss: 0.029697078446469592
Epoch: 10 Idx: 5000 Loss: 0.03480238062494878
Epoch: 11 Idx: 0 Loss: 0.005997845103200601
Epoch: 11 Idx: 5000 Loss: 0.021232081813270555
Epoch: 12 Idx: 0 Loss: 0.031586885960681536
Epoch: 12 Idx: 5000 Loss: 0.01702085596351751
Epoch: 13 Idx: 0 Loss: 0.03101006997989649
Epoch: 13 Idx: 5000 Loss: 0.018917123042661355
Epoch: 14 Idx: 0 Loss: 0.030820910980736527
Epoch: 14 Idx: 5000 Loss: 0.02582388992095269
Epoch: 15 Idx: 0 Loss: 0.016548393495298505
Epoch: 15 Idx: 5000 Loss: 0.006916980453921398
Epoch: 16 Idx: 0 Loss: 0.02339962329090481
Epoch: 16 Idx: 5000 Loss: 0.02670879950042827
Epoch: 17 Idx: 0 Loss: 0.03856561322880728
Epoch: 17 Idx: 5000 Loss: 0.010818736380927549
Epoch: 18 Idx: 0 Loss: 0.012900372574436565
Epoch: 18 Idx: 5000 Loss: 0.02645840112041673
Epoch: 19 Idx: 0 Loss: 0.0191148535638013
Epoch: 19 Idx: 5000 Loss: 0.019574892983907542
Epoch: 20 Idx: 0 Loss: 0.02597494670301753
Epoch: 20 Idx: 5000 Loss: 0.024976072942867057
Epoch: 21 Idx: 0 Loss: 0.021870316515628895
Epoch: 21 Idx: 5000 Loss: 0.030809008149419524
Epoch: 22 Idx: 0 Loss: 0.01306017930548087
Epoch: 22 Idx: 5000 Loss: 0.016585228821272634
Epoch: 23 Idx: 0 Loss: 0.012759817229707385
Epoch: 23 Idx: 5000 Loss: 0.02258609305778087
Epoch: 24 Idx: 0 Loss: 0.045430024146676906
Epoch: 24 Idx: 5000 Loss: 0.021407504153152693
Epoch: 25 Idx: 0 Loss: 0.02891572246348593
Epoch: 25 Idx: 5000 Loss: 0.015876969205873236
Epoch: 26 Idx: 0 Loss: 0.03174966880436151
Epoch: 26 Idx: 5000 Loss: 0.042630431056399354
Epoch: 27 Idx: 0 Loss: 0.006751637324287428
Epoch: 27 Idx: 5000 Loss: 0.03119405905289931
Epoch: 28 Idx: 0 Loss: 0.008114718986401787
Epoch: 28 Idx: 5000 Loss: 0.01625099000778954
Epoch: 29 Idx: 0 Loss: 0.012308659453608721
Epoch: 29 Idx: 5000 Loss: 0.021103225733884384
Epoch: 30 Idx: 0 Loss: 0.012803510037440152
Epoch: 30 Idx: 5000 Loss: 0.016591508644938267
Epoch: 31 Idx: 0 Loss: 0.0173397024488985
Epoch: 31 Idx: 5000 Loss: 0.02749244165661513
Epoch: 32 Idx: 0 Loss: 0.025007533894937643
Epoch: 32 Idx: 5000 Loss: 0.037286726166158926
Epoch: 33 Idx: 0 Loss: 0.014174148434141925
Epoch: 33 Idx: 5000 Loss: 0.03489882743254627
Epoch: 34 Idx: 0 Loss: 0.03921205016002001
Epoch: 34 Idx: 5000 Loss: 0.017377000427862166
Epoch: 35 Idx: 0 Loss: 0.022958780919834876
Epoch: 35 Idx: 5000 Loss: 0.016337727805447677
Epoch: 36 Idx: 0 Loss: 0.025542406587038068
Epoch: 36 Idx: 5000 Loss: 0.01873091187917196
Epoch: 37 Idx: 0 Loss: 0.022897576746542567
Epoch: 37 Idx: 5000 Loss: 0.01591789941495253
Epoch: 38 Idx: 0 Loss: 0.03568293942189251
Epoch: 38 Idx: 5000 Loss: 0.0279047713440478
Epoch: 39 Idx: 0 Loss: 0.012106257490676595
Epoch: 39 Idx: 5000 Loss: 0.014055270418973014
Epoch: 40 Idx: 0 Loss: 0.026733420760076565
Epoch: 40 Idx: 5000 Loss: 0.02593443051695725
Epoch: 41 Idx: 0 Loss: 0.012909958815211696
Epoch: 41 Idx: 5000 Loss: 0.010762776000750679
Epoch: 42 Idx: 0 Loss: 0.02789294133286681
Epoch: 42 Idx: 5000 Loss: 0.028664323519142908
Epoch: 43 Idx: 0 Loss: 0.018828004842238705
Epoch: 43 Idx: 5000 Loss: 0.027996776215400705
Epoch: 44 Idx: 0 Loss: 0.017068846673932207
Epoch: 44 Idx: 5000 Loss: 0.023945543870728947
Epoch: 45 Idx: 0 Loss: 0.022918405298395582
Epoch: 45 Idx: 5000 Loss: 0.01745477344109314
Epoch: 46 Idx: 0 Loss: 0.014739875291392472
Epoch: 46 Idx: 5000 Loss: 0.01564396957524822
Epoch: 47 Idx: 0 Loss: 0.015390803209808615
Epoch: 47 Idx: 5000 Loss: 0.03082484481094084
Epoch: 48 Idx: 0 Loss: 0.02054044540274501
Epoch: 48 Idx: 5000 Loss: 0.017099549306189718
Epoch: 49 Idx: 0 Loss: 0.020671410757640233
Epoch: 49 Idx: 5000 Loss: 0.04117252122486284
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 476, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 24 failed with Disk quota exceeded
k quota exceeded

ded
 quota exceeded
-----------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 3289916: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs_wtpath2_5.pkl Models/anatomy_aml_bagofnbrs_wtpath2_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs_wtpath2_5.pkl Models/anatomy_aml_bagofnbrs_wtpath2_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:57:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:57:45 2020
Terminated at Tue Sep  1 21:20:50 2020
Results reported at Tue Sep  1 21:20:50 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs_wtpath2_5.pkl Models/anatomy_aml_bagofnbrs_wtpath2_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26584.28 sec.
    Max Memory :                                 2933 MB
    Average Memory :                             2719.80 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40484.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   26585 sec.
    Turnaround time :                            26818 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc227>
Subject: Job 3289914: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 5 Output/test_anatomy_aml_bagofnbrs_wtpath1_5.pkl Models/anatomy_aml_bagofnbrs_wtpath1_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 5 Output/test_anatomy_aml_bagofnbrs_wtpath1_5.pkl Models/anatomy_aml_bagofnbrs_wtpath1_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc227>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Tue Sep  1 21:27:34 2020
Results reported at Tue Sep  1 21:27:34 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 5 Output/test_anatomy_aml_bagofnbrs_wtpath1_5.pkl Models/anatomy_aml_bagofnbrs_wtpath1_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27038.22 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2726.28 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27074 sec.
    Turnaround time :                            27222 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc235>
Subject: Job 3289918: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs_wtpath3_5.pkl Models/anatomy_aml_bagofnbrs_wtpath3_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs_wtpath3_5.pkl Models/anatomy_aml_bagofnbrs_wtpath3_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc235>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:57:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:57:45 2020
Terminated at Tue Sep  1 21:45:21 2020
Results reported at Tue Sep  1 21:45:21 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs_wtpath3_5.pkl Models/anatomy_aml_bagofnbrs_wtpath3_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28053.75 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2713.43 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28056 sec.
    Turnaround time :                            28288 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc235>
Subject: Job 3289920: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs_wtpath4_5.pkl Models/anatomy_aml_bagofnbrs_wtpath4_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs_wtpath4_5.pkl Models/anatomy_aml_bagofnbrs_wtpath4_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc235>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:58:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:58:45 2020
Terminated at Tue Sep  1 22:25:22 2020
Results reported at Tue Sep  1 22:25:22 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs_wtpath4_5.pkl Models/anatomy_aml_bagofnbrs_wtpath4_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30358.49 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2727.73 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30397 sec.
    Turnaround time :                            30689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc252>
Subject: Job 3289922: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs_wtpath5_5.pkl Models/anatomy_aml_bagofnbrs_wtpath5_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs_wtpath5_5.pkl Models/anatomy_aml_bagofnbrs_wtpath5_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc252>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:02:08 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:02:08 2020
Terminated at Tue Sep  1 23:00:58 2020
Results reported at Tue Sep  1 23:00:58 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs_wtpath5_5.pkl Models/anatomy_aml_bagofnbrs_wtpath5_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32265.50 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2718.70 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32330 sec.
    Turnaround time :                            32825 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc250>
Subject: Job 3289924: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs_wtpath7_5.pkl Models/anatomy_aml_bagofnbrs_wtpath7_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs_wtpath7_5.pkl Models/anatomy_aml_bagofnbrs_wtpath7_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc250>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:05:19 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:05:19 2020
Terminated at Tue Sep  1 23:59:52 2020
Results reported at Tue Sep  1 23:59:52 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs_wtpath7_5.pkl Models/anatomy_aml_bagofnbrs_wtpath7_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   35668.19 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2714.62 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   35673 sec.
    Turnaround time :                            36359 sec.

The output (if any) is above this job summary.

