Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1797887440920823
Epoch: 0 Idx: 5000 Loss: 0.02464447114973965
Epoch: 1 Idx: 0 Loss: 0.012527958397431847
Epoch: 1 Idx: 5000 Loss: 0.03360418079656759
Epoch: 2 Idx: 0 Loss: 0.031090745187104946
Epoch: 2 Idx: 5000 Loss: 0.009211543981106265
Epoch: 3 Idx: 0 Loss: 0.017648331151078386
Epoch: 3 Idx: 5000 Loss: 0.014356129318045925
Epoch: 4 Idx: 0 Loss: 0.015038951289427968
Epoch: 4 Idx: 5000 Loss: 0.01803988531955429
Epoch: 5 Idx: 0 Loss: 0.04180940561800746
Epoch: 5 Idx: 5000 Loss: 0.010909787753565357
Epoch: 6 Idx: 0 Loss: 0.03156381386116265
Epoch: 6 Idx: 5000 Loss: 0.025270517328119083
Epoch: 7 Idx: 0 Loss: 0.024343876216117194
Epoch: 7 Idx: 5000 Loss: 0.026963961518412627
Epoch: 8 Idx: 0 Loss: 0.01992941023537307
Epoch: 8 Idx: 5000 Loss: 0.013089287522087524
Epoch: 9 Idx: 0 Loss: 0.0252504264231809
Epoch: 9 Idx: 5000 Loss: 0.016654230697672898
Epoch: 10 Idx: 0 Loss: 0.01880596580434401
Epoch: 10 Idx: 5000 Loss: 0.01756938854203547
Epoch: 11 Idx: 0 Loss: 0.008713504913030145
Epoch: 11 Idx: 5000 Loss: 0.020630893046409565
Epoch: 12 Idx: 0 Loss: 0.04070633421710001
Epoch: 12 Idx: 5000 Loss: 0.026377840252146116
Epoch: 13 Idx: 0 Loss: 0.014530399041588517
Epoch: 13 Idx: 5000 Loss: 0.007748801534034908
Epoch: 14 Idx: 0 Loss: 0.02714671307113056
Epoch: 14 Idx: 5000 Loss: 0.008799474186016117
Epoch: 15 Idx: 0 Loss: 0.017320896688091968
Epoch: 15 Idx: 5000 Loss: 0.0077846691262386195
Epoch: 16 Idx: 0 Loss: 0.023966151021807103
Epoch: 16 Idx: 5000 Loss: 0.02142547479007963
Epoch: 17 Idx: 0 Loss: 0.020161054924100794
Epoch: 17 Idx: 5000 Loss: 0.03483430203667594
Epoch: 18 Idx: 0 Loss: 0.014266718683326147
Epoch: 18 Idx: 5000 Loss: 0.026271638485334082
Epoch: 19 Idx: 0 Loss: 0.03973507312735981
Epoch: 19 Idx: 5000 Loss: 0.018508656513060412
Epoch: 20 Idx: 0 Loss: 0.023177052892850464
Epoch: 20 Idx: 5000 Loss: 0.012319307749921835
Epoch: 21 Idx: 0 Loss: 0.016248985301554476
Epoch: 21 Idx: 5000 Loss: 0.017746871066677397
Epoch: 22 Idx: 0 Loss: 0.024983613735263963
Epoch: 22 Idx: 5000 Loss: 0.031173681273639023
Epoch: 23 Idx: 0 Loss: 0.0241815532612472
Epoch: 23 Idx: 5000 Loss: 0.007729224027859323
Epoch: 24 Idx: 0 Loss: 0.030751804658165828
Epoch: 24 Idx: 5000 Loss: 0.034091913325669396
Epoch: 25 Idx: 0 Loss: 0.020364274546393047
Epoch: 25 Idx: 5000 Loss: 0.02672242926244283
Epoch: 26 Idx: 0 Loss: 0.013093833015429568
Epoch: 26 Idx: 5000 Loss: 0.02855438097047954
Epoch: 27 Idx: 0 Loss: 0.015842693375332298
Epoch: 27 Idx: 5000 Loss: 0.03238822232763364
Epoch: 28 Idx: 0 Loss: 0.03222142028197825
Epoch: 28 Idx: 5000 Loss: 0.036556298366875586
Epoch: 29 Idx: 0 Loss: 0.020452123064446784
Epoch: 29 Idx: 5000 Loss: 0.02163549111396993
Epoch: 30 Idx: 0 Loss: 0.02210451724228601
Epoch: 30 Idx: 5000 Loss: 0.020323254279169554
Epoch: 31 Idx: 0 Loss: 0.022070641375158122
Epoch: 31 Idx: 5000 Loss: 0.03298648707182564
Epoch: 32 Idx: 0 Loss: 0.01664902334587471
Epoch: 32 Idx: 5000 Loss: 0.042646661777822295
Epoch: 33 Idx: 0 Loss: 0.025494711783503397
Epoch: 33 Idx: 5000 Loss: 0.020277377800516104
Epoch: 34 Idx: 0 Loss: 0.011759487307873528
Epoch: 34 Idx: 5000 Loss: 0.015808056726372057
Epoch: 35 Idx: 0 Loss: 0.0151078571116517
Epoch: 35 Idx: 5000 Loss: 0.011549252523719484
Epoch: 36 Idx: 0 Loss: 0.015571201911244923
Epoch: 36 Idx: 5000 Loss: 0.01921033968819892
Epoch: 37 Idx: 0 Loss: 0.03966837710512565
Epoch: 37 Idx: 5000 Loss: 0.01467724253824745
Epoch: 38 Idx: 0 Loss: 0.01953854861840488
Epoch: 38 Idx: 5000 Loss: 0.04311732279689602
Epoch: 39 Idx: 0 Loss: 0.024897232946149345
Epoch: 39 Idx: 5000 Loss: 0.02548059419109538
Epoch: 40 Idx: 0 Loss: 0.024104032757178637
Epoch: 40 Idx: 5000 Loss: 0.05246998621761414
Epoch: 41 Idx: 0 Loss: 0.013626379599155986
Epoch: 41 Idx: 5000 Loss: 0.02326736734041592
Epoch: 42 Idx: 0 Loss: 0.025411202648029423
Epoch: 42 Idx: 5000 Loss: 0.00946547193145773
Epoch: 43 Idx: 0 Loss: 0.013280971893077611
Epoch: 43 Idx: 5000 Loss: 0.022561367076802973
Epoch: 44 Idx: 0 Loss: 0.00979355515499378
Epoch: 44 Idx: 5000 Loss: 0.011252408808160856
Epoch: 45 Idx: 0 Loss: 0.05802760059674169
Epoch: 45 Idx: 5000 Loss: 0.017939727847878728
Epoch: 46 Idx: 0 Loss: 0.011254072946856401
Epoch: 46 Idx: 5000 Loss: 0.013029029520897129
Epoch: 47 Idx: 0 Loss: 0.01569351938748297
Epoch: 47 Idx: 5000 Loss: 0.018162094471076973
Epoch: 48 Idx: 0 Loss: 0.051244014415511754
Epoch: 48 Idx: 5000 Loss: 0.019137452808777055
Epoch: 49 Idx: 0 Loss: 0.03084475656813829
Epoch: 49 Idx: 5000 Loss: 0.013437104083918894
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.20480045275565292
Epoch: 0 Idx: 5000 Loss: 0.03561043573574253
Epoch: 1 Idx: 0 Loss: 0.018463377830558507
Epoch: 1 Idx: 5000 Loss: 0.029336904856922856
Epoch: 2 Idx: 0 Loss: 0.029616124320640702
Epoch: 2 Idx: 5000 Loss: 0.035404679740195136
Epoch: 3 Idx: 0 Loss: 0.02027863919584016
Epoch: 3 Idx: 5000 Loss: 0.023961059173001953
Epoch: 4 Idx: 0 Loss: 0.040067529272412195
Epoch: 4 Idx: 5000 Loss: 0.02646364217511289
Epoch: 5 Idx: 0 Loss: 0.01819041042867734
Epoch: 5 Idx: 5000 Loss: 0.019504944937150354
Epoch: 6 Idx: 0 Loss: 0.0246080060810736
Epoch: 6 Idx: 5000 Loss: 0.03493184539008559
Epoch: 7 Idx: 0 Loss: 0.009545528932980362
Epoch: 7 Idx: 5000 Loss: 0.011144808393448238
Epoch: 8 Idx: 0 Loss: 0.024611699151656737
Epoch: 8 Idx: 5000 Loss: 0.023408166289166935
Epoch: 9 Idx: 0 Loss: 0.027009062949395292
Epoch: 9 Idx: 5000 Loss: 0.02225889236745752
Epoch: 10 Idx: 0 Loss: 0.023136747441381854
Epoch: 10 Idx: 5000 Loss: 0.02549128281123536
Epoch: 11 Idx: 0 Loss: 0.0157333083026722
Epoch: 11 Idx: 5000 Loss: 0.028078211648621263
Epoch: 12 Idx: 0 Loss: 0.03285610771039838
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
26363
Epoch: 16 Idx: 5000 Loss: 0.014629511091333805
Epoch: 17 Idx: 0 Loss: 0.024151189019526002
Epoch: 17 Idx: 5000 Loss: 0.03217705345729251
Epoch: 18 Idx: 0 Loss: 0.03625324333872713
Epoch: 18 Idx: 5000 Loss: 0.025472775153571534
Epoch: 19 Idx: 0 Loss: 0.028593442378841507
Epoch: 19 Idx: 5000 Loss: 0.013439102388710315
Epoch: 20 Idx: 0 Loss: 0.013335891512010173
Epoch: 20 Idx: 5000 Loss: 0.014900348755779405
Epoch: 21 Idx: 0 Loss: 0.02344723244563982
Epoch: 21 Idx: 5000 Loss: 0.0071994840323683015
Epoch: 22 Idx: 0 Loss: 0.03506339221308017
Epoch: 22 Idx: 5000 Loss: 0.013205581580838973
Epoch: 23 Idx: 0 Loss: 0.013196559514290314
Epoch: 23 Idx: 5000 Loss: 0.023955884382760653
Epoch: 24 Idx: 0 Loss: 0.01621731954345817
Epoch: 24 Idx: 5000 Loss: 0.02123994136154847
Epoch: 25 Idx: 0 Loss: 0.019324206185056415
Epoch: 25 Idx: 5000 Loss: 0.02810598904621053
Epoch: 26 Idx: 0 Loss: 0.0124749085019039
Epoch: 26 Idx: 5000 Loss: 0.02530862614179169
Epoch: 27 Idx: 0 Loss: 0.046954348406531135
Epoch: 27 Idx: 5000 Loss: 0.030960273417358845
Epoch: 28 Idx: 0 Loss: 0.015793132879080244
Epoch: 28 Idx: 5000 Loss: 0.019826727018698018
Epoch: 29 Idx: 0 Loss: 0.03672431672570569
Epoch: 29 Idx: 5000 Loss: 0.01904711732539747
Epoch: 30 Idx: 0 Loss: 0.027509884468218768
Epoch: 30 Idx: 5000 Loss: 0.016376685487855012
Epoch: 31 Idx: 0 Loss: 0.02462435434456105
Epoch: 31 Idx: 5000 Loss: 0.02260362666650976
Epoch: 32 Idx: 0 Loss: 0.04415245790161913
Epoch: 32 Idx: 5000 Loss: 0.027016282493285325
Epoch: 33 Idx: 0 Loss: 0.019064967794406446
Epoch: 33 Idx: 5000 Loss: 0.01761951078151579
Epoch: 34 Idx: 0 Loss: 0.0335863372409123
Epoch: 34 Idx: 5000 Loss: 0.02703680641986904
Epoch: 35 Idx: 0 Loss: 0.0159189392586359
Epoch: 35 Idx: 5000 Loss: 0.02085275396135958
Epoch: 36 Idx: 0 Loss: 0.023425038756132485
Epoch: 36 Idx: 5000 Loss: 0.04578755061328413
Epoch: 37 Idx: 0 Loss: 0.0178525223811298
Epoch: 37 Idx: 5000 Loss: 0.031081358052959602
Epoch: 38 Idx: 0 Loss: 0.03198257352228753
Epoch: 38 Idx: 5000 Loss: 0.01173594584426699
Epoch: 39 Idx: 0 Loss: 0.04595944434326783
Epoch: 39 Idx: 5000 Loss: 0.013855438857333723
Epoch: 40 Idx: 0 Loss: 0.030778929516795707
Epoch: 40 Idx: 5000 Loss: 0.020047003928717864
Epoch: 41 Idx: 0 Loss: 0.03335972046679809
Epoch: 41 Idx: 5000 Loss: 0.03384248275231545
Epoch: 42 Idx: 0 Loss: 0.010844016483815116
Epoch: 42 Idx: 5000 Loss: 0.018568247450205452
Epoch: 43 Idx: 0 Loss: 0.015614834759963975
Epoch: 43 Idx: 5000 Loss: 0.014180414525832252
Epoch: 44 Idx: 0 Loss: 0.037797230344723876
Epoch: 44 Idx: 5000 Loss: 0.02016031422178339
Epoch: 45 Idx: 0 Loss: 0.02883379428520512
Epoch: 45 Idx: 5000 Loss: 0.010824612471401497
Epoch: 46 Idx: 0 Loss: 0.009841816368229249
Epoch: 46 Idx: 5000 Loss: 0.023433804955176836
Epoch: 47 Idx: 0 Loss: 0.0195455872625753
Epoch: 47 Idx: 5000 Loss: 0.02450425860527716
Epoch: 48 Idx: 0 Loss: 0.014380625735421565
Epoch: 48 Idx: 5000 Loss: 0.022610959282110484
Epoch: 49 Idx: 0 Loss: 0.018829234679565206
Epoch: 49 Idx: 5000 Loss: 0.04320398992923283
Len (direct inputs):  106
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2324991757419087
Epoch: 0 Idx: 5000 Loss: 0.017026189319907417
Epoch: 1 Idx: 0 Loss: 0.04250315378182749
Epoch: 1 Idx: 5000 Loss: 0.014062183201853606
Epoch: 2 Idx: 0 Loss: 0.02445174593093105
Epoch: 2 Idx: 5000 Loss: 0.03665060253079912
Epoch: 3 Idx: 0 Loss: 0.03485627125505167
Epoch: 3 Idx: 5000 Loss: 0.022578190646969358
Epoch: 4 Idx: 0 Loss: 0.01743218532469877
Epoch: 4 Idx: 5000 Loss: 0.02204150328751907
Epoch: 5 Idx: 0 Loss: 0.024166877123151524
Epoch: 5 Idx: 5000 Loss: 0.020830882680304967
Epoch: 6 Idx: 0 Loss: 0.011379388655433545
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
x: 5000 Loss: 0.024603367546329077
Epoch: 9 Idx: 0 Loss: 0.026730464041540315
Epoch: 9 Idx: 5000 Loss: 0.02403109302772705
Epoch: 10 Idx: 0 Loss: 0.022202811051252304
Epoch: 10 Idx: 5000 Loss: 0.024456622609351376
Epoch: 11 Idx: 0 Loss: 0.020087518695886326
Epoch: 11 Idx: 5000 Loss: 0.012123366863338269
Epoch: 12 Idx: 0 Loss: 0.04641432002989555
Epoch: 12 Idx: 5000 Loss: 0.024849098679560856
Epoch: 13 Idx: 0 Loss: 0.006387451357040363
Epoch: 13 Idx: 5000 Loss: 0.01554555368557385
Epoch: 14 Idx: 0 Loss: 0.01423058677635633
Epoch: 14 Idx: 5000 Loss: 0.03437777712752328
Epoch: 15 Idx: 0 Loss: 0.01963035909897672
Epoch: 15 Idx: 5000 Loss: 0.018212866389027076
Epoch: 16 Idx: 0 Loss: 0.024101755026499146
Epoch: 16 Idx: 5000 Loss: 0.011701525362162539
Epoch: 17 Idx: 0 Loss: 0.019014163980791858
Epoch: 17 Idx: 5000 Loss: 0.019527586941805463
Epoch: 18 Idx: 0 Loss: 0.01491568624050099
Epoch: 18 Idx: 5000 Loss: 0.02026500690892778
Epoch: 19 Idx: 0 Loss: 0.021122481597103625
Epoch: 19 Idx: 5000 Loss: 0.00784258913580985
Epoch: 20 Idx: 0 Loss: 0.023214360484027165
Epoch: 20 Idx: 5000 Loss: 0.04215694076021299
Epoch: 21 Idx: 0 Loss: 0.01848046317630765
Epoch: 21 Idx: 5000 Loss: 0.012820766154236336
Epoch: 22 Idx: 0 Loss: 0.022231541021987
Epoch: 22 Idx: 5000 Loss: 0.04150254064457204
Epoch: 23 Idx: 0 Loss: 0.0319782619142298
Epoch: 23 Idx: 5000 Loss: 0.012222287956989049
Epoch: 24 Idx: 0 Loss: 0.03065327241700873
Epoch: 24 Idx: 5000 Loss: 0.03314015439953191
Epoch: 25 Idx: 0 Loss: 0.0272886189701615
Epoch: 25 Idx: 5000 Loss: 0.018241562920991758
Epoch: 26 Idx: 0 Loss: 0.013362789435504981
Epoch: 26 Idx: 5000 Loss: 0.030422061775767447
Epoch: 27 Idx: 0 Loss: 0.030730984441844446
Epoch: 27 Idx: 5000 Loss: 0.01613189070960858
Epoch: 28 Idx: 0 Loss: 0.019700822883117888
Epoch: 28 Idx: 5000 Loss: 0.01938606811635046
Epoch: 29 Idx: 0 Loss: 0.018397238750066377
Epoch: 29 Idx: 5000 Loss: 0.006977841816303362
Epoch: 30 Idx: 0 Loss: 0.01750630589257647
Epoch: 30 Idx: 5000 Loss: 0.029114642880430988
Epoch: 31 Idx: 0 Loss: 0.015968495579042872
Epoch: 31 Idx: 5000 Loss: 0.023834423826270656
Epoch: 32 Idx: 0 Loss: 0.01960730730174506
Epoch: 32 Idx: 5000 Loss: 0.054431537350197234
Epoch: 33 Idx: 0 Loss: 0.016298860128100882
Epoch: 33 Idx: 5000 Loss: 0.0324682723110592
Epoch: 34 Idx: 0 Loss: 0.03338558357108026
Epoch: 34 Idx: 5000 Loss: 0.0379581417972291
Epoch: 35 Idx: 0 Loss: 0.011850610290118281
Epoch: 35 Idx: 5000 Loss: 0.012089204905196368
Epoch: 36 Idx: 0 Loss: 0.016297327599788886
Epoch: 36 Idx: 5000 Loss: 0.024427580595268387
Epoch: 37 Idx: 0 Loss: 0.023459444053525274
Epoch: 37 Idx: 5000 Loss: 0.017795140527530343
Epoch: 38 Idx: 0 Loss: 0.007343866189726928
Epoch: 38 Idx: 5000 Loss: 0.009957970154479096
Epoch: 39 Idx: 0 Loss: 0.015439479858085447
Epoch: 39 Idx: 5000 Loss: 0.021293660284712947
Epoch: 40 Idx: 0 Loss: 0.031456006810401185
Epoch: 40 Idx: 5000 Loss: 0.02495338714666876
Epoch: 41 Idx: 0 Loss: 0.011396597883157776
Epoch: 41 Idx: 5000 Loss: 0.012518627035882732
Epoch: 42 Idx: 0 Loss: 0.018231147951442126
Epoch: 42 Idx: 5000 Loss: 0.020124995905957413
Epoch: 43 Idx: 0 Loss: 0.018886206412358122
Epoch: 43 Idx: 5000 Loss: 0.01540457467376001
Epoch: 44 Idx: 0 Loss: 0.022758966264136003
Epoch: 44 Idx: 5000 Loss: 0.030673769071679808
Epoch: 45 Idx: 0 Loss: 0.04827089582151692
Epoch: 45 Idx: 5000 Loss: 0.03153875179477023
Epoch: 46 Idx: 0 Loss: 0.022838611782999636
Epoch: 46 Idx: 5000 Loss: 0.01958676921445318
Epoch: 47 Idx: 0 Loss: 0.0288201894765557
Epoch: 47 Idx: 5000 Loss: 0.040446941827323495
Epoch: 48 Idx: 0 Loss: 0.02205179584656071
Epoch: 48 Idx: 5000 Loss: 0.012872738460778197
Epoch: 49 Idx: 0 Loss: 0.01158225366057439
Epoch: 49 Idx: 5000 Loss: 0.030582486223424125
Len (direct inputs):  89
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.25720769021698386
Epoch: 0 Idx: 5000 Loss: 0.03332896298012983
Epoch: 1 Idx: 0 Loss: 0.0365336091463582
Epoch: 1 Idx: 5000 Loss: 0.010180748339887738
Epoch: 2 Idx: 0 Loss: 0.029243349169228085
Epoch: 2 Idx: 5000 Loss: 0.024854506703107366
Epoch: 3 Idx: 0 Loss: 0.016280948859101335
Epoch: 3 Idx: 5000 Loss: 0.022413345029642255
Epoch: 4 Idx: 0 Loss: 0.02419523029159735
Epoch: 4 Idx: 5000 Loss: 0.022564622898507247
Epoch: 5 Idx: 0 Loss: 0.021545464902710337
Epoch: 5 Idx: 5000 Loss: 0.010226369165028122
Epoch: 6 Idx: 0 Loss: 0.015585389823074367
Epoch: 6 Idx: 5000 Loss: 0.020036221232068996
Epoch: 7 Idx: 0 Loss: 0.013336427341148505
Epoch: 7 Idx: 5000 Loss: 0.009120351208256758
Epoch: 8 Idx: 0 Loss: 0.03223924929792936
Epoch: 8 Idx: 5000 Loss: 0.021406091243496593
Epoch: 9 Idx: 0 Loss: 0.030534697048781777
Epoch: 9 Idx: 5000 Loss: 0.013222262813387857
Epoch: 10 Idx: 0 Loss: 0.0073519811563511955
Epoch: 10 Idx: 5000 Loss: 0.024379112439708115
Epoch: 11 Idx: 0 Loss: 0.02418187223041777
Epoch: 11 Idx: 5000 Loss: 0.03485078923949769
Epoch: 12 Idx: 0 Loss: 0.021713629155611217
Epoch: 12 Idx: 5000 Loss: 0.02450121545831845
Epoch: 13 Idx: 0 Loss: 0.03243448910720441
Epoch: 13 Idx: 5000 Loss: 0.010591282899835184
Epoch: 14 Idx: 0 Loss: 0.03285095179498076
Epoch: 14 Idx: 5000 Loss: 0.026993466741436246
Epoch: 15 Idx: 0 Loss: 0.02334457591642746
Epoch: 15 Idx: 5000 Loss: 0.01556689291810329
Epoch: 16 Idx: 0 Loss: 0.027175819376940864
Epoch: 16 Idx: 5000 Loss: 0.021402708914648412
Epoch: 17 Idx: 0 Loss: 0.03385457072222229
Epoch: 17 Idx: 5000 Loss: 0.023301486590844984
Epoch: 18 Idx: 0 Loss: 0.022172776312144052
Epoch: 18 Idx: 5000 Loss: 0.014855640815081663
Epoch: 19 Idx: 0 Loss: 0.011602608489976345
Epoch: 19 Idx: 5000 Loss: 0.022996529801414603
Epoch: 20 Idx: 0 Loss: 0.0059453804123311155
Epoch: 20 Idx: 5000 Loss: 0.008033840585505513
Epoch: 21 Idx: 0 Loss: 0.02935983742083741
Epoch: 21 Idx: 5000 Loss: 0.022281260093881365
Epoch: 22 Idx: 0 Loss: 0.03176099863392529
Epoch: 22 Idx: 5000 Loss: 0.020584008467018973
Epoch: 23 Idx: 0 Loss: 0.015226543514637841
Epoch: 23 Idx: 5000 Loss: 0.019577607972399422
Epoch: 24 Idx: 0 Loss: 0.019402023255132905
Epoch: 24 Idx: 5000 Loss: 0.024727623171298875
Epoch: 25 Idx: 0 Loss: 0.016653051368533945
Epoch: 25 Idx: 5000 Loss: 0.022065144993062114
Epoch: 26 Idx: 0 Loss: 0.014650016874491575
Epoch: 26 Idx: 5000 Loss: 0.03096760280586049
Epoch: 27 Idx: 0 Loss: 0.015631133979796773
Epoch: 27 Idx: 5000 Loss: 0.026132410839385264
Epoch: 28 Idx: 0 Loss: 0.019033531049201403
Epoch: 28 Idx: 5000 Loss: 0.018512027607239356
Epoch: 29 Idx: 0 Loss: 0.015549263741689573
Epoch: 29 Idx: 5000 Loss: 0.01778802968516088
Epoch: 30 Idx: 0 Loss: 0.013929430677688624
Epoch: 30 Idx: 5000 Loss: 0.01974030334025663
Epoch: 31 Idx: 0 Loss: 0.018483719572225332
Epoch: 31 Idx: 5000 Loss: 0.01761437047051867
Epoch: 32 Idx: 0 Loss: 0.012639879203238294
Epoch: 32 Idx: 5000 Loss: 0.01849979455964367
Epoch: 33 Idx: 0 Loss: 0.03062823972525285
Epoch: 33 Idx: 5000 Loss: 0.029013701120344133
Epoch: 34 Idx: 0 Loss: 0.01675129425902245
Epoch: 34 Idx: 5000 Loss: 0.018283158100892555
Epoch: 35 Idx: 0 Loss: 0.016428917747368307
Epoch: 35 Idx: 5000 Loss: 0.03175233967598266
Epoch: 36 Idx: 0 Loss: 0.022116061634600122
Epoch: 36 Idx: 5000 Loss: 0.015695199211672624
Epoch: 37 Idx: 0 Loss: 0.012222646445601787
Epoch: 37 Idx: 5000 Loss: 0.022205975775858658
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 313, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml_weighted.py", line 312, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
Idx: 5000 Loss: 0.012561416128398914
Epoch: 48 Idx: 0 Loss: 0.016626580560773117
Epoch: 48 Idx: 5000 Loss: 0.017140488233208774
Epoch: 49 Idx: 0 Loss: 0.022667752809585873
Epoch: 49 Idx: 5000 Loss: 0.018150641928381818
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18329800388439405
Epoch: 0 Idx: 5000 Loss: 0.00979973375769766
Epoch: 1 Idx: 0 Loss: 0.03840450736792627
Epoch: 1 Idx: 5000 Loss: 0.015807237125280517
Epoch: 2 Idx: 0 Loss: 0.026369790811686042
Epoch: 2 Idx: 5000 Loss: 0.03115257332868427
Epoch: 3 Idx: 0 Loss: 0.020360355475031955
Epoch: 3 Idx: 5000 Loss: 0.010956634958462736
Epoch: 4 Idx: 0 Loss: 0.01789628852971258
Epoch: 4 Idx: 5000 Loss: 0.018488808962227897
Epoch: 5 Idx: 0 Loss: 0.02026818442657292
Epoch: 5 Idx: 5000 Loss: 0.017465467771692136
Epoch: 6 Idx: 0 Loss: 0.046918452982907546
Epoch: 6 Idx: 5000 Loss: 0.024086014969347532
Epoch: 7 Idx: 0 Loss: 0.022806291760805758
Epoch: 7 Idx: 5000 Loss: 0.022609245882381314
Epoch: 8 Idx: 0 Loss: 0.018937118613318364
Epoch: 8 Idx: 5000 Loss: 0.041414724691509655
Epoch: 9 Idx: 0 Loss: 0.017205381491017212
Epoch: 9 Idx: 5000 Loss: 0.022363585434797904
Epoch: 10 Idx: 0 Loss: 0.013622189107040859
Epoch: 10 Idx: 5000 Loss: 0.045269040095634365
Epoch: 11 Idx: 0 Loss: 0.014927606046648181
Epoch: 11 Idx: 5000 Loss: 0.022072703246071183
Epoch: 12 Idx: 0 Loss: 0.036637947777549515
Epoch: 12 Idx: 5000 Loss: 0.01077708438665068
Epoch: 13 Idx: 0 Loss: 0.02534635447508369
Epoch: 13 Idx: 5000 Loss: 0.015102117686439794
Epoch: 14 Idx: 0 Loss: 0.023927624170594686
Epoch: 14 Idx: 5000 Loss: 0.018581317806799162
Epoch: 15 Idx: 0 Loss: 0.021828872114355788
Epoch: 15 Idx: 5000 Loss: 0.020409022508777745
Epoch: 16 Idx: 0 Loss: 0.013333997104062757
Epoch: 16 Idx: 5000 Loss: 0.01904210241629214
Epoch: 17 Idx: 0 Loss: 0.014603787895284635
Epoch: 17 Idx: 5000 Loss: 0.025433214114779112
Epoch: 18 Idx: 0 Loss: 0.01223474296976925
Epoch: 18 Idx: 5000 Loss: 0.02195030489640646
Epoch: 19 Idx: 0 Loss: 0.010290410896898482
Epoch: 19 Idx: 5000 Loss: 0.009034131031471057
Epoch: 20 Idx: 0 Loss: 0.01759548988777263
Epoch: 20 Idx: 5000 Loss: 0.015515315403344115
Epoch: 21 Idx: 0 Loss: 0.015422155201229238
Epoch: 21 Idx: 5000 Loss: 0.021000336111980298
Epoch: 22 Idx: 0 Loss: 0.014606553049218856
Epoch: 22 Idx: 5000 Loss: 0.011640640947150512
Epoch: 23 Idx: 0 Loss: 0.03575986382263879
Epoch: 23 Idx: 5000 Loss: 0.02473039835042659
Epoch: 24 Idx: 0 Loss: 0.01443332926526451
Epoch: 24 Idx: 5000 Loss: 0.022636203468087052
Epoch: 25 Idx: 0 Loss: 0.0626232072087282
Epoch: 25 Idx: 5000 Loss: 0.03367032513005584
Epoch: 26 Idx: 0 Loss: 0.015935896096076558
Epoch: 26 Idx: 5000 Loss: 0.022210372994783678
Epoch: 27 Idx: 0 Loss: 0.009857519781015707
Epoch: 27 Idx: 5000 Loss: 0.011601896769670557
Epoch: 28 Idx: 0 Loss: 0.018645666469701595
Epoch: 28 Idx: 5000 Loss: 0.023894441209663345
Epoch: 29 Idx: 0 Loss: 0.014869024637647749
Epoch: 29 Idx: 5000 Loss: 0.015791669081825402
Epoch: 30 Idx: 0 Loss: 0.02221421114415945
Epoch: 30 Idx: 5000 Loss: 0.023683642769440526
Epoch: 31 Idx: 0 Loss: 0.020209937213030084
Epoch: 31 Idx: 5000 Loss: 0.03156306064173638
Epoch: 32 Idx: 0 Loss: 0.02279721198135294
Epoch: 32 Idx: 5000 Loss: 0.023606270585614764
Epoch: 33 Idx: 0 Loss: 0.013734322914066377
Epoch: 33 Idx: 5000 Loss: 0.02525307724107584
Epoch: 34 Idx: 0 Loss: 0.012286385086843918
Epoch: 34 Idx: 5000 Loss: 0.01973241882586324
Epoch: 35 Idx: 0 Loss: 0.029118621858075344
Epoch: 35 Idx: 5000 Loss: 0.022186505870906856
Epoch: 36 Idx: 0 Loss: 0.0161663328778369
Epoch: 36 Idx: 5000 Loss: 0.015745858004576563
Epoch: 37 Idx: 0 Loss: 0.018365396919843922
Epoch: 37 Idx: 5000 Loss: 0.01621673444811778
Epoch: 38 Idx: 0 Loss: 0.016406340219518128
Epoch: 38 Idx: 5000 Loss: 0.028237992673664093
Epoch: 39 Idx: 0 Loss: 0.019113096002710987
Epoch: 39 Idx: 5000 Loss: 0.026465002437693078
Epoch: 40 Idx: 0 Loss: 0.015663971687905256
Epoch: 40 Idx: 5000 Loss: 0.02377371256277283
Epoch: 41 Idx: 0 Loss: 0.015371993043578816
Epoch: 41 Idx: 5000 Loss: 0.029510042821107023
Epoch: 42 Idx: 0 Loss: 0.019179685462034177
Epoch: 42 Idx: 5000 Loss: 0.02447598251083499
Epoch: 43 Idx: 0 Loss: 0.025189193307970117
Epoch: 43 Idx: 5000 Loss: 0.012182641399478597
Epoch: 44 Idx: 0 Loss: 0.010877835733433573
Epoch: 44 Idx: 5000 Loss: 0.017752046748465592
Epoch: 45 Idx: 0 Loss: 0.01497405517571049
Epoch: 45 Idx: 5000 Loss: 0.028561413699076593
Epoch: 46 Idx: 0 Loss: 0.016338151692189135
Epoch: 46 Idx: 5000 Loss: 0.03615467690246793
Epoch: 47 Idx: 0 Loss: 0.02115612262976263
Epoch: 47 Idx: 5000 Loss: 0.011623350502459627
Epoch: 48 Idx: 0 Loss: 0.0182051715481427
Epoch: 48 Idx: 5000 Loss: 0.03909947516579974
Epoch: 49 Idx: 0 Loss: 0.015581509179593401
Epoch: 49 Idx: 5000 Loss: 0.03571928597566776
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zeTraining size: 12750Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19916504105304061
Epoch: 0 Idx: 5000 Loss: 0.03434615855551222
Epoch: 1 Idx: 0 Loss: 0.025976313777316316
Epoch: 1 Idx: 5000 Loss: 0.03520236954267552
Epoch: 2 Idx: 0 Loss: 0.025773609903624876
Epoch: 2 Idx: 5000 Loss: 0.03417306832560301
Epoch: 3 Idx: 0 Loss: 0.041157072167969155
Epoch: 3 Idx: 5000 Loss: 0.017675495308691738
Epoch: 4 Idx: 0 Loss: 0.02715847478822732
Epoch: 4 Idx: 5000 Loss: 0.01452972122119213
Epoch: 5 Idx: 0 Loss: 0.0276375623958985
Epoch: 5 Idx: 5000 Loss: 0.023937104901813604
Epoch: 6 Idx: 0 Loss: 0.02652885156282745
Epoch: 6 Idx: 5000 Loss: 0.023523060288936068
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 312, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
odule>
    loss.backward()
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
9 Idx: 5000 Loss: 0.03594890537680517
Epoch: 20 Idx: 0 Loss: 0.028782909718455957
Epoch: 20 Idx: 5000 Loss: 0.02462798461283749
Epoch: 21 Idx: 0 Loss: 0.02114460917530285
Epoch: 21 Idx: 5000 Loss: 0.014227011923614166
Epoch: 22 Idx: 0 Loss: 0.01498718012838747
Epoch: 22 Idx: 5000 Loss: 0.04272824473519543
Epoch: 23 Idx: 0 Loss: 0.01332109180511671
Epoch: 23 Idx: 5000 Loss: 0.015369390296639456
Epoch: 24 Idx: 0 Loss: 0.026273121813530594
Epoch: 24 Idx: 5000 Loss: 0.017853441036966844
Epoch: 25 Idx: 0 Loss: 0.01875218758396078
Epoch: 25 Idx: 5000 Loss: 0.011280557883434333
Epoch: 26 Idx: 0 Loss: 0.012549752144183683
Epoch: 26 Idx: 5000 Loss: 0.01759219709913152
Epoch: 27 Idx: 0 Loss: 0.024231836619950868
Epoch: 27 Idx: 5000 Loss: 0.021733399077769375
Epoch: 28 Idx: 0 Loss: 0.04354212853389557
Epoch: 28 Idx: 5000 Loss: 0.04674705077136134
Epoch: 29 Idx: 0 Loss: 0.0071058308843738646
Epoch: 29 Idx: 5000 Loss: 0.019973441431587198
Epoch: 30 Idx: 0 Loss: 0.010340315350734031
Epoch: 30 Idx: 5000 Loss: 0.020070205459603127
Epoch: 31 Idx: 0 Loss: 0.022939911057450198
Epoch: 31 Idx: 5000 Loss: 0.01641776233488572
Epoch: 32 Idx: 0 Loss: 0.03872747196035454
Epoch: 32 Idx: 5000 Loss: 0.01993598428709495
Epoch: 33 Idx: 0 Loss: 0.009554728327255213
Epoch: 33 Idx: 5000 Loss: 0.014358745289307594
Epoch: 34 Idx: 0 Loss: 0.02032778314929482
Epoch: 34 Idx: 5000 Loss: 0.009492454057628514
Epoch: 35 Idx: 0 Loss: 0.02283852083910557
Epoch: 35 Idx: 5000 Loss: 0.03203387915973004
Epoch: 36 Idx: 0 Loss: 0.02031056782333211
Epoch: 36 Idx: 5000 Loss: 0.016705456326229627
Epoch: 37 Idx: 0 Loss: 0.02477149242690539
Epoch: 37 Idx: 5000 Loss: 0.023794328997975775
Epoch: 38 Idx: 0 Loss: 0.02052028763755028
Epoch: 38 Idx: 5000 Loss: 0.012553870895826475
Epoch: 39 Idx: 0 Loss: 0.01797479564908882
Epoch: 39 Idx: 5000 Loss: 0.03381731043540643
Epoch: 40 Idx: 0 Loss: 0.02201924953486159
Epoch: 40 Idx: 5000 Loss: 0.020143705152840682
Epoch: 41 Idx: 0 Loss: 0.04411638147450901
Epoch: 41 Idx: 5000 Loss: 0.020834197136631184
Epoch: 42 Idx: 0 Loss: 0.01692040287514735
Epoch: 42 Idx: 5000 Loss: 0.012073153618842678
Epoch: 43 Idx: 0 Loss: 0.015045609787502079
Epoch: 43 Idx: 5000 Loss: 0.015194643702597547
Epoch: 44 Idx: 0 Loss: 0.019122711533823528
Epoch: 44 Idx: 5000 Loss: 0.013188585991669478
Epoch: 45 Idx: 0 Loss: 0.019294493642339955
Epoch: 45 Idx: 5000 Loss: 0.020008738812759733
Epoch: 46 Idx: 0 Loss: 0.021131603042960072
Epoch: 46 Idx: 5000 Loss: 0.036720935419988596
Epoch: 47 Idx: 0 Loss: 0.023036445301134596
Epoch: 47 Idx: 5000 Loss: 0.024029316377763787
Epoch: 48 Idx: 0 Loss: 0.03324882055539259
Epoch: 48 Idx: 5000 Loss: 0.03375928166852621
Epoch: 49 Idx: 0 Loss: 0.01560376466214502
Epoch: 49 Idx: 5000 Loss: 0.021226074254359087
Len (direct inputs):  95
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
0.8627920251440302
Parameter containing:
tensor([0.8628], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.25679366093544637
Epoch: 0 Idx: 5000 Loss: 0.022882521828486463
Epoch: 1 Idx: 0 Loss: 0.0173303236446678
Epoch: 1 Idx: 5000 Loss: 0.025522474672610942
Epoch: 2 Idx: 0 Loss: 0.015347230432069524
Epoch: 2 Idx: 5000 Loss: 0.02396227120471789
Epoch: 3 Idx: 0 Loss: 0.02432379451885386
Epoch: 3 Idx: 5000 Loss: 0.01650599728676348
Epoch: 4 Idx: 0 Loss: 0.01325139636128203
Epoch: 4 Idx: 5000 Loss: 0.03556916314272311
Epoch: 5 Idx: 0 Loss: 0.007012855815433089
Epoch: 5 Idx: 5000 Loss: 0.020021767873158864
Epoch: 6 Idx: 0 Loss: 0.03680670601324706
Epoch: 6 Idx: 5000 Loss: 0.020353862907867698
Epoch: 7 Idx: 0 Loss: 0.016851365589106715
Epoch: 7 Idx: 5000 Loss: 0.028078817167318935
Epoch: 8 Idx: 0 Loss: 0.01562782566588039
Epoch: 8 Idx: 5000 Loss: 0.014517845945951503
Epoch: 9 Idx: 0 Loss: 0.03757130456448961
Epoch: 9 Idx: 5000 Loss: 0.027109097991579796
Epoch: 10 Idx: 0 Loss: 0.026998859619896568
Epoch: 10 Idx: 5000 Loss: 0.01476889380022297
Epoch: 11 Idx: 0 Loss: 0.014923503070276338
Epoch: 11 Idx: 5000 Loss: 0.010835126838091375
Epoch: 12 Idx: 0 Loss: 0.016008598171792638
Epoch: 12 Idx: 5000 Loss: 0.05739278649022946
Epoch: 13 Idx: 0 Loss: 0.0060316230473520995
Epoch: 13 Idx: 5000 Loss: 0.040615955403633504
Epoch: 14 Idx: 0 Loss: 0.03179607114065207
Epoch: 14 Idx: 5000 Loss: 0.017585501405361623
Epoch: 15 Idx: 0 Loss: 0.02409369535096908
Epoch: 15 Idx: 5000 Loss: 0.027817415326763646
Epoch: 16 Idx: 0 Loss: 0.04518972310149693
Epoch: 16 Idx: 5000 Loss: 0.03104024692706702
Epoch: 17 Idx: 0 Loss: 0.012986044733896068
Epoch: 17 Idx: 5000 Loss: 0.03115403308442468
Epoch: 18 Idx: 0 Loss: 0.01582336725589152
Epoch: 18 Idx: 5000 Loss: 0.019095274231178506
Epoch: 19 Idx: 0 Loss: 0.02215089628661564
Epoch: 19 Idx: 5000 Loss: 0.02451291536403675
Epoch: 20 Idx: 0 Loss: 0.044651449329671314
Epoch: 20 Idx: 5000 Loss: 0.01622878050884327
Epoch: 21 Idx: 0 Loss: 0.0140931675751608
Epoch: 21 Idx: 5000 Loss: 0.026551680770766754
Epoch: 22 Idx: 0 Loss: 0.018784812092960984
Epoch: 22 Idx: 5000 Loss: 0.019014116294613904
Epoch: 23 Idx: 0 Loss: 0.018309150355797096
Epoch: 23 Idx: 5000 Loss: 0.03208670755065815
Epoch: 24 Idx: 0 Loss: 0.03777164963858337
Epoch: 24 Idx: 5000 Loss: 0.040467627277511076
Epoch: 25 Idx: 0 Loss: 0.015703993981755426
Epoch: 25 Idx: 5000 Loss: 0.027515371690493888
Epoch: 26 Idx: 0 Loss: 0.01853028928821361
Epoch: 26 Idx: 5000 Loss: 0.027346105915579146
Epoch: 27 Idx: 0 Loss: 0.017598543911529894
Epoch: 27 Idx: 5000 Loss: 0.005701136975096119
Epoch: 28 Idx: 0 Loss: 0.032713531266510644
Epoch: 28 Idx: 5000 Loss: 0.028918744332149148
Epoch: 29 Idx: 0 Loss: 0.042529855029918906
Epoch: 29 Idx: 5000 Loss: 0.022777067480460197
Epoch: 30 Idx: 0 Loss: 0.01639591031989992
Epoch: 30 Idx: 5000 Loss: 0.021921589120107395
Epoch: 31 Idx: 0 Loss: 0.02088735257373759
Epoch: 31 Idx: 5000 Loss: 0.027035512107430222
Epoch: 32 Idx: 0 Loss: 0.026401712579428298
Epoch: 32 Idx: 5000 Loss: 0.02297709518452897
Epoch: 33 Idx: 0 Loss: 0.02231629536096027
Epoch: 33 Idx: 5000 Loss: 0.016801635189917792
Epoch: 34 Idx: 0 Loss: 0.014846818293131864
Epoch: 34 Idx: 5000 Loss: 0.013375247744046123
Epoch: 35 Idx: 0 Loss: 0.01425581866466805
Epoch: 35 Idx: 5000 Loss: 0.022628707217374554
Epoch: 36 Idx: 0 Loss: 0.022038001748619706
Epoch: 36 Idx: 5000 Loss: 0.012892678241781346
Epoch: 37 Idx: 0 Loss: 0.01271085560911918
Epoch: 37 Idx: 5000 Loss: 0.040523467285731
Epoch: 38 Idx: 0 Loss: 0.02265136815362848
Epoch: 38 Idx: 5000 Loss: 0.019316465397097186
Epoch: 39 Idx: 0 Loss: 0.029282063527236838
Epoch: 39 Idx: 5000 Loss: 0.011171745593577444
Epoch: 40 Idx: 0 Loss: 0.02026152636200264
Epoch: 40 Idx: 5000 Loss: 0.01683133280568386
Epoch: 41 Idx: 0 Loss: 0.01896629031701673
Epoch: 41 Idx: 5000 Loss: 0.02125227088814892
Epoch: 42 Idx: 0 Loss: 0.015313023268284326
Epoch: 42 Idx: 5000 Loss: 0.01864471065733185
Epoch: 43 Idx: 0 Loss: 0.019608794117453685
Epoch: 43 Idx: 5000 Loss: 0.029894194267291334
Epoch: 44 Idx: 0 Loss: 0.014272396984367587
Epoch: 44 Idx: 5000 Loss: 0.020332369985640895
Epoch: 45 Idx: 0 Loss: 0.01968748330879997
Epoch: 45 Idx: 5000 Loss: 0.01339471509224038
Epoch: 46 Idx: 0 Loss: 0.03690765068807597
Epoch: 46 Idx: 5000 Loss: 0.03423594010337318
Epoch: 47 Idx: 0 Loss: 0.011005087560753574
Epoch: 47 Idx: 5000 Loss: 0.02642225006436891
Epoch: 48 Idx: 0 Loss: 0.019330583359570407
Epoch: 48 Idx: 5000 Loss: 0.02284029987089988
Epoch: 49 Idx: 0 Loss: 0.021780034357072375
Epoch: 49 Idx: 5000 Loss: 0.03302747631679952
Len (direct inputs):  642
Final Results: [0.96456086 0.83858004 0.89716947 0.8610729  0.93642483]
Threshold:  0.8627920251440302
ict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 24 failed with Disk quota exceeded
k quota exceeded

ded
 quota exceeded
-----------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 3289916: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs_wtpath2_5.pkl Models/anatomy_aml_bagofnbrs_wtpath2_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs_wtpath2_5.pkl Models/anatomy_aml_bagofnbrs_wtpath2_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:57:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:57:45 2020
Terminated at Tue Sep  1 21:20:50 2020
Results reported at Tue Sep  1 21:20:50 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs_wtpath2_5.pkl Models/anatomy_aml_bagofnbrs_wtpath2_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26584.28 sec.
    Max Memory :                                 2933 MB
    Average Memory :                             2719.80 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40484.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   26585 sec.
    Turnaround time :                            26818 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc227>
Subject: Job 3289914: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 5 Output/test_anatomy_aml_bagofnbrs_wtpath1_5.pkl Models/anatomy_aml_bagofnbrs_wtpath1_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 5 Output/test_anatomy_aml_bagofnbrs_wtpath1_5.pkl Models/anatomy_aml_bagofnbrs_wtpath1_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc227>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Tue Sep  1 21:27:34 2020
Results reported at Tue Sep  1 21:27:34 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 5 Output/test_anatomy_aml_bagofnbrs_wtpath1_5.pkl Models/anatomy_aml_bagofnbrs_wtpath1_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27038.22 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2726.28 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27074 sec.
    Turnaround time :                            27222 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc235>
Subject: Job 3289918: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs_wtpath3_5.pkl Models/anatomy_aml_bagofnbrs_wtpath3_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs_wtpath3_5.pkl Models/anatomy_aml_bagofnbrs_wtpath3_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc235>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:57:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:57:45 2020
Terminated at Tue Sep  1 21:45:21 2020
Results reported at Tue Sep  1 21:45:21 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs_wtpath3_5.pkl Models/anatomy_aml_bagofnbrs_wtpath3_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28053.75 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2713.43 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28056 sec.
    Turnaround time :                            28288 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc235>
Subject: Job 3289920: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs_wtpath4_5.pkl Models/anatomy_aml_bagofnbrs_wtpath4_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs_wtpath4_5.pkl Models/anatomy_aml_bagofnbrs_wtpath4_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc235>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:58:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:58:45 2020
Terminated at Tue Sep  1 22:25:22 2020
Results reported at Tue Sep  1 22:25:22 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs_wtpath4_5.pkl Models/anatomy_aml_bagofnbrs_wtpath4_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30358.49 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2727.73 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30397 sec.
    Turnaround time :                            30689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc252>
Subject: Job 3289922: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs_wtpath5_5.pkl Models/anatomy_aml_bagofnbrs_wtpath5_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs_wtpath5_5.pkl Models/anatomy_aml_bagofnbrs_wtpath5_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc252>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:02:08 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:02:08 2020
Terminated at Tue Sep  1 23:00:58 2020
Results reported at Tue Sep  1 23:00:58 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs_wtpath5_5.pkl Models/anatomy_aml_bagofnbrs_wtpath5_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32265.50 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2718.70 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32330 sec.
    Turnaround time :                            32825 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc250>
Subject: Job 3289924: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs_wtpath7_5.pkl Models/anatomy_aml_bagofnbrs_wtpath7_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs_wtpath7_5.pkl Models/anatomy_aml_bagofnbrs_wtpath7_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc250>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:05:19 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:05:19 2020
Terminated at Tue Sep  1 23:59:52 2020
Results reported at Tue Sep  1 23:59:52 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs_wtpath7_5.pkl Models/anatomy_aml_bagofnbrs_wtpath7_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   35668.19 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2714.62 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   35673 sec.
    Turnaround time :                            36359 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc239>
Subject: Job 3289942: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 5 Output/test_anatomy_aml_bagofnbrs_wtpath26_5.pkl Models/anatomy_aml_bagofnbrs_wtpath26_5.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 5 Output/test_anatomy_aml_bagofnbrs_wtpath26_5.pkl Models/anatomy_aml_bagofnbrs_wtpath26_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc239>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:56:59 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:56:59 2020
Terminated at Wed Sep  2 11:25:05 2020
Results reported at Wed Sep  2 11:25:05 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 5 Output/test_anatomy_aml_bagofnbrs_wtpath26_5.pkl Models/anatomy_aml_bagofnbrs_wtpath26_5.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   73675.41 sec.
    Max Memory :                                 2933 MB
    Average Memory :                             2710.39 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40484.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   73686 sec.
    Turnaround time :                            77471 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc246>
Subject: Job 3289946: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 5 Output/test_anatomy_aml_bagofnbrs_wtpath32_5.pkl Models/anatomy_aml_bagofnbrs_wtpath32_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 5 Output/test_anatomy_aml_bagofnbrs_wtpath32_5.pkl Models/anatomy_aml_bagofnbrs_wtpath32_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc246>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:41:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:41:45 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 5 Output/test_anatomy_aml_bagofnbrs_wtpath32_5.pkl Models/anatomy_aml_bagofnbrs_wtpath32_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   59812.15 sec.
    Max Memory :                                 2741 MB
    Average Memory :                             2671.58 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40676.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   59833 sec.
    Turnaround time :                            80694 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289950: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 5 Output/test_anatomy_aml_bagofnbrs_wtpath80_5.pkl Models/anatomy_aml_bagofnbrs_wtpath80_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 5 Output/test_anatomy_aml_bagofnbrs_wtpath80_5.pkl Models/anatomy_aml_bagofnbrs_wtpath80_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 20:00:42 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 20:00:42 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 5 Output/test_anatomy_aml_bagofnbrs_wtpath80_5.pkl Models/anatomy_aml_bagofnbrs_wtpath80_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   58686.36 sec.
    Max Memory :                                 2728 MB
    Average Memory :                             2621.06 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40689.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   58687 sec.
    Turnaround time :                            80695 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289944: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 5 Output/test_anatomy_aml_bagofnbrs_wtpath30_5.pkl Models/anatomy_aml_bagofnbrs_wtpath30_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 5 Output/test_anatomy_aml_bagofnbrs_wtpath30_5.pkl Models/anatomy_aml_bagofnbrs_wtpath30_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:35:33 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:35:33 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 5 Output/test_anatomy_aml_bagofnbrs_wtpath30_5.pkl Models/anatomy_aml_bagofnbrs_wtpath30_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   60101.00 sec.
    Max Memory :                                 2738 MB
    Average Memory :                             2670.61 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40679.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   60196 sec.
    Turnaround time :                            80695 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc276>
Subject: Job 3289952: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 5 Output/test_anatomy_aml_bagofnbrs_wtpath152_5.pkl Models/anatomy_aml_bagofnbrs_wtpath152_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 5 Output/test_anatomy_aml_bagofnbrs_wtpath152_5.pkl Models/anatomy_aml_bagofnbrs_wtpath152_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc276>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 20:05:57 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 20:05:57 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 5 Output/test_anatomy_aml_bagofnbrs_wtpath152_5.pkl Models/anatomy_aml_bagofnbrs_wtpath152_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   58190.39 sec.
    Max Memory :                                 2726 MB
    Average Memory :                             2604.99 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40691.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   58372 sec.
    Turnaround time :                            80695 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc265>
Subject: Job 3289948: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 5 Output/test_anatomy_aml_bagofnbrs_wtpath40_5.pkl Models/anatomy_aml_bagofnbrs_wtpath40_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 5 Output/test_anatomy_aml_bagofnbrs_wtpath40_5.pkl Models/anatomy_aml_bagofnbrs_wtpath40_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc265>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:51:06 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:51:06 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 5 Output/test_anatomy_aml_bagofnbrs_wtpath40_5.pkl Models/anatomy_aml_bagofnbrs_wtpath40_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   59214.62 sec.
    Max Memory :                                 2733 MB
    Average Memory :                             2659.94 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40684.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   59263 sec.
    Turnaround time :                            80695 sec.

The output (if any) is above this job summary.

