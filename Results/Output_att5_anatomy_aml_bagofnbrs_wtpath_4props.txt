Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22328219301899588
Epoch: 0 Idx: 5000 Loss: 0.046246047931630514
Epoch: 1 Idx: 0 Loss: 0.012331593373883135
Epoch: 1 Idx: 5000 Loss: 0.014258032648397784
Epoch: 2 Idx: 0 Loss: 0.03081631655951249
Epoch: 2 Idx: 5000 Loss: 0.00937357222771534
Epoch: 3 Idx: 0 Loss: 0.025232113875260524
Epoch: 3 Idx: 5000 Loss: 0.023209564911571376
Epoch: 4 Idx: 0 Loss: 0.03893242557978802
Epoch: 4 Idx: 5000 Loss: 0.03236610646453215
Epoch: 5 Idx: 0 Loss: 0.039443223837500097
Epoch: 5 Idx: 5000 Loss: 0.01137325469580687
Epoch: 6 Idx: 0 Loss: 0.016614431177176987
Epoch: 6 Idx: 5000 Loss: 0.012359715485590877
Epoch: 7 Idx: 0 Loss: 0.022579221873857497
Epoch: 7 Idx: 5000 Loss: 0.02430242894082077
Epoch: 8 Idx: 0 Loss: 0.023093293754824794
Epoch: 8 Idx: 5000 Loss: 0.02432762294092964
Epoch: 9 Idx: 0 Loss: 0.012917422935960788
Epoch: 9 Idx: 5000 Loss: 0.017372708802882768
Epoch: 10 Idx: 0 Loss: 0.03653505736215974
Epoch: 10 Idx: 5000 Loss: 0.019622358614664825
Epoch: 11 Idx: 0 Loss: 0.018868070754666524
Epoch: 11 Idx: 5000 Loss: 0.0153034908923989
Epoch: 12 Idx: 0 Loss: 0.02546458073850414
Epoch: 12 Idx: 5000 Loss: 0.02840662589739149
Epoch: 13 Idx: 0 Loss: 0.027632549175549873
Epoch: 13 Idx: 5000 Loss: 0.020232333975973044
Epoch: 14 Idx: 0 Loss: 0.03590397474282241
Epoch: 14 Idx: 5000 Loss: 0.023772908605925146
Epoch: 15 Idx: 0 Loss: 0.02522853597538698
Epoch: 15 Idx: 5000 Loss: 0.009806411691553411
Epoch: 16 Idx: 0 Loss: 0.015027403145233402
Epoch: 16 Idx: 5000 Loss: 0.024369347688487704
Epoch: 17 Idx: 0 Loss: 0.02291762025032794
Epoch: 17 Idx: 5000 Loss: 0.024534106660917484
Epoch: 18 Idx: 0 Loss: 0.019124676863742224
Epoch: 18 Idx: 5000 Loss: 0.020063157323101773
Epoch: 19 Idx: 0 Loss: 0.016146317552334477
Epoch: 19 Idx: 5000 Loss: 0.02925976984582195
Epoch: 20 Idx: 0 Loss: 0.012960565116645444
Epoch: 20 Idx: 5000 Loss: 0.020646135460900404
Epoch: 21 Idx: 0 Loss: 0.03588370517456295
Epoch: 21 Idx: 5000 Loss: 0.009222410731723626
Epoch: 22 Idx: 0 Loss: 0.03297230256829736
Epoch: 22 Idx: 5000 Loss: 0.030851228973246658
Epoch: 23 Idx: 0 Loss: 0.022812827139068872
Epoch: 23 Idx: 5000 Loss: 0.017424016327272192
Epoch: 24 Idx: 0 Loss: 0.021064009019840898
Epoch: 24 Idx: 5000 Loss: 0.020349085970726342
Epoch: 25 Idx: 0 Loss: 0.0158841305506514
Epoch: 25 Idx: 5000 Loss: 0.019634988823028734
Epoch: 26 Idx: 0 Loss: 0.012930260622086909
Epoch: 26 Idx: 5000 Loss: 0.013525677061942942
Epoch: 27 Idx: 0 Loss: 0.02608408587815174
Epoch: 27 Idx: 5000 Loss: 0.01470008593015756
Epoch: 28 Idx: 0 Loss: 0.029108386409201797
Epoch: 28 Idx: 5000 Loss: 0.01952986226043945
Epoch: 29 Idx: 0 Loss: 0.02203022981870504
Epoch: 29 Idx: 5000 Loss: 0.02443267976559016
Epoch: 30 Idx: 0 Loss: 0.019435397487640494
Epoch: 30 Idx: 5000 Loss: 0.010206337548477988
Epoch: 31 Idx: 0 Loss: 0.017629270128261078
Epoch: 31 Idx: 5000 Loss: 0.011988470376001066
Epoch: 32 Idx: 0 Loss: 0.020188180598150227
Epoch: 32 Idx: 5000 Loss: 0.025877495552998214
Epoch: 33 Idx: 0 Loss: 0.014744305791908389
Epoch: 33 Idx: 5000 Loss: 0.007873393414254658
Epoch: 34 Idx: 0 Loss: 0.011491869580793873
Epoch: 34 Idx: 5000 Loss: 0.03821131924795061
Epoch: 35 Idx: 0 Loss: 0.018771128342277524
Epoch: 35 Idx: 5000 Loss: 0.03155901144580561
Epoch: 36 Idx: 0 Loss: 0.03979475222179127
Epoch: 36 Idx: 5000 Loss: 0.026717656488728134
Epoch: 37 Idx: 0 Loss: 0.013352961171773033
Epoch: 37 Idx: 5000 Loss: 0.01894924384168487
Epoch: 38 Idx: 0 Loss: 0.024160106289368412
Epoch: 38 Idx: 5000 Loss: 0.026565963424571924
Epoch: 39 Idx: 0 Loss: 0.027473340705802762
Epoch: 39 Idx: 5000 Loss: 0.009130854233616977
Epoch: 40 Idx: 0 Loss: 0.021606631306164074
Epoch: 40 Idx: 5000 Loss: 0.009903272671087914
Epoch: 41 Idx: 0 Loss: 0.016369237910494834
Epoch: 41 Idx: 5000 Loss: 0.010649456141905704
Epoch: 42 Idx: 0 Loss: 0.01855229677863904
Epoch: 42 Idx: 5000 Loss: 0.025355034640821783
Epoch: 43 Idx: 0 Loss: 0.010278120982263775
Epoch: 43 Idx: 5000 Loss: 0.017278748665179887
Epoch: 44 Idx: 0 Loss: 0.013260112943797718
Epoch: 44 Idx: 5000 Loss: 0.03706051600582565
Epoch: 45 Idx: 0 Loss: 0.02648364977441179
Epoch: 45 Idx: 5000 Loss: 0.0337104746199188
Epoch: 46 Idx: 0 Loss: 0.028691643706885248
Epoch: 46 Idx: 5000 Loss: 0.02052258364222419
Epoch: 47 Idx: 0 Loss: 0.021197046004778636
Epoch: 47 Idx: 5000 Loss: 0.01729104019705493
Epoch: 48 Idx: 0 Loss: 0.012181881786821155
Epoch: 48 Idx: 5000 Loss: 0.016308399006158044
Epoch: 49 Idx: 0 Loss: 0.0473670205717139
Epoch: 49 Idx: 5000 Loss: 0.013187437685977962
Len (direct inputs):  117
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zerTrTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21021651265417063
Epoch: 0 Idx: 5000 Loss: 0.01881007479642283
Epoch: 1 Idx: 0 Loss: 0.021632702825791973
Epoch: 1 Idx: 5000 Loss: 0.027144834584160928
Epoch: 2 Idx: 0 Loss: 0.018504551707823703
Epoch: 2 Idx: 5000 Loss: 0.010144183145659229
Epoch: 3 Idx: 0 Loss: 0.03131662427775293
Epoch: 3 Idx: 5000 Loss: 0.016103031331659624
Epoch: 4 Idx: 0 Loss: 0.02607077258777577
Epoch: 4 Idx: 5000 Loss: 0.02057971179028683
Epoch: 5 Idx: 0 Loss: 0.02692247223054866
Epoch: 5 Idx: 5000 Loss: 0.017105515175514986
Epoch: 6 Idx: 0 Loss: 0.013629810865045418
Epoch: 6 Idx: 5000 Loss: 0.011362064086897813
Epoch: 7 Idx: 0 Loss: 0.02429548403371741
Epoch: 7 Idx: 5000 Loss: 0.017373604094536508
Epoch: 8 Idx: 0 Loss: 0.028127060593484472
Epoch: 8 Idx: 5000 Loss: 0.018245261180393938
Epoch: 9 Idx: 0 Loss: 0.011967607693657876
Epoch: 9 Idx: 5000 Loss: 0.02931910048783244
Epoch: 10 Idx: 0 Loss: 0.025016294351210222
Epoch: 10 Idx: 5000 Loss: 0.011067264047903208
Epoch: 11 Idx: 0 Loss: 0.01764790918818269
Epoch: 11 Idx: 5000 Loss: 0.03459552357493048
Epoch: 12 Idx: 0 Loss: 0.017972482532689932
Epoch: 12 Idx: 5000 Loss: 0.015811665766171035
Epoch: 13 Idx: 0 Loss: 0.024736724673896048
Epoch: 13 Idx: 5000 Loss: 0.014741320580981609
Epoch: 14 Idx: 0 Loss: 0.018388753021020825
Epoch: 14 Idx: 5000 Loss: 0.021318013404385786
Epoch: 15 Idx: 0 Loss: 0.025352086026421812
Epoch: 15 Idx: 5000 Loss: 0.017004414910714145
Epoch: 16 Idx: 0 Loss: 0.019132935003379724
Epoch: 16 Idx: 5000 Loss: 0.018068042918406073
Epoch: 17 Idx: 0 Loss: 0.02246540614641606
Epoch: 17 Idx: 5000 Loss: 0.03585846940436401
Epoch: 18 Idx: 0 Loss: 0.03091605520012089
Epoch: 18 Idx: 5000 Loss: 0.025680139020027755
Epoch: 19 Idx: 0 Loss: 0.00443513126054155
Epoch: 19 Idx: 5000 Loss: 0.023180394830815243
Epoch: 20 Idx: 0 Loss: 0.013185670337759503
Epoch: 20 Idx: 5000 Loss: 0.04633570227317477
Epoch: 21 Idx: 0 Loss: 0.011712136889319723
Epoch: 21 Idx: 5000 Loss: 0.014703943007306468
Epoch: 22 Idx: 0 Loss: 0.021620170668839513
Epoch: 22 Idx: 5000 Loss: 0.028531951130417413
Epoch: 23 Idx: 0 Loss: 0.02354885230790692
Epoch: 23 Idx: 5000 Loss: 0.018893033257643806
Epoch: 24 Idx: 0 Loss: 0.0188132333493752
Epoch: 24 Idx: 5000 Loss: 0.027126765322645175
Epoch: 25 Idx: 0 Loss: 0.014842821572530154
Epoch: 25 Idx: 5000 Loss: 0.02282557098427733
Epoch: 26 Idx: 0 Loss: 0.02295717281057436
Epoch: 26 Idx: 5000 Loss: 0.020892817289735972
Epoch: 27 Idx: 0 Loss: 0.022415258439364292
Epoch: 27 Idx: 5000 Loss: 0.014017934665175948
Epoch: 28 Idx: 0 Loss: 0.011783432304458837
Epoch: 28 Idx: 5000 Loss: 0.019892652712306554
Epoch: 29 Idx: 0 Loss: 0.02617038913146347
Epoch: 29 Idx: 5000 Loss: 0.019504500855377804
Epoch: 30 Idx: 0 Loss: 0.012314229146933435
Epoch: 30 Idx: 5000 Loss: 0.0299026464204018
Epoch: 31 Idx: 0 Loss: 0.018329638746510675
Epoch: 31 Idx: 5000 Loss: 0.036684278001635456
Epoch: 32 Idx: 0 Loss: 0.012242181621248448
Epoch: 32 Idx: 5000 Loss: 0.016649060087256508
Epoch: 33 Idx: 0 Loss: 0.025437449637625474
Epoch: 33 Idx: 5000 Loss: 0.02291889672203994
Epoch: 34 Idx: 0 Loss: 0.008829431240134
Epoch: 34 Idx: 5000 Loss: 0.026605775683650683
Epoch: 35 Idx: 0 Loss: 0.01736174241515043
Epoch: 35 Idx: 5000 Loss: 0.027872558020536987
Epoch: 36 Idx: 0 Loss: 0.02332172571247991
Epoch: 36 Idx: 5000 Loss: 0.01372673528778807
Epoch: 37 Idx: 0 Loss: 0.016272026502544685
Epoch: 37 Idx: 5000 Loss: 0.02808684949641417
Epoch: 38 Idx: 0 Loss: 0.02457324132515407
Epoch: 38 Idx: 5000 Loss: 0.02180970762534689
Epoch: 39 Idx: 0 Loss: 0.013815894446190826
Epoch: 39 Idx: 5000 Loss: 0.02489247215103173
Epoch: 40 Idx: 0 Loss: 0.017778874900460588
Epoch: 40 Idx: 5000 Loss: 0.01638698373940664
Epoch: 41 Idx: 0 Loss: 0.015966179046772767
Epoch: 41 Idx: 5000 Loss: 0.014817874461418318
Epoch: 42 Idx: 0 Loss: 0.015846399057482412
Epoch: 42 Idx: 5000 Loss: 0.009500362848549139
Epoch: 43 Idx: 0 Loss: 0.015455659944312711
Epoch: 43 Idx: 5000 Loss: 0.019411778046020012
Epoch: 44 Idx: 0 Loss: 0.018778115099379405
Epoch: 44 Idx: 5000 Loss: 0.03394730638775207
Epoch: 45 Idx: 0 Loss: 0.01906556263171696
Epoch: 45 Idx: 5000 Loss: 0.02747932059302744
Epoch: 46 Idx: 0 Loss: 0.032869645667608166
Epoch: 46 Idx: 5000 Loss: 0.04029969531380964
Epoch: 47 Idx: 0 Loss: 0.030072141092635337
Epoch: 47 Idx: 5000 Loss: 0.017269877496830087
Epoch: 48 Idx: 0 Loss: 0.028576678092098767
Epoch: 48 Idx: 5000 Loss: 0.021755134987949246
Epoch: 49 Idx: 0 Loss: 0.015928850353273728
Epoch: 49 Idx: 5000 Loss: 0.022437255736315304
Len (direct inputs):  88
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisTraining siTTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2403184010145024
Epoch: 0 Idx: 5000 Loss: 0.01429751724355944
Epoch: 1 Idx: 0 Loss: 0.02176672221896454
Epoch: 1 Idx: 5000 Loss: 0.014043783599179457
Epoch: 2 Idx: 0 Loss: 0.024166265443971338
Epoch: 2 Idx: 5000 Loss: 0.03422146094696309
Epoch: 3 Idx: 0 Loss: 0.029112355323721632
Epoch: 3 Idx: 5000 Loss: 0.023989641232910364
Epoch: 4 Idx: 0 Loss: 0.019784582412405397
Epoch: 4 Idx: 5000 Loss: 0.01920926382142256
Epoch: 5 Idx: 0 Loss: 0.02727818423860559
Epoch: 5 Idx: 5000 Loss: 0.02950060155114436
Epoch: 6 Idx: 0 Loss: 0.025598819384940304
Epoch: 6 Idx: 5000 Loss: 0.014513446313115132
Epoch: 7 Idx: 0 Loss: 0.021331912915292992
Epoch: 7 Idx: 5000 Loss: 0.022380790116455745
Epoch: 8 Idx: 0 Loss: 0.014982075434174305
Epoch: 8 Idx: 5000 Loss: 0.020473820351266395
Epoch: 9 Idx: 0 Loss: 0.025447281775489272
Epoch: 9 Idx: 5000 Loss: 0.019101057990250008
Epoch: 10 Idx: 0 Loss: 0.020483436044447183
Epoch: 10 Idx: 5000 Loss: 0.021185191988635152
Epoch: 11 Idx: 0 Loss: 0.02141761564502146
Epoch: 11 Idx: 5000 Loss: 0.01984681630963566
Epoch: 12 Idx: 0 Loss: 0.016665394470593793
Epoch: 12 Idx: 5000 Loss: 0.022048231349563994
Epoch: 13 Idx: 0 Loss: 0.016287071367204437
Epoch: 13 Idx: 5000 Loss: 0.022443363364086248
Epoch: 14 Idx: 0 Loss: 0.010525045217495607
Epoch: 14 Idx: 5000 Loss: 0.01319300056234626
Epoch: 15 Idx: 0 Loss: 0.017958840742975803
Epoch: 15 Idx: 5000 Loss: 0.01818206596520513
Epoch: 16 Idx: 0 Loss: 0.03162894700469025
Epoch: 16 Idx: 5000 Loss: 0.023039999694122782
Epoch: 17 Idx: 0 Loss: 0.017310416501381296
Epoch: 17 Idx: 5000 Loss: 0.03296319828865663
Epoch: 18 Idx: 0 Loss: 0.020245260221786777
Epoch: 18 Idx: 5000 Loss: 0.03348222080709283
Epoch: 19 Idx: 0 Loss: 0.017101591556932184
Epoch: 19 Idx: 5000 Loss: 0.013131517870576905
Epoch: 20 Idx: 0 Loss: 0.017083441963432817
Epoch: 20 Idx: 5000 Loss: 0.0225385924877358
Epoch: 21 Idx: 0 Loss: 0.018077206516821136
Epoch: 21 Idx: 5000 Loss: 0.013859825189681778
Epoch: 22 Idx: 0 Loss: 0.017849114524370782
Epoch: 22 Idx: 5000 Loss: 0.023351568587074184
Epoch: 23 Idx: 0 Loss: 0.03750855730069716
Epoch: 23 Idx: 5000 Loss: 0.00877449084077686
Epoch: 24 Idx: 0 Loss: 0.024101267324831973
Epoch: 24 Idx: 5000 Loss: 0.021302798107467533
Epoch: 25 Idx: 0 Loss: 0.028439652553594198
Epoch: 25 Idx: 5000 Loss: 0.032366507001068544
Epoch: 26 Idx: 0 Loss: 0.019662421753432364
Epoch: 26 Idx: 5000 Loss: 0.01530496757219534
Epoch: 27 Idx: 0 Loss: 0.015103095041229104
Epoch: 27 Idx: 5000 Loss: 0.024400120378368365
Epoch: 28 Idx: 0 Loss: 0.02207332627255238
Epoch: 28 Idx: 5000 Loss: 0.018367798759904866
Epoch: 29 Idx: 0 Loss: 0.019786429859325445
Epoch: 29 Idx: 5000 Loss: 0.02722761743586916
Epoch: 30 Idx: 0 Loss: 0.01729825137630087
Epoch: 30 Idx: 5000 Loss: 0.020307449624048384
Epoch: 31 Idx: 0 Loss: 0.016164037545140203
Epoch: 31 Idx: 5000 Loss: 0.013756769567380405
Epoch: 32 Idx: 0 Loss: 0.018756184979767006
Epoch: 32 Idx: 5000 Loss: 0.025678636077955595
Epoch: 33 Idx: 0 Loss: 0.032966597543068764
Epoch: 33 Idx: 5000 Loss: 0.01574983680895052
Epoch: 34 Idx: 0 Loss: 0.01597371259912387
Epoch: 34 Idx: 5000 Loss: 0.014626760785344653
Epoch: 35 Idx: 0 Loss: 0.022193394325236038
Epoch: 35 Idx: 5000 Loss: 0.026805244498433006
Epoch: 36 Idx: 0 Loss: 0.019081648902882886
Epoch: 36 Idx: 5000 Loss: 0.008513453783229392
Epoch: 37 Idx: 0 Loss: 0.012713539861091257
Epoch: 37 Idx: 5000 Loss: 0.020534579741477918
Epoch: 38 Idx: 0 Loss: 0.012572660707449671
Epoch: 38 Idx: 5000 Loss: 0.01921793085076445
Epoch: 39 Idx: 0 Loss: 0.015438502786118016
Epoch: 39 Idx: 5000 Loss: 0.01756267313457263
Epoch: 40 Idx: 0 Loss: 0.03130492891431729
Epoch: 40 Idx: 5000 Loss: 0.03102969592177965
Epoch: 41 Idx: 0 Loss: 0.015422948836728345
Epoch: 41 Idx: 5000 Loss: 0.01839378691957315
Epoch: 42 Idx: 0 Loss: 0.015912609637579997
Epoch: 42 Idx: 5000 Loss: 0.025963625170795722
Epoch: 43 Idx: 0 Loss: 0.0166045266787971
Epoch: 43 Idx: 5000 Loss: 0.00963916893910579
Epoch: 44 Idx: 0 Loss: 0.027774345338131126
Epoch: 44 Idx: 5000 Loss: 0.012884035527845904
Epoch: 45 Idx: 0 Loss: 0.023877048779805142
Epoch: 45 Idx: 5000 Loss: 0.021123741880673536
Epoch: 46 Idx: 0 Loss: 0.01920418205355441
Epoch: 46 Idx: 5000 Loss: 0.02177743587613304
Epoch: 47 Idx: 0 Loss: 0.012417884608105566
Epoch: 47 Idx: 5000 Loss: 0.027566369969089816
Epoch: 48 Idx: 0 Loss: 0.015093339461139476
Epoch: 48 Idx: 5000 Loss: 0.016914826127705833
Epoch: 49 Idx: 0 Loss: 0.020571783876671367
Epoch: 49 Idx: 5000 Loss: 0.029773214421035105
Len (direct inputs):  90
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1874929277908241
Epoch: 0 Idx: 5000 Loss: 0.034187077837951496
Epoch: 1 Idx: 0 Loss: 0.04077576259732024
Epoch: 1 Idx: 5000 Loss: 0.03187815862801818
Epoch: 2 Idx: 0 Loss: 0.01830546200530111
Epoch: 2 Idx: 5000 Loss: 0.02051052217809301
Epoch: 3 Idx: 0 Loss: 0.012352656118884218
Epoch: 3 Idx: 5000 Loss: 0.01208129553434938
Epoch: 4 Idx: 0 Loss: 0.019536241448405968
Epoch: 4 Idx: 5000 Loss: 0.02031334573333468
Epoch: 5 Idx: 0 Loss: 0.015933444143180736
Epoch: 5 Idx: 5000 Loss: 0.033584828534054506
Epoch: 6 Idx: 0 Loss: 0.017060498578981263
Epoch: 6 Idx: 5000 Loss: 0.018095628513939355
Epoch: 7 Idx: 0 Loss: 0.017774809169706092
Epoch: 7 Idx: 5000 Loss: 0.026308719321490145
Epoch: 8 Idx: 0 Loss: 0.01984251265007901
Epoch: 8 Idx: 5000 Loss: 0.048882721519717315
Epoch: 9 Idx: 0 Loss: 0.022895188417234327
Epoch: 9 Idx: 5000 Loss: 0.040253425243778
Epoch: 10 Idx: 0 Loss: 0.016037151412072823
Epoch: 10 Idx: 5000 Loss: 0.03950830320398875
Epoch: 11 Idx: 0 Loss: 0.03874400941941145
Epoch: 11 Idx: 5000 Loss: 0.01662354403096901
Epoch: 12 Idx: 0 Loss: 0.02129011500396139
Epoch: 12 Idx: 5000 Loss: 0.03464760615663493
Epoch: 13 Idx: 0 Loss: 0.0212445127994084
Epoch: 13 Idx: 5000 Loss: 0.03537448634885962
Epoch: 14 Idx: 0 Loss: 0.019649385666813273
Epoch: 14 Idx: 5000 Loss: 0.01898068084380348
Epoch: 15 Idx: 0 Loss: 0.019725203156592423
Epoch: 15 Idx: 5000 Loss: 0.024376884082384826
Epoch: 16 Idx: 0 Loss: 0.012014176376072916
Epoch: 16 Idx: 5000 Loss: 0.03246865295844028
Epoch: 17 Idx: 0 Loss: 0.01443122567541143
Epoch: 17 Idx: 5000 Loss: 0.02142885189465568
Epoch: 18 Idx: 0 Loss: 0.025470744862903627
Epoch: 18 Idx: 5000 Loss: 0.023808936394981883
Epoch: 19 Idx: 0 Loss: 0.03551640433874033
Epoch: 19 Idx: 5000 Loss: 0.017510861046654717
Epoch: 20 Idx: 0 Loss: 0.01806547183440064
Epoch: 20 Idx: 5000 Loss: 0.013093132989298477
Epoch: 21 Idx: 0 Loss: 0.01906440292722201
Epoch: 21 Idx: 5000 Loss: 0.015823683855388498
Epoch: 22 Idx: 0 Loss: 0.019640130851912296
Epoch: 22 Idx: 5000 Loss: 0.010079649104540723
Epoch: 23 Idx: 0 Loss: 0.008886119481554869
Epoch: 23 Idx: 5000 Loss: 0.011081578989985068
Epoch: 24 Idx: 0 Loss: 0.013754746440121693
Traceback (most recent call last):
  File "Attention_anatomy_aml_4props.py", line 400, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
h: 28 Idx: 5000 Loss: 0.03952384273904571
Epoch: 29 Idx: 0 Loss: 0.026414203243946034
Epoch: 29 Idx: 5000 Loss: 0.01371935404241357
Epoch: 30 Idx: 0 Loss: 0.011155987523672086
Epoch: 30 Idx: 5000 Loss: 0.018228597555275512
Epoch: 31 Idx: 0 Loss: 0.01675508257568681
Epoch: 31 Idx: 5000 Loss: 0.01690938719256275
Epoch: 32 Idx: 0 Loss: 0.02005645062542269
Epoch: 32 Idx: 5000 Loss: 0.02014006206457676
Epoch: 33 Idx: 0 Loss: 0.012008057958621497
Epoch: 33 Idx: 5000 Loss: 0.023901982513356825
Epoch: 34 Idx: 0 Loss: 0.02199171406460071
Epoch: 34 Idx: 5000 Loss: 0.021324961896472636
Epoch: 35 Idx: 0 Loss: 0.03875202681130875
Epoch: 35 Idx: 5000 Loss: 0.010259885168118692
Epoch: 36 Idx: 0 Loss: 0.04944843842125872
Epoch: 36 Idx: 5000 Loss: 0.0183180822788416
Epoch: 37 Idx: 0 Loss: 0.014795495584852974
Epoch: 37 Idx: 5000 Loss: 0.015723916902937157
Epoch: 38 Idx: 0 Loss: 0.02417506057441038
Epoch: 38 Idx: 5000 Loss: 0.019005857726870058
Epoch: 39 Idx: 0 Loss: 0.02179612860578884
Epoch: 39 Idx: 5000 Loss: 0.02678621126402534
Epoch: 40 Idx: 0 Loss: 0.021320789688933806
Epoch: 40 Idx: 5000 Loss: 0.024200541708978084
Epoch: 41 Idx: 0 Loss: 0.019525247988998504
Epoch: 41 Idx: 5000 Loss: 0.01341628369681288
Epoch: 42 Idx: 0 Loss: 0.026007639494623883
Epoch: 42 Idx: 5000 Loss: 0.03207514421430285
Epoch: 43 Idx: 0 Loss: 0.020136606967012433
Epoch: 43 Idx: 5000 Loss: 0.026440426544546515
Epoch: 44 Idx: 0 Loss: 0.010670200515124273
Epoch: 44 Idx: 5000 Loss: 0.02608285720997945
Epoch: 45 Idx: 0 Loss: 0.023794105660241992
Epoch: 45 Idx: 5000 Loss: 0.018101902535317134
Epoch: 46 Idx: 0 Loss: 0.015317370659201787
Epoch: 46 Idx: 5000 Loss: 0.030010596604896557
Epoch: 47 Idx: 0 Loss: 0.017643039971885675
Epoch: 47 Idx: 5000 Loss: 0.01768513243297895
Epoch: 48 Idx: 0 Loss: 0.012622595849311469
Epoch: 48 Idx: 5000 Loss: 0.01504294120507124
Epoch: 49 Idx: 0 Loss: 0.02675740214280025
Epoch: 49 Idx: 5000 Loss: 0.01171051867147493
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21013269005123592
Epoch: 0 Idx: 5000 Loss: 0.006053864062620911
Epoch: 1 Idx: 0 Loss: 0.03376491418412617
Epoch: 1 Idx: 5000 Loss: 0.013809240240159958
Epoch: 2 Idx: 0 Loss: 0.018157591380177255
Epoch: 2 Idx: 5000 Loss: 0.024045504396943758
Epoch: 3 Idx: 0 Loss: 0.017520650306438477
Epoch: 3 Idx: 5000 Loss: 0.019703484747951618
Epoch: 4 Idx: 0 Loss: 0.020677251033507753
Epoch: 4 Idx: 5000 Loss: 0.014390535747172268
Epoch: 5 Idx: 0 Loss: 0.006090986198984788
Epoch: 5 Idx: 5000 Loss: 0.037052923994817535
Epoch: 6 Idx: 0 Loss: 0.030545733132169788
Epoch: 6 Idx: 5000 Loss: 0.0328071266543028
Epoch: 7 Idx: 0 Loss: 0.022590656766544568
Epoch: 7 Idx: 5000 Loss: 0.019863702698991372
Epoch: 8 Idx: 0 Loss: 0.026966049211172363
Epoch: 8 Idx: 5000 Loss: 0.026987919348024685
Epoch: 9 Idx: 0 Loss: 0.03766099142326919
Epoch: 9 Idx: 5000 Loss: 0.020822801498068765
Epoch: 10 Idx: 0 Loss: 0.01581583362939385
Epoch: 10 Idx: 5000 Loss: 0.03479927246179062
Epoch: 11 Idx: 0 Loss: 0.02235099168864668
Epoch: 11 Idx: 5000 Loss: 0.026702403208932832
Epoch: 12 Idx: 0 Loss: 0.02980108619334544
Epoch: 12 Idx: 5000 Loss: 0.01988149469438711
Epoch: 13 Idx: 0 Loss: 0.024554383010872233
Epoch: 13 Idx: 5000 Loss: 0.02145715312732523
Epoch: 14 Idx: 0 Loss: 0.02768736135073055
Epoch: 14 Idx: 5000 Loss: 0.016875868015825052
Epoch: 15 Idx: 0 Loss: 0.018696523467293495
Epoch: 15 Idx: 5000 Loss: 0.024897789707049915
Epoch: 16 Idx: 0 Loss: 0.013407496251035537
Epoch: 16 Idx: 5000 Loss: 0.018816858860769392
Epoch: 17 Idx: 0 Loss: 0.015314649861834236
Epoch: 17 Idx: 5000 Loss: 0.007062938519103808
Epoch: 18 Idx: 0 Loss: 0.008342933768369296
Epoch: 18 Idx: 5000 Loss: 0.016823509696448607
Epoch: 19 Idx: 0 Loss: 0.015340447684518069
Epoch: 19 Idx: 5000 Loss: 0.014632550401081232
Epoch: 20 Idx: 0 Loss: 0.02821323893076368
Epoch: 20 Idx: 5000 Loss: 0.04895558689333239
Epoch: 21 Idx: 0 Loss: 0.012182314051294961
Epoch: 21 Idx: 5000 Loss: 0.03163436752570109
Epoch: 22 Idx: 0 Loss: 0.019798712497982117
Epoch: 22 Idx: 5000 Loss: 0.03806683804163878
Epoch: 23 Idx: 0 Loss: 0.012634489503546396
Epoch: 23 Idx: 5000 Loss: 0.03945660106726586
Epoch: 24 Idx: 0 Loss: 0.020342721661539116
Epoch: 24 Idx: 5000 Loss: 0.016229824581005935
Epoch: 25 Idx: 0 Loss: 0.021601026414780736
Traceback (most recent call last):
  File "Attention_anatomy_aml_4props.py", line 401, in <module>
    loss.backward()
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc209>
Subject: Job 3290120: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs_4props10_5.pkl Models/anatomy_aml_bagofnbrs_4props10_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs_4props10_5.pkl Models/anatomy_aml_bagofnbrs_4props10_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:04 2020
Job was executed on host(s) <dccxc209>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 05:53:51 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 05:53:51 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs_4props10_5.pkl Models/anatomy_aml_bagofnbrs_4props10_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23091.19 sec.
    Max Memory :                                 2744 MB
    Average Memory :                             2677.17 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40673.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23097 sec.
    Turnaround time :                            80684 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc227>
Subject: Job 3290121: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 12 5 Output/test_anatomy_aml_bagofnbrs_4props12_5.pkl Models/anatomy_aml_bagofnbrs_4props12_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 12 5 Output/test_anatomy_aml_bagofnbrs_4props12_5.pkl Models/anatomy_aml_bagofnbrs_4props12_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:04 2020
Job was executed on host(s) <dccxc227>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 05:57:26 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 05:57:26 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 12 5 Output/test_anatomy_aml_bagofnbrs_4props12_5.pkl Models/anatomy_aml_bagofnbrs_4props12_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22815.63 sec.
    Max Memory :                                 2740 MB
    Average Memory :                             2668.02 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40677.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22882 sec.
    Turnaround time :                            80685 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc202>
Subject: Job 3290122: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 15 5 Output/test_anatomy_aml_bagofnbrs_4props15_5.pkl Models/anatomy_aml_bagofnbrs_4props15_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 15 5 Output/test_anatomy_aml_bagofnbrs_4props15_5.pkl Models/anatomy_aml_bagofnbrs_4props15_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:04 2020
Job was executed on host(s) <dccxc202>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:00:23 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:00:23 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 15 5 Output/test_anatomy_aml_bagofnbrs_4props15_5.pkl Models/anatomy_aml_bagofnbrs_4props15_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22704.29 sec.
    Max Memory :                                 2735 MB
    Average Memory :                             2655.96 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40682.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22705 sec.
    Turnaround time :                            80685 sec.

The output (if any) is above this job summary.

