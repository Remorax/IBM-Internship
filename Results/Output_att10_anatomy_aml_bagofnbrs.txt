Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18073917830148395
Epoch: 0 Idx: 5000 Loss: 0.041884306391323706
Epoch: 1 Idx: 0 Loss: 0.020069661886673563
Epoch: 1 Idx: 5000 Loss: 0.009892251701286546
Epoch: 2 Idx: 0 Loss: 0.022365548859946235
Epoch: 2 Idx: 5000 Loss: 0.020688729284354102
Epoch: 3 Idx: 0 Loss: 0.02259934811957159
Epoch: 3 Idx: 5000 Loss: 0.026724056532305498
Epoch: 4 Idx: 0 Loss: 0.01384620988757768
Epoch: 4 Idx: 5000 Loss: 0.026873914135841233
Epoch: 5 Idx: 0 Loss: 0.019198631519983383
Epoch: 5 Idx: 5000 Loss: 0.01848772024769916
Epoch: 6 Idx: 0 Loss: 0.01903866722162793
Epoch: 6 Idx: 5000 Loss: 0.045535689323158164
Epoch: 7 Idx: 0 Loss: 0.029150900926412558
Epoch: 7 Idx: 5000 Loss: 0.007579241635467761
Epoch: 8 Idx: 0 Loss: 0.04479860431257295
Epoch: 8 Idx: 5000 Loss: 0.023913908863912824
Epoch: 9 Idx: 0 Loss: 0.013508958855976314
Epoch: 9 Idx: 5000 Loss: 0.017558266277619666
Epoch: 10 Idx: 0 Loss: 0.0224626092072522
Epoch: 10 Idx: 5000 Loss: 0.030421178050909832
Epoch: 11 Idx: 0 Loss: 0.013689765570139959
Epoch: 11 Idx: 5000 Loss: 0.018644910302271465
Epoch: 12 Idx: 0 Loss: 0.026750294186793824
Epoch: 12 Idx: 5000 Loss: 0.01483303111228682
Epoch: 13 Idx: 0 Loss: 0.01428758442001487
Epoch: 13 Idx: 5000 Loss: 0.05086414972489063
Epoch: 14 Idx: 0 Loss: 0.024102116595958824
Epoch: 14 Idx: 5000 Loss: 0.014660536050884139
Epoch: 15 Idx: 0 Loss: 0.03599783496528518
Epoch: 15 Idx: 5000 Loss: 0.022290898086406052
Epoch: 16 Idx: 0 Loss: 0.03407461144481695
Epoch: 16 Idx: 5000 Loss: 0.01774798518744801
Epoch: 17 Idx: 0 Loss: 0.02413729168952231
Epoch: 17 Idx: 5000 Loss: 0.01855163781658897
Epoch: 18 Idx: 0 Loss: 0.020680501583643186
Epoch: 18 Idx: 5000 Loss: 0.017694221088512034
Epoch: 19 Idx: 0 Loss: 0.015968058218622853
Epoch: 19 Idx: 5000 Loss: 0.021659803358512662
Epoch: 20 Idx: 0 Loss: 0.028657744557410877
Epoch: 20 Idx: 5000 Loss: 0.02619719565299196
Epoch: 21 Idx: 0 Loss: 0.03162116006845987
Epoch: 21 Idx: 5000 Loss: 0.04442145503561884
Epoch: 22 Idx: 0 Loss: 0.01856066630980875
Epoch: 22 Idx: 5000 Loss: 0.02118746966596373
Epoch: 23 Idx: 0 Loss: 0.013025978949659919
Epoch: 23 Idx: 5000 Loss: 0.012583791931354088
Epoch: 24 Idx: 0 Loss: 0.023188389158456512
Epoch: 24 Idx: 5000 Loss: 0.037656038817796486
Epoch: 25 Idx: 0 Loss: 0.02177389848846203
Epoch: 25 Idx: 5000 Loss: 0.013396144447570882
Epoch: 26 Idx: 0 Loss: 0.015529870483903871
Epoch: 26 Idx: 5000 Loss: 0.022527116496790768
Epoch: 27 Idx: 0 Loss: 0.02030534306096069
Epoch: 27 Idx: 5000 Loss: 0.032154952221015526
Epoch: 28 Idx: 0 Loss: 0.02464959321675836
Epoch: 28 Idx: 5000 Loss: 0.028977165249292808
Epoch: 29 Idx: 0 Loss: 0.02095439166901585
Epoch: 29 Idx: 5000 Loss: 0.04898036236735186
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml.py", line 314, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 314, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 312, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml.py", line 311, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
528767
Epoch: 39 Idx: 0 Loss: 0.017445975597344336
Epoch: 39 Idx: 5000 Loss: 0.017453517370863175
Epoch: 40 Idx: 0 Loss: 0.0140283940216907
Epoch: 40 Idx: 5000 Loss: 0.020329337152691397
Epoch: 41 Idx: 0 Loss: 0.008807472427898873
Epoch: 41 Idx: 5000 Loss: 0.018105061282822896
Epoch: 42 Idx: 0 Loss: 0.025887574191478534
Epoch: 42 Idx: 5000 Loss: 0.012928021940162498
Epoch: 43 Idx: 0 Loss: 0.02423093868287738
Epoch: 43 Idx: 5000 Loss: 0.014375739914350781
Epoch: 44 Idx: 0 Loss: 0.01793787310620843
Epoch: 44 Idx: 5000 Loss: 0.024953530258185988
Epoch: 45 Idx: 0 Loss: 0.033411113314443644
Epoch: 45 Idx: 5000 Loss: 0.009058009367229681
Epoch: 46 Idx: 0 Loss: 0.015896844638878062
Epoch: 46 Idx: 5000 Loss: 0.012100522376737177
Epoch: 47 Idx: 0 Loss: 0.019143327038592235
Epoch: 47 Idx: 5000 Loss: 0.014899067292905459
Epoch: 48 Idx: 0 Loss: 0.02860208931496694
Epoch: 48 Idx: 5000 Loss: 0.021640300419074356
Epoch: 49 Idx: 0 Loss: 0.015809291077849404
Epoch: 49 Idx: 5000 Loss: 0.010539931404747583
Len (direct inputs):  92
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22940229750732263
Epoch: 0 Idx: 5000 Loss: 0.04437944139779741
Epoch: 1 Idx: 0 Loss: 0.022512190344571914
Epoch: 1 Idx: 5000 Loss: 0.006285577821602993
Epoch: 2 Idx: 0 Loss: 0.046410245095083465
Epoch: 2 Idx: 5000 Loss: 0.020437091607554393
Epoch: 3 Idx: 0 Loss: 0.026517973232625763
Epoch: 3 Idx: 5000 Loss: 0.016640442428876356
Epoch: 4 Idx: 0 Loss: 0.015520300701102337
Epoch: 4 Idx: 5000 Loss: 0.04457533517707113
Epoch: 5 Idx: 0 Loss: 0.02779629219165684
Epoch: 5 Idx: 5000 Loss: 0.014200365806799592
Epoch: 6 Idx: 0 Loss: 0.014965658203704327
Epoch: 6 Idx: 5000 Loss: 0.018616946577924686
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 398, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
Epoch: 11 Idx: 0 Loss: 0.025473318378253132
Epoch: 11 Idx: 5000 Loss: 0.0202687597046232
Epoch: 12 Idx: 0 Loss: 0.02591942372374567
Epoch: 12 Idx: 5000 Loss: 0.013294307831434742
Epoch: 13 Idx: 0 Loss: 0.02199470893873394
Epoch: 13 Idx: 5000 Loss: 0.020880076307019064
Epoch: 14 Idx: 0 Loss: 0.025540174818475272
Epoch: 14 Idx: 5000 Loss: 0.047809981785366884
Epoch: 15 Idx: 0 Loss: 0.016468054690401426
Epoch: 15 Idx: 5000 Loss: 0.026493952253629424
Epoch: 16 Idx: 0 Loss: 0.018103017694550864
Epoch: 16 Idx: 5000 Loss: 0.028779372096486892
Epoch: 17 Idx: 0 Loss: 0.02675276570430391
Epoch: 17 Idx: 5000 Loss: 0.019242103833389364
Epoch: 18 Idx: 0 Loss: 0.01659904251858105
Epoch: 18 Idx: 5000 Loss: 0.01968332520513169
Epoch: 19 Idx: 0 Loss: 0.026503988216101646
Epoch: 19 Idx: 5000 Loss: 0.027922061286236727
Epoch: 20 Idx: 0 Loss: 0.013535810163437552
Epoch: 20 Idx: 5000 Loss: 0.032087297209789686
Epoch: 21 Idx: 0 Loss: 0.0173903907776356
Epoch: 21 Idx: 5000 Loss: 0.017266520632655862
Epoch: 22 Idx: 0 Loss: 0.021603828911236005
Epoch: 22 Idx: 5000 Loss: 0.035632072421915
Epoch: 23 Idx: 0 Loss: 0.01751107397292511
Epoch: 23 Idx: 5000 Loss: 0.031075699772127277
Epoch: 24 Idx: 0 Loss: 0.03723237997036729
Epoch: 24 Idx: 5000 Loss: 0.031890510322413745
Epoch: 25 Idx: 0 Loss: 0.027877993519888532
Epoch: 25 Idx: 5000 Loss: 0.0068058475314786045
Epoch: 26 Idx: 0 Loss: 0.012681869665647565
Epoch: 26 Idx: 5000 Loss: 0.012475167796979214
Epoch: 27 Idx: 0 Loss: 0.020341708729445893
Epoch: 27 Idx: 5000 Loss: 0.024122727086323417
Epoch: 28 Idx: 0 Loss: 0.04718829482771737
Epoch: 28 Idx: 5000 Loss: 0.019276272314789573
Epoch: 29 Idx: 0 Loss: 0.016832995711187168
Epoch: 29 Idx: 5000 Loss: 0.027967476471614704
Epoch: 30 Idx: 0 Loss: 0.02115081635426693
Epoch: 30 Idx: 5000 Loss: 0.019417254259971675
Epoch: 31 Idx: 0 Loss: 0.020227291577977403
Epoch: 31 Idx: 5000 Loss: 0.009476147208701354
Epoch: 32 Idx: 0 Loss: 0.032750950730124476
Epoch: 32 Idx: 5000 Loss: 0.023243738876567006
Epoch: 33 Idx: 0 Loss: 0.013265640356517368
Epoch: 33 Idx: 5000 Loss: 0.011736500000597908
Epoch: 34 Idx: 0 Loss: 0.03760764167022077
Epoch: 34 Idx: 5000 Loss: 0.021692250339463467
Epoch: 35 Idx: 0 Loss: 0.017623342936095552
Epoch: 35 Idx: 5000 Loss: 0.02722028831907066
Epoch: 36 Idx: 0 Loss: 0.024248184991109378
Epoch: 36 Idx: 5000 Loss: 0.030952507971950816
Epoch: 37 Idx: 0 Loss: 0.013368618637673156
Epoch: 37 Idx: 5000 Loss: 0.018068555100977747
Epoch: 38 Idx: 0 Loss: 0.0353868524416282
Epoch: 38 Idx: 5000 Loss: 0.017885073813263
Epoch: 39 Idx: 0 Loss: 0.027027872965067777
Epoch: 39 Idx: 5000 Loss: 0.016975015027405407
Epoch: 40 Idx: 0 Loss: 0.030323323127080533
Epoch: 40 Idx: 5000 Loss: 0.019499671040778797
Epoch: 41 Idx: 0 Loss: 0.021309704791648676
Epoch: 41 Idx: 5000 Loss: 0.017860422102801372
Epoch: 42 Idx: 0 Loss: 0.020439508828387864
Epoch: 42 Idx: 5000 Loss: 0.037734735382561405
Epoch: 43 Idx: 0 Loss: 0.010187839774929711
Epoch: 43 Idx: 5000 Loss: 0.017069136008577526
Epoch: 44 Idx: 0 Loss: 0.017534924942486353
Epoch: 44 Idx: 5000 Loss: 0.018835739354272275
Epoch: 45 Idx: 0 Loss: 0.010588742625027812
Epoch: 45 Idx: 5000 Loss: 0.037330204566527775
Epoch: 46 Idx: 0 Loss: 0.014701974793712633
Epoch: 46 Idx: 5000 Loss: 0.017361659558432656
Epoch: 47 Idx: 0 Loss: 0.019220565719094064
Epoch: 47 Idx: 5000 Loss: 0.021562712340950436
Epoch: 48 Idx: 0 Loss: 0.021085439027874076
Epoch: 48 Idx: 5000 Loss: 0.0166924582846672
Epoch: 49 Idx: 0 Loss: 0.015311671976968751
Epoch: 49 Idx: 5000 Loss: 0.016223800596546302
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division TrainingTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.259603327387734
Epoch: 0 Idx: 5000 Loss: 0.02343145824714008
Epoch: 1 Idx: 0 Loss: 0.01788596321948894
Epoch: 1 Idx: 5000 Loss: 0.023356712983743845
Epoch: 2 Idx: 0 Loss: 0.011703738483020419
Epoch: 2 Idx: 5000 Loss: 0.018025542899803286
Epoch: 3 Idx: 0 Loss: 0.034918282868533705
Epoch: 3 Idx: 5000 Loss: 0.02154124367711775
Epoch: 4 Idx: 0 Loss: 0.02525389567016752
Epoch: 4 Idx: 5000 Loss: 0.019359875715763218
Epoch: 5 Idx: 0 Loss: 0.023063239669054
Epoch: 5 Idx: 5000 Loss: 0.031992690444287
Epoch: 6 Idx: 0 Loss: 0.034493900216596214
Epoch: 6 Idx: 5000 Loss: 0.02563639417745488
Epoch: 7 Idx: 0 Loss: 0.03926570926347271
Epoch: 7 Idx: 5000 Loss: 0.014421323654698587
Epoch: 8 Idx: 0 Loss: 0.012814394521165992
Epoch: 8 Idx: 5000 Loss: 0.01767137446225362
Epoch: 9 Idx: 0 Loss: 0.025362024972078574
Epoch: 9 Idx: 5000 Loss: 0.051545782057805814
Epoch: 10 Idx: 0 Loss: 0.02573936069748598
Epoch: 10 Idx: 5000 Loss: 0.02429107292791169
Epoch: 11 Idx: 0 Loss: 0.007241480935330844
Epoch: 11 Idx: 5000 Loss: 0.02029948459770337
Epoch: 12 Idx: 0 Loss: 0.025276697165271884
Epoch: 12 Idx: 5000 Loss: 0.02316429275144436
Epoch: 13 Idx: 0 Loss: 0.01989041474597033
Epoch: 13 Idx: 5000 Loss: 0.012103683657020078
Epoch: 14 Idx: 0 Loss: 0.035781677211965915
Epoch: 14 Idx: 5000 Loss: 0.020691529205334066
Epoch: 15 Idx: 0 Loss: 0.018090107662426125
Epoch: 15 Idx: 5000 Loss: 0.019551824777189593
Epoch: 16 Idx: 0 Loss: 0.019819208179454748
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 398, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
386546390039
Epoch: 20 Idx: 5000 Loss: 0.018519327654635456
Epoch: 21 Idx: 0 Loss: 0.02673276343427783
Epoch: 21 Idx: 5000 Loss: 0.01100597823650161
Epoch: 22 Idx: 0 Loss: 0.013653006641963086
Epoch: 22 Idx: 5000 Loss: 0.026866298983622406
Epoch: 23 Idx: 0 Loss: 0.015426754897416364
Epoch: 23 Idx: 5000 Loss: 0.03402078689973162
Epoch: 24 Idx: 0 Loss: 0.013956794523386414
Epoch: 24 Idx: 5000 Loss: 0.023375145392236424
Epoch: 25 Idx: 0 Loss: 0.017640839180209204
Epoch: 25 Idx: 5000 Loss: 0.01219955283049023
Epoch: 26 Idx: 0 Loss: 0.010514275997017708
Epoch: 26 Idx: 5000 Loss: 0.019039950868473862
Epoch: 27 Idx: 0 Loss: 0.029970385032771607
Epoch: 27 Idx: 5000 Loss: 0.014589551629646577
Epoch: 28 Idx: 0 Loss: 0.03459278041707539
Epoch: 28 Idx: 5000 Loss: 0.017818022596781433
Epoch: 29 Idx: 0 Loss: 0.025122116051926782
Epoch: 29 Idx: 5000 Loss: 0.009470861827328137
Epoch: 30 Idx: 0 Loss: 0.0109637739259294
Epoch: 30 Idx: 5000 Loss: 0.013549423869045809
Epoch: 31 Idx: 0 Loss: 0.03180688928436168
Epoch: 31 Idx: 5000 Loss: 0.02183199980981653
Epoch: 32 Idx: 0 Loss: 0.02127233178868395
Epoch: 32 Idx: 5000 Loss: 0.014486198038472318
Epoch: 33 Idx: 0 Loss: 0.012289224048609963
Epoch: 33 Idx: 5000 Loss: 0.019368854158850247
Epoch: 34 Idx: 0 Loss: 0.012608895612612326
Epoch: 34 Idx: 5000 Loss: 0.01156936344000344
Epoch: 35 Idx: 0 Loss: 0.024351076846721716
Epoch: 35 Idx: 5000 Loss: 0.023381503891361812
Epoch: 36 Idx: 0 Loss: 0.030384871357036304
Epoch: 36 Idx: 5000 Loss: 0.012891703253428522
Epoch: 37 Idx: 0 Loss: 0.019118482233980456
Epoch: 37 Idx: 5000 Loss: 0.01912451116424927
Epoch: 38 Idx: 0 Loss: 0.021707818043996928
Epoch: 38 Idx: 5000 Loss: 0.011110530299152111
Epoch: 39 Idx: 0 Loss: 0.02331660231755816
Epoch: 39 Idx: 5000 Loss: 0.03318490734611775
Epoch: 40 Idx: 0 Loss: 0.018999891994759974
Epoch: 40 Idx: 5000 Loss: 0.05061813936707356
Epoch: 41 Idx: 0 Loss: 0.03318488412823084
Epoch: 41 Idx: 5000 Loss: 0.02131088013534011
Epoch: 42 Idx: 0 Loss: 0.027808052832351514
Epoch: 42 Idx: 5000 Loss: 0.01323689219890951
Epoch: 43 Idx: 0 Loss: 0.01930116516883374
Epoch: 43 Idx: 5000 Loss: 0.027566970568935114
Epoch: 44 Idx: 0 Loss: 0.02358927705724244
Epoch: 44 Idx: 5000 Loss: 0.02478228729004345
Epoch: 45 Idx: 0 Loss: 0.014352516789828278
Epoch: 45 Idx: 5000 Loss: 0.03440226142260899
Epoch: 46 Idx: 0 Loss: 0.013965297913530298
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 398, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt

division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zTraiTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.25248447138796637
Epoch: 0 Idx: 5000 Loss: 0.019251929717930424
Epoch: 1 Idx: 0 Loss: 0.038131295621474146
Epoch: 1 Idx: 5000 Loss: 0.027161507829274158
Epoch: 2 Idx: 0 Loss: 0.017835543954154924
Epoch: 2 Idx: 5000 Loss: 0.011195669018853638
Epoch: 3 Idx: 0 Loss: 0.01996279354569075
Epoch: 3 Idx: 5000 Loss: 0.0365606612069604
Epoch: 4 Idx: 0 Loss: 0.030865224230391473
Epoch: 4 Idx: 5000 Loss: 0.018175171011872875
Epoch: 5 Idx: 0 Loss: 0.01813983127341256
Epoch: 5 Idx: 5000 Loss: 0.022398713117584415
Epoch: 6 Idx: 0 Loss: 0.035822814909481554
Epoch: 6 Idx: 5000 Loss: 0.03459253648205968
Epoch: 7 Idx: 0 Loss: 0.0197976676639153
Epoch: 7 Idx: 5000 Loss: 0.017067638057485585
Epoch: 8 Idx: 0 Loss: 0.039601477043076316
Epoch: 8 Idx: 5000 Loss: 0.015600868840100413
Epoch: 9 Idx: 0 Loss: 0.014461370241510993
Epoch: 9 Idx: 5000 Loss: 0.02567420175160049
Epoch: 10 Idx: 0 Loss: 0.02790884514252053
Epoch: 10 Idx: 5000 Loss: 0.022030672514705918
Epoch: 11 Idx: 0 Loss: 0.009623842289277543
Epoch: 11 Idx: 5000 Loss: 0.03256973317902213
Epoch: 12 Idx: 0 Loss: 0.02324476766507539
Epoch: 12 Idx: 5000 Loss: 0.019797827243796983
Epoch: 13 Idx: 0 Loss: 0.018379291093865477
Epoch: 13 Idx: 5000 Loss: 0.02226990304928124
Epoch: 14 Idx: 0 Loss: 0.020858913642335303
Epoch: 14 Idx: 5000 Loss: 0.01484559981422236
Epoch: 15 Idx: 0 Loss: 0.019934150287361067
Epoch: 15 Idx: 5000 Loss: 0.014122455384911067
Epoch: 16 Idx: 0 Loss: 0.037643771067358056
Epoch: 16 Idx: 5000 Loss: 0.009391384181396623
Epoch: 17 Idx: 0 Loss: 0.01578672163795962
Epoch: 17 Idx: 5000 Loss: 0.022207515771492055
Epoch: 18 Idx: 0 Loss: 0.021926289905968004
Epoch: 18 Idx: 5000 Loss: 0.02273067065185584
Epoch: 19 Idx: 0 Loss: 0.02051971870756769
Epoch: 19 Idx: 5000 Loss: 0.03146147866739748
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml.py", line 310, in to_feature
    for elem in inputs]
  File "Attention_anatomy_aml.py", line 310, in <listcomp>
    for elem in inputs]
  File "Attention_anatomy_aml.py", line 309, in <listcomp>
    for ent in elem]
  File "Attention_anatomy_aml.py", line 308, in <listcomp>
    for nbr_type in ent[:max_types]]
KeyboardInterrupt
017542970310090764
Epoch: 26 Idx: 0 Loss: 0.010223172180210646
Epoch: 26 Idx: 5000 Loss: 0.02237135959610108
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt

Epoch: 29 Idx: 0 Loss: 0.018629498867126067
Epoch: 29 Idx: 5000 Loss: 0.018014092499617114
Epoch: 30 Idx: 0 Loss: 0.01991141731421408
Epoch: 30 Idx: 5000 Loss: 0.027098010599208085
Epoch: 31 Idx: 0 Loss: 0.02310521310667621
Epoch: 31 Idx: 5000 Loss: 0.01889579249561625
Epoch: 32 Idx: 0 Loss: 0.017598339593385415
Epoch: 32 Idx: 5000 Loss: 0.022443335825130403
Epoch: 33 Idx: 0 Loss: 0.016716579307171443
Epoch: 33 Idx: 5000 Loss: 0.01507709295719635
Epoch: 34 Idx: 0 Loss: 0.018062272757410583
Epoch: 34 Idx: 5000 Loss: 0.03138043359020367
Epoch: 35 Idx: 0 Loss: 0.023563444494431453
Epoch: 35 Idx: 5000 Loss: 0.015327465507270023
Epoch: 36 Idx: 0 Loss: 0.009908884098993044
Epoch: 36 Idx: 5000 Loss: 0.015189940340604237
Epoch: 37 Idx: 0 Loss: 0.013676481725020298
Epoch: 37 Idx: 5000 Loss: 0.0451270422390803
Epoch: 38 Idx: 0 Loss: 0.014362629266641485
Epoch: 38 Idx: 5000 Loss: 0.018524357172808967
Epoch: 39 Idx: 0 Loss: 0.018088156223694474
Epoch: 39 Idx: 5000 Loss: 0.023583879659181278
Epoch: 40 Idx: 0 Loss: 0.017187282784362122
Epoch: 40 Idx: 5000 Loss: 0.016093599685415162
Epoch: 41 Idx: 0 Loss: 0.03118887367714697
Epoch: 41 Idx: 5000 Loss: 0.012468419462145216
Epoch: 42 Idx: 0 Loss: 0.05014465650867235
Epoch: 42 Idx: 5000 Loss: 0.013647287848372809
Epoch: 43 Idx: 0 Loss: 0.013678971440292266
Epoch: 43 Idx: 5000 Loss: 0.02261318771001033
Epoch: 44 Idx: 0 Loss: 0.024338450849105728
Epoch: 44 Idx: 5000 Loss: 0.028935609351799757
Epoch: 45 Idx: 0 Loss: 0.0328143064612565
Epoch: 45 Idx: 5000 Loss: 0.022394097188324007
Epoch: 46 Idx: 0 Loss: 0.0173555323836311
Epoch: 46 Idx: 5000 Loss: 0.03901648983672802
Epoch: 47 Idx: 0 Loss: 0.01931804380992873
Epoch: 47 Idx: 5000 Loss: 0.028998605253528924
Epoch: 48 Idx: 0 Loss: 0.02487110993865348
Epoch: 48 Idx: 5000 Loss: 0.02382007271904669
Epoch: 49 Idx: 0 Loss: 0.024157395062149232
Epoch: 49 Idx: 5000 Loss: 0.017500221946316737
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.23855755042771634
Epoch: 0 Idx: 5000 Loss: 0.019705761244665533
Epoch: 1 Idx: 0 Loss: 0.02070783309124307
Epoch: 1 Idx: 5000 Loss: 0.02305545530116878
Epoch: 2 Idx: 0 Loss: 0.034741802865112845
Epoch: 2 Idx: 5000 Loss: 0.01943742355852541
Epoch: 3 Idx: 0 Loss: 0.025541480336057527
Epoch: 3 Idx: 5000 Loss: 0.02657382808625706
Epoch: 4 Idx: 0 Loss: 0.012111608231272058
Epoch: 4 Idx: 5000 Loss: 0.020840894862080604
Epoch: 5 Idx: 0 Loss: 0.021240538971088296
Epoch: 5 Idx: 5000 Loss: 0.024257459557188187
Epoch: 6 Idx: 0 Loss: 0.02609462171457807
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 396, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "Attention_anatomy_aml.py", line 283, in forward
    contextual_node_emb = torch.cat((node_emb, context_emb), dim=1)
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc222>
Subject: Job 3290015: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 20 10 Output/test_anatomy_aml_bagofnbrs20_10.pkl Models/anatomy_aml_bagofnbrs20_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 20 10 Output/test_anatomy_aml_bagofnbrs20_10.pkl Models/anatomy_aml_bagofnbrs20_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:58 2020
Job was executed on host(s) <dccxc222>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:16:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:16:47 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 20 10 Output/test_anatomy_aml_bagofnbrs20_10.pkl Models/anatomy_aml_bagofnbrs20_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46920.18 sec.
    Max Memory :                                 2743 MB
    Average Memory :                             2673.67 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40674.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   46924 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc202>
Subject: Job 3290029: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 10 Output/test_anatomy_aml_bagofnbrs80_10.pkl Models/anatomy_aml_bagofnbrs80_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 10 Output/test_anatomy_aml_bagofnbrs80_10.pkl Models/anatomy_aml_bagofnbrs80_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc202>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 00:02:36 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 00:02:36 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 10 Output/test_anatomy_aml_bagofnbrs80_10.pkl Models/anatomy_aml_bagofnbrs80_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   44167.20 sec.
    Max Memory :                                 2710 MB
    Average Memory :                             2598.85 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40707.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   44191 sec.
    Turnaround time :                            80689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc218>
Subject: Job 3290019: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 24 10 Output/test_anatomy_aml_bagofnbrs24_10.pkl Models/anatomy_aml_bagofnbrs24_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 24 10 Output/test_anatomy_aml_bagofnbrs24_10.pkl Models/anatomy_aml_bagofnbrs24_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc218>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:35:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:35:47 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 24 10 Output/test_anatomy_aml_bagofnbrs24_10.pkl Models/anatomy_aml_bagofnbrs24_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45665.66 sec.
    Max Memory :                                 2741 MB
    Average Memory :                             2670.18 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40676.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45781 sec.
    Turnaround time :                            80689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc210>
Subject: Job 3290023: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 10 Output/test_anatomy_aml_bagofnbrs30_10.pkl Models/anatomy_aml_bagofnbrs30_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 10 Output/test_anatomy_aml_bagofnbrs30_10.pkl Models/anatomy_aml_bagofnbrs30_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc210>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:45:11 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:45:11 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 10 Output/test_anatomy_aml_bagofnbrs30_10.pkl Models/anatomy_aml_bagofnbrs30_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45128.71 sec.
    Max Memory :                                 2733 MB
    Average Memory :                             2647.00 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40684.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45217 sec.
    Turnaround time :                            80689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc223>
Subject: Job 3290031: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 10 Output/test_anatomy_aml_bagofnbrs152_10.pkl Models/anatomy_aml_bagofnbrs152_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 10 Output/test_anatomy_aml_bagofnbrs152_10.pkl Models/anatomy_aml_bagofnbrs152_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc223>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 00:06:55 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 00:06:55 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 10 Output/test_anatomy_aml_bagofnbrs152_10.pkl Models/anatomy_aml_bagofnbrs152_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   43822.58 sec.
    Max Memory :                                 2679 MB
    Average Memory :                             2570.08 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40738.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   43913 sec.
    Turnaround time :                            80689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc265>
Subject: Job 3290021: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 10 Output/test_anatomy_aml_bagofnbrs26_10.pkl Models/anatomy_aml_bagofnbrs26_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 10 Output/test_anatomy_aml_bagofnbrs26_10.pkl Models/anatomy_aml_bagofnbrs26_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc265>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:42:59 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:42:59 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 10 Output/test_anatomy_aml_bagofnbrs26_10.pkl Models/anatomy_aml_bagofnbrs26_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45331.30 sec.
    Max Memory :                                 2734 MB
    Average Memory :                             2660.75 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40683.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45374 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc206>
Subject: Job 3290025: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 10 Output/test_anatomy_aml_bagofnbrs32_10.pkl Models/anatomy_aml_bagofnbrs32_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 10 Output/test_anatomy_aml_bagofnbrs32_10.pkl Models/anatomy_aml_bagofnbrs32_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc206>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:47:29 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:47:29 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 10 Output/test_anatomy_aml_bagofnbrs32_10.pkl Models/anatomy_aml_bagofnbrs32_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45069.69 sec.
    Max Memory :                                 2733 MB
    Average Memory :                             2659.44 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40684.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45079 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 3290027: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 10 Output/test_anatomy_aml_bagofnbrs40_10.pkl Models/anatomy_aml_bagofnbrs40_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 10 Output/test_anatomy_aml_bagofnbrs40_10.pkl Models/anatomy_aml_bagofnbrs40_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 00:00:00 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 00:00:00 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 10 Output/test_anatomy_aml_bagofnbrs40_10.pkl Models/anatomy_aml_bagofnbrs40_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   44235.21 sec.
    Max Memory :                                 2731 MB
    Average Memory :                             2641.11 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40686.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   44329 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.

