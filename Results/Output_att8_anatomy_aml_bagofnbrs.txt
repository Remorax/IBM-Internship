Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17690339337759103
Epoch: 0 Idx: 5000 Loss: 0.02337019838145586
Epoch: 1 Idx: 0 Loss: 0.011059219904961176
Epoch: 1 Idx: 5000 Loss: 0.015786861199242715
Epoch: 2 Idx: 0 Loss: 0.015854701915509143
Epoch: 2 Idx: 5000 Loss: 0.031751388690915885
Epoch: 3 Idx: 0 Loss: 0.039397660744938584
Epoch: 3 Idx: 5000 Loss: 0.012135006946533109
Epoch: 4 Idx: 0 Loss: 0.03400843895392493
Epoch: 4 Idx: 5000 Loss: 0.01659219897993255
Epoch: 5 Idx: 0 Loss: 0.03019575554483452
Epoch: 5 Idx: 5000 Loss: 0.01240578997056213
Epoch: 6 Idx: 0 Loss: 0.039017582558819076
Epoch: 6 Idx: 5000 Loss: 0.02596579400851257
Epoch: 7 Idx: 0 Loss: 0.011133416262433953
Epoch: 7 Idx: 5000 Loss: 0.0267722184899957
Epoch: 8 Idx: 0 Loss: 0.022424402727575145
Epoch: 8 Idx: 5000 Loss: 0.027571296867508463
Epoch: 9 Idx: 0 Loss: 0.020119838786380732
Epoch: 9 Idx: 5000 Loss: 0.025971552357616976
Epoch: 10 Idx: 0 Loss: 0.023435346705277853
Epoch: 10 Idx: 5000 Loss: 0.018998441161104555
Epoch: 11 Idx: 0 Loss: 0.0225524000909591
Epoch: 11 Idx: 5000 Loss: 0.03745999324186749
Epoch: 12 Idx: 0 Loss: 0.0378174951444442
Epoch: 12 Idx: 5000 Loss: 0.02202615764029698
Epoch: 13 Idx: 0 Loss: 0.019627488465559614
Epoch: 13 Idx: 5000 Loss: 0.02497433334424178
Epoch: 14 Idx: 0 Loss: 0.04217542346858187
Epoch: 14 Idx: 5000 Loss: 0.014973981576100543
Epoch: 15 Idx: 0 Loss: 0.028690784464517574
Epoch: 15 Idx: 5000 Loss: 0.0220863782050654
Epoch: 16 Idx: 0 Loss: 0.01871467837573046
Epoch: 16 Idx: 5000 Loss: 0.012427098682941276
Epoch: 17 Idx: 0 Loss: 0.024694698199523683
Epoch: 17 Idx: 5000 Loss: 0.01776972903360758
Epoch: 18 Idx: 0 Loss: 0.026611251352961798
Epoch: 18 Idx: 5000 Loss: 0.018579526848207656
Epoch: 19 Idx: 0 Loss: 0.013833294219594776
Epoch: 19 Idx: 5000 Loss: 0.03106333978725628
Epoch: 20 Idx: 0 Loss: 0.02158697477879116
Epoch: 20 Idx: 5000 Loss: 0.03912230820887255
Epoch: 21 Idx: 0 Loss: 0.02386843966675198
Epoch: 21 Idx: 5000 Loss: 0.013421440938572449
Epoch: 22 Idx: 0 Loss: 0.00945189313892688
Epoch: 22 Idx: 5000 Loss: 0.03377541312698831
Epoch: 23 Idx: 0 Loss: 0.019263738687953745
Epoch: 23 Idx: 5000 Loss: 0.015277315236035384
Epoch: 24 Idx: 0 Loss: 0.013265314912367564
Epoch: 24 Idx: 5000 Loss: 0.02113889947355526
Epoch: 25 Idx: 0 Loss: 0.025935316649130437
Epoch: 25 Idx: 5000 Loss: 0.03413284551660995
Epoch: 26 Idx: 0 Loss: 0.03847492315157299
Epoch: 26 Idx: 5000 Loss: 0.024366904302148926
Epoch: 27 Idx: 0 Loss: 0.021207089934766263
Epoch: 27 Idx: 5000 Loss: 0.02854144699365549
Epoch: 28 Idx: 0 Loss: 0.013662301604194115
Epoch: 28 Idx: 5000 Loss: 0.03209616068366159
Epoch: 29 Idx: 0 Loss: 0.03154324911252446
Epoch: 29 Idx: 5000 Loss: 0.015207383698308014
Epoch: 30 Idx: 0 Loss: 0.016822716589281382
Epoch: 30 Idx: 5000 Loss: 0.01253262773047017
Epoch: 31 Idx: 0 Loss: 0.022512799709354395
Epoch: 31 Idx: 5000 Loss: 0.021824548176382536
Epoch: 32 Idx: 0 Loss: 0.01815323543152796
Epoch: 32 Idx: 5000 Loss: 0.018307724425761514
Epoch: 33 Idx: 0 Loss: 0.017396197477222958
Epoch: 33 Idx: 5000 Loss: 0.03642995390633537
Epoch: 34 Idx: 0 Loss: 0.008261821423267262
Epoch: 34 Idx: 5000 Loss: 0.01355695623790448
Epoch: 35 Idx: 0 Loss: 0.013842528499010354
Epoch: 35 Idx: 5000 Loss: 0.022098431983611846
Epoch: 36 Idx: 0 Loss: 0.016340080234920375
Epoch: 36 Idx: 5000 Loss: 0.019949251218072464
Epoch: 37 Idx: 0 Loss: 0.014654831723174392
Epoch: 37 Idx: 5000 Loss: 0.014924626600964636
Epoch: 38 Idx: 0 Loss: 0.038633039814000264
Epoch: 38 Idx: 5000 Loss: 0.029847940477833592
Epoch: 39 Idx: 0 Loss: 0.03376569091767506
Epoch: 39 Idx: 5000 Loss: 0.019672629268412643
Epoch: 40 Idx: 0 Loss: 0.026233011272707763
Epoch: 40 Idx: 5000 Loss: 0.021958865020009568
Epoch: 41 Idx: 0 Loss: 0.020159144766419043
Epoch: 41 Idx: 5000 Loss: 0.019760276801590863
Epoch: 42 Idx: 0 Loss: 0.020614355479668865
Epoch: 42 Idx: 5000 Loss: 0.028655559961981065
Epoch: 43 Idx: 0 Loss: 0.027163166568781136
Epoch: 43 Idx: 5000 Loss: 0.01079314197486331
Epoch: 44 Idx: 0 Loss: 0.02161794174499053
Epoch: 44 Idx: 5000 Loss: 0.028751542932762112
Epoch: 45 Idx: 0 Loss: 0.022582692214549588
Epoch: 45 Idx: 5000 Loss: 0.012020199456114983
Epoch: 46 Idx: 0 Loss: 0.026445321711555624
Epoch: 46 Idx: 5000 Loss: 0.016527244516600082
Epoch: 47 Idx: 0 Loss: 0.029584951688141015
Epoch: 47 Idx: 5000 Loss: 0.02616582835075581
Epoch: 48 Idx: 0 Loss: 0.010852088108213014
Epoch: 48 Idx: 5000 Loss: 0.010718050884416977
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml.py", line 314, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 314, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 312, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml.py", line 311, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc279>
Subject: Job 3289991: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 8 Output/test_anatomy_aml_bagofnbrs152_8.pkl Models/anatomy_aml_bagofnbrs152_8.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 8 Output/test_anatomy_aml_bagofnbrs152_8.pkl Models/anatomy_aml_bagofnbrs152_8.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:56 2020
Job was executed on host(s) <dccxc279>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 22:15:32 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 22:15:32 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 8 Output/test_anatomy_aml_bagofnbrs152_8.pkl Models/anatomy_aml_bagofnbrs152_8.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   50591.85 sec.
    Max Memory :                                 2678 MB
    Average Memory :                             2599.44 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40739.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   50597 sec.
    Turnaround time :                            80693 sec.

The output (if any) is above this job summary.

