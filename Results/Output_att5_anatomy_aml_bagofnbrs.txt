Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2702318803901369
Epoch: 0 Idx: 5000 Loss: 0.022213667852124266
Epoch: 1 Idx: 0 Loss: 0.019675960486567484
Epoch: 1 Idx: 5000 Loss: 0.018352310138649033
Epoch: 2 Idx: 0 Loss: 0.019335807063892162
Epoch: 2 Idx: 5000 Loss: 0.01476245310222751
Epoch: 3 Idx: 0 Loss: 0.010613864387782949
Epoch: 3 Idx: 5000 Loss: 0.024651835508734142
Epoch: 4 Idx: 0 Loss: 0.02324235991807462
Epoch: 4 Idx: 5000 Loss: 0.012999068785019469
Epoch: 5 Idx: 0 Loss: 0.026485497301791026
Epoch: 5 Idx: 5000 Loss: 0.03470808017834693
Epoch: 6 Idx: 0 Loss: 0.024504134895324554
Epoch: 6 Idx: 5000 Loss: 0.027703077292358456
Epoch: 7 Idx: 0 Loss: 0.01678019869267177
Epoch: 7 Idx: 5000 Loss: 0.0340196894586593
Epoch: 8 Idx: 0 Loss: 0.012413742501097677
Epoch: 8 Idx: 5000 Loss: 0.013423142744293438
Epoch: 9 Idx: 0 Loss: 0.024565375593678638
Epoch: 9 Idx: 5000 Loss: 0.024426308939558977
Epoch: 10 Idx: 0 Loss: 0.00726189837218484
Epoch: 10 Idx: 5000 Loss: 0.012197054025147626
Epoch: 11 Idx: 0 Loss: 0.028817788512908504
Epoch: 11 Idx: 5000 Loss: 0.012760322005538293
Epoch: 12 Idx: 0 Loss: 0.03322079352184051
Epoch: 12 Idx: 5000 Loss: 0.021080339920476216
Epoch: 13 Idx: 0 Loss: 0.04182746863060692
Epoch: 13 Idx: 5000 Loss: 0.031248138384149013
Epoch: 14 Idx: 0 Loss: 0.029868506902588382
Epoch: 14 Idx: 5000 Loss: 0.023225122276048584
Epoch: 15 Idx: 0 Loss: 0.0233162925803644
Epoch: 15 Idx: 5000 Loss: 0.020291879888406514
Epoch: 16 Idx: 0 Loss: 0.0384379547233486
Epoch: 16 Idx: 5000 Loss: 0.023970702853887253
Epoch: 17 Idx: 0 Loss: 0.02230106548496163
Epoch: 17 Idx: 5000 Loss: 0.016698129754953824
Epoch: 18 Idx: 0 Loss: 0.026978841678223044
Epoch: 18 Idx: 5000 Loss: 0.027637814731691954
Epoch: 19 Idx: 0 Loss: 0.03607539829158519
Epoch: 19 Idx: 5000 Loss: 0.020057532874118447
Epoch: 20 Idx: 0 Loss: 0.011225389559937432
Epoch: 20 Idx: 5000 Loss: 0.011333583903174084
Epoch: 21 Idx: 0 Loss: 0.013253123686584049
Epoch: 21 Idx: 5000 Loss: 0.01412012128416721
Epoch: 22 Idx: 0 Loss: 0.02104240867617193
Epoch: 22 Idx: 5000 Loss: 0.011714887818724685
Epoch: 23 Idx: 0 Loss: 0.011082393737760744
Epoch: 23 Idx: 5000 Loss: 0.027593477101027615
Epoch: 24 Idx: 0 Loss: 0.03509857553423158
Epoch: 24 Idx: 5000 Loss: 0.020894159093194763
Epoch: 25 Idx: 0 Loss: 0.022237832558521114
Epoch: 25 Idx: 5000 Loss: 0.020666072563213615
Epoch: 26 Idx: 0 Loss: 0.020951934605423266
Epoch: 26 Idx: 5000 Loss: 0.03700784049851929
Epoch: 27 Idx: 0 Loss: 0.016985122654663988
Epoch: 27 Idx: 5000 Loss: 0.0168523811751178
Epoch: 28 Idx: 0 Loss: 0.015561859248656152
Epoch: 28 Idx: 5000 Loss: 0.03492798830518298
Epoch: 29 Idx: 0 Loss: 0.011199329061734032
Epoch: 29 Idx: 5000 Loss: 0.021120522121281442
Epoch: 30 Idx: 0 Loss: 0.013832508739146439
Epoch: 30 Idx: 5000 Loss: 0.007217571368095392
Epoch: 31 Idx: 0 Loss: 0.034095048810648745
Epoch: 31 Idx: 5000 Loss: 0.02388952264934903
Epoch: 32 Idx: 0 Loss: 0.020943422472030508
Epoch: 32 Idx: 5000 Loss: 0.019065057947557217
Epoch: 33 Idx: 0 Loss: 0.01634068180434879
Epoch: 33 Idx: 5000 Loss: 0.01797740686318471
Epoch: 34 Idx: 0 Loss: 0.03318600262155457
Epoch: 34 Idx: 5000 Loss: 0.0261424720713737
Epoch: 35 Idx: 0 Loss: 0.02230679228634895
Epoch: 35 Idx: 5000 Loss: 0.014668380235402362
Epoch: 36 Idx: 0 Loss: 0.01921368928768876
Epoch: 36 Idx: 5000 Loss: 0.02462064755555802
Epoch: 37 Idx: 0 Loss: 0.020483842961895777
Epoch: 37 Idx: 5000 Loss: 0.024153224614212848
Epoch: 38 Idx: 0 Loss: 0.020465683950792547
Epoch: 38 Idx: 5000 Loss: 0.01635778940481012
Epoch: 39 Idx: 0 Loss: 0.03651862765561741
Epoch: 39 Idx: 5000 Loss: 0.04768252564107718
Epoch: 40 Idx: 0 Loss: 0.020792216478608273
Epoch: 40 Idx: 5000 Loss: 0.018383698370980694
Epoch: 41 Idx: 0 Loss: 0.05511553382402892
Epoch: 41 Idx: 5000 Loss: 0.020020683901592316
Epoch: 42 Idx: 0 Loss: 0.016506306378225612
Epoch: 42 Idx: 5000 Loss: 0.02730432467823344
Epoch: 43 Idx: 0 Loss: 0.015701282916611888
Epoch: 43 Idx: 5000 Loss: 0.014846411347391019
Epoch: 44 Idx: 0 Loss: 0.029571042136934032
Epoch: 44 Idx: 5000 Loss: 0.020886354805687776
Epoch: 45 Idx: 0 Loss: 0.024740451304681717
Epoch: 45 Idx: 5000 Loss: 0.019443603935140767
Epoch: 46 Idx: 0 Loss: 0.036254644325377346
Epoch: 46 Idx: 5000 Loss: 0.03322100995066725
Epoch: 47 Idx: 0 Loss: 0.019697615831945185
Epoch: 47 Idx: 5000 Loss: 0.028354183869265304
Epoch: 48 Idx: 0 Loss: 0.029461858506343395
Epoch: 48 Idx: 5000 Loss: 0.011867067724950067
Epoch: 49 Idx: 0 Loss: 0.030125508517248746
Epoch: 49 Idx: 5000 Loss: 0.016839284287102425
Len (direct inputs):  114
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17356659167036298
Epoch: 0 Idx: 5000 Loss: 0.028996264182261956
Epoch: 1 Idx: 0 Loss: 0.025278830933004
Epoch: 1 Idx: 5000 Loss: 0.024327687947505533
Epoch: 2 Idx: 0 Loss: 0.01867954069178219
Epoch: 2 Idx: 5000 Loss: 0.021795932970388003
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml.py", line 314, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 314, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 312, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml.py", line 311, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
0 Loss: 0.03383515754486581
Epoch: 12 Idx: 5000 Loss: 0.030705775084967718
Epoch: 13 Idx: 0 Loss: 0.01544432211905683
Epoch: 13 Idx: 5000 Loss: 0.017844225278846985
Epoch: 14 Idx: 0 Loss: 0.011101049131487236
Epoch: 14 Idx: 5000 Loss: 0.024258482470070616
Epoch: 15 Idx: 0 Loss: 0.01580936860883795
Epoch: 15 Idx: 5000 Loss: 0.03681866181511622
Epoch: 16 Idx: 0 Loss: 0.035735225712189206
Epoch: 16 Idx: 5000 Loss: 0.017823452706281752
Epoch: 17 Idx: 0 Loss: 0.030738493313164264
Epoch: 17 Idx: 5000 Loss: 0.04494537535686284
Epoch: 18 Idx: 0 Loss: 0.017021518337477386
Epoch: 18 Idx: 5000 Loss: 0.013202415217812492
Epoch: 19 Idx: 0 Loss: 0.04419075502362171
Epoch: 19 Idx: 5000 Loss: 0.029903426649733618
Epoch: 20 Idx: 0 Loss: 0.02237897949365075
Epoch: 20 Idx: 5000 Loss: 0.020827856690419942
Epoch: 21 Idx: 0 Loss: 0.025676227342406835
Epoch: 21 Idx: 5000 Loss: 0.027887637213797327
Epoch: 22 Idx: 0 Loss: 0.009119969201547628
Epoch: 22 Idx: 5000 Loss: 0.022055178277252428
Epoch: 23 Idx: 0 Loss: 0.017170063695139647
Epoch: 23 Idx: 5000 Loss: 0.017322577678821525
Epoch: 24 Idx: 0 Loss: 0.023172264395765585
Epoch: 24 Idx: 5000 Loss: 0.022376564633612983
Epoch: 25 Idx: 0 Loss: 0.028480465937866496
Epoch: 25 Idx: 5000 Loss: 0.01249565194100285
Epoch: 26 Idx: 0 Loss: 0.013096128624384935
Epoch: 26 Idx: 5000 Loss: 0.03623318318669624
Epoch: 27 Idx: 0 Loss: 0.030249430439774495
Epoch: 27 Idx: 5000 Loss: 0.013780781096096734
Epoch: 28 Idx: 0 Loss: 0.015273047926782727
Epoch: 28 Idx: 5000 Loss: 0.015683616218558717
Epoch: 29 Idx: 0 Loss: 0.019418060790917874
Epoch: 29 Idx: 5000 Loss: 0.020795610363421008
Epoch: 30 Idx: 0 Loss: 0.02205622930029246
Epoch: 30 Idx: 5000 Loss: 0.02382873696084235
Epoch: 31 Idx: 0 Loss: 0.027787869453009793
Epoch: 31 Idx: 5000 Loss: 0.015712703447902755
Epoch: 32 Idx: 0 Loss: 0.017331763975744166
Epoch: 32 Idx: 5000 Loss: 0.02550304972563918
Epoch: 33 Idx: 0 Loss: 0.03569015493021691
Epoch: 33 Idx: 5000 Loss: 0.027264720015977746
Epoch: 34 Idx: 0 Loss: 0.03610033381724358
Epoch: 34 Idx: 5000 Loss: 0.016322501604661702
Epoch: 35 Idx: 0 Loss: 0.04613785486810441
Epoch: 35 Idx: 5000 Loss: 0.011785760466535247
Epoch: 36 Idx: 0 Loss: 0.030215192233867923
Epoch: 36 Idx: 5000 Loss: 0.02888547075897365
Epoch: 37 Idx: 0 Loss: 0.022312137636444365
Epoch: 37 Idx: 5000 Loss: 0.026976376456960466
Epoch: 38 Idx: 0 Loss: 0.022291775683538965
Epoch: 38 Idx: 5000 Loss: 0.040390168922072624
Epoch: 39 Idx: 0 Loss: 0.039991579124910415
Epoch: 39 Idx: 5000 Loss: 0.014953408583616623
Epoch: 40 Idx: 0 Loss: 0.013346068529733567
Epoch: 40 Idx: 5000 Loss: 0.029816756697739956
Epoch: 41 Idx: 0 Loss: 0.018590789264760668
Epoch: 41 Idx: 5000 Loss: 0.010132528383847605
Epoch: 42 Idx: 0 Loss: 0.024914229830359326
Epoch: 42 Idx: 5000 Loss: 0.01997456696908824
Epoch: 43 Idx: 0 Loss: 0.011511576869354476
Epoch: 43 Idx: 5000 Loss: 0.016745797829721104
Epoch: 44 Idx: 0 Loss: 0.026714203265813127
Epoch: 44 Idx: 5000 Loss: 0.05046905335670789
Epoch: 45 Idx: 0 Loss: 0.018758822127777464
Epoch: 45 Idx: 5000 Loss: 0.016626343292307395
Epoch: 46 Idx: 0 Loss: 0.02117016186119736
Epoch: 46 Idx: 5000 Loss: 0.02725800842229218
Epoch: 47 Idx: 0 Loss: 0.009299118150529091
Epoch: 47 Idx: 5000 Loss: 0.03208485356632185
Epoch: 48 Idx: 0 Loss: 0.027817322820083867
Epoch: 48 Idx: 5000 Loss: 0.026486680377899086
Epoch: 49 Idx: 0 Loss: 0.018211861860989508
Epoch: 49 Idx: 5000 Loss: 0.023992144527285392
Len (direct inputs):  96
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1767132100529698
Epoch: 0 Idx: 5000 Loss: 0.017944621800019565
Epoch: 1 Idx: 0 Loss: 0.012092596098674668
Epoch: 1 Idx: 5000 Loss: 0.019116553361547915
Epoch: 2 Idx: 0 Loss: 0.013940603494527555
Epoch: 2 Idx: 5000 Loss: 0.02022909677754455
Epoch: 3 Idx: 0 Loss: 0.024023142336046823
Epoch: 3 Idx: 5000 Loss: 0.026427317546556674
Epoch: 4 Idx: 0 Loss: 0.021477539433868206
Epoch: 4 Idx: 5000 Loss: 0.023371670476481504
Epoch: 5 Idx: 0 Loss: 0.023843755953575275
Epoch: 5 Idx: 5000 Loss: 0.007926021244828354
Epoch: 6 Idx: 0 Loss: 0.0203757390423212
Epoch: 6 Idx: 5000 Loss: 0.023572414848745667
Epoch: 7 Idx: 0 Loss: 0.035336737317413706
Epoch: 7 Idx: 5000 Loss: 0.024976890974090324
Epoch: 8 Idx: 0 Loss: 0.03317497094170315
Epoch: 8 Idx: 5000 Loss: 0.021366182872253132
Epoch: 9 Idx: 0 Loss: 0.04385891438473828
Epoch: 9 Idx: 5000 Loss: 0.023574571183247423
Epoch: 10 Idx: 0 Loss: 0.022130481962600153
Epoch: 10 Idx: 5000 Loss: 0.016513987346032783
Epoch: 11 Idx: 0 Loss: 0.014914952477537137
Epoch: 11 Idx: 5000 Loss: 0.018332642418998298
Epoch: 12 Idx: 0 Loss: 0.024482470336357157
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
538408766
Epoch: 14 Idx: 5000 Loss: 0.021206601863852554
Epoch: 15 Idx: 0 Loss: 0.025299121329072034
Epoch: 15 Idx: 5000 Loss: 0.012369187746180777
Epoch: 16 Idx: 0 Loss: 0.016837947793821244
Epoch: 16 Idx: 5000 Loss: 0.012441079091302326
Epoch: 17 Idx: 0 Loss: 0.017843916963658127
Epoch: 17 Idx: 5000 Loss: 0.02104284344246721
Epoch: 18 Idx: 0 Loss: 0.02014201410945493
Epoch: 18 Idx: 5000 Loss: 0.012098066766681907
Epoch: 19 Idx: 0 Loss: 0.02103137621791553
Epoch: 19 Idx: 5000 Loss: 0.0216365572751704
Epoch: 20 Idx: 0 Loss: 0.03310847220673201
Epoch: 20 Idx: 5000 Loss: 0.02042486557933853
Epoch: 21 Idx: 0 Loss: 0.024932792704067158
Epoch: 21 Idx: 5000 Loss: 0.04273750734611996
Epoch: 22 Idx: 0 Loss: 0.011867006685731695
Epoch: 22 Idx: 5000 Loss: 0.02020962715369698
Epoch: 23 Idx: 0 Loss: 0.02248643427995154
Epoch: 23 Idx: 5000 Loss: 0.017117117106956188
Epoch: 24 Idx: 0 Loss: 0.003927966513471503
Epoch: 24 Idx: 5000 Loss: 0.010816886091889135
Epoch: 25 Idx: 0 Loss: 0.017453690185138367
Epoch: 25 Idx: 5000 Loss: 0.013822270703901452
Epoch: 26 Idx: 0 Loss: 0.025979794666350198
Epoch: 26 Idx: 5000 Loss: 0.04556441416379231
Epoch: 27 Idx: 0 Loss: 0.028924888017002236
Epoch: 27 Idx: 5000 Loss: 0.017179175519310048
Epoch: 28 Idx: 0 Loss: 0.015976720800754646
Epoch: 28 Idx: 5000 Loss: 0.01287275446641608
Epoch: 29 Idx: 0 Loss: 0.013161925354311474
Epoch: 29 Idx: 5000 Loss: 0.021169524001729702
Epoch: 30 Idx: 0 Loss: 0.021364204328002023
Epoch: 30 Idx: 5000 Loss: 0.014697226739782025
Epoch: 31 Idx: 0 Loss: 0.0228434105443261
Epoch: 31 Idx: 5000 Loss: 0.017632196773839877
Epoch: 32 Idx: 0 Loss: 0.02861828109086449
Epoch: 32 Idx: 5000 Loss: 0.03900289332621494
Epoch: 33 Idx: 0 Loss: 0.013759908579591284
Epoch: 33 Idx: 5000 Loss: 0.014142099157499453
Epoch: 34 Idx: 0 Loss: 0.02570030578069129
Epoch: 34 Idx: 5000 Loss: 0.03288074526358377
Epoch: 35 Idx: 0 Loss: 0.024401237919197398
Epoch: 35 Idx: 5000 Loss: 0.023643049244636095
Epoch: 36 Idx: 0 Loss: 0.02107154976596444
Epoch: 36 Idx: 5000 Loss: 0.026949989738201036
Epoch: 37 Idx: 0 Loss: 0.014440268617189729
Epoch: 37 Idx: 5000 Loss: 0.04262941860562361
Epoch: 38 Idx: 0 Loss: 0.02157125746015052
Epoch: 38 Idx: 5000 Loss: 0.014463895782397172
Epoch: 39 Idx: 0 Loss: 0.022337649747555494
Epoch: 39 Idx: 5000 Loss: 0.021749487552475046
Epoch: 40 Idx: 0 Loss: 0.02675725746312304
Epoch: 40 Idx: 5000 Loss: 0.01834456111030804
Epoch: 41 Idx: 0 Loss: 0.029729869008555863
Epoch: 41 Idx: 5000 Loss: 0.018430516435725727
Epoch: 42 Idx: 0 Loss: 0.034844819460488036
Epoch: 42 Idx: 5000 Loss: 0.04692505950307067
Epoch: 43 Idx: 0 Loss: 0.019512178635025135
Epoch: 43 Idx: 5000 Loss: 0.037768972668473316
Epoch: 44 Idx: 0 Loss: 0.020202100767930917
Epoch: 44 Idx: 5000 Loss: 0.025479930908214245
Epoch: 45 Idx: 0 Loss: 0.011905073324844345
Epoch: 45 Idx: 5000 Loss: 0.017430079438953132
Epoch: 46 Idx: 0 Loss: 0.016894475290276108
Epoch: 46 Idx: 5000 Loss: 0.015962870609328814
Epoch: 47 Idx: 0 Loss: 0.024124695652762387
Epoch: 47 Idx: 5000 Loss: 0.04199886546799962
Epoch: 48 Idx: 0 Loss: 0.009037932784722752
Epoch: 48 Idx: 5000 Loss: 0.020371149051974922
Epoch: 49 Idx: 0 Loss: 0.010098978647287347
Epoch: 49 Idx: 5000 Loss: 0.012941081355518575
Len (direct inputs):  89
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by Training size: 127500 Validation size: Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19561548273711227
Epoch: 0 Idx: 5000 Loss: 0.017813299764948234
Epoch: 1 Idx: 0 Loss: 0.01204798266400289
Epoch: 1 Idx: 5000 Loss: 0.015664175099923593
Epoch: 2 Idx: 0 Loss: 0.021720422863289544
Epoch: 2 Idx: 5000 Loss: 0.018472607368511765
Epoch: 3 Idx: 0 Loss: 0.01670082411796423
Epoch: 3 Idx: 5000 Loss: 0.028058129482245626
Epoch: 4 Idx: 0 Loss: 0.016951768685906796
Epoch: 4 Idx: 5000 Loss: 0.017386740996187193
Epoch: 5 Idx: 0 Loss: 0.01998648543466731
Epoch: 5 Idx: 5000 Loss: 0.014593374762736285
Epoch: 6 Idx: 0 Loss: 0.015095876196548985
Epoch: 6 Idx: 5000 Loss: 0.01863089897033378
Epoch: 7 Idx: 0 Loss: 0.021527692297440615
Epoch: 7 Idx: 5000 Loss: 0.022584316733196545
Epoch: 8 Idx: 0 Loss: 0.033440419370589516
Epoch: 8 Idx: 5000 Loss: 0.022107773997726725
Epoch: 9 Idx: 0 Loss: 0.03573500393387856
Epoch: 9 Idx: 5000 Loss: 0.010737224395687351
Epoch: 10 Idx: 0 Loss: 0.02673294913746472
Epoch: 10 Idx: 5000 Loss: 0.016610463073849856
Epoch: 11 Idx: 0 Loss: 0.017866224246199717
Epoch: 11 Idx: 5000 Loss: 0.019130402145933804
Epoch: 12 Idx: 0 Loss: 0.018438421756342246
Epoch: 12 Idx: 5000 Loss: 0.03475455934180517
Epoch: 13 Idx: 0 Loss: 0.03434328704235678
Epoch: 13 Idx: 5000 Loss: 0.02835769474994146
Epoch: 14 Idx: 0 Loss: 0.011966477694295072
Epoch: 14 Idx: 5000 Loss: 0.02308941206940435
Epoch: 15 Idx: 0 Loss: 0.036919777476676026
Epoch: 15 Idx: 5000 Loss: 0.030453379760463715
Epoch: 16 Idx: 0 Loss: 0.025381591247529825
Epoch: 16 Idx: 5000 Loss: 0.01031770606473372
Epoch: 17 Idx: 0 Loss: 0.01888922856967266
Epoch: 17 Idx: 5000 Loss: 0.02386063209017355
Epoch: 18 Idx: 0 Loss: 0.0170234279133723
Epoch: 18 Idx: 5000 Loss: 0.019411109931469318
Epoch: 19 Idx: 0 Loss: 0.04013874052787893
Epoch: 19 Idx: 5000 Loss: 0.02286308303322209
Epoch: 20 Idx: 0 Loss: 0.009330988046471077
Epoch: 20 Idx: 5000 Loss: 0.016897059861587523
Epoch: 21 Idx: 0 Loss: 0.016801365389188472
Epoch: 21 Idx: 5000 Loss: 0.04229473464628603
Epoch: 22 Idx: 0 Loss: 0.02654539385961479
Epoch: 22 Idx: 5000 Loss: 0.023543853617097914
Epoch: 23 Idx: 0 Loss: 0.026614006872036966
Epoch: 23 Idx: 5000 Loss: 0.02110305355717277
Epoch: 24 Idx: 0 Loss: 0.018569918304308063
Epoch: 24 Idx: 5000 Loss: 0.021975468461511813
Epoch: 25 Idx: 0 Loss: 0.024901455798705063
Epoch: 25 Idx: 5000 Loss: 0.01817553422069995
Epoch: 26 Idx: 0 Loss: 0.014151291027965519
Epoch: 26 Idx: 5000 Loss: 0.025282643452789542
Epoch: 27 Idx: 0 Loss: 0.011763954494045267
Epoch: 27 Idx: 5000 Loss: 0.045572189521802434
Epoch: 28 Idx: 0 Loss: 0.03783270375856229
Epoch: 28 Idx: 5000 Loss: 0.017844979025087297
Epoch: 29 Idx: 0 Loss: 0.031415635542330886
Epoch: 29 Idx: 5000 Loss: 0.0191961291736769
Epoch: 30 Idx: 0 Loss: 0.03205827313682075
Epoch: 30 Idx: 5000 Loss: 0.030726641263339075
Epoch: 31 Idx: 0 Loss: 0.012373432555692204
Epoch: 31 Idx: 5000 Loss: 0.020128414653119757
Epoch: 32 Idx: 0 Loss: 0.031637399782053044
Epoch: 32 Idx: 5000 Loss: 0.025058396804058594
Epoch: 33 Idx: 0 Loss: 0.031098782512554407
Epoch: 33 Idx: 5000 Loss: 0.01217314082046794
Epoch: 34 Idx: 0 Loss: 0.021971669418216787
Epoch: 34 Idx: 5000 Loss: 0.022342749424829757
Epoch: 35 Idx: 0 Loss: 0.018012699310890782
Epoch: 35 Idx: 5000 Loss: 0.01509631024600988
Epoch: 36 Idx: 0 Loss: 0.014413408183046393
Epoch: 36 Idx: 5000 Loss: 0.017638711279887816
Epoch: 37 Idx: 0 Loss: 0.029331546507718798
Epoch: 37 Idx: 5000 Loss: 0.00887292221839169
Epoch: 38 Idx: 0 Loss: 0.019730091261398165
Epoch: 38 Idx: 5000 Loss: 0.025685967347865577
Epoch: 39 Idx: 0 Loss: 0.01822459929216095
Epoch: 39 Idx: 5000 Loss: 0.018867247857574182
Epoch: 40 Idx: 0 Loss: 0.016872442280007203
Epoch: 40 Idx: 5000 Loss: 0.020782205698778894
Epoch: 41 Idx: 0 Loss: 0.030260515243903587
Epoch: 41 Idx: 5000 Loss: 0.02957916143023812
Epoch: 42 Idx: 0 Loss: 0.013559310280801604
Epoch: 42 Idx: 5000 Loss: 0.010795183981110214
Epoch: 43 Idx: 0 Loss: 0.026453379082414633
Epoch: 43 Idx: 5000 Loss: 0.030751781172310266
Epoch: 44 Idx: 0 Loss: 0.014262331959878378
Epoch: 44 Idx: 5000 Loss: 0.02085135630315472
Epoch: 45 Idx: 0 Loss: 0.026006273109853845
Epoch: 45 Idx: 5000 Loss: 0.036026259161677336
Epoch: 46 Idx: 0 Loss: 0.022079121114559355
Epoch: 46 Idx: 5000 Loss: 0.023974159617386054
Epoch: 47 Idx: 0 Loss: 0.02507157590435692
Epoch: 47 Idx: 5000 Loss: 0.01650251758029294
Epoch: 48 Idx: 0 Loss: 0.012184623705405502
Epoch: 48 Idx: 5000 Loss: 0.022981210587137302
Epoch: 49 Idx: 0 Loss: 0.022083515740803625
Epoch: 49 Idx: 5000 Loss: 0.043333118309553
Len (direct inputs):  105
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Terminated
: 0 Loss: 0.1595620876136854
Epoch: 0 Idx: 5000 Loss: 0.030392813897831394
Epoch: 1 Idx: 0 Loss: 0.033756176604881144
Epoch: 1 Idx: 5000 Loss: 0.018945418895376598
Epoch: 2 Idx: 0 Loss: 0.02764561637110554
Epoch: 2 Idx: 5000 Loss: 0.014789988706419368
Epoch: 3 Idx: 0 Loss: 0.020936497117278914
Epoch: 3 Idx: 5000 Loss: 0.023169788799504577
Epoch: 4 Idx: 0 Loss: 0.03126370246938942
Epoch: 4 Idx: 5000 Loss: 0.02292771517893589
Epoch: 5 Idx: 0 Loss: 0.014925548637720675
Epoch: 5 Idx: 5000 Loss: 0.0148675455448807
Epoch: 6 Idx: 0 Loss: 0.01845340888464434
Epoch: 6 Idx: 5000 Loss: 0.03663740895044547
Epoch: 7 Idx: 0 Loss: 0.01507907415412554
Epoch: 7 Idx: 5000 Loss: 0.019505173295114035
Epoch: 8 Idx: 0 Loss: 0.016485685578062383
Epoch: 8 Idx: 5000 Loss: 0.0161974499219052
Epoch: 9 Idx: 0 Loss: 0.012838263980290265
Epoch: 9 Idx: 5000 Loss: 0.01236626460192617
Epoch: 10 Idx: 0 Loss: 0.02451093113823139
Epoch: 10 Idx: 5000 Loss: 0.03355585570893422
Epoch: 11 Idx: 0 Loss: 0.0196944095976581
Epoch: 11 Idx: 5000 Loss: 0.025661317388695523
Epoch: 12 Idx: 0 Loss: 0.016693688444866947
Epoch: 12 Idx: 5000 Loss: 0.03871944551385263
Epoch: 13 Idx: 0 Loss: 0.016661486072084404
Epoch: 13 Idx: 5000 Loss: 0.02512433139144371
Epoch: 14 Idx: 0 Loss: 0.02354449801330276
Epoch: 14 Idx: 5000 Loss: 0.022851677280575508
Epoch: 15 Idx: 0 Loss: 0.02838086882863706
Epoch: 15 Idx: 5000 Loss: 0.01308966498716129
Epoch: 16 Idx: 0 Loss: 0.018572806825370082
Epoch: 16 Idx: 5000 Loss: 0.03308374651177041
Epoch: 17 Idx: 0 Loss: 0.02627673866250848
Epoch: 17 Idx: 5000 Loss: 0.0167964926139152
Epoch: 18 Idx: 0 Loss: 0.010312684183406275
Epoch: 18 Idx: 5000 Loss: 0.01576124617531567
Epoch: 19 Idx: 0 Loss: 0.013843019052225337
Epoch: 19 Idx: 5000 Loss: 0.010775746473506537
Epoch: 20 Idx: 0 Loss: 0.02553942802954305
Epoch: 20 Idx: 5000 Loss: 0.020610167611533402
Epoch: 21 Idx: 0 Loss: 0.023634189251502076
Epoch: 21 Idx: 5000 Loss: 0.020362922804453773
Epoch: 22 Idx: 0 Loss: 0.022297672018640587
Epoch: 22 Idx: 5000 Loss: 0.026704165761084822
Epoch: 23 Idx: 0 Loss: 0.03195064889012091
Epoch: 23 Idx: 5000 Loss: 0.009918750690800456
Epoch: 24 Idx: 0 Loss: 0.014661571134467267
Epoch: 24 Idx: 5000 Loss: 0.022578177175707058
Epoch: 25 Idx: 0 Loss: 0.010238224938923651
Epoch: 25 Idx: 5000 Loss: 0.0205226359588877
Epoch: 26 Idx: 0 Loss: 0.011309969223916444
Epoch: 26 Idx: 5000 Loss: 0.022463713910419594
Epoch: 27 Idx: 0 Loss: 0.021833683418638273
Epoch: 27 Idx: 5000 Loss: 0.03907655523357121
Epoch: 28 Idx: 0 Loss: 0.018715459239633442
Epoch: 28 Idx: 5000 Loss: 0.03307188698856378
Epoch: 29 Idx: 0 Loss: 0.022387352794004157
Epoch: 29 Idx: 5000 Loss: 0.013784618933537872
Epoch: 30 Idx: 0 Loss: 0.00843191882315026
Epoch: 30 Idx: 5000 Loss: 0.025060160607101804
Epoch: 31 Idx: 0 Loss: 0.010649530499731424
Epoch: 31 Idx: 5000 Loss: 0.014126140981023199
Epoch: 32 Idx: 0 Loss: 0.021909884234136694
Epoch: 32 Idx: 5000 Loss: 0.01922811487185725
Epoch: 33 Idx: 0 Loss: 0.019473941525859954
Epoch: 33 Idx: 5000 Loss: 0.01443103222793058
Epoch: 34 Idx: 0 Loss: 0.03244188185032047
Epoch: 34 Idx: 5000 Loss: 0.01937502175512728
Epoch: 35 Idx: 0 Loss: 0.012722023956536039
Epoch: 35 Idx: 5000 Loss: 0.026212148603631566
Epoch: 36 Idx: 0 Loss: 0.01836207561427717
Epoch: 36 Idx: 5000 Loss: 0.014314003903218367
Epoch: 37 Idx: 0 Loss: 0.017332877466130762
Epoch: 37 Idx: 5000 Loss: 0.015296300760425827
Epoch: 38 Idx: 0 Loss: 0.010264280975527656
Epoch: 38 Idx: 5000 Loss: 0.02318353607473763
Epoch: 39 Idx: 0 Loss: 0.020402437968423663
Epoch: 39 Idx: 5000 Loss: 0.014419231782334467
Epoch: 40 Idx: 0 Loss: 0.030771581845180675
Epoch: 40 Idx: 5000 Loss: 0.030371000920139307
Epoch: 41 Idx: 0 Loss: 0.030079493389663967
Epoch: 41 Idx: 5000 Loss: 0.008329563590277759
Epoch: 42 Idx: 0 Loss: 0.02271018449263551
Epoch: 42 Idx: 5000 Loss: 0.01591998463647277
Epoch: 43 Idx: 0 Loss: 0.02525504705083302
Epoch: 43 Idx: 5000 Loss: 0.009522680379131789
Epoch: 44 Idx: 0 Loss: 0.02860096336633877
Epoch: 44 Idx: 5000 Loss: 0.030475538603639912
Epoch: 45 Idx: 0 Loss: 0.013369744952546295
Epoch: 45 Idx: 5000 Loss: 0.035855626947444444
Epoch: 46 Idx: 0 Loss: 0.01849560749739859
Epoch: 46 Idx: 5000 Loss: 0.01490080694535623
Epoch: 47 Idx: 0 Loss: 0.02443294442190864
Epoch: 47 Idx: 5000 Loss: 0.013233928374004809
Epoch: 48 Idx: 0 Loss: 0.016905853182611577
Epoch: 48 Idx: 5000 Loss: 0.01229461793058571
Epoch: 49 Idx: 0 Loss: 0.014334356945268228
Epoch: 49 Idx: 5000 Loss: 0.02090470566512581
Len (direct inputs):  102
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.24301327859569244
Epoch: 0 Idx: 5000 Loss: 0.015958495729317575
Epoch: 1 Idx: 0 Loss: 0.025929987689756552
Epoch: 1 Idx: 5000 Loss: 0.02330026922115316
Epoch: 2 Idx: 0 Loss: 0.010332404185139863
Epoch: 2 Idx: 5000 Loss: 0.026277794053831592
Epoch: 3 Idx: 0 Loss: 0.025756663840970116
Epoch: 3 Idx: 5000 Loss: 0.018341715134426277
Epoch: 4 Idx: 0 Loss: 0.019583779571292953
Epoch: 4 Idx: 5000 Loss: 0.019808734222319997
Epoch: 5 Idx: 0 Loss: 0.016803228327019627
Epoch: 5 Idx: 5000 Loss: 0.02709569237117597
Epoch: 6 Idx: 0 Loss: 0.013735223631219144
Epoch: 6 Idx: 5000 Loss: 0.018437116787151916
Epoch: 7 Idx: 0 Loss: 0.030176162356114096
Epoch: 7 Idx: 5000 Loss: 0.013167800781701423
Epoch: 8 Idx: 0 Loss: 0.04181962404365587
Epoch: 8 Idx: 5000 Loss: 0.02554740593144518
Epoch: 9 Idx: 0 Loss: 0.026635862927153166
Epoch: 9 Idx: 5000 Loss: 0.029886043226213174
Epoch: 10 Idx: 0 Loss: 0.008006417743096218
Epoch: 10 Idx: 5000 Loss: 0.020489568151258478
Epoch: 11 Idx: 0 Loss: 0.02736260645995055
Epoch: 11 Idx: 5000 Loss: 0.020056459608898944
Epoch: 12 Idx: 0 Loss: 0.028411537588901414
Epoch: 12 Idx: 5000 Loss: 0.018859794091147526
Epoch: 13 Idx: 0 Loss: 0.024346646859682214
Epoch: 13 Idx: 5000 Loss: 0.01725737200140179
Epoch: 14 Idx: 0 Loss: 0.028452223606012565
Epoch: 14 Idx: 5000 Loss: 0.020570881345254956
Epoch: 15 Idx: 0 Loss: 0.010982874102140515
Epoch: 15 Idx: 5000 Loss: 0.04591249042500829
Epoch: 16 Idx: 0 Loss: 0.012395874038292082
Epoch: 16 Idx: 5000 Loss: 0.015251860704174733
Epoch: 17 Idx: 0 Loss: 0.018074274742729393
Epoch: 17 Idx: 5000 Loss: 0.02883947985934242
Epoch: 18 Idx: 0 Loss: 0.019995585773758203
Epoch: 18 Idx: 5000 Loss: 0.019533188267899335
Epoch: 19 Idx: 0 Loss: 0.016529513169473146
Epoch: 19 Idx: 5000 Loss: 0.015476067425569371
Epoch: 20 Idx: 0 Loss: 0.03411184841241249
Epoch: 20 Idx: 5000 Loss: 0.046810254532105806
Epoch: 21 Idx: 0 Loss: 0.021087683642491674
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 396, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "Attention_anatomy_aml.py", line 273, in forward
    node_weights = masked_softmax(node_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_pathlen)) # batch_size * 4 * max_pathlen
  File "Attention_anatomy_aml.py", line 218, in masked_softmax
    inp = inp.double()
KeyboardInterrupt
x: 5000 Loss: 0.025506210753718166
Epoch: 28 Idx: 0 Loss: 0.030695938018699766
Epoch: 28 Idx: 5000 Loss: 0.02299860027323873
Epoch: 29 Idx: 0 Loss: 0.028486198500104993
Epoch: 29 Idx: 5000 Loss: 0.013393274779521114
Epoch: 30 Idx: 0 Loss: 0.010512576024254964
Epoch: 30 Idx: 5000 Loss: 0.015361568552564427
Epoch: 31 Idx: 0 Loss: 0.009521783304593627
Epoch: 31 Idx: 5000 Loss: 0.028612053954205647
Epoch: 32 Idx: 0 Loss: 0.022187440388071352
Epoch: 32 Idx: 5000 Loss: 0.006858759842944352
Epoch: 33 Idx: 0 Loss: 0.02904108278124312
Epoch: 33 Idx: 5000 Loss: 0.019775103512164865
Epoch: 34 Idx: 0 Loss: 0.03459314320395096
Epoch: 34 Idx: 5000 Loss: 0.015111359825364068
Epoch: 35 Idx: 0 Loss: 0.01936593688080049
Epoch: 35 Idx: 5000 Loss: 0.02897051528626817
Epoch: 36 Idx: 0 Loss: 0.012478969864174776
Epoch: 36 Idx: 5000 Loss: 0.018164263871291254
Epoch: 37 Idx: 0 Loss: 0.020918203267762878
Epoch: 37 Idx: 5000 Loss: 0.012007501861004669
Epoch: 38 Idx: 0 Loss: 0.015230625185134648
Epoch: 38 Idx: 5000 Loss: 0.023507885033248868
Epoch: 39 Idx: 0 Loss: 0.02238403713669665
Epoch: 39 Idx: 5000 Loss: 0.02557707239697603
Epoch: 40 Idx: 0 Loss: 0.03610052250826985
Epoch: 40 Idx: 5000 Loss: 0.01884787106339402
Epoch: 41 Idx: 0 Loss: 0.014513742599960567
Epoch: 41 Idx: 5000 Loss: 0.03621687150480683
Epoch: 42 Idx: 0 Loss: 0.016872767035929173
Epoch: 42 Idx: 5000 Loss: 0.024902394959559628
Epoch: 43 Idx: 0 Loss: 0.040824879018366164
Epoch: 43 Idx: 5000 Loss: 0.01556726546320268
Epoch: 44 Idx: 0 Loss: 0.04168209890877682
Epoch: 44 Idx: 5000 Loss: 0.015227910875294601
Epoch: 45 Idx: 0 Loss: 0.012304711664644134
Epoch: 45 Idx: 5000 Loss: 0.02017082435101345
Epoch: 46 Idx: 0 Loss: 0.020613711639681113
Epoch: 46 Idx: 5000 Loss: 0.025020384031189113
Epoch: 47 Idx: 0 Loss: 0.017372035732617353
Epoch: 47 Idx: 5000 Loss: 0.013993250585830739
Epoch: 48 Idx: 0 Loss: 0.0183610857798086
Epoch: 48 Idx: 5000 Loss: 0.03352653305269587
Epoch: 49 Idx: 0 Loss: 0.017274777343661223
Epoch: 49 Idx: 5000 Loss: 0.023026209636212838
Len (direct inputs):  100
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division b0.828140.8003085336542344
Parameter containing:
tensor([0.8003], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.16890203937411247
Epoch: 0 Idx: 5000 Loss: 0.014135620027775488
Epoch: 1 Idx: 0 Loss: 0.013483667872377623
Epoch: 1 Idx: 5000 Loss: 0.01349763536849778
Epoch: 2 Idx: 0 Loss: 0.026157474160899433
Epoch: 2 Idx: 5000 Loss: 0.023812562811454094
Epoch: 3 Idx: 0 Loss: 0.016066905858123044
Epoch: 3 Idx: 5000 Loss: 0.016825411005489115
Epoch: 4 Idx: 0 Loss: 0.026797499371430104
Epoch: 4 Idx: 5000 Loss: 0.028680288877808054
Epoch: 5 Idx: 0 Loss: 0.02675326474199582
Epoch: 5 Idx: 5000 Loss: 0.035441749636344694
Epoch: 6 Idx: 0 Loss: 0.012160206627814458
Epoch: 6 Idx: 5000 Loss: 0.012261841239802767
Epoch: 7 Idx: 0 Loss: 0.01948637945844485
Epoch: 7 Idx: 5000 Loss: 0.03720800184122437
Epoch: 8 Idx: 0 Loss: 0.049405109124173216
Epoch: 8 Idx: 5000 Loss: 0.014157805645507974
Epoch: 9 Idx: 0 Loss: 0.03154591997726593
Epoch: 9 Idx: 5000 Loss: 0.02213835512214555
Epoch: 10 Idx: 0 Loss: 0.0105513261975741
Epoch: 10 Idx: 5000 Loss: 0.01956026641188276
Epoch: 11 Idx: 0 Loss: 0.019637364108320548
Epoch: 11 Idx: 5000 Loss: 0.015717732428185402
Epoch: 12 Idx: 0 Loss: 0.03231763177031955
Epoch: 12 Idx: 5000 Loss: 0.014778325638922627
Epoch: 13 Idx: 0 Loss: 0.007894693962217343
Epoch: 13 Idx: 5000 Loss: 0.015467218182557009
Epoch: 14 Idx: 0 Loss: 0.022170936431510753
Epoch: 14 Idx: 5000 Loss: 0.02824474437609218
Epoch: 15 Idx: 0 Loss: 0.019847391010463372
Epoch: 15 Idx: 5000 Loss: 0.015750250140112373
Epoch: 16 Idx: 0 Loss: 0.018871406718123533
Epoch: 16 Idx: 5000 Loss: 0.03415808268237917
Epoch: 17 Idx: 0 Loss: 0.038473155040073224
Epoch: 17 Idx: 5000 Loss: 0.01507612738210315
Epoch: 18 Idx: 0 Loss: 0.03169761893202144
Epoch: 18 Idx: 5000 Loss: 0.02847830781013387
Epoch: 19 Idx: 0 Loss: 0.026231396836033455
Epoch: 19 Idx: 5000 Loss: 0.024122054603417528
Epoch: 20 Idx: 0 Loss: 0.010464744186558555
Epoch: 20 Idx: 5000 Loss: 0.0145863847152695
Epoch: 21 Idx: 0 Loss: 0.0188883798760868
Epoch: 21 Idx: 5000 Loss: 0.018001829219140026
Epoch: 22 Idx: 0 Loss: 0.017829991075766514
Epoch: 22 Idx: 5000 Loss: 0.03352776835637229
Epoch: 23 Idx: 0 Loss: 0.01589201929816857
Epoch: 23 Idx: 5000 Loss: 0.023192349939812913
Epoch: 24 Idx: 0 Loss: 0.017309463153095637
Epoch: 24 Idx: 5000 Loss: 0.02698036384930448
Epoch: 25 Idx: 0 Loss: 0.031006576305365564
Epoch: 25 Idx: 5000 Loss: 0.018837657043214464
Epoch: 26 Idx: 0 Loss: 0.02912066667536744
Epoch: 26 Idx: 5000 Loss: 0.02267683666838872
Epoch: 27 Idx: 0 Loss: 0.043999468178454224
Epoch: 27 Idx: 5000 Loss: 0.02122348096432969
Epoch: 28 Idx: 0 Loss: 0.023595230076477777
Epoch: 28 Idx: 5000 Loss: 0.024445677088483574
Epoch: 29 Idx: 0 Loss: 0.00910968352825339
Epoch: 29 Idx: 5000 Loss: 0.014701106471712692
Epoch: 30 Idx: 0 Loss: 0.01194226059725604
Epoch: 30 Idx: 5000 Loss: 0.03639498326879577
Epoch: 31 Idx: 0 Loss: 0.014937738329981033
Epoch: 31 Idx: 5000 Loss: 0.012631138881365266
Epoch: 32 Idx: 0 Loss: 0.025752600845140593
Epoch: 32 Idx: 5000 Loss: 0.01564777046936351
Epoch: 33 Idx: 0 Loss: 0.022489398537080776
Epoch: 33 Idx: 5000 Loss: 0.04143149288521947
Epoch: 34 Idx: 0 Loss: 0.026162883425676128
Epoch: 34 Idx: 5000 Loss: 0.013734346438579598
Epoch: 35 Idx: 0 Loss: 0.007897829793719294
Epoch: 35 Idx: 5000 Loss: 0.011707626780519193
Epoch: 36 Idx: 0 Loss: 0.022312043543383044
Epoch: 36 Idx: 5000 Loss: 0.0417881785595747
Epoch: 37 Idx: 0 Loss: 0.02098370590547519
Epoch: 37 Idx: 5000 Loss: 0.014744804719908595
Epoch: 38 Idx: 0 Loss: 0.012709083266361582
Epoch: 38 Idx: 5000 Loss: 0.04054688532422898
Epoch: 39 Idx: 0 Loss: 0.01992185240997331
Epoch: 39 Idx: 5000 Loss: 0.024805651063346222
Epoch: 40 Idx: 0 Loss: 0.02565635403735425
Epoch: 40 Idx: 5000 Loss: 0.01913781644258363
Epoch: 41 Idx: 0 Loss: 0.016146774548056048
Epoch: 41 Idx: 5000 Loss: 0.03331104505396082
Epoch: 42 Idx: 0 Loss: 0.02481480171839758
Epoch: 42 Idx: 5000 Loss: 0.027780463495413563
Epoch: 43 Idx: 0 Loss: 0.03762606843243122
Epoch: 43 Idx: 5000 Loss: 0.030555586039121525
Epoch: 44 Idx: 0 Loss: 0.00748124367075592
Epoch: 44 Idx: 5000 Loss: 0.01063826388875334
Epoch: 45 Idx: 0 Loss: 0.029726543204168142
Epoch: 45 Idx: 5000 Loss: 0.016949800303976015
Epoch: 46 Idx: 0 Loss: 0.02361440801008277
Epoch: 46 Idx: 5000 Loss: 0.018811984661694682
Epoch: 47 Idx: 0 Loss: 0.037013062308861255
Epoch: 47 Idx: 5000 Loss: 0.014372136300945821
Epoch: 48 Idx: 0 Loss: 0.019684474568211062
Epoch: 48 Idx: 5000 Loss: 0.013424579025720536
Epoch: 49 Idx: 0 Loss: 0.015658312332488083
Epoch: 49 Idx: 5000 Loss: 0.0322410761337747
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 475, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 24 failed with Disk quota exceeded
a exceeded

s used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:57:45 2020
Terminated at Tue Sep  1 20:18:37 2020
Results reported at Tue Sep  1 20:18:37 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs3_5.pkl Models/anatomy_aml_bagofnbrs3_5.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22823.20 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2717.66 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22852 sec.
    Turnaround time :                            23085 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc213>
Subject: Job 3289915: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs2_5.pkl Models/anatomy_aml_bagofnbrs2_5.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs2_5.pkl Models/anatomy_aml_bagofnbrs2_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc213>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Tue Sep  1 20:23:49 2020
Results reported at Tue Sep  1 20:23:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs2_5.pkl Models/anatomy_aml_bagofnbrs2_5.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23161.00 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2722.06 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23221 sec.
    Turnaround time :                            23397 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 3289919: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs4_5.pkl Models/anatomy_aml_bagofnbrs4_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs4_5.pkl Models/anatomy_aml_bagofnbrs4_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:58:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:58:45 2020
Terminated at Tue Sep  1 20:59:23 2020
Results reported at Tue Sep  1 20:59:23 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs4_5.pkl Models/anatomy_aml_bagofnbrs4_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25229.92 sec.
    Max Memory :                                 2922 MB
    Average Memory :                             2717.41 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40495.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25238 sec.
    Turnaround time :                            25530 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc271>
Subject: Job 3289921: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs5_5.pkl Models/anatomy_aml_bagofnbrs5_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs5_5.pkl Models/anatomy_aml_bagofnbrs5_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc271>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:58:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:58:45 2020
Terminated at Tue Sep  1 21:31:25 2020
Results reported at Tue Sep  1 21:31:25 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs5_5.pkl Models/anatomy_aml_bagofnbrs5_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27124.00 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2726.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27160 sec.
    Turnaround time :                            27452 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289925: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 5 Output/test_anatomy_aml_bagofnbrs8_5.pkl Models/anatomy_aml_bagofnbrs8_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 5 Output/test_anatomy_aml_bagofnbrs8_5.pkl Models/anatomy_aml_bagofnbrs8_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:05:28 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:05:28 2020
Terminated at Tue Sep  1 23:05:04 2020
Results reported at Tue Sep  1 23:05:04 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 5 Output/test_anatomy_aml_bagofnbrs8_5.pkl Models/anatomy_aml_bagofnbrs8_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32366.17 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2722.71 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32376 sec.
    Turnaround time :                            33071 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc242>
Subject: Job 3289923: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs7_5.pkl Models/anatomy_aml_bagofnbrs7_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs7_5.pkl Models/anatomy_aml_bagofnbrs7_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc242>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:04:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:04:47 2020
Terminated at Tue Sep  1 23:09:09 2020
Results reported at Tue Sep  1 23:09:09 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs7_5.pkl Models/anatomy_aml_bagofnbrs7_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32507.88 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2717.33 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32662 sec.
    Turnaround time :                            33316 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289927: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs10_5.pkl Models/anatomy_aml_bagofnbrs10_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs10_5.pkl Models/anatomy_aml_bagofnbrs10_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:06:28 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:06:28 2020
Terminated at Wed Sep  2 00:17:45 2020
Results reported at Wed Sep  2 00:17:45 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs10_5.pkl Models/anatomy_aml_bagofnbrs10_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   36620.23 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2717.25 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   36677 sec.
    Turnaround time :                            37432 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc271>
Subject: Job 3289949: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 5 Output/test_anatomy_aml_bagofnbrs80_5.pkl Models/anatomy_aml_bagofnbrs80_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 5 Output/test_anatomy_aml_bagofnbrs80_5.pkl Models/anatomy_aml_bagofnbrs80_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc271>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:53:06 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:53:06 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 5 Output/test_anatomy_aml_bagofnbrs80_5.pkl Models/anatomy_aml_bagofnbrs80_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   59136.73 sec.
    Max Memory :                                 2721 MB
    Average Memory :                             2637.54 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40696.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   59172 sec.
    Turnaround time :                            80694 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc205>
Subject: Job 3289951: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 5 Output/test_anatomy_aml_bagofnbrs152_5.pkl Models/anatomy_aml_bagofnbrs152_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 5 Output/test_anatomy_aml_bagofnbrs152_5.pkl Models/anatomy_aml_bagofnbrs152_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc205>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 20:03:43 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 20:03:43 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 5 Output/test_anatomy_aml_bagofnbrs152_5.pkl Models/anatomy_aml_bagofnbrs152_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   58444.37 sec.
    Max Memory :                                 2723 MB
    Average Memory :                             2604.84 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40694.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   58526 sec.
    Turnaround time :                            80694 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc233>
Subject: Job 3289943: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 5 Output/test_anatomy_aml_bagofnbrs30_5.pkl Models/anatomy_aml_bagofnbrs30_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 5 Output/test_anatomy_aml_bagofnbrs30_5.pkl Models/anatomy_aml_bagofnbrs30_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc233>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:19:35 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:19:35 2020
Terminated at Wed Sep  2 12:18:50 2020
Results reported at Wed Sep  2 12:18:50 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 5 Output/test_anatomy_aml_bagofnbrs30_5.pkl Models/anatomy_aml_bagofnbrs30_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   61148.86 sec.
    Max Memory :                                 2742 MB
    Average Memory :                             2674.90 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40675.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   61169 sec.
    Turnaround time :                            80696 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc276>
Subject: Job 3289945: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 5 Output/test_anatomy_aml_bagofnbrs32_5.pkl Models/anatomy_aml_bagofnbrs32_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 5 Output/test_anatomy_aml_bagofnbrs32_5.pkl Models/anatomy_aml_bagofnbrs32_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc276>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:36:19 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:36:19 2020
Terminated at Wed Sep  2 12:18:50 2020
Results reported at Wed Sep  2 12:18:50 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 5 Output/test_anatomy_aml_bagofnbrs32_5.pkl Models/anatomy_aml_bagofnbrs32_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   60137.66 sec.
    Max Memory :                                 2746 MB
    Average Memory :                             2684.55 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40671.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   60150 sec.
    Turnaround time :                            80696 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289947: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 5 Output/test_anatomy_aml_bagofnbrs40_5.pkl Models/anatomy_aml_bagofnbrs40_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 5 Output/test_anatomy_aml_bagofnbrs40_5.pkl Models/anatomy_aml_bagofnbrs40_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:54 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 19:50:03 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 19:50:03 2020
Terminated at Wed Sep  2 12:18:54 2020
Results reported at Wed Sep  2 12:18:54 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 5 Output/test_anatomy_aml_bagofnbrs40_5.pkl Models/anatomy_aml_bagofnbrs40_5.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 143.

Resource usage summary:

    CPU time :                                   59140.12 sec.
    Max Memory :                                 2744 MB
    Average Memory :                             2679.16 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40673.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   59331 sec.
    Turnaround time :                            80700 sec.

The output (if any) is above this job summary.

