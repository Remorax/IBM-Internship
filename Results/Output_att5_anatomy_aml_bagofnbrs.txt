Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2640040262695228
Epoch: 0 Idx: 5000 Loss: 0.02118390419411081
Epoch: 1 Idx: 0 Loss: 0.02972945407658322
Epoch: 1 Idx: 5000 Loss: 0.03008839769090796
Epoch: 2 Idx: 0 Loss: 0.02000661569625223
Epoch: 2 Idx: 5000 Loss: 0.02640670250055341
Epoch: 3 Idx: 0 Loss: 0.02575195884150059
Epoch: 3 Idx: 5000 Loss: 0.026737034123258147
Epoch: 4 Idx: 0 Loss: 0.017821847951739717
Epoch: 4 Idx: 5000 Loss: 0.02415040602168874
Epoch: 5 Idx: 0 Loss: 0.018225710748137666
Epoch: 5 Idx: 5000 Loss: 0.043439640970999616
Epoch: 6 Idx: 0 Loss: 0.02740496520667171
Epoch: 6 Idx: 5000 Loss: 0.020146488003004902
Epoch: 7 Idx: 0 Loss: 0.021405207064325617
Epoch: 7 Idx: 5000 Loss: 0.02118824038896996
Epoch: 8 Idx: 0 Loss: 0.020133878144564604
Epoch: 8 Idx: 5000 Loss: 0.016629168125577364
Epoch: 9 Idx: 0 Loss: 0.03690576117385897
Epoch: 9 Idx: 5000 Loss: 0.025666841889870715
Epoch: 10 Idx: 0 Loss: 0.01392274207010831
Epoch: 10 Idx: 5000 Loss: 0.015055961678359467
Epoch: 11 Idx: 0 Loss: 0.030302468776717124
Epoch: 11 Idx: 5000 Loss: 0.025790551032467926
Epoch: 12 Idx: 0 Loss: 0.027611008254156886
Epoch: 12 Idx: 5000 Loss: 0.023045212492924308
Epoch: 13 Idx: 0 Loss: 0.027933404966331423
Epoch: 13 Idx: 5000 Loss: 0.00985436597834366
Epoch: 14 Idx: 0 Loss: 0.02348218124598028
Epoch: 14 Idx: 5000 Loss: 0.01470467750751908
Epoch: 15 Idx: 0 Loss: 0.019690381842079115
Epoch: 15 Idx: 5000 Loss: 0.024820369024364863
Epoch: 16 Idx: 0 Loss: 0.029183885316499944
Epoch: 16 Idx: 5000 Loss: 0.019935580338691524
Epoch: 17 Idx: 0 Loss: 0.045126900209244106
Epoch: 17 Idx: 5000 Loss: 0.0110935897840613
Epoch: 18 Idx: 0 Loss: 0.0314035043667467
Epoch: 18 Idx: 5000 Loss: 0.03216260157819449
Epoch: 19 Idx: 0 Loss: 0.032380419319518974
Epoch: 19 Idx: 5000 Loss: 0.022514923880893198
Epoch: 20 Idx: 0 Loss: 0.015557579204744019
Epoch: 20 Idx: 5000 Loss: 0.006109700691112769
Epoch: 21 Idx: 0 Loss: 0.007976214903506383
Epoch: 21 Idx: 5000 Loss: 0.019987955749343346
Epoch: 22 Idx: 0 Loss: 0.021842799187709247
Epoch: 22 Idx: 5000 Loss: 0.06061209958118254
Epoch: 23 Idx: 0 Loss: 0.01079697491780833
Epoch: 23 Idx: 5000 Loss: 0.014473681337515582
Epoch: 24 Idx: 0 Loss: 0.025714784833357658
Epoch: 24 Idx: 5000 Loss: 0.02615956223894511
Epoch: 25 Idx: 0 Loss: 0.031533182062239325
Epoch: 25 Idx: 5000 Loss: 0.024349700618744058
Epoch: 26 Idx: 0 Loss: 0.011124305339988639
Epoch: 26 Idx: 5000 Loss: 0.02109059864346148
Epoch: 27 Idx: 0 Loss: 0.03430064179132164
Epoch: 27 Idx: 5000 Loss: 0.014953726819126361
Epoch: 28 Idx: 0 Loss: 0.014183202847715806
Epoch: 28 Idx: 5000 Loss: 0.015177332429844396
Epoch: 29 Idx: 0 Loss: 0.02516702374112932
Epoch: 29 Idx: 5000 Loss: 0.01668672173716639
Epoch: 30 Idx: 0 Loss: 0.032068544883195334
Epoch: 30 Idx: 5000 Loss: 0.03290359497688821
Epoch: 31 Idx: 0 Loss: 0.021054576724395584
Epoch: 31 Idx: 5000 Loss: 0.032927091977917655
Epoch: 32 Idx: 0 Loss: 0.02356650850881954
Epoch: 32 Idx: 5000 Loss: 0.01942515474486947
Epoch: 33 Idx: 0 Loss: 0.022135189667987203
Epoch: 33 Idx: 5000 Loss: 0.027549089781953952
Epoch: 34 Idx: 0 Loss: 0.013740453053439235
Epoch: 34 Idx: 5000 Loss: 0.019940616773037486
Epoch: 35 Idx: 0 Loss: 0.028376676816648152
Epoch: 35 Idx: 5000 Loss: 0.046022374574846245
Epoch: 36 Idx: 0 Loss: 0.019719964856712
Epoch: 36 Idx: 5000 Loss: 0.022106871603166493
Epoch: 37 Idx: 0 Loss: 0.014851096635234123
Epoch: 37 Idx: 5000 Loss: 0.04092679451637701
Epoch: 38 Idx: 0 Loss: 0.021836828745820536
Epoch: 38 Idx: 5000 Loss: 0.008952860053735592
Epoch: 39 Idx: 0 Loss: 0.0109146405433764
Epoch: 39 Idx: 5000 Loss: 0.017334730655167736
Epoch: 40 Idx: 0 Loss: 0.022842962199178335
Epoch: 40 Idx: 5000 Loss: 0.015318153959842044
Epoch: 41 Idx: 0 Loss: 0.016656863551854192
Epoch: 41 Idx: 5000 Loss: 0.03918380306975499
Epoch: 42 Idx: 0 Loss: 0.01915906558679043
Epoch: 42 Idx: 5000 Loss: 0.04141344617784319
Epoch: 43 Idx: 0 Loss: 0.009656196176807507
Epoch: 43 Idx: 5000 Loss: 0.04791914624958328
Epoch: 44 Idx: 0 Loss: 0.028262065647806214
Epoch: 44 Idx: 5000 Loss: 0.017230428383606274
Epoch: 45 Idx: 0 Loss: 0.01738856815549594
Epoch: 45 Idx: 5000 Loss: 0.022460768417411153
Epoch: 46 Idx: 0 Loss: 0.03368654739655823
Epoch: 46 Idx: 5000 Loss: 0.015818225287125492
Epoch: 47 Idx: 0 Loss: 0.028443562062548517
Epoch: 47 Idx: 5000 Loss: 0.032964118753402
Epoch: 48 Idx: 0 Loss: 0.034953407679597356
Epoch: 48 Idx: 5000 Loss: 0.0227700620053497
Epoch: 49 Idx: 0 Loss: 0.019842501939246554
Epoch: 49 Idx: 5000 Loss: 0.0337942300777486
Len (direct inputs):  105
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18550868834181677
Epoch: 0 Idx: 5000 Loss: 0.027991936921772112
Epoch: 1 Idx: 0 Loss: 0.021179572908622046
Epoch: 1 Idx: 5000 Loss: 0.013557351434879614
Epoch: 2 Idx: 0 Loss: 0.019523266243140776
Epoch: 2 Idx: 5000 Loss: 0.02566365583503767
Epoch: 3 Idx: 0 Loss: 0.02667715529342579
Epoch: 3 Idx: 5000 Loss: 0.01708788764909664
Epoch: 4 Idx: 0 Loss: 0.0179272690606194
Epoch: 4 Idx: 5000 Loss: 0.015909979683575413
Epoch: 5 Idx: 0 Loss: 0.011121744910467498
Epoch: 5 Idx: 5000 Loss: 0.03316742269311729
Epoch: 6 Idx: 0 Loss: 0.00965625662958717
Epoch: 6 Idx: 5000 Loss: 0.019187150992883397
Epoch: 7 Idx: 0 Loss: 0.015651248350029776
Epoch: 7 Idx: 5000 Loss: 0.030577632992457988
Epoch: 8 Idx: 0 Loss: 0.019061406852333363
Epoch: 8 Idx: 5000 Loss: 0.019922779349223124
Epoch: 9 Idx: 0 Loss: 0.020100999384778884
Epoch: 9 Idx: 5000 Loss: 0.0293698552092642
Epoch: 10 Idx: 0 Loss: 0.014527231436864326
Epoch: 10 Idx: 5000 Loss: 0.018530238326224604
Epoch: 11 Idx: 0 Loss: 0.011726675618649722
Epoch: 11 Idx: 5000 Loss: 0.025651659514222925
Epoch: 12 Idx: 0 Loss: 0.03383515754486581
Epoch: 12 Idx: 5000 Loss: 0.030705775084967718
Epoch: 13 Idx: 0 Loss: 0.01544432211905683
Epoch: 13 Idx: 5000 Loss: 0.017844225278846985
Epoch: 14 Idx: 0 Loss: 0.011101049131487236
Epoch: 14 Idx: 5000 Loss: 0.024258482470070616
Epoch: 15 Idx: 0 Loss: 0.01580936860883795
Epoch: 15 Idx: 5000 Loss: 0.03681866181511622
Epoch: 16 Idx: 0 Loss: 0.035735225712189206
Epoch: 16 Idx: 5000 Loss: 0.017823452706281752
Epoch: 17 Idx: 0 Loss: 0.030738493313164264
Epoch: 17 Idx: 5000 Loss: 0.04494537535686284
Epoch: 18 Idx: 0 Loss: 0.017021518337477386
Epoch: 18 Idx: 5000 Loss: 0.013202415217812492
Epoch: 19 Idx: 0 Loss: 0.04419075502362171
Epoch: 19 Idx: 5000 Loss: 0.029903426649733618
Epoch: 20 Idx: 0 Loss: 0.02237897949365075
Epoch: 20 Idx: 5000 Loss: 0.020827856690419942
Epoch: 21 Idx: 0 Loss: 0.025676227342406835
Epoch: 21 Idx: 5000 Loss: 0.027887637213797327
Epoch: 22 Idx: 0 Loss: 0.009119969201547628
Epoch: 22 Idx: 5000 Loss: 0.022055178277252428
Epoch: 23 Idx: 0 Loss: 0.017170063695139647
Epoch: 23 Idx: 5000 Loss: 0.017322577678821525
Epoch: 24 Idx: 0 Loss: 0.023172264395765585
Epoch: 24 Idx: 5000 Loss: 0.022376564633612983
Epoch: 25 Idx: 0 Loss: 0.028480465937866496
Epoch: 25 Idx: 5000 Loss: 0.01249565194100285
Epoch: 26 Idx: 0 Loss: 0.013096128624384935
Epoch: 26 Idx: 5000 Loss: 0.03623318318669624
Epoch: 27 Idx: 0 Loss: 0.030249430439774495
Epoch: 27 Idx: 5000 Loss: 0.013780781096096734
Epoch: 28 Idx: 0 Loss: 0.015273047926782727
Epoch: 28 Idx: 5000 Loss: 0.015683616218558717
Epoch: 29 Idx: 0 Loss: 0.019418060790917874
Epoch: 29 Idx: 5000 Loss: 0.020795610363421008
Epoch: 30 Idx: 0 Loss: 0.02205622930029246
Epoch: 30 Idx: 5000 Loss: 0.02382873696084235
Epoch: 31 Idx: 0 Loss: 0.027787869453009793
Epoch: 31 Idx: 5000 Loss: 0.015712703447902755
Epoch: 32 Idx: 0 Loss: 0.017331763975744166
Epoch: 32 Idx: 5000 Loss: 0.02550304972563918
Epoch: 33 Idx: 0 Loss: 0.03569015493021691
Epoch: 33 Idx: 5000 Loss: 0.027264720015977746
Epoch: 34 Idx: 0 Loss: 0.03610033381724358
Epoch: 34 Idx: 5000 Loss: 0.016322501604661702
Epoch: 35 Idx: 0 Loss: 0.04613785486810441
Epoch: 35 Idx: 5000 Loss: 0.011785760466535247
Epoch: 36 Idx: 0 Loss: 0.030215192233867923
Epoch: 36 Idx: 5000 Loss: 0.02888547075897365
Epoch: 37 Idx: 0 Loss: 0.022312137636444365
Epoch: 37 Idx: 5000 Loss: 0.026976376456960466
Epoch: 38 Idx: 0 Loss: 0.022291775683538965
Epoch: 38 Idx: 5000 Loss: 0.040390168922072624
Epoch: 39 Idx: 0 Loss: 0.039991579124910415
Epoch: 39 Idx: 5000 Loss: 0.014953408583616623
Epoch: 40 Idx: 0 Loss: 0.013346068529733567
Epoch: 40 Idx: 5000 Loss: 0.029816756697739956
Epoch: 41 Idx: 0 Loss: 0.018590789264760668
Epoch: 41 Idx: 5000 Loss: 0.010132528383847605
Epoch: 42 Idx: 0 Loss: 0.024914229830359326
Epoch: 42 Idx: 5000 Loss: 0.01997456696908824
Epoch: 43 Idx: 0 Loss: 0.011511576869354476
Epoch: 43 Idx: 5000 Loss: 0.016745797829721104
Epoch: 44 Idx: 0 Loss: 0.026714203265813127
Epoch: 44 Idx: 5000 Loss: 0.05046905335670789
Epoch: 45 Idx: 0 Loss: 0.018758822127777464
Epoch: 45 Idx: 5000 Loss: 0.016626343292307395
Epoch: 46 Idx: 0 Loss: 0.02117016186119736
Epoch: 46 Idx: 5000 Loss: 0.02725800842229218
Epoch: 47 Idx: 0 Loss: 0.009299118150529091
Epoch: 47 Idx: 5000 Loss: 0.03208485356632185
Epoch: 48 Idx: 0 Loss: 0.027817322820083867
Epoch: 48 Idx: 5000 Loss: 0.026486680377899086
Epoch: 49 Idx: 0 Loss: 0.018211861860989508
Epoch: 49 Idx: 5000 Loss: 0.023992144527285392
Len (direct inputs):  96
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
ze: 12Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.24042464157922472
Epoch: 0 Idx: 5000 Loss: 0.032140470883697037
Epoch: 1 Idx: 0 Loss: 0.027258144073671862
Epoch: 1 Idx: 5000 Loss: 0.013749451618668014
Epoch: 2 Idx: 0 Loss: 0.02530080261176429
Epoch: 2 Idx: 5000 Loss: 0.014825242041852676
Epoch: 3 Idx: 0 Loss: 0.021583982292996167
Epoch: 3 Idx: 5000 Loss: 0.019335707517020316
Epoch: 4 Idx: 0 Loss: 0.030717178070384613
Epoch: 4 Idx: 5000 Loss: 0.012871328730654785
Epoch: 5 Idx: 0 Loss: 0.012892599581679817
Epoch: 5 Idx: 5000 Loss: 0.03062085040796462
Epoch: 6 Idx: 0 Loss: 0.051349924812430246
Epoch: 6 Idx: 5000 Loss: 0.019432796422682926
Epoch: 7 Idx: 0 Loss: 0.034340118867686245
Epoch: 7 Idx: 5000 Loss: 0.014489925639753012
Epoch: 8 Idx: 0 Loss: 0.020831122074009736
Epoch: 8 Idx: 5000 Loss: 0.03117750727861559
Epoch: 9 Idx: 0 Loss: 0.032181204263626646
Epoch: 9 Idx: 5000 Loss: 0.017411334989649245
Epoch: 10 Idx: 0 Loss: 0.042178191803100284
Epoch: 10 Idx: 5000 Loss: 0.017058260922009783
Epoch: 11 Idx: 0 Loss: 0.009767449654536877
Epoch: 11 Idx: 5000 Loss: 0.024362548893273835
Epoch: 12 Idx: 0 Loss: 0.015381352107621473
Epoch: 12 Idx: 5000 Loss: 0.022002987232020564
Epoch: 13 Idx: 0 Loss: 0.031456020180993305
Epoch: 13 Idx: 5000 Loss: 0.03170455025491572
Epoch: 14 Idx: 0 Loss: 0.014140131538408766
Epoch: 14 Idx: 5000 Loss: 0.021206601863852554
Epoch: 15 Idx: 0 Loss: 0.025299121329072034
Epoch: 15 Idx: 5000 Loss: 0.012369187746180777
Epoch: 16 Idx: 0 Loss: 0.016837947793821244
Epoch: 16 Idx: 5000 Loss: 0.012441079091302326
Epoch: 17 Idx: 0 Loss: 0.017843916963658127
Epoch: 17 Idx: 5000 Loss: 0.02104284344246721
Epoch: 18 Idx: 0 Loss: 0.02014201410945493
Epoch: 18 Idx: 5000 Loss: 0.012098066766681907
Epoch: 19 Idx: 0 Loss: 0.02103137621791553
Epoch: 19 Idx: 5000 Loss: 0.0216365572751704
Epoch: 20 Idx: 0 Loss: 0.03310847220673201
Epoch: 20 Idx: 5000 Loss: 0.02042486557933853
Epoch: 21 Idx: 0 Loss: 0.024932792704067158
Epoch: 21 Idx: 5000 Loss: 0.04273750734611996
Epoch: 22 Idx: 0 Loss: 0.011867006685731695
Epoch: 22 Idx: 5000 Loss: 0.02020962715369698
Epoch: 23 Idx: 0 Loss: 0.02248643427995154
Epoch: 23 Idx: 5000 Loss: 0.017117117106956188
Epoch: 24 Idx: 0 Loss: 0.003927966513471503
Epoch: 24 Idx: 5000 Loss: 0.010816886091889135
Epoch: 25 Idx: 0 Loss: 0.017453690185138367
Epoch: 25 Idx: 5000 Loss: 0.013822270703901452
Epoch: 26 Idx: 0 Loss: 0.025979794666350198
Epoch: 26 Idx: 5000 Loss: 0.04556441416379231
Epoch: 27 Idx: 0 Loss: 0.028924888017002236
Epoch: 27 Idx: 5000 Loss: 0.017179175519310048
Epoch: 28 Idx: 0 Loss: 0.015976720800754646
Epoch: 28 Idx: 5000 Loss: 0.01287275446641608
Epoch: 29 Idx: 0 Loss: 0.013161925354311474
Epoch: 29 Idx: 5000 Loss: 0.021169524001729702
Epoch: 30 Idx: 0 Loss: 0.021364204328002023
Epoch: 30 Idx: 5000 Loss: 0.014697226739782025
Epoch: 31 Idx: 0 Loss: 0.0228434105443261
Epoch: 31 Idx: 5000 Loss: 0.017632196773839877
Epoch: 32 Idx: 0 Loss: 0.02861828109086449
Epoch: 32 Idx: 5000 Loss: 0.03900289332621494
Epoch: 33 Idx: 0 Loss: 0.013759908579591284
Epoch: 33 Idx: 5000 Loss: 0.014142099157499453
Epoch: 34 Idx: 0 Loss: 0.02570030578069129
Epoch: 34 Idx: 5000 Loss: 0.03288074526358377
Epoch: 35 Idx: 0 Loss: 0.024401237919197398
Epoch: 35 Idx: 5000 Loss: 0.023643049244636095
Epoch: 36 Idx: 0 Loss: 0.02107154976596444
Epoch: 36 Idx: 5000 Loss: 0.026949989738201036
Epoch: 37 Idx: 0 Loss: 0.014440268617189729
Epoch: 37 Idx: 5000 Loss: 0.04262941860562361
Epoch: 38 Idx: 0 Loss: 0.02157125746015052
Epoch: 38 Idx: 5000 Loss: 0.014463895782397172
Epoch: 39 Idx: 0 Loss: 0.022337649747555494
Epoch: 39 Idx: 5000 Loss: 0.021749487552475046
Epoch: 40 Idx: 0 Loss: 0.02675725746312304
Epoch: 40 Idx: 5000 Loss: 0.01834456111030804
Epoch: 41 Idx: 0 Loss: 0.029729869008555863
Epoch: 41 Idx: 5000 Loss: 0.018430516435725727
Epoch: 42 Idx: 0 Loss: 0.034844819460488036
Epoch: 42 Idx: 5000 Loss: 0.04692505950307067
Epoch: 43 Idx: 0 Loss: 0.019512178635025135
Epoch: 43 Idx: 5000 Loss: 0.037768972668473316
Epoch: 44 Idx: 0 Loss: 0.020202100767930917
Epoch: 44 Idx: 5000 Loss: 0.025479930908214245
Epoch: 45 Idx: 0 Loss: 0.011905073324844345
Epoch: 45 Idx: 5000 Loss: 0.017430079438953132
Epoch: 46 Idx: 0 Loss: 0.016894475290276108
Epoch: 46 Idx: 5000 Loss: 0.015962870609328814
Epoch: 47 Idx: 0 Loss: 0.024124695652762387
Epoch: 47 Idx: 5000 Loss: 0.04199886546799962
Epoch: 48 Idx: 0 Loss: 0.009037932784722752
Epoch: 48 Idx: 5000 Loss: 0.020371149051974922
Epoch: 49 Idx: 0 Loss: 0.010098978647287347
Epoch: 49 Idx: 5000 Loss: 0.012941081355518575
Len (direct inputs):  89
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1973945271270379
Epoch: 0 Idx: 5000 Loss: 0.025803572462521997
Epoch: 1 Idx: 0 Loss: 0.014439567324997566
Epoch: 1 Idx: 5000 Loss: 0.015599958538045456
Epoch: 2 Idx: 0 Loss: 0.01632130708883352
Epoch: 2 Idx: 5000 Loss: 0.04732998774836153
Epoch: 3 Idx: 0 Loss: 0.03004125098868958
Epoch: 3 Idx: 5000 Loss: 0.02526987183760385
Epoch: 4 Idx: 0 Loss: 0.02414690403939264
Epoch: 4 Idx: 5000 Loss: 0.024452916295090568
Epoch: 5 Idx: 0 Loss: 0.033986286894698146
Epoch: 5 Idx: 5000 Loss: 0.0182449258247774
Epoch: 6 Idx: 0 Loss: 0.021274913563071006
Epoch: 6 Idx: 5000 Loss: 0.01203865944191498
Epoch: 7 Idx: 0 Loss: 0.016814244809269525
Epoch: 7 Idx: 5000 Loss: 0.025249447307596904
Epoch: 8 Idx: 0 Loss: 0.018146189736103806
Epoch: 8 Idx: 5000 Loss: 0.024176714662988315
Epoch: 9 Idx: 0 Loss: 0.043270646895413864
Epoch: 9 Idx: 5000 Loss: 0.01209440572144881
Epoch: 10 Idx: 0 Loss: 0.020719195502648474
Epoch: 10 Idx: 5000 Loss: 0.015791189761686062
Epoch: 11 Idx: 0 Loss: 0.010892293786488438
Epoch: 11 Idx: 5000 Loss: 0.024924884338362527
Epoch: 12 Idx: 0 Loss: 0.01884121367163677
Epoch: 12 Idx: 5000 Loss: 0.01464874393761676
Epoch: 13 Idx: 0 Loss: 0.015003780117138198
Epoch: 13 Idx: 5000 Loss: 0.012155756143542699
Epoch: 14 Idx: 0 Loss: 0.020442048820286926
Epoch: 14 Idx: 5000 Loss: 0.022921980218181537
Epoch: 15 Idx: 0 Loss: 0.016163001195597822
Epoch: 15 Idx: 5000 Loss: 0.03825597107380687
Epoch: 16 Idx: 0 Loss: 0.014914837943284263
Epoch: 16 Idx: 5000 Loss: 0.032176677606463526
Epoch: 17 Idx: 0 Loss: 0.016700861796965415
Epoch: 17 Idx: 5000 Loss: 0.01816696890720767
Epoch: 18 Idx: 0 Loss: 0.015258309497956342
Epoch: 18 Idx: 5000 Loss: 0.016931959641951322
Epoch: 19 Idx: 0 Loss: 0.016893621165883298
Epoch: 19 Idx: 5000 Loss: 0.01994240454336523
Epoch: 20 Idx: 0 Loss: 0.0383637440407785
Epoch: 20 Idx: 5000 Loss: 0.015156060647627518
Epoch: 21 Idx: 0 Loss: 0.023571219212542918
Epoch: 21 Idx: 5000 Loss: 0.020591638984593444
Epoch: 22 Idx: 0 Loss: 0.034120198485690645
Epoch: 22 Idx: 5000 Loss: 0.023249716219577138
Epoch: 23 Idx: 0 Loss: 0.02358666233557674
Epoch: 23 Idx: 5000 Loss: 0.015368495201234835
Epoch: 24 Idx: 0 Loss: 0.024800064454077397
Epoch: 24 Idx: 5000 Loss: 0.0223676799941045
Epoch: 25 Idx: 0 Loss: 0.01447361874316307
Epoch: 25 Idx: 5000 Loss: 0.02926442412312979
Epoch: 26 Idx: 0 Loss: 0.026511344715988195
Epoch: 26 Idx: 5000 Loss: 0.010904735796120946
Epoch: 27 Idx: 0 Loss: 0.029736889237750058
Epoch: 27 Idx: 5000 Loss: 0.05839464497400386
Epoch: 28 Idx: 0 Loss: 0.017668307914530983
Epoch: 28 Idx: 5000 Loss: 0.010892280278290025
Epoch: 29 Idx: 0 Loss: 0.020318392490875962
Epoch: 29 Idx: 5000 Loss: 0.02656739453460107
Epoch: 30 Idx: 0 Loss: 0.016831375976436056
Epoch: 30 Idx: 5000 Loss: 0.05389924538149676
Epoch: 31 Idx: 0 Loss: 0.014449454616781156
Epoch: 31 Idx: 5000 Loss: 0.012426408419168903
Epoch: 32 Idx: 0 Loss: 0.017629656054478665
Epoch: 32 Idx: 5000 Loss: 0.023106128683948943
Epoch: 33 Idx: 0 Loss: 0.012800788381049855
Epoch: 33 Idx: 5000 Loss: 0.04485847897306501
Epoch: 34 Idx: 0 Loss: 0.01974874668750837
Epoch: 34 Idx: 5000 Loss: 0.01763075884991363
Epoch: 35 Idx: 0 Loss: 0.020763632623380157
Epoch: 35 Idx: 5000 Loss: 0.021239074387098787
Epoch: 36 Idx: 0 Loss: 0.02855292873752153
Epoch: 36 Idx: 5000 Loss: 0.02747558055960328
Epoch: 37 Idx: 0 Loss: 0.017200189504693722
Epoch: 37 Idx: 5000 Loss: 0.01724884301343075
Epoch: 38 Idx: 0 Loss: 0.04240312309701066
Epoch: 38 Idx: 5000 Loss: 0.034180706155868545
Epoch: 39 Idx: 0 Loss: 0.012850501901296274
Epoch: 39 Idx: 5000 Loss: 0.0253471722283826
Epoch: 40 Idx: 0 Loss: 0.016545040940964588
Epoch: 40 Idx: 5000 Loss: 0.021217010137330662
Epoch: 41 Idx: 0 Loss: 0.014735440477920043
Epoch: 41 Idx: 5000 Loss: 0.025825765890481592
Epoch: 42 Idx: 0 Loss: 0.01582528863398993
Epoch: 42 Idx: 5000 Loss: 0.03411348097732658
Epoch: 43 Idx: 0 Loss: 0.02876815942153318
Epoch: 43 Idx: 5000 Loss: 0.02674050411507949
Epoch: 44 Idx: 0 Loss: 0.012787685977959717
Epoch: 44 Idx: 5000 Loss: 0.008608727588381007
Epoch: 45 Idx: 0 Loss: 0.022370979638757726
Epoch: 45 Idx: 5000 Loss: 0.011594716570709264
Epoch: 46 Idx: 0 Loss: 0.00885637245782592
Epoch: 46 Idx: 5000 Loss: 0.02096198887004047
Epoch: 47 Idx: 0 Loss: 0.02526579292872453
Epoch: 47 Idx: 5000 Loss: 0.021154060363810838
Epoch: 48 Idx: 0 Loss: 0.02397395092895352
Epoch: 48 Idx: 5000 Loss: 0.014738605891330052
Epoch: 49 Idx: 0 Loss: 0.01294009026409753
Epoch: 49 Idx: 5000 Loss: 0.018285655784239005
Len (direct inputs):  102
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1595620876136854
Epoch: 0 Idx: 5000 Loss: 0.030392813897831394
Epoch: 1 Idx: 0 Loss: 0.033756176604881144
Epoch: 1 Idx: 5000 Loss: 0.018945418895376598
Epoch: 2 Idx: 0 Loss: 0.02764561637110554
Epoch: 2 Idx: 5000 Loss: 0.014789988706419368
Epoch: 3 Idx: 0 Loss: 0.020936497117278914
Epoch: 3 Idx: 5000 Loss: 0.023169788799504577
Epoch: 4 Idx: 0 Loss: 0.03126370246938942
Epoch: 4 Idx: 5000 Loss: 0.02292771517893589
Epoch: 5 Idx: 0 Loss: 0.014925548637720675
Epoch: 5 Idx: 5000 Loss: 0.0148675455448807
Epoch: 6 Idx: 0 Loss: 0.01845340888464434
Epoch: 6 Idx: 5000 Loss: 0.03663740895044547
Epoch: 7 Idx: 0 Loss: 0.01507907415412554
Epoch: 7 Idx: 5000 Loss: 0.019505173295114035
Epoch: 8 Idx: 0 Loss: 0.016485685578062383
Epoch: 8 Idx: 5000 Loss: 0.0161974499219052
Epoch: 9 Idx: 0 Loss: 0.012838263980290265
Epoch: 9 Idx: 5000 Loss: 0.01236626460192617
Epoch: 10 Idx: 0 Loss: 0.02451093113823139
Epoch: 10 Idx: 5000 Loss: 0.03355585570893422
Epoch: 11 Idx: 0 Loss: 0.0196944095976581
Epoch: 11 Idx: 5000 Loss: 0.025661317388695523
Epoch: 12 Idx: 0 Loss: 0.016693688444866947
Epoch: 12 Idx: 5000 Loss: 0.03871944551385263
Epoch: 13 Idx: 0 Loss: 0.016661486072084404
Epoch: 13 Idx: 5000 Loss: 0.02512433139144371
Epoch: 14 Idx: 0 Loss: 0.02354449801330276
Epoch: 14 Idx: 5000 Loss: 0.022851677280575508
Epoch: 15 Idx: 0 Loss: 0.02838086882863706
Epoch: 15 Idx: 5000 Loss: 0.01308966498716129
Epoch: 16 Idx: 0 Loss: 0.018572806825370082
Epoch: 16 Idx: 5000 Loss: 0.03308374651177041
Epoch: 17 Idx: 0 Loss: 0.02627673866250848
Epoch: 17 Idx: 5000 Loss: 0.0167964926139152
Epoch: 18 Idx: 0 Loss: 0.010312684183406275
Epoch: 18 Idx: 5000 Loss: 0.01576124617531567
Epoch: 19 Idx: 0 Loss: 0.013843019052225337
Epoch: 19 Idx: 5000 Loss: 0.010775746473506537
Epoch: 20 Idx: 0 Loss: 0.02553942802954305
Epoch: 20 Idx: 5000 Loss: 0.020610167611533402
Epoch: 21 Idx: 0 Loss: 0.023634189251502076
Epoch: 21 Idx: 5000 Loss: 0.020362922804453773
Epoch: 22 Idx: 0 Loss: 0.022297672018640587
Epoch: 22 Idx: 5000 Loss: 0.026704165761084822
Epoch: 23 Idx: 0 Loss: 0.03195064889012091
Epoch: 23 Idx: 5000 Loss: 0.009918750690800456
Epoch: 24 Idx: 0 Loss: 0.014661571134467267
Epoch: 24 Idx: 5000 Loss: 0.022578177175707058
Epoch: 25 Idx: 0 Loss: 0.010238224938923651
Epoch: 25 Idx: 5000 Loss: 0.0205226359588877
Epoch: 26 Idx: 0 Loss: 0.011309969223916444
Epoch: 26 Idx: 5000 Loss: 0.022463713910419594
Epoch: 27 Idx: 0 Loss: 0.021833683418638273
Epoch: 27 Idx: 5000 Loss: 0.03907655523357121
Epoch: 28 Idx: 0 Loss: 0.018715459239633442
Epoch: 28 Idx: 5000 Loss: 0.03307188698856378
Epoch: 29 Idx: 0 Loss: 0.022387352794004157
Epoch: 29 Idx: 5000 Loss: 0.013784618933537872
Epoch: 30 Idx: 0 Loss: 0.00843191882315026
Epoch: 30 Idx: 5000 Loss: 0.025060160607101804
Epoch: 31 Idx: 0 Loss: 0.010649530499731424
Epoch: 31 Idx: 5000 Loss: 0.014126140981023199
Epoch: 32 Idx: 0 Loss: 0.021909884234136694
Epoch: 32 Idx: 5000 Loss: 0.01922811487185725
Epoch: 33 Idx: 0 Loss: 0.019473941525859954
Epoch: 33 Idx: 5000 Loss: 0.01443103222793058
Epoch: 34 Idx: 0 Loss: 0.03244188185032047
Epoch: 34 Idx: 5000 Loss: 0.01937502175512728
Epoch: 35 Idx: 0 Loss: 0.012722023956536039
Epoch: 35 Idx: 5000 Loss: 0.026212148603631566
Epoch: 36 Idx: 0 Loss: 0.01836207561427717
Epoch: 36 Idx: 5000 Loss: 0.014314003903218367
Epoch: 37 Idx: 0 Loss: 0.017332877466130762
Epoch: 37 Idx: 5000 Loss: 0.015296300760425827
Epoch: 38 Idx: 0 Loss: 0.010264280975527656
Epoch: 38 Idx: 5000 Loss: 0.02318353607473763
Epoch: 39 Idx: 0 Loss: 0.020402437968423663
Epoch: 39 Idx: 5000 Loss: 0.014419231782334467
Epoch: 40 Idx: 0 Loss: 0.030771581845180675
Epoch: 40 Idx: 5000 Loss: 0.030371000920139307
Epoch: 41 Idx: 0 Loss: 0.030079493389663967
Epoch: 41 Idx: 5000 Loss: 0.008329563590277759
Epoch: 42 Idx: 0 Loss: 0.02271018449263551
Epoch: 42 Idx: 5000 Loss: 0.01591998463647277
Epoch: 43 Idx: 0 Loss: 0.02525504705083302
Epoch: 43 Idx: 5000 Loss: 0.009522680379131789
Epoch: 44 Idx: 0 Loss: 0.02860096336633877
Epoch: 44 Idx: 5000 Loss: 0.030475538603639912
Epoch: 45 Idx: 0 Loss: 0.013369744952546295
Epoch: 45 Idx: 5000 Loss: 0.035855626947444444
Epoch: 46 Idx: 0 Loss: 0.01849560749739859
Epoch: 46 Idx: 5000 Loss: 0.01490080694535623
Epoch: 47 Idx: 0 Loss: 0.02443294442190864
Epoch: 47 Idx: 5000 Loss: 0.013233928374004809
Epoch: 48 Idx: 0 Loss: 0.016905853182611577
Epoch: 48 Idx: 5000 Loss: 0.01229461793058571
Epoch: 49 Idx: 0 Loss: 0.014334356945268228
Epoch: 49 Idx: 5000 Loss: 0.02090470566512581
Len (direct inputs):  102
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
o
division by zero
division by zero
ning size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.3093636013838238
Epoch: 0 Idx: 5000 Loss: 0.01124998930063887
Epoch: 1 Idx: 0 Loss: 0.0211087966589592
Epoch: 1 Idx: 5000 Loss: 0.04504730808854185
Epoch: 2 Idx: 0 Loss: 0.020429433306243187
Epoch: 2 Idx: 5000 Loss: 0.03146946903102456
Epoch: 3 Idx: 0 Loss: 0.012719739773981666
Epoch: 3 Idx: 5000 Loss: 0.03242133821020009
Epoch: 4 Idx: 0 Loss: 0.010578600575857298
Epoch: 4 Idx: 5000 Loss: 0.024947089080396716
Epoch: 5 Idx: 0 Loss: 0.02151491195789991
Epoch: 5 Idx: 5000 Loss: 0.011910597369474956
Epoch: 6 Idx: 0 Loss: 0.018123692238696722
Epoch: 6 Idx: 5000 Loss: 0.017219165520773655
Epoch: 7 Idx: 0 Loss: 0.03428874070560724
Epoch: 7 Idx: 5000 Loss: 0.015952299205599218
Epoch: 8 Idx: 0 Loss: 0.023763914528425842
Epoch: 8 Idx: 5000 Loss: 0.012001715645744641
Epoch: 9 Idx: 0 Loss: 0.020786123464272964
Epoch: 9 Idx: 5000 Loss: 0.017241522655530417
Epoch: 10 Idx: 0 Loss: 0.02172806097455945
Epoch: 10 Idx: 5000 Loss: 0.020008971518355782
Epoch: 11 Idx: 0 Loss: 0.032220190319437375
Epoch: 11 Idx: 5000 Loss: 0.028026072220240367
Epoch: 12 Idx: 0 Loss: 0.01248890573225148
Epoch: 12 Idx: 5000 Loss: 0.012853064413136344
Epoch: 13 Idx: 0 Loss: 0.015303138727699915
Epoch: 13 Idx: 5000 Loss: 0.020166905057667647
Epoch: 14 Idx: 0 Loss: 0.014579228341205145
Epoch: 14 Idx: 5000 Loss: 0.021672554587390674
Epoch: 15 Idx: 0 Loss: 0.021679059275076913
Epoch: 15 Idx: 5000 Loss: 0.023520353318805143
Epoch: 16 Idx: 0 Loss: 0.014838403691971081
Epoch: 16 Idx: 5000 Loss: 0.026702395380446022
Epoch: 17 Idx: 0 Loss: 0.020771389908446722
Epoch: 17 Idx: 5000 Loss: 0.010688472926946512
Epoch: 18 Idx: 0 Loss: 0.012165652595148006
Epoch: 18 Idx: 5000 Loss: 0.018342573368910826
Epoch: 19 Idx: 0 Loss: 0.025264772463242446
Epoch: 19 Idx: 5000 Loss: 0.014797429747193414
Epoch: 20 Idx: 0 Loss: 0.033022053013320146
Epoch: 20 Idx: 5000 Loss: 0.01374701076763549
Epoch: 21 Idx: 0 Loss: 0.014021947356386915
Epoch: 21 Idx: 5000 Loss: 0.018336303900141942
Epoch: 22 Idx: 0 Loss: 0.023868678944010444
Epoch: 22 Idx: 5000 Loss: 0.031228244039470124
Epoch: 23 Idx: 0 Loss: 0.012725840741869231
Epoch: 23 Idx: 5000 Loss: 0.024566978259655283
Epoch: 24 Idx: 0 Loss: 0.019019081765599077
Epoch: 24 Idx: 5000 Loss: 0.019960502209287034
Epoch: 25 Idx: 0 Loss: 0.035914336053695754
Epoch: 25 Idx: 5000 Loss: 0.01866746336306782
Epoch: 26 Idx: 0 Loss: 0.01444357513951216
Epoch: 26 Idx: 5000 Loss: 0.012548737318882966
Epoch: 27 Idx: 0 Loss: 0.01535907964936636
Epoch: 27 Idx: 5000 Loss: 0.025506210753718166
Epoch: 28 Idx: 0 Loss: 0.030695938018699766
Epoch: 28 Idx: 5000 Loss: 0.02299860027323873
Epoch: 29 Idx: 0 Loss: 0.028486198500104993
Epoch: 29 Idx: 5000 Loss: 0.013393274779521114
Epoch: 30 Idx: 0 Loss: 0.010512576024254964
Epoch: 30 Idx: 5000 Loss: 0.015361568552564427
Epoch: 31 Idx: 0 Loss: 0.009521783304593627
Epoch: 31 Idx: 5000 Loss: 0.028612053954205647
Epoch: 32 Idx: 0 Loss: 0.022187440388071352
Epoch: 32 Idx: 5000 Loss: 0.006858759842944352
Epoch: 33 Idx: 0 Loss: 0.02904108278124312
Epoch: 33 Idx: 5000 Loss: 0.019775103512164865
Epoch: 34 Idx: 0 Loss: 0.03459314320395096
Epoch: 34 Idx: 5000 Loss: 0.015111359825364068
Epoch: 35 Idx: 0 Loss: 0.01936593688080049
Epoch: 35 Idx: 5000 Loss: 0.02897051528626817
Epoch: 36 Idx: 0 Loss: 0.012478969864174776
Epoch: 36 Idx: 5000 Loss: 0.018164263871291254
Epoch: 37 Idx: 0 Loss: 0.020918203267762878
Epoch: 37 Idx: 5000 Loss: 0.012007501861004669
Epoch: 38 Idx: 0 Loss: 0.015230625185134648
Epoch: 38 Idx: 5000 Loss: 0.023507885033248868
Epoch: 39 Idx: 0 Loss: 0.02238403713669665
Epoch: 39 Idx: 5000 Loss: 0.02557707239697603
Epoch: 40 Idx: 0 Loss: 0.03610052250826985
Epoch: 40 Idx: 5000 Loss: 0.01884787106339402
Epoch: 41 Idx: 0 Loss: 0.014513742599960567
Epoch: 41 Idx: 5000 Loss: 0.03621687150480683
Epoch: 42 Idx: 0 Loss: 0.016872767035929173
Epoch: 42 Idx: 5000 Loss: 0.024902394959559628
Epoch: 43 Idx: 0 Loss: 0.040824879018366164
Epoch: 43 Idx: 5000 Loss: 0.01556726546320268
Epoch: 44 Idx: 0 Loss: 0.04168209890877682
Epoch: 44 Idx: 5000 Loss: 0.015227910875294601
Epoch: 45 Idx: 0 Loss: 0.012304711664644134
Epoch: 45 Idx: 5000 Loss: 0.02017082435101345
Epoch: 46 Idx: 0 Loss: 0.020613711639681113
Epoch: 46 Idx: 5000 Loss: 0.025020384031189113
Epoch: 47 Idx: 0 Loss: 0.017372035732617353
Epoch: 47 Idx: 5000 Loss: 0.013993250585830739
Epoch: 48 Idx: 0 Loss: 0.0183610857798086
Epoch: 48 Idx: 5000 Loss: 0.03352653305269587
Epoch: 49 Idx: 0 Loss: 0.017274777343661223
Epoch: 49 Idx: 5000 Loss: 0.023026209636212838
Len (direct inputs):  100
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division b0.828140.8003085336542344
Parameter containing:
tensor([0.8003], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.16890203937411247
Epoch: 0 Idx: 5000 Loss: 0.014135620027775488
Epoch: 1 Idx: 0 Loss: 0.013483667872377623
Epoch: 1 Idx: 5000 Loss: 0.01349763536849778
Epoch: 2 Idx: 0 Loss: 0.026157474160899433
Epoch: 2 Idx: 5000 Loss: 0.023812562811454094
Epoch: 3 Idx: 0 Loss: 0.016066905858123044
Epoch: 3 Idx: 5000 Loss: 0.016825411005489115
Epoch: 4 Idx: 0 Loss: 0.026797499371430104
Epoch: 4 Idx: 5000 Loss: 0.028680288877808054
Epoch: 5 Idx: 0 Loss: 0.02675326474199582
Epoch: 5 Idx: 5000 Loss: 0.035441749636344694
Epoch: 6 Idx: 0 Loss: 0.012160206627814458
Epoch: 6 Idx: 5000 Loss: 0.012261841239802767
Epoch: 7 Idx: 0 Loss: 0.01948637945844485
Epoch: 7 Idx: 5000 Loss: 0.03720800184122437
Epoch: 8 Idx: 0 Loss: 0.049405109124173216
Epoch: 8 Idx: 5000 Loss: 0.014157805645507974
Epoch: 9 Idx: 0 Loss: 0.03154591997726593
Epoch: 9 Idx: 5000 Loss: 0.02213835512214555
Epoch: 10 Idx: 0 Loss: 0.0105513261975741
Epoch: 10 Idx: 5000 Loss: 0.01956026641188276
Epoch: 11 Idx: 0 Loss: 0.019637364108320548
Epoch: 11 Idx: 5000 Loss: 0.015717732428185402
Epoch: 12 Idx: 0 Loss: 0.03231763177031955
Epoch: 12 Idx: 5000 Loss: 0.014778325638922627
Epoch: 13 Idx: 0 Loss: 0.007894693962217343
Epoch: 13 Idx: 5000 Loss: 0.015467218182557009
Epoch: 14 Idx: 0 Loss: 0.022170936431510753
Epoch: 14 Idx: 5000 Loss: 0.02824474437609218
Epoch: 15 Idx: 0 Loss: 0.019847391010463372
Epoch: 15 Idx: 5000 Loss: 0.015750250140112373
Epoch: 16 Idx: 0 Loss: 0.018871406718123533
Epoch: 16 Idx: 5000 Loss: 0.03415808268237917
Epoch: 17 Idx: 0 Loss: 0.038473155040073224
Epoch: 17 Idx: 5000 Loss: 0.01507612738210315
Epoch: 18 Idx: 0 Loss: 0.03169761893202144
Epoch: 18 Idx: 5000 Loss: 0.02847830781013387
Epoch: 19 Idx: 0 Loss: 0.026231396836033455
Epoch: 19 Idx: 5000 Loss: 0.024122054603417528
Epoch: 20 Idx: 0 Loss: 0.010464744186558555
Epoch: 20 Idx: 5000 Loss: 0.0145863847152695
Epoch: 21 Idx: 0 Loss: 0.0188883798760868
Epoch: 21 Idx: 5000 Loss: 0.018001829219140026
Epoch: 22 Idx: 0 Loss: 0.017829991075766514
Epoch: 22 Idx: 5000 Loss: 0.03352776835637229
Epoch: 23 Idx: 0 Loss: 0.01589201929816857
Epoch: 23 Idx: 5000 Loss: 0.023192349939812913
Epoch: 24 Idx: 0 Loss: 0.017309463153095637
Epoch: 24 Idx: 5000 Loss: 0.02698036384930448
Epoch: 25 Idx: 0 Loss: 0.031006576305365564
Epoch: 25 Idx: 5000 Loss: 0.018837657043214464
Epoch: 26 Idx: 0 Loss: 0.02912066667536744
Epoch: 26 Idx: 5000 Loss: 0.02267683666838872
Epoch: 27 Idx: 0 Loss: 0.043999468178454224
Epoch: 27 Idx: 5000 Loss: 0.02122348096432969
Epoch: 28 Idx: 0 Loss: 0.023595230076477777
Epoch: 28 Idx: 5000 Loss: 0.024445677088483574
Epoch: 29 Idx: 0 Loss: 0.00910968352825339
Epoch: 29 Idx: 5000 Loss: 0.014701106471712692
Epoch: 30 Idx: 0 Loss: 0.01194226059725604
Epoch: 30 Idx: 5000 Loss: 0.03639498326879577
Epoch: 31 Idx: 0 Loss: 0.014937738329981033
Epoch: 31 Idx: 5000 Loss: 0.012631138881365266
Epoch: 32 Idx: 0 Loss: 0.025752600845140593
Epoch: 32 Idx: 5000 Loss: 0.01564777046936351
Epoch: 33 Idx: 0 Loss: 0.022489398537080776
Epoch: 33 Idx: 5000 Loss: 0.04143149288521947
Epoch: 34 Idx: 0 Loss: 0.026162883425676128
Epoch: 34 Idx: 5000 Loss: 0.013734346438579598
Epoch: 35 Idx: 0 Loss: 0.007897829793719294
Epoch: 35 Idx: 5000 Loss: 0.011707626780519193
Epoch: 36 Idx: 0 Loss: 0.022312043543383044
Epoch: 36 Idx: 5000 Loss: 0.0417881785595747
Epoch: 37 Idx: 0 Loss: 0.02098370590547519
Epoch: 37 Idx: 5000 Loss: 0.014744804719908595
Epoch: 38 Idx: 0 Loss: 0.012709083266361582
Epoch: 38 Idx: 5000 Loss: 0.04054688532422898
Epoch: 39 Idx: 0 Loss: 0.01992185240997331
Epoch: 39 Idx: 5000 Loss: 0.024805651063346222
Epoch: 40 Idx: 0 Loss: 0.02565635403735425
Epoch: 40 Idx: 5000 Loss: 0.01913781644258363
Epoch: 41 Idx: 0 Loss: 0.016146774548056048
Epoch: 41 Idx: 5000 Loss: 0.03331104505396082
Epoch: 42 Idx: 0 Loss: 0.02481480171839758
Epoch: 42 Idx: 5000 Loss: 0.027780463495413563
Epoch: 43 Idx: 0 Loss: 0.03762606843243122
Epoch: 43 Idx: 5000 Loss: 0.030555586039121525
Epoch: 44 Idx: 0 Loss: 0.00748124367075592
Epoch: 44 Idx: 5000 Loss: 0.01063826388875334
Epoch: 45 Idx: 0 Loss: 0.029726543204168142
Epoch: 45 Idx: 5000 Loss: 0.016949800303976015
Epoch: 46 Idx: 0 Loss: 0.02361440801008277
Epoch: 46 Idx: 5000 Loss: 0.018811984661694682
Epoch: 47 Idx: 0 Loss: 0.037013062308861255
Epoch: 47 Idx: 5000 Loss: 0.014372136300945821
Epoch: 48 Idx: 0 Loss: 0.019684474568211062
Epoch: 48 Idx: 5000 Loss: 0.013424579025720536
Epoch: 49 Idx: 0 Loss: 0.015658312332488083
Epoch: 49 Idx: 5000 Loss: 0.0322410761337747
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 475, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 24 failed with Disk quota exceeded
a exceeded

s used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:57:45 2020
Terminated at Tue Sep  1 20:18:37 2020
Results reported at Tue Sep  1 20:18:37 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 5 Output/test_anatomy_aml_bagofnbrs3_5.pkl Models/anatomy_aml_bagofnbrs3_5.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22823.20 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2717.66 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22852 sec.
    Turnaround time :                            23085 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc213>
Subject: Job 3289915: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs2_5.pkl Models/anatomy_aml_bagofnbrs2_5.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs2_5.pkl Models/anatomy_aml_bagofnbrs2_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc213>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Tue Sep  1 20:23:49 2020
Results reported at Tue Sep  1 20:23:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 5 Output/test_anatomy_aml_bagofnbrs2_5.pkl Models/anatomy_aml_bagofnbrs2_5.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23161.00 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2722.06 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23221 sec.
    Turnaround time :                            23397 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 3289919: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs4_5.pkl Models/anatomy_aml_bagofnbrs4_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs4_5.pkl Models/anatomy_aml_bagofnbrs4_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:58:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:58:45 2020
Terminated at Tue Sep  1 20:59:23 2020
Results reported at Tue Sep  1 20:59:23 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 5 Output/test_anatomy_aml_bagofnbrs4_5.pkl Models/anatomy_aml_bagofnbrs4_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25229.92 sec.
    Max Memory :                                 2922 MB
    Average Memory :                             2717.41 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40495.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25238 sec.
    Turnaround time :                            25530 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc271>
Subject: Job 3289921: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs5_5.pkl Models/anatomy_aml_bagofnbrs5_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs5_5.pkl Models/anatomy_aml_bagofnbrs5_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc271>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:58:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:58:45 2020
Terminated at Tue Sep  1 21:31:25 2020
Results reported at Tue Sep  1 21:31:25 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 5 Output/test_anatomy_aml_bagofnbrs5_5.pkl Models/anatomy_aml_bagofnbrs5_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27124.00 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2726.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27160 sec.
    Turnaround time :                            27452 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289925: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 5 Output/test_anatomy_aml_bagofnbrs8_5.pkl Models/anatomy_aml_bagofnbrs8_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 5 Output/test_anatomy_aml_bagofnbrs8_5.pkl Models/anatomy_aml_bagofnbrs8_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:05:28 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:05:28 2020
Terminated at Tue Sep  1 23:05:04 2020
Results reported at Tue Sep  1 23:05:04 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 5 Output/test_anatomy_aml_bagofnbrs8_5.pkl Models/anatomy_aml_bagofnbrs8_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32366.17 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2722.71 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32376 sec.
    Turnaround time :                            33071 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc242>
Subject: Job 3289923: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs7_5.pkl Models/anatomy_aml_bagofnbrs7_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs7_5.pkl Models/anatomy_aml_bagofnbrs7_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc242>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:04:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:04:47 2020
Terminated at Tue Sep  1 23:09:09 2020
Results reported at Tue Sep  1 23:09:09 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 5 Output/test_anatomy_aml_bagofnbrs7_5.pkl Models/anatomy_aml_bagofnbrs7_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32507.88 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2717.33 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32662 sec.
    Turnaround time :                            33316 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289927: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs10_5.pkl Models/anatomy_aml_bagofnbrs10_5.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs10_5.pkl Models/anatomy_aml_bagofnbrs10_5.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:53 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 14:06:28 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 14:06:28 2020
Terminated at Wed Sep  2 00:17:45 2020
Results reported at Wed Sep  2 00:17:45 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 5 Output/test_anatomy_aml_bagofnbrs10_5.pkl Models/anatomy_aml_bagofnbrs10_5.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   36620.23 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2717.25 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   36677 sec.
    Turnaround time :                            37432 sec.

The output (if any) is above this job summary.

