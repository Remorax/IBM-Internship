Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1350006971765818
Epoch: 0 Idx: 5000 Loss: 0.013429329888145977
Epoch: 1 Idx: 0 Loss: 0.026658417279906168
Epoch: 1 Idx: 5000 Loss: 0.025606580416958628
Epoch: 2 Idx: 0 Loss: 0.008530294582100922
Epoch: 2 Idx: 5000 Loss: 0.017613605667723173
Epoch: 3 Idx: 0 Loss: 0.030781354534690435
Epoch: 3 Idx: 5000 Loss: 0.01228140174696384
Epoch: 4 Idx: 0 Loss: 0.031764814054923485
Epoch: 4 Idx: 5000 Loss: 0.01876860065749597
Epoch: 5 Idx: 0 Loss: 0.010543799482845412
Epoch: 5 Idx: 5000 Loss: 0.01881359767640007
Epoch: 6 Idx: 0 Loss: 0.010982003259171433
Epoch: 6 Idx: 5000 Loss: 0.012354323832864788
Epoch: 7 Idx: 0 Loss: 0.030461987787046882
Epoch: 7 Idx: 5000 Loss: 0.01599470647988909
Epoch: 8 Idx: 0 Loss: 0.01956169139372495
Epoch: 8 Idx: 5000 Loss: 0.026464201321969534
Epoch: 9 Idx: 0 Loss: 0.032421882936718126
Epoch: 9 Idx: 5000 Loss: 0.015791599951323412
Epoch: 10 Idx: 0 Loss: 0.012108935468944666
Epoch: 10 Idx: 5000 Loss: 0.009602329733099546
Epoch: 11 Idx: 0 Loss: 0.015451388923670385
Epoch: 11 Idx: 5000 Loss: 0.03327685863544431
Epoch: 12 Idx: 0 Loss: 0.02244789058649019
Epoch: 12 Idx: 5000 Loss: 0.007802394192328175
Epoch: 13 Idx: 0 Loss: 0.013436739551037082
Epoch: 13 Idx: 5000 Loss: 0.032070280253263535
Epoch: 14 Idx: 0 Loss: 0.0243946131610921
Epoch: 14 Idx: 5000 Loss: 0.023656420474702358
Epoch: 15 Idx: 0 Loss: 0.007586338497014467
Epoch: 15 Idx: 5000 Loss: 0.022916783091409043
Epoch: 16 Idx: 0 Loss: 0.022434237788984537
Epoch: 16 Idx: 5000 Loss: 0.01748452261952524
Epoch: 17 Idx: 0 Loss: 0.05057208131761572
Epoch: 17 Idx: 5000 Loss: 0.013008017772255208
Epoch: 18 Idx: 0 Loss: 0.029731456150811165
Epoch: 18 Idx: 5000 Loss: 0.02741804526581118
Epoch: 19 Idx: 0 Loss: 0.012380886386484524
Epoch: 19 Idx: 5000 Loss: 0.011476896019185646
Epoch: 20 Idx: 0 Loss: 0.014238314690792345
Epoch: 20 Idx: 5000 Loss: 0.01591401344743956
Epoch: 21 Idx: 0 Loss: 0.026021956729500924
Epoch: 21 Idx: 5000 Loss: 0.015599122594989209
Epoch: 22 Idx: 0 Loss: 0.02385404734557755
Epoch: 22 Idx: 5000 Loss: 0.019605444208530373
Epoch: 23 Idx: 0 Loss: 0.021576844380670323
Epoch: 23 Idx: 5000 Loss: 0.015074576250843345
Epoch: 24 Idx: 0 Loss: 0.032174073540501155
Epoch: 24 Idx: 5000 Loss: 0.029657626047125928
Epoch: 25 Idx: 0 Loss: 0.03286722711084107
Epoch: 25 Idx: 5000 Loss: 0.0342173311966461
Epoch: 26 Idx: 0 Loss: 0.00938095663398393
Epoch: 26 Idx: 5000 Loss: 0.02333946617729652
Epoch: 27 Idx: 0 Loss: 0.01835314092722142
Epoch: 27 Idx: 5000 Loss: 0.03701591276426114
Epoch: 28 Idx: 0 Loss: 0.016780025807468475
Epoch: 28 Idx: 5000 Loss: 0.016607441412496822
Epoch: 29 Idx: 0 Loss: 0.01211891459157918
Epoch: 29 Idx: 5000 Loss: 0.018802670158310806
Epoch: 30 Idx: 0 Loss: 0.01202866290221352
Epoch: 30 Idx: 5000 Loss: 0.02389612785265252
Epoch: 31 Idx: 0 Loss: 0.039761555004525995
Epoch: 31 Idx: 5000 Loss: 0.01627670184358345
Epoch: 32 Idx: 0 Loss: 0.02301773217261829
Epoch: 32 Idx: 5000 Loss: 0.03486345590797307
Epoch: 33 Idx: 0 Loss: 0.007874474736898432
Epoch: 33 Idx: 5000 Loss: 0.01966442845576584
Epoch: 34 Idx: 0 Loss: 0.0194342644703112
Epoch: 34 Idx: 5000 Loss: 0.022773569682033493
Epoch: 35 Idx: 0 Loss: 0.015761556191218922
Epoch: 35 Idx: 5000 Loss: 0.01979355614496662
Epoch: 36 Idx: 0 Loss: 0.022381132135306347
Epoch: 36 Idx: 5000 Loss: 0.00886275864825644
Epoch: 37 Idx: 0 Loss: 0.02556549187260016
Epoch: 37 Idx: 5000 Loss: 0.039597752528548726
Epoch: 38 Idx: 0 Loss: 0.030637545525684404
Epoch: 38 Idx: 5000 Loss: 0.02184279396656654
Epoch: 39 Idx: 0 Loss: 0.01990650954922981
Epoch: 39 Idx: 5000 Loss: 0.014148953212683751
Epoch: 40 Idx: 0 Loss: 0.020798709249069312
Epoch: 40 Idx: 5000 Loss: 0.02660780059794208
Epoch: 41 Idx: 0 Loss: 0.020114525047764044
Epoch: 41 Idx: 5000 Loss: 0.02160820067891967
Epoch: 42 Idx: 0 Loss: 0.05629730152144012
Epoch: 42 Idx: 5000 Loss: 0.01332605886264799
Epoch: 43 Idx: 0 Loss: 0.019443628656581856
Epoch: 43 Idx: 5000 Loss: 0.01956638959405263
Epoch: 44 Idx: 0 Loss: 0.015529889976946449
Epoch: 44 Idx: 5000 Loss: 0.032045044675845624
Epoch: 45 Idx: 0 Loss: 0.0416956027740128
Epoch: 45 Idx: 5000 Loss: 0.028620900212387144
Epoch: 46 Idx: 0 Loss: 0.044798962925524696
Epoch: 46 Idx: 5000 Loss: 0.023384619007285945
Epoch: 47 Idx: 0 Loss: 0.01272677085933523
Epoch: 47 Idx: 5000 Loss: 0.02553565413395885
Epoch: 48 Idx: 0 Loss: 0.02535086740430508
Epoch: 48 Idx: 5000 Loss: 0.039048468549031476
Epoch: 49 Idx: 0 Loss: 0.032954660975739104
Epoch: 49 Idx: 5000 Loss: 0.024704737998296776
Len (direct inputs):  99
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2829600269742744
Epoch: 0 Idx: 5000 Loss: 0.04405950473800944
Epoch: 1 Idx: 0 Loss: 0.035902639347876505
Epoch: 1 Idx: 5000 Loss: 0.021719162384262004
Epoch: 2 Idx: 0 Loss: 0.010512831395517869
Epoch: 2 Idx: 5000 Loss: 0.02103642735193835
Epoch: 3 Idx: 0 Loss: 0.018381848953563408
Epoch: 3 Idx: 5000 Loss: 0.01609987541799042
Epoch: 4 Idx: 0 Loss: 0.03136732323617496
Epoch: 4 Idx: 5000 Loss: 0.026412816992142926
Epoch: 5 Idx: 0 Loss: 0.022158418323682344
Epoch: 5 Idx: 5000 Loss: 0.01568186633764302
Epoch: 6 Idx: 0 Loss: 0.028187682963875728
Epoch: 6 Idx: 5000 Loss: 0.011428030566661088
Epoch: 7 Idx: 0 Loss: 0.02696489772355817
Epoch: 7 Idx: 5000 Loss: 0.03898751217799584
Epoch: 8 Idx: 0 Loss: 0.034480951467979144
Epoch: 8 Idx: 5000 Loss: 0.01175792401703009
Epoch: 9 Idx: 0 Loss: 0.0288890779828787
Epoch: 9 Idx: 5000 Loss: 0.02881575547950792
Epoch: 10 Idx: 0 Loss: 0.019001554571647632
Epoch: 10 Idx: 5000 Loss: 0.014076457320208244
Epoch: 11 Idx: 0 Loss: 0.019136847402229786
Epoch: 11 Idx: 5000 Loss: 0.019477463888299112
Epoch: 12 Idx: 0 Loss: 0.028005873661640042
Epoch: 12 Idx: 5000 Loss: 0.012252828629635153
Epoch: 13 Idx: 0 Loss: 0.01620350280295782
Epoch: 13 Idx: 5000 Loss: 0.027622909338543528
Epoch: 14 Idx: 0 Loss: 0.01621331508836442
Epoch: 14 Idx: 5000 Loss: 0.0174842061778807
Epoch: 15 Idx: 0 Loss: 0.030032839235457322
Epoch: 15 Idx: 5000 Loss: 0.02585240114296558
Epoch: 16 Idx: 0 Loss: 0.024192914601797462
Epoch: 16 Idx: 5000 Loss: 0.01993617975551033
Epoch: 17 Idx: 0 Loss: 0.02051349319112867
Epoch: 17 Idx: 5000 Loss: 0.02650707257913659
Epoch: 18 Idx: 0 Loss: 0.02419592675587893
Epoch: 18 Idx: 5000 Loss: 0.018446923723245277
Epoch: 19 Idx: 0 Loss: 0.01570010758012025
Epoch: 19 Idx: 5000 Loss: 0.03125246919739502
Epoch: 20 Idx: 0 Loss: 0.036037074182002075
Epoch: 20 Idx: 5000 Loss: 0.02244969127145905
Epoch: 21 Idx: 0 Loss: 0.03722342394766742
Epoch: 21 Idx: 5000 Loss: 0.016913337856094085
Epoch: 22 Idx: 0 Loss: 0.032101888322956226
Epoch: 22 Idx: 5000 Loss: 0.01966477045252081
Epoch: 23 Idx: 0 Loss: 0.016299270113958146
Epoch: 23 Idx: 5000 Loss: 0.019614217803671112
Epoch: 24 Idx: 0 Loss: 0.019729972570859906
Epoch: 24 Idx: 5000 Loss: 0.009627590715846953
Epoch: 25 Idx: 0 Loss: 0.018332894679282993
Epoch: 25 Idx: 5000 Loss: 0.019627492325184855
Epoch: 26 Idx: 0 Loss: 0.03898697200872524
Epoch: 26 Idx: 5000 Loss: 0.008310546119407528
Epoch: 27 Idx: 0 Loss: 0.019069134731885803
Epoch: 27 Idx: 5000 Loss: 0.01357012548402204
Epoch: 28 Idx: 0 Loss: 0.02153572689608079
Epoch: 28 Idx: 5000 Loss: 0.017251838920450305
Epoch: 29 Idx: 0 Loss: 0.03751267345401944
Epoch: 29 Idx: 5000 Loss: 0.026903465214108444
Epoch: 30 Idx: 0 Loss: 0.01751521790092376
Epoch: 30 Idx: 5000 Loss: 0.0123325904438402
Epoch: 31 Idx: 0 Loss: 0.023181579721543492
Epoch: 31 Idx: 5000 Loss: 0.02079716586660281
Epoch: 32 Idx: 0 Loss: 0.01841949671475086
Epoch: 32 Idx: 5000 Loss: 0.021869304115603172
Epoch: 33 Idx: 0 Loss: 0.025211805186726576
Epoch: 33 Idx: 5000 Loss: 0.015751315176643972
Epoch: 34 Idx: 0 Loss: 0.013300080468786652
Epoch: 34 Idx: 5000 Loss: 0.017933140874661187
Epoch: 35 Idx: 0 Loss: 0.006198137997534594
Epoch: 35 Idx: 5000 Loss: 0.018095951045795967
Epoch: 36 Idx: 0 Loss: 0.01660278195856338
Epoch: 36 Idx: 5000 Loss: 0.02323673686153694
Epoch: 37 Idx: 0 Loss: 0.029473847221627457
Epoch: 37 Idx: 5000 Loss: 0.01605877445415666
Epoch: 38 Idx: 0 Loss: 0.01819406487989364
Epoch: 38 Idx: 5000 Loss: 0.01866768996126589
Epoch: 39 Idx: 0 Loss: 0.03041865983105264
Epoch: 39 Idx: 5000 Loss: 0.010403688782845559
Epoch: 40 Idx: 0 Loss: 0.021059437324623783
Epoch: 40 Idx: 5000 Loss: 0.024865056298949653
Epoch: 41 Idx: 0 Loss: 0.034565396942996554
Epoch: 41 Idx: 5000 Loss: 0.015681259375236282
Epoch: 42 Idx: 0 Loss: 0.035175416326759815
Epoch: 42 Idx: 5000 Loss: 0.01367477729145083
Epoch: 43 Idx: 0 Loss: 0.01477550781226722
Epoch: 43 Idx: 5000 Loss: 0.016269194736429644
Epoch: 44 Idx: 0 Loss: 0.011796986846331174
Epoch: 44 Idx: 5000 Loss: 0.0198899019152143
Epoch: 45 Idx: 0 Loss: 0.01930211045006923
Epoch: 45 Idx: 5000 Loss: 0.024839662039875268
Epoch: 46 Idx: 0 Loss: 0.014409900989134165
Epoch: 46 Idx: 5000 Loss: 0.017894004455114982
Epoch: 47 Idx: 0 Loss: 0.02162496020165617
Epoch: 47 Idx: 5000 Loss: 0.022181560885965512
Epoch: 48 Idx: 0 Loss: 0.02072532944999045
Epoch: 48 Idx: 5000 Loss: 0.03165241089855104
Epoch: 49 Idx: 0 Loss: 0.01905837414516957
Epoch: 49 Idx: 5000 Loss: 0.027029057531159674
Len (direct inputs):  116
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19774366035506108
Epoch: 0 Idx: 5000 Loss: 0.027612873488188222
Epoch: 1 Idx: 0 Loss: 0.033354777231258244
Epoch: 1 Idx: 5000 Loss: 0.01846871930638892
Epoch: 2 Idx: 0 Loss: 0.019986918794252857
Epoch: 2 Idx: 5000 Loss: 0.023104409101325673
Epoch: 3 Idx: 0 Loss: 0.03723391624459996
Epoch: 3 Idx: 5000 Loss: 0.024768329969895156
Epoch: 4 Idx: 0 Loss: 0.015831266179883056
Epoch: 4 Idx: 5000 Loss: 0.025813935757366964
Epoch: 5 Idx: 0 Loss: 0.026406200319089843
Epoch: 5 Idx: 5000 Loss: 0.031653767105436846
Epoch: 6 Idx: 0 Loss: 0.00946627168614744
Epoch: 6 Idx: 5000 Loss: 0.014843776143672122
Epoch: 7 Idx: 0 Loss: 0.028149828630269907
Epoch: 7 Idx: 5000 Loss: 0.01730237731428025
Epoch: 8 Idx: 0 Loss: 0.01660092471628522
Epoch: 8 Idx: 5000 Loss: 0.030850532161265237
Epoch: 9 Idx: 0 Loss: 0.02976408239475811
Epoch: 9 Idx: 5000 Loss: 0.017197275236912488
Epoch: 10 Idx: 0 Loss: 0.009514347382337262
Epoch: 10 Idx: 5000 Loss: 0.02059257097793698
Epoch: 11 Idx: 0 Loss: 0.02374554879189012
Epoch: 11 Idx: 5000 Loss: 0.02981083850169526
Epoch: 12 Idx: 0 Loss: 0.031555691745298105
Epoch: 12 Idx: 5000 Loss: 0.015591166419066157
Epoch: 13 Idx: 0 Loss: 0.015813825654228973
Epoch: 13 Idx: 5000 Loss: 0.026739436305306057
Epoch: 14 Idx: 0 Loss: 0.030359832898289925
Epoch: 14 Idx: 5000 Loss: 0.01112141198295935
Epoch: 15 Idx: 0 Loss: 0.016273021765491568
Epoch: 15 Idx: 5000 Loss: 0.017327524006432227
Epoch: 16 Idx: 0 Loss: 0.013396941231215978
Epoch: 16 Idx: 5000 Loss: 0.024498732564563197
Epoch: 17 Idx: 0 Loss: 0.02162463886319232
Epoch: 17 Idx: 5000 Loss: 0.026227836878145117
Epoch: 18 Idx: 0 Loss: 0.038148310482004316
Epoch: 18 Idx: 5000 Loss: 0.02610130236320686
Epoch: 19 Idx: 0 Loss: 0.032231032955751825
Epoch: 19 Idx: 5000 Loss: 0.0230101873374104
Epoch: 20 Idx: 0 Loss: 0.019886859178712836
Epoch: 20 Idx: 5000 Loss: 0.022376655302040388
Epoch: 21 Idx: 0 Loss: 0.010246120203245546
Epoch: 21 Idx: 5000 Loss: 0.012717931399467726
Epoch: 22 Idx: 0 Loss: 0.03047708100635392
Epoch: 22 Idx: 5000 Loss: 0.018260798681709055
Epoch: 23 Idx: 0 Loss: 0.022385132712528174
Epoch: 23 Idx: 5000 Loss: 0.011565327842454407
Epoch: 24 Idx: 0 Loss: 0.018857472462047865
Epoch: 24 Idx: 5000 Loss: 0.019346705079325482
Epoch: 25 Idx: 0 Loss: 0.019265332976508464
Epoch: 25 Idx: 5000 Loss: 0.0193219203267338
Epoch: 26 Idx: 0 Loss: 0.006172846646599953
Epoch: 26 Idx: 5000 Loss: 0.015547323803045635
Epoch: 27 Idx: 0 Loss: 0.016614322127697492
Epoch: 27 Idx: 5000 Loss: 0.02736592748314457
Epoch: 28 Idx: 0 Loss: 0.039648776432853666
Epoch: 28 Idx: 5000 Loss: 0.010146609855720395
Epoch: 29 Idx: 0 Loss: 0.026706451844310596
Epoch: 29 Idx: 5000 Loss: 0.01869359686215221
Epoch: 30 Idx: 0 Loss: 0.0290043583448686
Epoch: 30 Idx: 5000 Loss: 0.023664072589552197
Epoch: 31 Idx: 0 Loss: 0.023976060491467915
Epoch: 31 Idx: 5000 Loss: 0.019969029730827512
Epoch: 32 Idx: 0 Loss: 0.033089591674040394
Epoch: 32 Idx: 5000 Loss: 0.028033665483465732
Epoch: 33 Idx: 0 Loss: 0.022890018279709424
Epoch: 33 Idx: 5000 Loss: 0.0415598177906927
Epoch: 34 Idx: 0 Loss: 0.020958366110370948
Epoch: 34 Idx: 5000 Loss: 0.040834723279272664
Epoch: 35 Idx: 0 Loss: 0.02927884019366359
Epoch: 35 Idx: 5000 Loss: 0.032796531547133266
Epoch: 36 Idx: 0 Loss: 0.019067878429953583
Epoch: 36 Idx: 5000 Loss: 0.01536301812731318
Epoch: 37 Idx: 0 Loss: 0.016300114944656465
Epoch: 37 Idx: 5000 Loss: 0.02499720577891509
Epoch: 38 Idx: 0 Loss: 0.028422450135556235
Epoch: 38 Idx: 5000 Loss: 0.016245546320358264
Epoch: 39 Idx: 0 Loss: 0.015946817693487398
Epoch: 39 Idx: 5000 Loss: 0.02802304580780587
Epoch: 40 Idx: 0 Loss: 0.030644010771474255
Traceback (most recent call last):
  File "Attention_anatomy_aml_4props.py", line 400, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
dx: 5000 Loss: 0.022739762935940513
Epoch: 45 Idx: 0 Loss: 0.02509224171036508
Epoch: 45 Idx: 5000 Loss: 0.0287714915999834
Epoch: 46 Idx: 0 Loss: 0.014928223172420527
Epoch: 46 Idx: 5000 Loss: 0.01599661690476242
Epoch: 47 Idx: 0 Loss: 0.01166670372977642
Epoch: 47 Idx: 5000 Loss: 0.017568915658385746
Epoch: 48 Idx: 0 Loss: 0.019746366495747795
Epoch: 48 Idx: 5000 Loss: 0.023437270819677378
Epoch: 49 Idx: 0 Loss: 0.019455921739108893
Epoch: 49 Idx: 5000 Loss: 0.021289456190626485
Len (direct inputs):  109
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2477527439445623
Epoch: 0 Idx: 5000 Loss: 0.022651951220308143
Epoch: 1 Idx: 0 Loss: 0.015211717699452298
Epoch: 1 Idx: 5000 Loss: 0.01078626904365415
Epoch: 2 Idx: 0 Loss: 0.024378945207914345
Epoch: 2 Idx: 5000 Loss: 0.011038945677877674
Epoch: 3 Idx: 0 Loss: 0.029546147866167043
Epoch: 3 Idx: 5000 Loss: 0.019121058740604272
Epoch: 4 Idx: 0 Loss: 0.01913464361305771
Epoch: 4 Idx: 5000 Loss: 0.011986376625037222
Epoch: 5 Idx: 0 Loss: 0.037021587852551004
Epoch: 5 Idx: 5000 Loss: 0.010096989041269721
Epoch: 6 Idx: 0 Loss: 0.010205581840124678
Traceback (most recent call last):
  File "Attention_anatomy_aml_4props.py", line 401, in <module>
    loss.backward()
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
00 Loss: 0.02796266288312768
Epoch: 12 Idx: 0 Loss: 0.018009694877085387
Epoch: 12 Idx: 5000 Loss: 0.01007777205339748
Epoch: 13 Idx: 0 Loss: 0.03973459394617852
Epoch: 13 Idx: 5000 Loss: 0.03139810128629315
Epoch: 14 Idx: 0 Loss: 0.033214290653507575
Epoch: 14 Idx: 5000 Loss: 0.021386158298739164
Epoch: 15 Idx: 0 Loss: 0.022372628054075656
Epoch: 15 Idx: 5000 Loss: 0.024022010180784308
Epoch: 16 Idx: 0 Loss: 0.03500450060836717
Epoch: 16 Idx: 5000 Loss: 0.01852978754735412
Epoch: 17 Idx: 0 Loss: 0.022287626810611055
Epoch: 17 Idx: 5000 Loss: 0.016770665547283296
Epoch: 18 Idx: 0 Loss: 0.02651340830697088
Epoch: 18 Idx: 5000 Loss: 0.01732071944765916
Epoch: 19 Idx: 0 Loss: 0.023963272238825883
Epoch: 19 Idx: 5000 Loss: 0.013169065053271822
Epoch: 20 Idx: 0 Loss: 0.016189963282924515
Epoch: 20 Idx: 5000 Loss: 0.014265134767535092
Epoch: 21 Idx: 0 Loss: 0.03038459118613622
Epoch: 21 Idx: 5000 Loss: 0.02240035928137578
Epoch: 22 Idx: 0 Loss: 0.02649391042329585
Epoch: 22 Idx: 5000 Loss: 0.0449916689378493
Epoch: 23 Idx: 0 Loss: 0.018721223804927273
Epoch: 23 Idx: 5000 Loss: 0.02179170754793447
Epoch: 24 Idx: 0 Loss: 0.026428846825381505
Epoch: 24 Idx: 5000 Loss: 0.01116315585490579
Epoch: 25 Idx: 0 Loss: 0.031561866957945474
Epoch: 25 Idx: 5000 Loss: 0.037460440125712695
Epoch: 26 Idx: 0 Loss: 0.03859087671459284
Epoch: 26 Idx: 5000 Loss: 0.008613474787821446
Epoch: 27 Idx: 0 Loss: 0.02721885279798771
Epoch: 27 Idx: 5000 Loss: 0.011304742116693592
Epoch: 28 Idx: 0 Loss: 0.024970360054856842
Epoch: 28 Idx: 5000 Loss: 0.02097903292669188
Epoch: 29 Idx: 0 Loss: 0.015633710115505725
Epoch: 29 Idx: 5000 Loss: 0.019177678426732767
Epoch: 30 Idx: 0 Loss: 0.023659754073354126
Epoch: 30 Idx: 5000 Loss: 0.032508742551961685
Epoch: 31 Idx: 0 Loss: 0.010962766606058147
Epoch: 31 Idx: 5000 Loss: 0.02018876848075227
Epoch: 32 Idx: 0 Loss: 0.01662821988139184
Traceback (most recent call last):
  File "Attention_anatomy_aml_4props.py", line 398, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "Attention_anatomy_aml_4props.py", line 283, in forward
    + self.w_data_neighbours * distance_weighted_path[:,3,:]
KeyboardInterrupt
rch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc221>
Subject: Job 3290128: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 10 Output/test_anatomy_aml_bagofnbrs_4props7_10.pkl Models/anatomy_aml_bagofnbrs_4props7_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 10 Output/test_anatomy_aml_bagofnbrs_4props7_10.pkl Models/anatomy_aml_bagofnbrs_4props7_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:05 2020
Job was executed on host(s) <dccxc221>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:22:19 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:22:19 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 10 Output/test_anatomy_aml_bagofnbrs_4props7_10.pkl Models/anatomy_aml_bagofnbrs_4props7_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21381.48 sec.
    Max Memory :                                 2740 MB
    Average Memory :                             2673.72 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40677.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21418 sec.
    Turnaround time :                            80683 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc273>
Subject: Job 3290132: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 15 10 Output/test_anatomy_aml_bagofnbrs_4props15_10.pkl Models/anatomy_aml_bagofnbrs_4props15_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 15 10 Output/test_anatomy_aml_bagofnbrs_4props15_10.pkl Models/anatomy_aml_bagofnbrs_4props15_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc273>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:40:18 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:40:18 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 15 10 Output/test_anatomy_aml_bagofnbrs_4props15_10.pkl Models/anatomy_aml_bagofnbrs_4props15_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20262.20 sec.
    Max Memory :                                 2735 MB
    Average Memory :                             2642.30 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40682.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   20310 sec.
    Turnaround time :                            80682 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc243>
Subject: Job 3290130: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 10 Output/test_anatomy_aml_bagofnbrs_4props10_10.pkl Models/anatomy_aml_bagofnbrs_4props10_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 10 Output/test_anatomy_aml_bagofnbrs_4props10_10.pkl Models/anatomy_aml_bagofnbrs_4props10_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:05 2020
Job was executed on host(s) <dccxc243>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:32:55 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:32:55 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 10 10 Output/test_anatomy_aml_bagofnbrs_4props10_10.pkl Models/anatomy_aml_bagofnbrs_4props10_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20717.68 sec.
    Max Memory :                                 2733 MB
    Average Memory :                             2661.14 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40684.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   20763 sec.
    Turnaround time :                            80683 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc264>
Subject: Job 3290129: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 10 Output/test_anatomy_aml_bagofnbrs_4props8_10.pkl Models/anatomy_aml_bagofnbrs_4props8_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 10 Output/test_anatomy_aml_bagofnbrs_4props8_10.pkl Models/anatomy_aml_bagofnbrs_4props8_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:05 2020
Job was executed on host(s) <dccxc264>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:24:20 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:24:20 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 10 Output/test_anatomy_aml_bagofnbrs_4props8_10.pkl Models/anatomy_aml_bagofnbrs_4props8_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21221.62 sec.
    Max Memory :                                 2732 MB
    Average Memory :                             2658.71 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40685.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21268 sec.
    Turnaround time :                            80683 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc251>
Subject: Job 3290131: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 12 10 Output/test_anatomy_aml_bagofnbrs_4props12_10.pkl Models/anatomy_aml_bagofnbrs_4props12_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 12 10 Output/test_anatomy_aml_bagofnbrs_4props12_10.pkl Models/anatomy_aml_bagofnbrs_4props12_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc251>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:36:16 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:36:16 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_4props.py Input/data_anatomy_oaei_bagofnbrs.pkl 12 10 Output/test_anatomy_aml_bagofnbrs_4props12_10.pkl Models/anatomy_aml_bagofnbrs_4props12_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20547.32 sec.
    Max Memory :                                 2724 MB
    Average Memory :                             2641.58 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40693.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   20555 sec.
    Turnaround time :                            80683 sec.

The output (if any) is above this job summary.

