Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21107186607937006
Epoch: 0 Idx: 5000 Loss: 0.027152570131569367
Epoch: 1 Idx: 0 Loss: 0.02769560642552728
Epoch: 1 Idx: 5000 Loss: 0.01795065632319946
Epoch: 2 Idx: 0 Loss: 0.020275210686618958
Epoch: 2 Idx: 5000 Loss: 0.019762048847571233
Epoch: 3 Idx: 0 Loss: 0.034000684077328924
Epoch: 3 Idx: 5000 Loss: 0.01983389476333986
Epoch: 4 Idx: 0 Loss: 0.01638753696999904
Epoch: 4 Idx: 5000 Loss: 0.009635247754958338
Epoch: 5 Idx: 0 Loss: 0.010855030538550718
Epoch: 5 Idx: 5000 Loss: 0.024420896084304908
Epoch: 6 Idx: 0 Loss: 0.023967576246282596
Epoch: 6 Idx: 5000 Loss: 0.015198240376074573
Epoch: 7 Idx: 0 Loss: 0.0324770169943548
Epoch: 7 Idx: 5000 Loss: 0.02671991147884079
Epoch: 8 Idx: 0 Loss: 0.015484588945019442
Epoch: 8 Idx: 5000 Loss: 0.0280457812375719
Epoch: 9 Idx: 0 Loss: 0.014110287853555423
Epoch: 9 Idx: 5000 Loss: 0.011675393695915895
Epoch: 10 Idx: 0 Loss: 0.02644561755305391
Epoch: 10 Idx: 5000 Loss: 0.018748166129752154
Epoch: 11 Idx: 0 Loss: 0.013331437315278671
Epoch: 11 Idx: 5000 Loss: 0.01697652220100283
Epoch: 12 Idx: 0 Loss: 0.007276343533414411
Epoch: 12 Idx: 5000 Loss: 0.01900005380740309
Epoch: 13 Idx: 0 Loss: 0.02383279079228054
Epoch: 13 Idx: 5000 Loss: 0.018242959335115175
Epoch: 14 Idx: 0 Loss: 0.02294603539385439
Epoch: 14 Idx: 5000 Loss: 0.041305259669002746
Epoch: 15 Idx: 0 Loss: 0.01755279751645555
Epoch: 15 Idx: 5000 Loss: 0.018849933529175526
Epoch: 16 Idx: 0 Loss: 0.013040859638427832
Epoch: 16 Idx: 5000 Loss: 0.01381955886578131
Epoch: 17 Idx: 0 Loss: 0.012329646782244448
Epoch: 17 Idx: 5000 Loss: 0.018605826995729335
Epoch: 18 Idx: 0 Loss: 0.034178004312227034
Epoch: 18 Idx: 5000 Loss: 0.011395236269422505
Epoch: 19 Idx: 0 Loss: 0.01888323090542228
Epoch: 19 Idx: 5000 Loss: 0.008649892681885128
Epoch: 20 Idx: 0 Loss: 0.011220942171333032
Epoch: 20 Idx: 5000 Loss: 0.010425359003334553
Epoch: 21 Idx: 0 Loss: 0.02064650764875533
Epoch: 21 Idx: 5000 Loss: 0.030929209253746127
Epoch: 22 Idx: 0 Loss: 0.031919135263101035
Epoch: 22 Idx: 5000 Loss: 0.028713564108374782
Epoch: 23 Idx: 0 Loss: 0.008762284260152083
Epoch: 23 Idx: 5000 Loss: 0.021814261374499707
Epoch: 24 Idx: 0 Loss: 0.02151883050502148
Epoch: 24 Idx: 5000 Loss: 0.02513628013099757
Epoch: 25 Idx: 0 Loss: 0.01996444936849396
Epoch: 25 Idx: 5000 Loss: 0.03758625366162824
Epoch: 26 Idx: 0 Loss: 0.018105271670106393
Epoch: 26 Idx: 5000 Loss: 0.018087298266053074
Epoch: 27 Idx: 0 Loss: 0.023268463666270577
Epoch: 27 Idx: 5000 Loss: 0.02124707778223554
Epoch: 28 Idx: 0 Loss: 0.03146958879383075
Epoch: 28 Idx: 5000 Loss: 0.020977981470837397
Epoch: 29 Idx: 0 Loss: 0.015091182587164862
Epoch: 29 Idx: 5000 Loss: 0.012372145532720657
Epoch: 30 Idx: 0 Loss: 0.013973210264949219
Epoch: 30 Idx: 5000 Loss: 0.02181813865553608
Epoch: 31 Idx: 0 Loss: 0.01058041910692049
Epoch: 31 Idx: 5000 Loss: 0.014410248514682153
Epoch: 32 Idx: 0 Loss: 0.028834620466012126
Epoch: 32 Idx: 5000 Loss: 0.016972651493435117
Epoch: 33 Idx: 0 Loss: 0.010809036917866478
Epoch: 33 Idx: 5000 Loss: 0.022386254945112333
Epoch: 34 Idx: 0 Loss: 0.014348906358662204
Epoch: 34 Idx: 5000 Loss: 0.011885172033600974
Epoch: 35 Idx: 0 Loss: 0.012145522315327707
Epoch: 35 Idx: 5000 Loss: 0.03398230237493151
Epoch: 36 Idx: 0 Loss: 0.02296351208823099
Epoch: 36 Idx: 5000 Loss: 0.009422430195686067
Epoch: 37 Idx: 0 Loss: 0.024944257177025208
Epoch: 37 Idx: 5000 Loss: 0.014911560622135323
Epoch: 38 Idx: 0 Loss: 0.014945115019360699
Epoch: 38 Idx: 5000 Loss: 0.038193602358420016
Epoch: 39 Idx: 0 Loss: 0.029240073258460147
Epoch: 39 Idx: 5000 Loss: 0.014236965266894493
Epoch: 40 Idx: 0 Loss: 0.030263240684789732
Epoch: 40 Idx: 5000 Loss: 0.013801514613824022
Epoch: 41 Idx: 0 Loss: 0.019626922887842436
Epoch: 41 Idx: 5000 Loss: 0.019431275274762346
Epoch: 42 Idx: 0 Loss: 0.02760071995893521
Epoch: 42 Idx: 5000 Loss: 0.007602322374182983
Epoch: 43 Idx: 0 Loss: 0.028824670900553902
Epoch: 43 Idx: 5000 Loss: 0.042285464382336366
Epoch: 44 Idx: 0 Loss: 0.014551166537815848
Epoch: 44 Idx: 5000 Loss: 0.018316784145976872
Epoch: 45 Idx: 0 Loss: 0.03272018358527444
Epoch: 45 Idx: 5000 Loss: 0.01476208552285286
Epoch: 46 Idx: 0 Loss: 0.03778520127480316
Epoch: 46 Idx: 5000 Loss: 0.01921785312492652
Epoch: 47 Idx: 0 Loss: 0.013728118935364877
Epoch: 47 Idx: 5000 Loss: 0.020447734751427915
Epoch: 48 Idx: 0 Loss: 0.015502809392358294
Epoch: 48 Idx: 5000 Loss: 0.015457730161308327
Epoch: 49 Idx: 0 Loss: 0.033598773507608835
Epoch: 49 Idx: 5000 Loss: 0.024676398164157565
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2423504008932823
Epoch: 0 Idx: 5000 Loss: 0.022308966602930677
Epoch: 1 Idx: 0 Loss: 0.03366079853116646
Epoch: 1 Idx: 5000 Loss: 0.01710845023351682
Epoch: 2 Idx: 0 Loss: 0.021089452220769564
Epoch: 2 Idx: 5000 Loss: 0.03591715262963157
Epoch: 3 Idx: 0 Loss: 0.01814247728682316
Epoch: 3 Idx: 5000 Loss: 0.03234589002502643
Epoch: 4 Idx: 0 Loss: 0.010106788933346043
Epoch: 4 Idx: 5000 Loss: 0.013393357375154973
Epoch: 5 Idx: 0 Loss: 0.01181179384303129
Epoch: 5 Idx: 5000 Loss: 0.025025805329998534
Epoch: 6 Idx: 0 Loss: 0.027852636198771333
Epoch: 6 Idx: 5000 Loss: 0.010168051436409113
Epoch: 7 Idx: 0 Loss: 0.0468255101989645
Epoch: 7 Idx: 5000 Loss: 0.01741396838498204
Epoch: 8 Idx: 0 Loss: 0.018548330404116707
Epoch: 8 Idx: 5000 Loss: 0.017383898805382145
Epoch: 9 Idx: 0 Loss: 0.01810315633169182
Epoch: 9 Idx: 5000 Loss: 0.020724953085930684
Epoch: 10 Idx: 0 Loss: 0.020097929670628775
Epoch: 10 Idx: 5000 Loss: 0.02328337539729295
Epoch: 11 Idx: 0 Loss: 0.028814539115222443
Epoch: 11 Idx: 5000 Loss: 0.01517975852409345
Epoch: 12 Idx: 0 Loss: 0.021404928780755117
Epoch: 12 Idx: 5000 Loss: 0.05111411154509414
Epoch: 13 Idx: 0 Loss: 0.023627842443897925
Epoch: 13 Idx: 5000 Loss: 0.033008089595622916
Epoch: 14 Idx: 0 Loss: 0.03468318889093579
Epoch: 14 Idx: 5000 Loss: 0.030206306424455653
Epoch: 15 Idx: 0 Loss: 0.012759495432044157
Epoch: 15 Idx: 5000 Loss: 0.01868507910556648
Epoch: 16 Idx: 0 Loss: 0.010414268616764477
Epoch: 16 Idx: 5000 Loss: 0.015555488605076488
Epoch: 17 Idx: 0 Loss: 0.01845139427659126
Epoch: 17 Idx: 5000 Loss: 0.02418819505275904
Epoch: 18 Idx: 0 Loss: 0.02746491254395619
Epoch: 18 Idx: 5000 Loss: 0.021259126370576056
Epoch: 19 Idx: 0 Loss: 0.02717269532052946
Epoch: 19 Idx: 5000 Loss: 0.025836957389473635
Epoch: 20 Idx: 0 Loss: 0.01142326596652917
Epoch: 20 Idx: 5000 Loss: 0.02205982871653679
Epoch: 21 Idx: 0 Loss: 0.018587089241942308
Epoch: 21 Idx: 5000 Loss: 0.010856837999163847
Epoch: 22 Idx: 0 Loss: 0.020690552976815223
Epoch: 22 Idx: 5000 Loss: 0.04139838263958266
Epoch: 23 Idx: 0 Loss: 0.019392473612518085
Epoch: 23 Idx: 5000 Loss: 0.04241808319426705
Epoch: 24 Idx: 0 Loss: 0.01775164904739282
Epoch: 24 Idx: 5000 Loss: 0.017448117985195504
Epoch: 25 Idx: 0 Loss: 0.04674024364126507
Epoch: 25 Idx: 5000 Loss: 0.026425191622393082
Epoch: 26 Idx: 0 Loss: 0.020419912509611775
Epoch: 26 Idx: 5000 Loss: 0.01161195093781366
Epoch: 27 Idx: 0 Loss: 0.01938833525798869
Epoch: 27 Idx: 5000 Loss: 0.0277151778788683
Epoch: 28 Idx: 0 Loss: 0.01663630828012779
Epoch: 28 Idx: 5000 Loss: 0.0330952088327046
Epoch: 29 Idx: 0 Loss: 0.01714246983793603
Epoch: 29 Idx: 5000 Loss: 0.01250307409994544
Epoch: 30 Idx: 0 Loss: 0.025100573461667
Epoch: 30 Idx: 5000 Loss: 0.025946828412338013
Epoch: 31 Idx: 0 Loss: 0.028080175155373102
Epoch: 31 Idx: 5000 Loss: 0.02582265579979374
Epoch: 32 Idx: 0 Loss: 0.028159749147475568
Epoch: 32 Idx: 5000 Loss: 0.011284336486338968
Epoch: 33 Idx: 0 Loss: 0.0184732175733058
Epoch: 33 Idx: 5000 Loss: 0.024091762222960438
Epoch: 34 Idx: 0 Loss: 0.010749143008455347
Epoch: 34 Idx: 5000 Loss: 0.027219531394531947
Epoch: 35 Idx: 0 Loss: 0.03154273291719234
Epoch: 35 Idx: 5000 Loss: 0.03412922347071045
Epoch: 36 Idx: 0 Loss: 0.028751496232177483
Epoch: 36 Idx: 5000 Loss: 0.019599901849773185
Epoch: 37 Idx: 0 Loss: 0.018540991645880946
Epoch: 37 Idx: 5000 Loss: 0.018415070786407087
Epoch: 38 Idx: 0 Loss: 0.021098385946847212
Epoch: 38 Idx: 5000 Loss: 0.014588164962465112
Epoch: 39 Idx: 0 Loss: 0.032961547698309335
Epoch: 39 Idx: 5000 Loss: 0.011097606130059726
Epoch: 40 Idx: 0 Loss: 0.02371030377174734
Epoch: 40 Idx: 5000 Loss: 0.013291556082701883
Epoch: 41 Idx: 0 Loss: 0.022025479477227362
Epoch: 41 Idx: 5000 Loss: 0.03630331723990224
Epoch: 42 Idx: 0 Loss: 0.013506017078810668
Epoch: 42 Idx: 5000 Loss: 0.022388287188736686
Epoch: 43 Idx: 0 Loss: 0.020463357063564662
Epoch: 43 Idx: 5000 Loss: 0.01287980656515134
Epoch: 44 Idx: 0 Loss: 0.0180332327069178
Epoch: 44 Idx: 5000 Loss: 0.013346506048086463
Epoch: 45 Idx: 0 Loss: 0.03270612079161675
Epoch: 45 Idx: 5000 Loss: 0.015481200856677067
Epoch: 46 Idx: 0 Loss: 0.03243006138821986
Epoch: 46 Idx: 5000 Loss: 0.02126814141166815
Epoch: 47 Idx: 0 Loss: 0.03440799091947468
Epoch: 47 Idx: 5000 Loss: 0.029419550859051985
Epoch: 48 Idx: 0 Loss: 0.016843515527739317
Epoch: 48 Idx: 5000 Loss: 0.02232771630269486
Epoch: 49 Idx: 0 Loss: 0.026960734330568135
Epoch: 49 Idx: 5000 Loss: 0.037362038333845826
Len (direct inputs):  100
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22174017243932737
Epoch: 0 Idx: 5000 Loss: 0.022492430062904217
Epoch: 1 Idx: 0 Loss: 0.02030183022080536
Epoch: 1 Idx: 5000 Loss: 0.027598584690665236
Epoch: 2 Idx: 0 Loss: 0.009332230401708013
Epoch: 2 Idx: 5000 Loss: 0.022802214197383534
Epoch: 3 Idx: 0 Loss: 0.033551203213421844
Epoch: 3 Idx: 5000 Loss: 0.029068265060194998
Epoch: 4 Idx: 0 Loss: 0.019099995420185586
Epoch: 4 Idx: 5000 Loss: 0.03887497485570843
Epoch: 5 Idx: 0 Loss: 0.016915464140651808
Epoch: 5 Idx: 5000 Loss: 0.02441014771314602
Epoch: 6 Idx: 0 Loss: 0.03054549435549779
Epoch: 6 Idx: 5000 Loss: 0.026554401054188
Epoch: 7 Idx: 0 Loss: 0.01289799607473437
Epoch: 7 Idx: 5000 Loss: 0.055363677235623646
Epoch: 8 Idx: 0 Loss: 0.011562016456293329
Epoch: 8 Idx: 5000 Loss: 0.018764479643315613
Epoch: 9 Idx: 0 Loss: 0.018675918947268076
Epoch: 9 Idx: 5000 Loss: 0.019928772139063057
Epoch: 10 Idx: 0 Loss: 0.03374863689295815
Epoch: 10 Idx: 5000 Loss: 0.03325552081139531
Epoch: 11 Idx: 0 Loss: 0.019060111656345385
Epoch: 11 Idx: 5000 Loss: 0.017021011518709
Epoch: 12 Idx: 0 Loss: 0.018181111268184634
Epoch: 12 Idx: 5000 Loss: 0.025224171964814343
Epoch: 13 Idx: 0 Loss: 0.014171870936663658
Epoch: 13 Idx: 5000 Loss: 0.027254730336423644
Epoch: 14 Idx: 0 Loss: 0.016056895855033684
Epoch: 14 Idx: 5000 Loss: 0.02934994468955573
Epoch: 15 Idx: 0 Loss: 0.009808596257228687
Epoch: 15 Idx: 5000 Loss: 0.03436748408348342
Epoch: 16 Idx: 0 Loss: 0.04219810741410547
Epoch: 16 Idx: 5000 Loss: 0.022327321531064648
Epoch: 17 Idx: 0 Loss: 0.039003852338571385
Epoch: 17 Idx: 5000 Loss: 0.033198856313551614
Epoch: 18 Idx: 0 Loss: 0.02126077262563707
Epoch: 18 Idx: 5000 Loss: 0.013376090568152008
Epoch: 19 Idx: 0 Loss: 0.01942707584421908
Epoch: 19 Idx: 5000 Loss: 0.028479815021036255
Epoch: 20 Idx: 0 Loss: 0.02504273435050227
Epoch: 20 Idx: 5000 Loss: 0.02652480127139191
Epoch: 21 Idx: 0 Loss: 0.028136524674773764
Epoch: 21 Idx: 5000 Loss: 0.014105139696155962
Epoch: 22 Idx: 0 Loss: 0.018167542128655158
Epoch: 22 Idx: 5000 Loss: 0.011903049855701937
Epoch: 23 Idx: 0 Loss: 0.015540791483655633
Epoch: 23 Idx: 5000 Loss: 0.018924004733533284
Epoch: 24 Idx: 0 Loss: 0.016002567786898066
Epoch: 24 Idx: 5000 Loss: 0.022588934931428085
Epoch: 25 Idx: 0 Loss: 0.024491693429873434
Epoch: 25 Idx: 5000 Loss: 0.014930866527547756
Epoch: 26 Idx: 0 Loss: 0.014334860843618819
Epoch: 26 Idx: 5000 Loss: 0.019786324136301082
Epoch: 27 Idx: 0 Loss: 0.02573197457444753
Epoch: 27 Idx: 5000 Loss: 0.02461560514606305
Epoch: 28 Idx: 0 Loss: 0.018979263656275704
Epoch: 28 Idx: 5000 Loss: 0.027741609207628644
Epoch: 29 Idx: 0 Loss: 0.028240902260012815
Epoch: 29 Idx: 5000 Loss: 0.02272203158758354
Epoch: 30 Idx: 0 Loss: 0.035931467879416465
Epoch: 30 Idx: 5000 Loss: 0.022057006581595387
Epoch: 31 Idx: 0 Loss: 0.02272240384546049
Epoch: 31 Idx: 5000 Loss: 0.03731228360831594
Epoch: 32 Idx: 0 Loss: 0.02804337287158763
Epoch: 32 Idx: 5000 Loss: 0.029104213077866467
Epoch: 33 Idx: 0 Loss: 0.026701924999294434
Epoch: 33 Idx: 5000 Loss: 0.028244122557071004
Epoch: 34 Idx: 0 Loss: 0.02896860680031463
Epoch: 34 Idx: 5000 Loss: 0.012585089636067477
Epoch: 35 Idx: 0 Loss: 0.024662265557181318
Epoch: 35 Idx: 5000 Loss: 0.02947820756935147
Epoch: 36 Idx: 0 Loss: 0.03248095769976933
Epoch: 36 Idx: 5000 Loss: 0.019584100389636924
Epoch: 37 Idx: 0 Loss: 0.04454269586467138
Epoch: 37 Idx: 5000 Loss: 0.02207976850910275
Epoch: 38 Idx: 0 Loss: 0.02363683773708984
Epoch: 38 Idx: 5000 Loss: 0.028500699032758307
Epoch: 39 Idx: 0 Loss: 0.01944566411972093
Epoch: 39 Idx: 5000 Loss: 0.03649100039400334
Epoch: 40 Idx: 0 Loss: 0.02678391329569529
Epoch: 40 Idx: 5000 Loss: 0.03868102087596208
Epoch: 41 Idx: 0 Loss: 0.01810722474653473
Epoch: 41 Idx: 5000 Loss: 0.0136530804757681
Epoch: 42 Idx: 0 Loss: 0.03396964958024712
Epoch: 42 Idx: 5000 Loss: 0.035115651465007995
Epoch: 43 Idx: 0 Loss: 0.013027015066578954
Epoch: 43 Idx: 5000 Loss: 0.028893504224312194
Epoch: 44 Idx: 0 Loss: 0.019888206265867736
Epoch: 44 Idx: 5000 Loss: 0.023770899375155564
Epoch: 45 Idx: 0 Loss: 0.040935708246650854
Epoch: 45 Idx: 5000 Loss: 0.033422179674304324
Epoch: 46 Idx: 0 Loss: 0.00500717701601258
Epoch: 46 Idx: 5000 Loss: 0.03799902634053477
Epoch: 47 Idx: 0 Loss: 0.010360000049659892
Epoch: 47 Idx: 5000 Loss: 0.014138966177111068
Epoch: 48 Idx: 0 Loss: 0.026457415861579986
Epoch: 48 Idx: 5000 Loss: 0.019965383233871096
Epoch: 49 Idx: 0 Loss: 0.02563452189226373
Epoch: 49 Idx: 5000 Loss: 0.018869380753386577
Len (direct inputs):  96
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
vision by zeTraining sizTrainingTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.16528437441856325
Epoch: 0 Idx: 5000 Loss: 0.019203578589384283
Epoch: 1 Idx: 0 Loss: 0.027628633674099477
Epoch: 1 Idx: 5000 Loss: 0.03755430290633657
Epoch: 2 Idx: 0 Loss: 0.014895091525763909
Epoch: 2 Idx: 5000 Loss: 0.025413329655460847
Epoch: 3 Idx: 0 Loss: 0.017803792634668244
Epoch: 3 Idx: 5000 Loss: 0.031210932699674752
Epoch: 4 Idx: 0 Loss: 0.027355490417006648
Epoch: 4 Idx: 5000 Loss: 0.01434027533115289
Epoch: 5 Idx: 0 Loss: 0.027716907573538303
Epoch: 5 Idx: 5000 Loss: 0.025736643952663098
Epoch: 6 Idx: 0 Loss: 0.020376725965819333
Epoch: 6 Idx: 5000 Loss: 0.02379365525044185
Epoch: 7 Idx: 0 Loss: 0.012148283113649204
Epoch: 7 Idx: 5000 Loss: 0.03345587321430822
Epoch: 8 Idx: 0 Loss: 0.01994268189587303
Epoch: 8 Idx: 5000 Loss: 0.014861099278678774
Epoch: 9 Idx: 0 Loss: 0.017403674042582883
Epoch: 9 Idx: 5000 Loss: 0.015503200757586905
Epoch: 10 Idx: 0 Loss: 0.02996573072778331
Epoch: 10 Idx: 5000 Loss: 0.0184619747407909
Epoch: 11 Idx: 0 Loss: 0.01765989091406807
Epoch: 11 Idx: 5000 Loss: 0.02651320071765996
Epoch: 12 Idx: 0 Loss: 0.022803639668780164
Epoch: 12 Idx: 5000 Loss: 0.03257395532913331
Epoch: 13 Idx: 0 Loss: 0.03028756419230854
Epoch: 13 Idx: 5000 Loss: 0.013261850823971029
Epoch: 14 Idx: 0 Loss: 0.012419363622832674
Epoch: 14 Idx: 5000 Loss: 0.012172999893653804
Epoch: 15 Idx: 0 Loss: 0.01643761813146595
Epoch: 15 Idx: 5000 Loss: 0.017596225460331693
Epoch: 16 Idx: 0 Loss: 0.007218563932467172
Epoch: 16 Idx: 5000 Loss: 0.019345195768163993
Epoch: 17 Idx: 0 Loss: 0.011380795375890679
Epoch: 17 Idx: 5000 Loss: 0.02169916188482282
Epoch: 18 Idx: 0 Loss: 0.014574405215033276
Epoch: 18 Idx: 5000 Loss: 0.0241001868815453
Epoch: 19 Idx: 0 Loss: 0.02577619414332466
Epoch: 19 Idx: 5000 Loss: 0.022891822239002366
Epoch: 20 Idx: 0 Loss: 0.03517490352384771
Epoch: 20 Idx: 5000 Loss: 0.023601432170839994
Epoch: 21 Idx: 0 Loss: 0.03326198681753689
Epoch: 21 Idx: 5000 Loss: 0.019409683426858705
Epoch: 22 Idx: 0 Loss: 0.016246079976125254
Epoch: 22 Idx: 5000 Loss: 0.014617341630895873
Epoch: 23 Idx: 0 Loss: 0.02020185718250108
Epoch: 23 Idx: 5000 Loss: 0.012078339233142723
Epoch: 24 Idx: 0 Loss: 0.02014266475871398
Epoch: 24 Idx: 5000 Loss: 0.019132006927051207
Epoch: 25 Idx: 0 Loss: 0.026210386307867886
Epoch: 25 Idx: 5000 Loss: 0.03226952677045663
Epoch: 26 Idx: 0 Loss: 0.018798603632641506
Epoch: 26 Idx: 5000 Loss: 0.021970342017178842
Epoch: 27 Idx: 0 Loss: 0.010337340934241261
Epoch: 27 Idx: 5000 Loss: 0.02073176570599315
Epoch: 28 Idx: 0 Loss: 0.01845116556148764
Epoch: 28 Idx: 5000 Loss: 0.02084874066433664
Epoch: 29 Idx: 0 Loss: 0.023188348197634587
Epoch: 29 Idx: 5000 Loss: 0.02185514039604748
Epoch: 30 Idx: 0 Loss: 0.013056742746666714
Epoch: 30 Idx: 5000 Loss: 0.025328042776109908
Epoch: 31 Idx: 0 Loss: 0.026870543317010616
Epoch: 31 Idx: 5000 Loss: 0.01569805237284265
Epoch: 32 Idx: 0 Loss: 0.028575408158151634
Epoch: 32 Idx: 5000 Loss: 0.011072361793121998
Epoch: 33 Idx: 0 Loss: 0.030710985272575615
Epoch: 33 Idx: 5000 Loss: 0.014737242342406613
Epoch: 34 Idx: 0 Loss: 0.01710418715781596
Epoch: 34 Idx: 5000 Loss: 0.023546908869888146
Epoch: 35 Idx: 0 Loss: 0.02250474851628433
Epoch: 35 Idx: 5000 Loss: 0.02372505612918923
Epoch: 36 Idx: 0 Loss: 0.014077413756791234
Epoch: 36 Idx: 5000 Loss: 0.02782670859361928
Epoch: 37 Idx: 0 Loss: 0.02257555083674386
Epoch: 37 Idx: 5000 Loss: 0.01970291830994523
Epoch: 38 Idx: 0 Loss: 0.025850195210334215
Epoch: 38 Idx: 5000 Loss: 0.027479256451449233
Epoch: 39 Idx: 0 Loss: 0.011516241624654023
Epoch: 39 Idx: 5000 Loss: 0.013720396667266646
Epoch: 40 Idx: 0 Loss: 0.020965394338972203
Epoch: 40 Idx: 5000 Loss: 0.025800542509863987
Epoch: 41 Idx: 0 Loss: 0.05346716277740496
Epoch: 41 Idx: 5000 Loss: 0.01258285140393511
Epoch: 42 Idx: 0 Loss: 0.017594550821618194
Epoch: 42 Idx: 5000 Loss: 0.036375616650530455
Epoch: 43 Idx: 0 Loss: 0.0165004134853549
Epoch: 43 Idx: 5000 Loss: 0.020874477783899177
Epoch: 44 Idx: 0 Loss: 0.05618874452973632
Epoch: 44 Idx: 5000 Loss: 0.01900028300851412
Epoch: 45 Idx: 0 Loss: 0.02666394759956442
Epoch: 45 Idx: 5000 Loss: 0.03446101352648997
Epoch: 46 Idx: 0 Loss: 0.0199945724032546
Epoch: 46 Idx: 5000 Loss: 0.02822767222038286
Epoch: 47 Idx: 0 Loss: 0.01515027218983975
Epoch: 47 Idx: 5000 Loss: 0.027461855973571887
Epoch: 48 Idx: 0 Loss: 0.024035916210998508
Epoch: 48 Idx: 5000 Loss: 0.020425429826711566
Epoch: 49 Idx: 0 Loss: 0.036184679875407154
Epoch: 49 Idx: 5000 Loss: 0.02708424960665676
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21216924502201745
Epoch: 0 Idx: 5000 Loss: 0.019895028108866987
Epoch: 1 Idx: 0 Loss: 0.02784515837927054
Epoch: 1 Idx: 5000 Loss: 0.020971047803110893
Epoch: 2 Idx: 0 Loss: 0.014284785244502465
Epoch: 2 Idx: 5000 Loss: 0.02124234620867293
Epoch: 3 Idx: 0 Loss: 0.029175601587769644
Epoch: 3 Idx: 5000 Loss: 0.020658251523697532
Epoch: 4 Idx: 0 Loss: 0.017182392867308688
Epoch: 4 Idx: 5000 Loss: 0.019729067004958068
Epoch: 5 Idx: 0 Loss: 0.021316083857508984
Epoch: 5 Idx: 5000 Loss: 0.01894239038937553
Epoch: 6 Idx: 0 Loss: 0.02215032173847152
Epoch: 6 Idx: 5000 Loss: 0.013040588917341278
Epoch: 7 Idx: 0 Loss: 0.015988493361749172
Epoch: 7 Idx: 5000 Loss: 0.02118690788461207
Epoch: 8 Idx: 0 Loss: 0.02720745113618572
Epoch: 8 Idx: 5000 Loss: 0.029692162839458223
Epoch: 9 Idx: 0 Loss: 0.008038359176107283
Epoch: 9 Idx: 5000 Loss: 0.022253755306935897
Epoch: 10 Idx: 0 Loss: 0.025652975890508974
Epoch: 10 Idx: 5000 Loss: 0.010557726154313295
Epoch: 11 Idx: 0 Loss: 0.017562430416957008
Epoch: 11 Idx: 5000 Loss: 0.012528097710568897
Epoch: 12 Idx: 0 Loss: 0.02457016698903019
Epoch: 12 Idx: 5000 Loss: 0.009319531804781624
Epoch: 13 Idx: 0 Loss: 0.018539391708815797
Epoch: 13 Idx: 5000 Loss: 0.02325775109050748
Epoch: 14 Idx: 0 Loss: 0.019276806697073694
Epoch: 14 Idx: 5000 Loss: 0.01862666886920869
Epoch: 15 Idx: 0 Loss: 0.021060945234723773
Epoch: 15 Idx: 5000 Loss: 0.034745000091116224
Epoch: 16 Idx: 0 Loss: 0.03128143242828964
Epoch: 16 Idx: 5000 Loss: 0.012333302648150885
Epoch: 17 Idx: 0 Loss: 0.013776974841778027
Epoch: 17 Idx: 5000 Loss: 0.02253045174970874
Epoch: 18 Idx: 0 Loss: 0.022550057719355483
Epoch: 18 Idx: 5000 Loss: 0.02717523024060419
Epoch: 19 Idx: 0 Loss: 0.022976004618682468
Epoch: 19 Idx: 5000 Loss: 0.012851364625954424
Epoch: 20 Idx: 0 Loss: 0.00884818793818183
Epoch: 20 Idx: 5000 Loss: 0.018722860327470676
Epoch: 21 Idx: 0 Loss: 0.01485020059417803
Epoch: 21 Idx: 5000 Loss: 0.03667432623792836
Epoch: 22 Idx: 0 Loss: 0.014185413340351491
Epoch: 22 Idx: 5000 Loss: 0.023327862320543992
Epoch: 23 Idx: 0 Loss: 0.014383736399705365
Epoch: 23 Idx: 5000 Loss: 0.016200104306985978
Epoch: 24 Idx: 0 Loss: 0.030194288952955072
Epoch: 24 Idx: 5000 Loss: 0.04670453085521188
Epoch: 25 Idx: 0 Loss: 0.014493654549141465
Epoch: 25 Idx: 5000 Loss: 0.030887273632363967
Epoch: 26 Idx: 0 Loss: 0.02358977890155073
Epoch: 26 Idx: 5000 Loss: 0.02423982603218669
Epoch: 27 Idx: 0 Loss: 0.008940413094297392
Epoch: 27 Idx: 5000 Loss: 0.008301788429721218
Epoch: 28 Idx: 0 Loss: 0.017658581325524537
Epoch: 28 Idx: 5000 Loss: 0.019766887619388274
Epoch: 29 Idx: 0 Loss: 0.01966178828983084
Epoch: 29 Idx: 5000 Loss: 0.02589193804784234
Epoch: 30 Idx: 0 Loss: 0.014342896774739974
Epoch: 30 Idx: 5000 Loss: 0.017631385612657785
Epoch: 31 Idx: 0 Loss: 0.016706917464541754
Epoch: 31 Idx: 5000 Loss: 0.011261868278882503
Epoch: 32 Idx: 0 Loss: 0.03659505067791237
Epoch: 32 Idx: 5000 Loss: 0.016739526175561802
Epoch: 33 Idx: 0 Loss: 0.020465992052500173
Epoch: 33 Idx: 5000 Loss: 0.022250513641370824
Epoch: 34 Idx: 0 Loss: 0.008306288224255009
Epoch: 34 Idx: 5000 Loss: 0.034228221701495774
Epoch: 35 Idx: 0 Loss: 0.015533092607622204
Epoch: 35 Idx: 5000 Loss: 0.02673058323954189
Epoch: 36 Idx: 0 Loss: 0.01344924540987186
Epoch: 36 Idx: 5000 Loss: 0.011671813239910926
Epoch: 37 Idx: 0 Loss: 0.021435931613890763
Epoch: 37 Idx: 5000 Loss: 0.02905528634222449
Epoch: 38 Idx: 0 Loss: 0.04078079293770259
Epoch: 38 Idx: 5000 Loss: 0.024215814163418822
Epoch: 39 Idx: 0 Loss: 0.027557754998503284
Epoch: 39 Idx: 5000 Loss: 0.00993330353459201
Epoch: 40 Idx: 0 Loss: 0.018418424785732884
Epoch: 40 Idx: 5000 Loss: 0.009885178830847129
Epoch: 41 Idx: 0 Loss: 0.04015173561725395
Epoch: 41 Idx: 5000 Loss: 0.02223009775821857
Epoch: 42 Idx: 0 Loss: 0.033760303299301284
Epoch: 42 Idx: 5000 Loss: 0.02192487015627427
Epoch: 43 Idx: 0 Loss: 0.019202572214733517
Epoch: 43 Idx: 5000 Loss: 0.025116754297510313
Epoch: 44 Idx: 0 Loss: 0.024322950547281754
Epoch: 44 Idx: 5000 Loss: 0.03082884562472232
Epoch: 45 Idx: 0 Loss: 0.01938658698026159
Epoch: 45 Idx: 5000 Loss: 0.011586341526740295
Epoch: 46 Idx: 0 Loss: 0.015920189386511383
Epoch: 46 Idx: 5000 Loss: 0.04482148390772119
Epoch: 47 Idx: 0 Loss: 0.01462446859076661
Epoch: 47 Idx: 5000 Loss: 0.013615838849908189
Epoch: 48 Idx: 0 Loss: 0.01727951887037809
Epoch: 48 Idx: 5000 Loss: 0.028777299645946195
Epoch: 49 Idx: 0 Loss: 0.015258619967267864
Epoch: 49 Idx: 5000 Loss: 0.0079733648439371
Len (direct inputs):  83
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
1Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.15604691266278325
Epoch: 0 Idx: 5000 Loss: 0.029579761089520196
Epoch: 1 Idx: 0 Loss: 0.029204637368286348
Epoch: 1 Idx: 5000 Loss: 0.028567958058023493
Epoch: 2 Idx: 0 Loss: 0.017519382096746264
Epoch: 2 Idx: 5000 Loss: 0.026637007317762272
Epoch: 3 Idx: 0 Loss: 0.01098265840497302
Epoch: 3 Idx: 5000 Loss: 0.01600632561928832
Epoch: 4 Idx: 0 Loss: 0.011004690972780312
Epoch: 4 Idx: 5000 Loss: 0.018575482102060843
Epoch: 5 Idx: 0 Loss: 0.01751751894495101
Epoch: 5 Idx: 5000 Loss: 0.017112746202936086
Epoch: 6 Idx: 0 Loss: 0.012005545921300462
Epoch: 6 Idx: 5000 Loss: 0.02598699342030454
Epoch: 7 Idx: 0 Loss: 0.02053228667442275
Epoch: 7 Idx: 5000 Loss: 0.023465577084149803
Epoch: 8 Idx: 0 Loss: 0.015641096172788933
Epoch: 8 Idx: 5000 Loss: 0.011115048343270088
Epoch: 9 Idx: 0 Loss: 0.023815914782057017
Epoch: 9 Idx: 5000 Loss: 0.011381704579768405
Epoch: 10 Idx: 0 Loss: 0.03247420261845712
Epoch: 10 Idx: 5000 Loss: 0.0219203104002523
Epoch: 11 Idx: 0 Loss: 0.02005482444972262
Epoch: 11 Idx: 5000 Loss: 0.020028865829686233
Epoch: 12 Idx: 0 Loss: 0.008567962834112222
Epoch: 12 Idx: 5000 Loss: 0.022946782352659807
Epoch: 13 Idx: 0 Loss: 0.026552858223151565
Epoch: 13 Idx: 5000 Loss: 0.0339941938596572
Epoch: 14 Idx: 0 Loss: 0.016462407175278325
Epoch: 14 Idx: 5000 Loss: 0.01901023462983295
Epoch: 15 Idx: 0 Loss: 0.037663101874882324
Epoch: 15 Idx: 5000 Loss: 0.01967750140360161
Epoch: 16 Idx: 0 Loss: 0.031155218517887037
Epoch: 16 Idx: 5000 Loss: 0.014735984952975524
Epoch: 17 Idx: 0 Loss: 0.008977826918035415
Epoch: 17 Idx: 5000 Loss: 0.020251491037690268
Epoch: 18 Idx: 0 Loss: 0.013353730774985948
Epoch: 18 Idx: 5000 Loss: 0.02147087959286125
Epoch: 19 Idx: 0 Loss: 0.00860179278351889
Epoch: 19 Idx: 5000 Loss: 0.022456112767267576
Epoch: 20 Idx: 0 Loss: 0.04223931219381779
Epoch: 20 Idx: 5000 Loss: 0.019125209876206622
Epoch: 21 Idx: 0 Loss: 0.03558395039271223
Epoch: 21 Idx: 5000 Loss: 0.030029057101109938
Epoch: 22 Idx: 0 Loss: 0.027033399835121426
Epoch: 22 Idx: 5000 Loss: 0.013030433709465323
Epoch: 23 Idx: 0 Loss: 0.018907178302533
Epoch: 23 Idx: 5000 Loss: 0.027989533007317595
Epoch: 24 Idx: 0 Loss: 0.013137155562279438
Epoch: 24 Idx: 5000 Loss: 0.028980986865786332
Epoch: 25 Idx: 0 Loss: 0.033340137973837086
Epoch: 25 Idx: 5000 Loss: 0.02341294577594328
Epoch: 26 Idx: 0 Loss: 0.025316407865457264
Epoch: 26 Idx: 5000 Loss: 0.013458833741117004
Epoch: 27 Idx: 0 Loss: 0.011410782267477854
Epoch: 27 Idx: 5000 Loss: 0.023911341045271876
Epoch: 28 Idx: 0 Loss: 0.015489046044985083
Epoch: 28 Idx: 5000 Loss: 0.02121315558687593
Epoch: 29 Idx: 0 Loss: 0.012083585368167839
Epoch: 29 Idx: 5000 Loss: 0.016776445709657122
Epoch: 30 Idx: 0 Loss: 0.03749453222965775
Epoch: 30 Idx: 5000 Loss: 0.013969743700200777
Epoch: 31 Idx: 0 Loss: 0.018264769858502283
Epoch: 31 Idx: 5000 Loss: 0.028328202894414605
Epoch: 32 Idx: 0 Loss: 0.01811535645615414
Epoch: 32 Idx: 5000 Loss: 0.012608257035810092
Epoch: 33 Idx: 0 Loss: 0.018900139521363508
Epoch: 33 Idx: 5000 Loss: 0.01922076454455707
Epoch: 34 Idx: 0 Loss: 0.04596939008607051
Epoch: 34 Idx: 5000 Loss: 0.03540747396491207
Epoch: 35 Idx: 0 Loss: 0.026019416545327537
Epoch: 35 Idx: 5000 Loss: 0.009428297752177218
Epoch: 36 Idx: 0 Loss: 0.014459870272605115
Epoch: 36 Idx: 5000 Loss: 0.020878116838624994
Epoch: 37 Idx: 0 Loss: 0.01899769754848467
Epoch: 37 Idx: 5000 Loss: 0.017580200183489908
Epoch: 38 Idx: 0 Loss: 0.012395248535410292
Epoch: 38 Idx: 5000 Loss: 0.03185597972345467
Epoch: 39 Idx: 0 Loss: 0.02808945097698621
Epoch: 39 Idx: 5000 Loss: 0.01598331035169989
Epoch: 40 Idx: 0 Loss: 0.025832652189569063
Epoch: 40 Idx: 5000 Loss: 0.0293497233031193
Epoch: 41 Idx: 0 Loss: 0.03585011236467352
Epoch: 41 Idx: 5000 Loss: 0.02157152126087995
Epoch: 42 Idx: 0 Loss: 0.02311270899991661
Epoch: 42 Idx: 5000 Loss: 0.031766475214529764
Epoch: 43 Idx: 0 Loss: 0.037328904388000766
Epoch: 43 Idx: 5000 Loss: 0.011197749086616294
Epoch: 44 Idx: 0 Loss: 0.013432277213046167
Epoch: 44 Idx: 5000 Loss: 0.01174160428078197
Epoch: 45 Idx: 0 Loss: 0.03586022864171413
Epoch: 45 Idx: 5000 Loss: 0.016876396356393337
Epoch: 46 Idx: 0 Loss: 0.018501978227836342
Epoch: 46 Idx: 5000 Loss: 0.03884523196334131
Epoch: 47 Idx: 0 Loss: 0.04203442467141461
Epoch: 47 Idx: 5000 Loss: 0.012049524839217538
Epoch: 48 Idx: 0 Loss: 0.004757945134430666
Epoch: 48 Idx: 5000 Loss: 0.01383111775575077
Epoch: 49 Idx: 0 Loss: 0.025281827389155017
Epoch: 49 Idx: 5000 Loss: 0.031019472950872106
Len (direct inputs):  94
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divi0.8451646654093949
Parameter containing:
tensor([0.8452], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.29186727331602536
Epoch: 0 Idx: 5000 Loss: 0.027140907626541505
Epoch: 1 Idx: 0 Loss: 0.03416004866380873
Epoch: 1 Idx: 5000 Loss: 0.017717429387638556
Epoch: 2 Idx: 0 Loss: 0.017370861790283083
Epoch: 2 Idx: 5000 Loss: 0.01619025736172072
Epoch: 3 Idx: 0 Loss: 0.025834453802262066
Epoch: 3 Idx: 5000 Loss: 0.026187317324537888
Epoch: 4 Idx: 0 Loss: 0.021824864325454073
Epoch: 4 Idx: 5000 Loss: 0.019094809627626903
Epoch: 5 Idx: 0 Loss: 0.011313440490590534
Epoch: 5 Idx: 5000 Loss: 0.03865593595635712
Epoch: 6 Idx: 0 Loss: 0.01317128570802895
Epoch: 6 Idx: 5000 Loss: 0.06399280668058163
Epoch: 7 Idx: 0 Loss: 0.028032418962912094
Epoch: 7 Idx: 5000 Loss: 0.028291420349241078
Epoch: 8 Idx: 0 Loss: 0.01271525040341723
Epoch: 8 Idx: 5000 Loss: 0.01999877544494614
Epoch: 9 Idx: 0 Loss: 0.031099107426295357
Epoch: 9 Idx: 5000 Loss: 0.02945996640605408
Epoch: 10 Idx: 0 Loss: 0.019099762596988014
Epoch: 10 Idx: 5000 Loss: 0.019293926318628654
Epoch: 11 Idx: 0 Loss: 0.014347940360369262
Epoch: 11 Idx: 5000 Loss: 0.021882992531595376
Epoch: 12 Idx: 0 Loss: 0.01605005714707489
Epoch: 12 Idx: 5000 Loss: 0.026321269161253158
Epoch: 13 Idx: 0 Loss: 0.02623696240988317
Epoch: 13 Idx: 5000 Loss: 0.014725638634872643
Epoch: 14 Idx: 0 Loss: 0.016606786706688213
Epoch: 14 Idx: 5000 Loss: 0.006308348654156309
Epoch: 15 Idx: 0 Loss: 0.022668416612848463
Epoch: 15 Idx: 5000 Loss: 0.017483060520273062
Epoch: 16 Idx: 0 Loss: 0.017775948382124044
Epoch: 16 Idx: 5000 Loss: 0.02245156551478544
Epoch: 17 Idx: 0 Loss: 0.022746034983918008
Epoch: 17 Idx: 5000 Loss: 0.014021057777949174
Epoch: 18 Idx: 0 Loss: 0.027727331147850212
Epoch: 18 Idx: 5000 Loss: 0.017065163650284786
Epoch: 19 Idx: 0 Loss: 0.02075349819251693
Epoch: 19 Idx: 5000 Loss: 0.01870617320993267
Epoch: 20 Idx: 0 Loss: 0.023042708773015737
Epoch: 20 Idx: 5000 Loss: 0.010911374394135469
Epoch: 21 Idx: 0 Loss: 0.025959475555194768
Epoch: 21 Idx: 5000 Loss: 0.045836881348173764
Epoch: 22 Idx: 0 Loss: 0.02694477357340266
Epoch: 22 Idx: 5000 Loss: 0.017138692197892533
Epoch: 23 Idx: 0 Loss: 0.01739931335269752
Epoch: 23 Idx: 5000 Loss: 0.03127362734574696
Epoch: 24 Idx: 0 Loss: 0.01829310681517872
Epoch: 24 Idx: 5000 Loss: 0.03493078050776645
Epoch: 25 Idx: 0 Loss: 0.04218957276794133
Epoch: 25 Idx: 5000 Loss: 0.016525860660050268
Epoch: 26 Idx: 0 Loss: 0.012273380925472917
Epoch: 26 Idx: 5000 Loss: 0.028216468191747897
Epoch: 27 Idx: 0 Loss: 0.03105937422035676
Epoch: 27 Idx: 5000 Loss: 0.017933677558630578
Epoch: 28 Idx: 0 Loss: 0.01776325051480117
Epoch: 28 Idx: 5000 Loss: 0.03145154574362545
Epoch: 29 Idx: 0 Loss: 0.02160219644309517
Epoch: 29 Idx: 5000 Loss: 0.03920825369074351
Epoch: 30 Idx: 0 Loss: 0.037373893305917893
Epoch: 30 Idx: 5000 Loss: 0.022197066758077574
Epoch: 31 Idx: 0 Loss: 0.021550365010061738
Epoch: 31 Idx: 5000 Loss: 0.021892593262961217
Epoch: 32 Idx: 0 Loss: 0.03265752803003455
Epoch: 32 Idx: 5000 Loss: 0.014437235091508974
Epoch: 33 Idx: 0 Loss: 0.030598261956989573
Epoch: 33 Idx: 5000 Loss: 0.045631955539271116
Epoch: 34 Idx: 0 Loss: 0.037094747403275945
Epoch: 34 Idx: 5000 Loss: 0.016465782508073054
Epoch: 35 Idx: 0 Loss: 0.01567900897128112
Epoch: 35 Idx: 5000 Loss: 0.03526186291496149
Epoch: 36 Idx: 0 Loss: 0.028955593625405014
Epoch: 36 Idx: 5000 Loss: 0.023665515235471706
Epoch: 37 Idx: 0 Loss: 0.022970117477701505
Epoch: 37 Idx: 5000 Loss: 0.03437190747578189
Epoch: 38 Idx: 0 Loss: 0.028698518039496426
Epoch: 38 Idx: 5000 Loss: 0.018171331412213762
Epoch: 39 Idx: 0 Loss: 0.014336374592872342
Epoch: 39 Idx: 5000 Loss: 0.024567135315115902
Epoch: 40 Idx: 0 Loss: 0.032480255378886894
Epoch: 40 Idx: 5000 Loss: 0.02366944000746812
Epoch: 41 Idx: 0 Loss: 0.03563819697071374
Epoch: 41 Idx: 5000 Loss: 0.030895836303483717
Epoch: 42 Idx: 0 Loss: 0.018462802636111532
Epoch: 42 Idx: 5000 Loss: 0.028037118894953514
Epoch: 43 Idx: 0 Loss: 0.018038138362109195
Epoch: 43 Idx: 5000 Loss: 0.01673240760636388
Epoch: 44 Idx: 0 Loss: 0.02725393891824913
Epoch: 44 Idx: 5000 Loss: 0.017985226480568862
Epoch: 45 Idx: 0 Loss: 0.031644638469073276
Epoch: 45 Idx: 5000 Loss: 0.05095055763777297
Epoch: 46 Idx: 0 Loss: 0.030296327687996172
Epoch: 46 Idx: 5000 Loss: 0.03063093407867695
Epoch: 47 Idx: 0 Loss: 0.018657568568005815
Epoch: 47 Idx: 5000 Loss: 0.03321637987979875
Epoch: 48 Idx: 0 Loss: 0.0222028307384617
Epoch: 48 Idx: 5000 Loss: 0.02703385743866628
Epoch: 49 Idx: 0 Loss: 0.018059320936533514
Epoch: 49 Idx: 5000 Loss: 0.015204302570844512
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 476, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
th Disk quota exceeded

-------------------------
Sender: LSF System <rer@dccxc254>
Subject: Job 3289874: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs_wtpath1_4.pkl Models/anatomy_aml_bagofnbrs_wtpath1_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs_wtpath1_4.pkl Models/anatomy_aml_bagofnbrs_wtpath1_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc254>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Tue Sep  1 20:42:03 2020
Results reported at Tue Sep  1 20:42:03 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs_wtpath1_4.pkl Models/anatomy_aml_bagofnbrs_wtpath1_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24335.67 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2723.00 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   24437 sec.
    Turnaround time :                            24493 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc255>
Subject: Job 3289876: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs_wtpath2_4.pkl Models/anatomy_aml_bagofnbrs_wtpath2_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs_wtpath2_4.pkl Models/anatomy_aml_bagofnbrs_wtpath2_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc255>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:07:37 2020
Results reported at Tue Sep  1 21:07:37 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs_wtpath2_4.pkl Models/anatomy_aml_bagofnbrs_wtpath2_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25965.07 sec.
    Max Memory :                                 2918 MB
    Average Memory :                             2699.26 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40499.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25996 sec.
    Turnaround time :                            26027 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc236>
Subject: Job 3289878: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs_wtpath3_4.pkl Models/anatomy_aml_bagofnbrs_wtpath3_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs_wtpath3_4.pkl Models/anatomy_aml_bagofnbrs_wtpath3_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc236>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:25:29 2020
Results reported at Tue Sep  1 21:25:29 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs_wtpath3_4.pkl Models/anatomy_aml_bagofnbrs_wtpath3_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26981.45 sec.
    Max Memory :                                 2925 MB
    Average Memory :                             2714.09 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40492.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27041 sec.
    Turnaround time :                            27099 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc258>
Subject: Job 3289880: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs_wtpath4_4.pkl Models/anatomy_aml_bagofnbrs_wtpath4_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs_wtpath4_4.pkl Models/anatomy_aml_bagofnbrs_wtpath4_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc258>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:40:48 2020
Results reported at Tue Sep  1 21:40:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs_wtpath4_4.pkl Models/anatomy_aml_bagofnbrs_wtpath4_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27891.24 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2712.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27986 sec.
    Turnaround time :                            28018 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc273>
Subject: Job 3289882: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs_wtpath5_4.pkl Models/anatomy_aml_bagofnbrs_wtpath5_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs_wtpath5_4.pkl Models/anatomy_aml_bagofnbrs_wtpath5_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc273>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 22:26:58 2020
Results reported at Tue Sep  1 22:26:58 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs_wtpath5_4.pkl Models/anatomy_aml_bagofnbrs_wtpath5_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30639.92 sec.
    Max Memory :                                 2934 MB
    Average Memory :                             2726.53 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40483.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30730 sec.
    Turnaround time :                            30788 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc238>
Subject: Job 3289884: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs_wtpath7_4.pkl Models/anatomy_aml_bagofnbrs_wtpath7_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs_wtpath7_4.pkl Models/anatomy_aml_bagofnbrs_wtpath7_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc238>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 22:49:55 2020
Results reported at Tue Sep  1 22:49:55 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs_wtpath7_4.pkl Models/anatomy_aml_bagofnbrs_wtpath7_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32058.21 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2710.76 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32134 sec.
    Turnaround time :                            32165 sec.

The output (if any) is above this job summary.

