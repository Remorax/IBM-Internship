Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21107186607937006
Epoch: 0 Idx: 5000 Loss: 0.027152570131569367
Epoch: 1 Idx: 0 Loss: 0.02769560642552728
Epoch: 1 Idx: 5000 Loss: 0.01795065632319946
Epoch: 2 Idx: 0 Loss: 0.020275210686618958
Epoch: 2 Idx: 5000 Loss: 0.019762048847571233
Epoch: 3 Idx: 0 Loss: 0.034000684077328924
Epoch: 3 Idx: 5000 Loss: 0.01983389476333986
Epoch: 4 Idx: 0 Loss: 0.01638753696999904
Epoch: 4 Idx: 5000 Loss: 0.009635247754958338
Epoch: 5 Idx: 0 Loss: 0.010855030538550718
Epoch: 5 Idx: 5000 Loss: 0.024420896084304908
Epoch: 6 Idx: 0 Loss: 0.023967576246282596
Epoch: 6 Idx: 5000 Loss: 0.015198240376074573
Epoch: 7 Idx: 0 Loss: 0.0324770169943548
Epoch: 7 Idx: 5000 Loss: 0.02671991147884079
Epoch: 8 Idx: 0 Loss: 0.015484588945019442
Epoch: 8 Idx: 5000 Loss: 0.0280457812375719
Epoch: 9 Idx: 0 Loss: 0.014110287853555423
Epoch: 9 Idx: 5000 Loss: 0.011675393695915895
Epoch: 10 Idx: 0 Loss: 0.02644561755305391
Epoch: 10 Idx: 5000 Loss: 0.018748166129752154
Epoch: 11 Idx: 0 Loss: 0.013331437315278671
Epoch: 11 Idx: 5000 Loss: 0.01697652220100283
Epoch: 12 Idx: 0 Loss: 0.007276343533414411
Epoch: 12 Idx: 5000 Loss: 0.01900005380740309
Epoch: 13 Idx: 0 Loss: 0.02383279079228054
Epoch: 13 Idx: 5000 Loss: 0.018242959335115175
Epoch: 14 Idx: 0 Loss: 0.02294603539385439
Epoch: 14 Idx: 5000 Loss: 0.041305259669002746
Epoch: 15 Idx: 0 Loss: 0.01755279751645555
Epoch: 15 Idx: 5000 Loss: 0.018849933529175526
Epoch: 16 Idx: 0 Loss: 0.013040859638427832
Epoch: 16 Idx: 5000 Loss: 0.01381955886578131
Epoch: 17 Idx: 0 Loss: 0.012329646782244448
Epoch: 17 Idx: 5000 Loss: 0.018605826995729335
Epoch: 18 Idx: 0 Loss: 0.034178004312227034
Epoch: 18 Idx: 5000 Loss: 0.011395236269422505
Epoch: 19 Idx: 0 Loss: 0.01888323090542228
Epoch: 19 Idx: 5000 Loss: 0.008649892681885128
Epoch: 20 Idx: 0 Loss: 0.011220942171333032
Epoch: 20 Idx: 5000 Loss: 0.010425359003334553
Epoch: 21 Idx: 0 Loss: 0.02064650764875533
Epoch: 21 Idx: 5000 Loss: 0.030929209253746127
Epoch: 22 Idx: 0 Loss: 0.031919135263101035
Epoch: 22 Idx: 5000 Loss: 0.028713564108374782
Epoch: 23 Idx: 0 Loss: 0.008762284260152083
Epoch: 23 Idx: 5000 Loss: 0.021814261374499707
Epoch: 24 Idx: 0 Loss: 0.02151883050502148
Epoch: 24 Idx: 5000 Loss: 0.02513628013099757
Epoch: 25 Idx: 0 Loss: 0.01996444936849396
Epoch: 25 Idx: 5000 Loss: 0.03758625366162824
Epoch: 26 Idx: 0 Loss: 0.018105271670106393
Epoch: 26 Idx: 5000 Loss: 0.018087298266053074
Epoch: 27 Idx: 0 Loss: 0.023268463666270577
Epoch: 27 Idx: 5000 Loss: 0.02124707778223554
Epoch: 28 Idx: 0 Loss: 0.03146958879383075
Epoch: 28 Idx: 5000 Loss: 0.020977981470837397
Epoch: 29 Idx: 0 Loss: 0.015091182587164862
Epoch: 29 Idx: 5000 Loss: 0.012372145532720657
Epoch: 30 Idx: 0 Loss: 0.013973210264949219
Epoch: 30 Idx: 5000 Loss: 0.02181813865553608
Epoch: 31 Idx: 0 Loss: 0.01058041910692049
Epoch: 31 Idx: 5000 Loss: 0.014410248514682153
Epoch: 32 Idx: 0 Loss: 0.028834620466012126
Epoch: 32 Idx: 5000 Loss: 0.016972651493435117
Epoch: 33 Idx: 0 Loss: 0.010809036917866478
Epoch: 33 Idx: 5000 Loss: 0.022386254945112333
Epoch: 34 Idx: 0 Loss: 0.014348906358662204
Epoch: 34 Idx: 5000 Loss: 0.011885172033600974
Epoch: 35 Idx: 0 Loss: 0.012145522315327707
Epoch: 35 Idx: 5000 Loss: 0.03398230237493151
Epoch: 36 Idx: 0 Loss: 0.02296351208823099
Epoch: 36 Idx: 5000 Loss: 0.009422430195686067
Epoch: 37 Idx: 0 Loss: 0.024944257177025208
Epoch: 37 Idx: 5000 Loss: 0.014911560622135323
Epoch: 38 Idx: 0 Loss: 0.014945115019360699
Epoch: 38 Idx: 5000 Loss: 0.038193602358420016
Epoch: 39 Idx: 0 Loss: 0.029240073258460147
Epoch: 39 Idx: 5000 Loss: 0.014236965266894493
Epoch: 40 Idx: 0 Loss: 0.030263240684789732
Epoch: 40 Idx: 5000 Loss: 0.013801514613824022
Epoch: 41 Idx: 0 Loss: 0.019626922887842436
Epoch: 41 Idx: 5000 Loss: 0.019431275274762346
Epoch: 42 Idx: 0 Loss: 0.02760071995893521
Epoch: 42 Idx: 5000 Loss: 0.007602322374182983
Epoch: 43 Idx: 0 Loss: 0.028824670900553902
Epoch: 43 Idx: 5000 Loss: 0.042285464382336366
Epoch: 44 Idx: 0 Loss: 0.014551166537815848
Epoch: 44 Idx: 5000 Loss: 0.018316784145976872
Epoch: 45 Idx: 0 Loss: 0.03272018358527444
Epoch: 45 Idx: 5000 Loss: 0.01476208552285286
Epoch: 46 Idx: 0 Loss: 0.03778520127480316
Epoch: 46 Idx: 5000 Loss: 0.01921785312492652
Epoch: 47 Idx: 0 Loss: 0.013728118935364877
Epoch: 47 Idx: 5000 Loss: 0.020447734751427915
Epoch: 48 Idx: 0 Loss: 0.015502809392358294
Epoch: 48 Idx: 5000 Loss: 0.015457730161308327
Epoch: 49 Idx: 0 Loss: 0.033598773507608835
Epoch: 49 Idx: 5000 Loss: 0.024676398164157565
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisTraining sizTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21851599827073878
Epoch: 0 Idx: 5000 Loss: 0.01758923588863891
Epoch: 1 Idx: 0 Loss: 0.022423887398224186
Epoch: 1 Idx: 5000 Loss: 0.02034493189237139
Epoch: 2 Idx: 0 Loss: 0.016081141823573942
Epoch: 2 Idx: 5000 Loss: 0.02262103214357552
Epoch: 3 Idx: 0 Loss: 0.02822417152191043
Epoch: 3 Idx: 5000 Loss: 0.019392215979597175
Epoch: 4 Idx: 0 Loss: 0.021120490859309508
Epoch: 4 Idx: 5000 Loss: 0.03578146889694534
Epoch: 5 Idx: 0 Loss: 0.011652137576075732
Epoch: 5 Idx: 5000 Loss: 0.04238325005272798
Epoch: 6 Idx: 0 Loss: 0.017536400818510066
Epoch: 6 Idx: 5000 Loss: 0.016396870602813748
Epoch: 7 Idx: 0 Loss: 0.015739422467676907
Epoch: 7 Idx: 5000 Loss: 0.011541892011487077
Epoch: 8 Idx: 0 Loss: 0.021078632674692156
Epoch: 8 Idx: 5000 Loss: 0.045623957407309075
Epoch: 9 Idx: 0 Loss: 0.020280191291618097
Epoch: 9 Idx: 5000 Loss: 0.011901501669075526
Epoch: 10 Idx: 0 Loss: 0.02912918439948113
Epoch: 10 Idx: 5000 Loss: 0.025150676176857824
Epoch: 11 Idx: 0 Loss: 0.017389722026835037
Epoch: 11 Idx: 5000 Loss: 0.014711934012182019
Epoch: 12 Idx: 0 Loss: 0.016226482522450517
Epoch: 12 Idx: 5000 Loss: 0.01977863928029875
Epoch: 13 Idx: 0 Loss: 0.030340664003231176
Epoch: 13 Idx: 5000 Loss: 0.03633309250018658
Epoch: 14 Idx: 0 Loss: 0.026845941166831998
Epoch: 14 Idx: 5000 Loss: 0.03307112614310717
Epoch: 15 Idx: 0 Loss: 0.024628224748819066
Epoch: 15 Idx: 5000 Loss: 0.040793959706287006
Epoch: 16 Idx: 0 Loss: 0.025495960165447765
Epoch: 16 Idx: 5000 Loss: 0.020320688494595625
Epoch: 17 Idx: 0 Loss: 0.017918246521520746
Epoch: 17 Idx: 5000 Loss: 0.03256091523680345
Epoch: 18 Idx: 0 Loss: 0.03590401385876281
Epoch: 18 Idx: 5000 Loss: 0.034304038183246024
Epoch: 19 Idx: 0 Loss: 0.0161586712614709
Epoch: 19 Idx: 5000 Loss: 0.017543500659194335
Epoch: 20 Idx: 0 Loss: 0.018428136184819043
Epoch: 20 Idx: 5000 Loss: 0.02540473357590672
Epoch: 21 Idx: 0 Loss: 0.011828866894428126
Epoch: 21 Idx: 5000 Loss: 0.029027709094067697
Epoch: 22 Idx: 0 Loss: 0.019959654264476965
Epoch: 22 Idx: 5000 Loss: 0.015277921979116637
Epoch: 23 Idx: 0 Loss: 0.022427604508019484
Epoch: 23 Idx: 5000 Loss: 0.014362322629109435
Epoch: 24 Idx: 0 Loss: 0.042562853810781134
Epoch: 24 Idx: 5000 Loss: 0.016787198771496013
Epoch: 25 Idx: 0 Loss: 0.015190828339077169
Epoch: 25 Idx: 5000 Loss: 0.028547831501614135
Epoch: 26 Idx: 0 Loss: 0.014117663292247114
Epoch: 26 Idx: 5000 Loss: 0.03336198573917462
Epoch: 27 Idx: 0 Loss: 0.020862896122653227
Epoch: 27 Idx: 5000 Loss: 0.016339719294302377
Epoch: 28 Idx: 0 Loss: 0.028385397155366088
Epoch: 28 Idx: 5000 Loss: 0.017467542462081598
Epoch: 29 Idx: 0 Loss: 0.026941806545075754
Epoch: 29 Idx: 5000 Loss: 0.029512171976190334
Epoch: 30 Idx: 0 Loss: 0.019191985159938602
Epoch: 30 Idx: 5000 Loss: 0.02520315118432428
Epoch: 31 Idx: 0 Loss: 0.015177645547358745
Epoch: 31 Idx: 5000 Loss: 0.04795540600268897
Epoch: 32 Idx: 0 Loss: 0.023565182613637232
Epoch: 32 Idx: 5000 Loss: 0.012039171654534948
Epoch: 33 Idx: 0 Loss: 0.024226991838963586
Epoch: 33 Idx: 5000 Loss: 0.0097284330515732
Epoch: 34 Idx: 0 Loss: 0.02951454109561721
Epoch: 34 Idx: 5000 Loss: 0.01855798141636375
Epoch: 35 Idx: 0 Loss: 0.016606941446345995
Epoch: 35 Idx: 5000 Loss: 0.017562702088664527
Epoch: 36 Idx: 0 Loss: 0.010801713944592964
Epoch: 36 Idx: 5000 Loss: 0.016910303707025346
Epoch: 37 Idx: 0 Loss: 0.04948754121111343
Epoch: 37 Idx: 5000 Loss: 0.04383739874456535
Epoch: 38 Idx: 0 Loss: 0.017647466251953327
Epoch: 38 Idx: 5000 Loss: 0.015502129727263041
Epoch: 39 Idx: 0 Loss: 0.01790638655773697
Epoch: 39 Idx: 5000 Loss: 0.010240750639178134
Epoch: 40 Idx: 0 Loss: 0.04160185980617323
Epoch: 40 Idx: 5000 Loss: 0.02360864147463086
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
12079161675
Epoch: 45 Idx: 5000 Loss: 0.015481200856677067
Epoch: 46 Idx: 0 Loss: 0.03243006138821986
Epoch: 46 Idx: 5000 Loss: 0.02126814141166815
Epoch: 47 Idx: 0 Loss: 0.03440799091947468
Epoch: 47 Idx: 5000 Loss: 0.029419550859051985
Epoch: 48 Idx: 0 Loss: 0.016843515527739317
Epoch: 48 Idx: 5000 Loss: 0.02232771630269486
Epoch: 49 Idx: 0 Loss: 0.026960734330568135
Epoch: 49 Idx: 5000 Loss: 0.037362038333845826
Len (direct inputs):  100
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22174017243932737
Epoch: 0 Idx: 5000 Loss: 0.022492430062904217
Epoch: 1 Idx: 0 Loss: 0.02030183022080536
Epoch: 1 Idx: 5000 Loss: 0.027598584690665236
Epoch: 2 Idx: 0 Loss: 0.009332230401708013
Epoch: 2 Idx: 5000 Loss: 0.022802214197383534
Epoch: 3 Idx: 0 Loss: 0.033551203213421844
Epoch: 3 Idx: 5000 Loss: 0.029068265060194998
Epoch: 4 Idx: 0 Loss: 0.019099995420185586
Epoch: 4 Idx: 5000 Loss: 0.03887497485570843
Epoch: 5 Idx: 0 Loss: 0.016915464140651808
Epoch: 5 Idx: 5000 Loss: 0.02441014771314602
Epoch: 6 Idx: 0 Loss: 0.03054549435549779
Epoch: 6 Idx: 5000 Loss: 0.026554401054188
Epoch: 7 Idx: 0 Loss: 0.01289799607473437
Epoch: 7 Idx: 5000 Loss: 0.055363677235623646
Epoch: 8 Idx: 0 Loss: 0.011562016456293329
Epoch: 8 Idx: 5000 Loss: 0.018764479643315613
Epoch: 9 Idx: 0 Loss: 0.018675918947268076
Epoch: 9 Idx: 5000 Loss: 0.019928772139063057
Epoch: 10 Idx: 0 Loss: 0.03374863689295815
Epoch: 10 Idx: 5000 Loss: 0.03325552081139531
Epoch: 11 Idx: 0 Loss: 0.019060111656345385
Epoch: 11 Idx: 5000 Loss: 0.017021011518709
Epoch: 12 Idx: 0 Loss: 0.018181111268184634
Epoch: 12 Idx: 5000 Loss: 0.025224171964814343
Epoch: 13 Idx: 0 Loss: 0.014171870936663658
Epoch: 13 Idx: 5000 Loss: 0.027254730336423644
Epoch: 14 Idx: 0 Loss: 0.016056895855033684
Epoch: 14 Idx: 5000 Loss: 0.02934994468955573
Epoch: 15 Idx: 0 Loss: 0.009808596257228687
Epoch: 15 Idx: 5000 Loss: 0.03436748408348342
Epoch: 16 Idx: 0 Loss: 0.04219810741410547
Epoch: 16 Idx: 5000 Loss: 0.022327321531064648
Epoch: 17 Idx: 0 Loss: 0.039003852338571385
Epoch: 17 Idx: 5000 Loss: 0.033198856313551614
Epoch: 18 Idx: 0 Loss: 0.02126077262563707
Epoch: 18 Idx: 5000 Loss: 0.013376090568152008
Epoch: 19 Idx: 0 Loss: 0.01942707584421908
Epoch: 19 Idx: 5000 Loss: 0.028479815021036255
Epoch: 20 Idx: 0 Loss: 0.02504273435050227
Epoch: 20 Idx: 5000 Loss: 0.02652480127139191
Epoch: 21 Idx: 0 Loss: 0.028136524674773764
Epoch: 21 Idx: 5000 Loss: 0.014105139696155962
Epoch: 22 Idx: 0 Loss: 0.018167542128655158
Epoch: 22 Idx: 5000 Loss: 0.011903049855701937
Epoch: 23 Idx: 0 Loss: 0.015540791483655633
Epoch: 23 Idx: 5000 Loss: 0.018924004733533284
Epoch: 24 Idx: 0 Loss: 0.016002567786898066
Epoch: 24 Idx: 5000 Loss: 0.022588934931428085
Epoch: 25 Idx: 0 Loss: 0.024491693429873434
Epoch: 25 Idx: 5000 Loss: 0.014930866527547756
Epoch: 26 Idx: 0 Loss: 0.014334860843618819
Epoch: 26 Idx: 5000 Loss: 0.019786324136301082
Epoch: 27 Idx: 0 Loss: 0.02573197457444753
Epoch: 27 Idx: 5000 Loss: 0.02461560514606305
Epoch: 28 Idx: 0 Loss: 0.018979263656275704
Epoch: 28 Idx: 5000 Loss: 0.027741609207628644
Epoch: 29 Idx: 0 Loss: 0.028240902260012815
Epoch: 29 Idx: 5000 Loss: 0.02272203158758354
Epoch: 30 Idx: 0 Loss: 0.035931467879416465
Epoch: 30 Idx: 5000 Loss: 0.022057006581595387
Epoch: 31 Idx: 0 Loss: 0.02272240384546049
Epoch: 31 Idx: 5000 Loss: 0.03731228360831594
Epoch: 32 Idx: 0 Loss: 0.02804337287158763
Epoch: 32 Idx: 5000 Loss: 0.029104213077866467
Epoch: 33 Idx: 0 Loss: 0.026701924999294434
Epoch: 33 Idx: 5000 Loss: 0.028244122557071004
Epoch: 34 Idx: 0 Loss: 0.02896860680031463
Epoch: 34 Idx: 5000 Loss: 0.012585089636067477
Epoch: 35 Idx: 0 Loss: 0.024662265557181318
Epoch: 35 Idx: 5000 Loss: 0.02947820756935147
Epoch: 36 Idx: 0 Loss: 0.03248095769976933
Epoch: 36 Idx: 5000 Loss: 0.019584100389636924
Epoch: 37 Idx: 0 Loss: 0.04454269586467138
Epoch: 37 Idx: 5000 Loss: 0.02207976850910275
Epoch: 38 Idx: 0 Loss: 0.02363683773708984
Epoch: 38 Idx: 5000 Loss: 0.028500699032758307
Epoch: 39 Idx: 0 Loss: 0.01944566411972093
Epoch: 39 Idx: 5000 Loss: 0.03649100039400334
Epoch: 40 Idx: 0 Loss: 0.02678391329569529
Epoch: 40 Idx: 5000 Loss: 0.03868102087596208
Epoch: 41 Idx: 0 Loss: 0.01810722474653473
Epoch: 41 Idx: 5000 Loss: 0.0136530804757681
Epoch: 42 Idx: 0 Loss: 0.03396964958024712
Epoch: 42 Idx: 5000 Loss: 0.035115651465007995
Epoch: 43 Idx: 0 Loss: 0.013027015066578954
Epoch: 43 Idx: 5000 Loss: 0.028893504224312194
Epoch: 44 Idx: 0 Loss: 0.019888206265867736
Epoch: 44 Idx: 5000 Loss: 0.023770899375155564
Epoch: 45 Idx: 0 Loss: 0.040935708246650854
Epoch: 45 Idx: 5000 Loss: 0.033422179674304324
Epoch: 46 Idx: 0 Loss: 0.00500717701601258
Epoch: 46 Idx: 5000 Loss: 0.03799902634053477
Epoch: 47 Idx: 0 Loss: 0.010360000049659892
Epoch: 47 Idx: 5000 Loss: 0.014138966177111068
Epoch: 48 Idx: 0 Loss: 0.026457415861579986
Epoch: 48 Idx: 5000 Loss: 0.019965383233871096
Epoch: 49 Idx: 0 Loss: 0.02563452189226373
Epoch: 49 Idx: 5000 Loss: 0.018869380753386577
Len (direct inputs):  96
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17678734137658159
Epoch: 0 Idx: 5000 Loss: 0.021600825121353005
Epoch: 1 Idx: 0 Loss: 0.01498117565827076
Epoch: 1 Idx: 5000 Loss: 0.036505556992591615
Epoch: 2 Idx: 0 Loss: 0.018985326864676272
Epoch: 2 Idx: 5000 Loss: 0.018934800086272793
Epoch: 3 Idx: 0 Loss: 0.022013787915905386
Epoch: 3 Idx: 5000 Loss: 0.014237345026884332
Epoch: 4 Idx: 0 Loss: 0.01678091068518835
Epoch: 4 Idx: 5000 Loss: 0.019128651180446956
Epoch: 5 Idx: 0 Loss: 0.02464821913351691
Epoch: 5 Idx: 5000 Loss: 0.03133771763710811
Epoch: 6 Idx: 0 Loss: 0.012614975201061366
Epoch: 6 Idx: 5000 Loss: 0.011233996778634323
Epoch: 7 Idx: 0 Loss: 0.0305329630646339
Epoch: 7 Idx: 5000 Loss: 0.02677552245711471
Epoch: 8 Idx: 0 Loss: 0.010546481456683281
Epoch: 8 Idx: 5000 Loss: 0.015131526015349542
Epoch: 9 Idx: 0 Loss: 0.026846298904294293
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 313, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
KeyboardInterrupt
ss: 0.011380795375890679
Epoch: 17 Idx: 5000 Loss: 0.02169916188482282
Epoch: 18 Idx: 0 Loss: 0.014574405215033276
Epoch: 18 Idx: 5000 Loss: 0.0241001868815453
Epoch: 19 Idx: 0 Loss: 0.02577619414332466
Epoch: 19 Idx: 5000 Loss: 0.022891822239002366
Epoch: 20 Idx: 0 Loss: 0.03517490352384771
Epoch: 20 Idx: 5000 Loss: 0.023601432170839994
Epoch: 21 Idx: 0 Loss: 0.03326198681753689
Epoch: 21 Idx: 5000 Loss: 0.019409683426858705
Epoch: 22 Idx: 0 Loss: 0.016246079976125254
Epoch: 22 Idx: 5000 Loss: 0.014617341630895873
Epoch: 23 Idx: 0 Loss: 0.02020185718250108
Epoch: 23 Idx: 5000 Loss: 0.012078339233142723
Epoch: 24 Idx: 0 Loss: 0.02014266475871398
Epoch: 24 Idx: 5000 Loss: 0.019132006927051207
Epoch: 25 Idx: 0 Loss: 0.026210386307867886
Epoch: 25 Idx: 5000 Loss: 0.03226952677045663
Epoch: 26 Idx: 0 Loss: 0.018798603632641506
Epoch: 26 Idx: 5000 Loss: 0.021970342017178842
Epoch: 27 Idx: 0 Loss: 0.010337340934241261
Epoch: 27 Idx: 5000 Loss: 0.02073176570599315
Epoch: 28 Idx: 0 Loss: 0.01845116556148764
Epoch: 28 Idx: 5000 Loss: 0.02084874066433664
Epoch: 29 Idx: 0 Loss: 0.023188348197634587
Epoch: 29 Idx: 5000 Loss: 0.02185514039604748
Epoch: 30 Idx: 0 Loss: 0.013056742746666714
Epoch: 30 Idx: 5000 Loss: 0.025328042776109908
Epoch: 31 Idx: 0 Loss: 0.026870543317010616
Epoch: 31 Idx: 5000 Loss: 0.01569805237284265
Epoch: 32 Idx: 0 Loss: 0.028575408158151634
Epoch: 32 Idx: 5000 Loss: 0.011072361793121998
Epoch: 33 Idx: 0 Loss: 0.030710985272575615
Epoch: 33 Idx: 5000 Loss: 0.014737242342406613
Epoch: 34 Idx: 0 Loss: 0.01710418715781596
Epoch: 34 Idx: 5000 Loss: 0.023546908869888146
Epoch: 35 Idx: 0 Loss: 0.02250474851628433
Epoch: 35 Idx: 5000 Loss: 0.02372505612918923
Epoch: 36 Idx: 0 Loss: 0.014077413756791234
Epoch: 36 Idx: 5000 Loss: 0.02782670859361928
Epoch: 37 Idx: 0 Loss: 0.02257555083674386
Epoch: 37 Idx: 5000 Loss: 0.01970291830994523
Epoch: 38 Idx: 0 Loss: 0.025850195210334215
Epoch: 38 Idx: 5000 Loss: 0.027479256451449233
Epoch: 39 Idx: 0 Loss: 0.011516241624654023
Epoch: 39 Idx: 5000 Loss: 0.013720396667266646
Epoch: 40 Idx: 0 Loss: 0.020965394338972203
Epoch: 40 Idx: 5000 Loss: 0.025800542509863987
Epoch: 41 Idx: 0 Loss: 0.05346716277740496
Epoch: 41 Idx: 5000 Loss: 0.01258285140393511
Epoch: 42 Idx: 0 Loss: 0.017594550821618194
Epoch: 42 Idx: 5000 Loss: 0.036375616650530455
Epoch: 43 Idx: 0 Loss: 0.0165004134853549
Epoch: 43 Idx: 5000 Loss: 0.020874477783899177
Epoch: 44 Idx: 0 Loss: 0.05618874452973632
Epoch: 44 Idx: 5000 Loss: 0.01900028300851412
Epoch: 45 Idx: 0 Loss: 0.02666394759956442
Epoch: 45 Idx: 5000 Loss: 0.03446101352648997
Epoch: 46 Idx: 0 Loss: 0.0199945724032546
Epoch: 46 Idx: 5000 Loss: 0.02822767222038286
Epoch: 47 Idx: 0 Loss: 0.01515027218983975
Epoch: 47 Idx: 5000 Loss: 0.027461855973571887
Epoch: 48 Idx: 0 Loss: 0.024035916210998508
Epoch: 48 Idx: 5000 Loss: 0.020425429826711566
Epoch: 49 Idx: 0 Loss: 0.036184679875407154
Epoch: 49 Idx: 5000 Loss: 0.02708424960665676
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21216924502201745
Epoch: 0 Idx: 5000 Loss: 0.019895028108866987
Epoch: 1 Idx: 0 Loss: 0.02784515837927054
Epoch: 1 Idx: 5000 Loss: 0.020971047803110893
Epoch: 2 Idx: 0 Loss: 0.014284785244502465
Epoch: 2 Idx: 5000 Loss: 0.02124234620867293
Epoch: 3 Idx: 0 Loss: 0.029175601587769644
Epoch: 3 Idx: 5000 Loss: 0.020658251523697532
Epoch: 4 Idx: 0 Loss: 0.017182392867308688
Epoch: 4 Idx: 5000 Loss: 0.019729067004958068
Epoch: 5 Idx: 0 Loss: 0.021316083857508984
Epoch: 5 Idx: 5000 Loss: 0.01894239038937553
Epoch: 6 Idx: 0 Loss: 0.02215032173847152
Epoch: 6 Idx: 5000 Loss: 0.013040588917341278
Epoch: 7 Idx: 0 Loss: 0.015988493361749172
Epoch: 7 Idx: 5000 Loss: 0.02118690788461207
Epoch: 8 Idx: 0 Loss: 0.02720745113618572
Epoch: 8 Idx: 5000 Loss: 0.029692162839458223
Epoch: 9 Idx: 0 Loss: 0.008038359176107283
Epoch: 9 Idx: 5000 Loss: 0.022253755306935897
Epoch: 10 Idx: 0 Loss: 0.025652975890508974
Epoch: 10 Idx: 5000 Loss: 0.010557726154313295
Epoch: 11 Idx: 0 Loss: 0.017562430416957008
Epoch: 11 Idx: 5000 Loss: 0.012528097710568897
Epoch: 12 Idx: 0 Loss: 0.02457016698903019
Epoch: 12 Idx: 5000 Loss: 0.009319531804781624
Epoch: 13 Idx: 0 Loss: 0.018539391708815797
Epoch: 13 Idx: 5000 Loss: 0.02325775109050748
Epoch: 14 Idx: 0 Loss: 0.019276806697073694
Epoch: 14 Idx: 5000 Loss: 0.01862666886920869
Epoch: 15 Idx: 0 Loss: 0.021060945234723773
Epoch: 15 Idx: 5000 Loss: 0.034745000091116224
Epoch: 16 Idx: 0 Loss: 0.03128143242828964
Epoch: 16 Idx: 5000 Loss: 0.012333302648150885
Epoch: 17 Idx: 0 Loss: 0.013776974841778027
Epoch: 17 Idx: 5000 Loss: 0.02253045174970874
Epoch: 18 Idx: 0 Loss: 0.022550057719355483
Epoch: 18 Idx: 5000 Loss: 0.02717523024060419
Epoch: 19 Idx: 0 Loss: 0.022976004618682468
Epoch: 19 Idx: 5000 Loss: 0.012851364625954424
Epoch: 20 Idx: 0 Loss: 0.00884818793818183
Epoch: 20 Idx: 5000 Loss: 0.018722860327470676
Epoch: 21 Idx: 0 Loss: 0.01485020059417803
Epoch: 21 Idx: 5000 Loss: 0.03667432623792836
Epoch: 22 Idx: 0 Loss: 0.014185413340351491
Epoch: 22 Idx: 5000 Loss: 0.023327862320543992
Epoch: 23 Idx: 0 Loss: 0.014383736399705365
Epoch: 23 Idx: 5000 Loss: 0.016200104306985978
Epoch: 24 Idx: 0 Loss: 0.030194288952955072
Epoch: 24 Idx: 5000 Loss: 0.04670453085521188
Epoch: 25 Idx: 0 Loss: 0.014493654549141465
Epoch: 25 Idx: 5000 Loss: 0.030887273632363967
Epoch: 26 Idx: 0 Loss: 0.02358977890155073
Epoch: 26 Idx: 5000 Loss: 0.02423982603218669
Epoch: 27 Idx: 0 Loss: 0.008940413094297392
Epoch: 27 Idx: 5000 Loss: 0.008301788429721218
Epoch: 28 Idx: 0 Loss: 0.017658581325524537
Epoch: 28 Idx: 5000 Loss: 0.019766887619388274
Epoch: 29 Idx: 0 Loss: 0.01966178828983084
Epoch: 29 Idx: 5000 Loss: 0.02589193804784234
Epoch: 30 Idx: 0 Loss: 0.014342896774739974
Epoch: 30 Idx: 5000 Loss: 0.017631385612657785
Epoch: 31 Idx: 0 Loss: 0.016706917464541754
Epoch: 31 Idx: 5000 Loss: 0.011261868278882503
Epoch: 32 Idx: 0 Loss: 0.03659505067791237
Epoch: 32 Idx: 5000 Loss: 0.016739526175561802
Epoch: 33 Idx: 0 Loss: 0.020465992052500173
Epoch: 33 Idx: 5000 Loss: 0.022250513641370824
Epoch: 34 Idx: 0 Loss: 0.008306288224255009
Epoch: 34 Idx: 5000 Loss: 0.034228221701495774
Epoch: 35 Idx: 0 Loss: 0.015533092607622204
Epoch: 35 Idx: 5000 Loss: 0.02673058323954189
Epoch: 36 Idx: 0 Loss: 0.01344924540987186
Epoch: 36 Idx: 5000 Loss: 0.011671813239910926
Epoch: 37 Idx: 0 Loss: 0.021435931613890763
Epoch: 37 Idx: 5000 Loss: 0.02905528634222449
Epoch: 38 Idx: 0 Loss: 0.04078079293770259
Epoch: 38 Idx: 5000 Loss: 0.024215814163418822
Epoch: 39 Idx: 0 Loss: 0.027557754998503284
Epoch: 39 Idx: 5000 Loss: 0.00993330353459201
Epoch: 40 Idx: 0 Loss: 0.018418424785732884
Epoch: 40 Idx: 5000 Loss: 0.009885178830847129
Epoch: 41 Idx: 0 Loss: 0.04015173561725395
Epoch: 41 Idx: 5000 Loss: 0.02223009775821857
Epoch: 42 Idx: 0 Loss: 0.033760303299301284
Epoch: 42 Idx: 5000 Loss: 0.02192487015627427
Epoch: 43 Idx: 0 Loss: 0.019202572214733517
Epoch: 43 Idx: 5000 Loss: 0.025116754297510313
Epoch: 44 Idx: 0 Loss: 0.024322950547281754
Epoch: 44 Idx: 5000 Loss: 0.03082884562472232
Epoch: 45 Idx: 0 Loss: 0.01938658698026159
Epoch: 45 Idx: 5000 Loss: 0.011586341526740295
Epoch: 46 Idx: 0 Loss: 0.015920189386511383
Epoch: 46 Idx: 5000 Loss: 0.04482148390772119
Epoch: 47 Idx: 0 Loss: 0.01462446859076661
Epoch: 47 Idx: 5000 Loss: 0.013615838849908189
Epoch: 48 Idx: 0 Loss: 0.01727951887037809
Epoch: 48 Idx: 5000 Loss: 0.028777299645946195
Epoch: 49 Idx: 0 Loss: 0.015258619967267864
Epoch: 49 Idx: 5000 Loss: 0.0079733648439371
Len (direct inputs):  83
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.207754679962096
Epoch: 0 Idx: 5000 Loss: 0.03550882167071285
Epoch: 1 Idx: 0 Loss: 0.015064685085736369
Epoch: 1 Idx: 5000 Loss: 0.023110719904037208
Epoch: 2 Idx: 0 Loss: 0.022660791853127877
Epoch: 2 Idx: 5000 Loss: 0.011088054507850895
Epoch: 3 Idx: 0 Loss: 0.03750509625573507
Epoch: 3 Idx: 5000 Loss: 0.038632314492149995
Epoch: 4 Idx: 0 Loss: 0.025249164325263765
Epoch: 4 Idx: 5000 Loss: 0.014483222379629634
Epoch: 5 Idx: 0 Loss: 0.026041542831014133
Epoch: 5 Idx: 5000 Loss: 0.013976999564829121
Epoch: 6 Idx: 0 Loss: 0.0223244824721383
Epoch: 6 Idx: 5000 Loss: 0.009823517399298572
Epoch: 7 Idx: 0 Loss: 0.033477504554392676
Epoch: 7 Idx: 5000 Loss: 0.015098551003314368
Epoch: 8 Idx: 0 Loss: 0.02695450472780659
Epoch: 8 Idx: 5000 Loss: 0.013331693523120346
Epoch: 9 Idx: 0 Loss: 0.03014219744370485
Epoch: 9 Idx: 5000 Loss: 0.02149236505956676
Epoch: 10 Idx: 0 Loss: 0.026483299775496676
Epoch: 10 Idx: 5000 Loss: 0.02079350880317888
Epoch: 11 Idx: 0 Loss: 0.024709051939481048
Epoch: 11 Idx: 5000 Loss: 0.012280552764692976
Epoch: 12 Idx: 0 Loss: 0.027705782502086253
Epoch: 12 Idx: 5000 Loss: 0.022233459091352846
Epoch: 13 Idx: 0 Loss: 0.010476182266476265
Epoch: 13 Idx: 5000 Loss: 0.016548735585943084
Epoch: 14 Idx: 0 Loss: 0.019400496626117307
Epoch: 14 Idx: 5000 Loss: 0.030194059335445417
Epoch: 15 Idx: 0 Loss: 0.017296367542143868
Epoch: 15 Idx: 5000 Loss: 0.034548165100587606
Epoch: 16 Idx: 0 Loss: 0.04860971661172732
Epoch: 16 Idx: 5000 Loss: 0.018614755450738533
Epoch: 17 Idx: 0 Loss: 0.021083930941987156
Epoch: 17 Idx: 5000 Loss: 0.015833114820671026
Epoch: 18 Idx: 0 Loss: 0.021586192969513966
Epoch: 18 Idx: 5000 Loss: 0.027423027113101996
Epoch: 19 Idx: 0 Loss: 0.02269835574156921
Epoch: 19 Idx: 5000 Loss: 0.02323319675903156
Epoch: 20 Idx: 0 Loss: 0.03164138901889034
Epoch: 20 Idx: 5000 Loss: 0.01878842710093153
Epoch: 21 Idx: 0 Loss: 0.022431412349171943
Epoch: 21 Idx: 5000 Loss: 0.032810904826798964
Epoch: 22 Idx: 0 Loss: 0.011412710357078644
Epoch: 22 Idx: 5000 Loss: 0.03265508184684169
Epoch: 23 Idx: 0 Loss: 0.02765345594491405
Epoch: 23 Idx: 5000 Loss: 0.01508418078382066
Epoch: 24 Idx: 0 Loss: 0.024190756461009166
Epoch: 24 Idx: 5000 Loss: 0.022692386337387464
Epoch: 25 Idx: 0 Loss: 0.022976133814084716
Epoch: 25 Idx: 5000 Loss: 0.021285584395468876
Epoch: 26 Idx: 0 Loss: 0.017130702227734607
Epoch: 26 Idx: 5000 Loss: 0.015134116695090433
Epoch: 27 Idx: 0 Loss: 0.0206533299685882
Epoch: 27 Idx: 5000 Loss: 0.028258300664732672
Epoch: 28 Idx: 0 Loss: 0.02403169101215908
Epoch: 28 Idx: 5000 Loss: 0.04573928006119342
Epoch: 29 Idx: 0 Loss: 0.021494185965930973
Epoch: 29 Idx: 5000 Loss: 0.032882702182400796
Epoch: 30 Idx: 0 Loss: 0.01804986604040146
Epoch: 30 Idx: 5000 Loss: 0.026820495259052113
Epoch: 31 Idx: 0 Loss: 0.05333402023607158
Epoch: 31 Idx: 5000 Loss: 0.011412649904461886
Epoch: 32 Idx: 0 Loss: 0.014006856458286011
Epoch: 32 Idx: 5000 Loss: 0.012379372466381381
Epoch: 33 Idx: 0 Loss: 0.023781488333349726
Epoch: 33 Idx: 5000 Loss: 0.031106181319037196
Epoch: 34 Idx: 0 Loss: 0.02309004087946879
Epoch: 34 Idx: 5000 Loss: 0.018358890154718744
Epoch: 35 Idx: 0 Loss: 0.03843629005533996
Epoch: 35 Idx: 5000 Loss: 0.0074595374753659025
Epoch: 36 Idx: 0 Loss: 0.024830945290591565
Epoch: 36 Idx: 5000 Loss: 0.024057939553019925
Epoch: 37 Idx: 0 Loss: 0.02666867874539982
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 313, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml_weighted.py", line 312, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
: 0 Loss: 0.04203442467141461
Epoch: 47 Idx: 5000 Loss: 0.012049524839217538
Epoch: 48 Idx: 0 Loss: 0.004757945134430666
Epoch: 48 Idx: 5000 Loss: 0.01383111775575077
Epoch: 49 Idx: 0 Loss: 0.025281827389155017
Epoch: 49 Idx: 5000 Loss: 0.031019472950872106
Len (direct inputs):  94
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divi0.845164665400.8446796498192247
Parameter containing:
tensor([0.8447], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.23120377817435817
Epoch: 0 Idx: 5000 Loss: 0.0245378121238623
Epoch: 1 Idx: 0 Loss: 0.01477218429638434
Epoch: 1 Idx: 5000 Loss: 0.0175047073560889
Epoch: 2 Idx: 0 Loss: 0.01825619418534532
Epoch: 2 Idx: 5000 Loss: 0.02329060811651127
Epoch: 3 Idx: 0 Loss: 0.01629124535930627
Epoch: 3 Idx: 5000 Loss: 0.014258857597865236
Epoch: 4 Idx: 0 Loss: 0.030667138150029007
Epoch: 4 Idx: 5000 Loss: 0.015552838225201506
Epoch: 5 Idx: 0 Loss: 0.016796008129205037
Epoch: 5 Idx: 5000 Loss: 0.009816198615055883
Epoch: 6 Idx: 0 Loss: 0.02427333101282772
Epoch: 6 Idx: 5000 Loss: 0.03834905394320146
Epoch: 7 Idx: 0 Loss: 0.039668779908951256
Epoch: 7 Idx: 5000 Loss: 0.020180815156031623
Epoch: 8 Idx: 0 Loss: 0.026575739609278837
Epoch: 8 Idx: 5000 Loss: 0.027126035564381784
Epoch: 9 Idx: 0 Loss: 0.008365202406770396
Epoch: 9 Idx: 5000 Loss: 0.0176401929084221
Epoch: 10 Idx: 0 Loss: 0.006260566988589176
Epoch: 10 Idx: 5000 Loss: 0.033340237254794555
Epoch: 11 Idx: 0 Loss: 0.034973807781956044
Epoch: 11 Idx: 5000 Loss: 0.019000048773910462
Epoch: 12 Idx: 0 Loss: 0.031732246240189874
Epoch: 12 Idx: 5000 Loss: 0.02023404534133892
Epoch: 13 Idx: 0 Loss: 0.027271878694474123
Epoch: 13 Idx: 5000 Loss: 0.026317421281103585
Epoch: 14 Idx: 0 Loss: 0.013037891615155752
Epoch: 14 Idx: 5000 Loss: 0.0184638933984808
Epoch: 15 Idx: 0 Loss: 0.02726226635160838
Epoch: 15 Idx: 5000 Loss: 0.019124324676134563
Epoch: 16 Idx: 0 Loss: 0.010521423943817
Epoch: 16 Idx: 5000 Loss: 0.020256768804702237
Epoch: 17 Idx: 0 Loss: 0.018962460163444855
Epoch: 17 Idx: 5000 Loss: 0.007939826414668076
Epoch: 18 Idx: 0 Loss: 0.020682426561684338
Epoch: 18 Idx: 5000 Loss: 0.025946504767149388
Epoch: 19 Idx: 0 Loss: 0.00921869079359897
Epoch: 19 Idx: 5000 Loss: 0.009913543342651982
Epoch: 20 Idx: 0 Loss: 0.020541179145630223
Epoch: 20 Idx: 5000 Loss: 0.01676117947223568
Epoch: 21 Idx: 0 Loss: 0.02988158019455586
Epoch: 21 Idx: 5000 Loss: 0.046437899097608516
Epoch: 22 Idx: 0 Loss: 0.016417426561377603
Epoch: 22 Idx: 5000 Loss: 0.022628913928205572
Epoch: 23 Idx: 0 Loss: 0.01608776213403389
Epoch: 23 Idx: 5000 Loss: 0.021161603161905553
Epoch: 24 Idx: 0 Loss: 0.03454702549771719
Epoch: 24 Idx: 5000 Loss: 0.01742404450248946
Epoch: 25 Idx: 0 Loss: 0.0176469039400147
Epoch: 25 Idx: 5000 Loss: 0.0164674783770214
Epoch: 26 Idx: 0 Loss: 0.031592338398733384
Epoch: 26 Idx: 5000 Loss: 0.02164759041047027
Epoch: 27 Idx: 0 Loss: 0.017852152381660123
Epoch: 27 Idx: 5000 Loss: 0.02193833136277102
Epoch: 28 Idx: 0 Loss: 0.030976037882632868
Epoch: 28 Idx: 5000 Loss: 0.020609437427420746
Epoch: 29 Idx: 0 Loss: 0.015303201907558676
Epoch: 29 Idx: 5000 Loss: 0.030078331892632232
Epoch: 30 Idx: 0 Loss: 0.03173088042325389
Epoch: 30 Idx: 5000 Loss: 0.014620092729237495
Epoch: 31 Idx: 0 Loss: 0.011987770975775129
Epoch: 31 Idx: 5000 Loss: 0.014362060998558156
Epoch: 32 Idx: 0 Loss: 0.015495929946603974
Epoch: 32 Idx: 5000 Loss: 0.019217752542327382
Epoch: 33 Idx: 0 Loss: 0.03484150964972462
Epoch: 33 Idx: 5000 Loss: 0.020075641233653782
Epoch: 34 Idx: 0 Loss: 0.017959171737578453
Epoch: 34 Idx: 5000 Loss: 0.01824162558588937
Epoch: 35 Idx: 0 Loss: 0.013355833773635074
Epoch: 35 Idx: 5000 Loss: 0.02216856557682243
Epoch: 36 Idx: 0 Loss: 0.013229487087759855
Epoch: 36 Idx: 5000 Loss: 0.03193309476259286
Epoch: 37 Idx: 0 Loss: 0.01197676019114155
Epoch: 37 Idx: 5000 Loss: 0.015863963902814825
Epoch: 38 Idx: 0 Loss: 0.027442440690411598
Epoch: 38 Idx: 5000 Loss: 0.01305059881766335
Epoch: 39 Idx: 0 Loss: 0.019626234262424384
Epoch: 39 Idx: 5000 Loss: 0.017932645522174877
Epoch: 40 Idx: 0 Loss: 0.014180187075209521
Epoch: 40 Idx: 5000 Loss: 0.0405596470326056
Epoch: 41 Idx: 0 Loss: 0.024641345653806244
Epoch: 41 Idx: 5000 Loss: 0.023607107400719654
Epoch: 42 Idx: 0 Loss: 0.01492618142575845
Epoch: 42 Idx: 5000 Loss: 0.021156695912554643
Epoch: 43 Idx: 0 Loss: 0.021188524566500964
Epoch: 43 Idx: 5000 Loss: 0.028724716614770725
Epoch: 44 Idx: 0 Loss: 0.006899946193992471
Epoch: 44 Idx: 5000 Loss: 0.018030224290804874
Epoch: 45 Idx: 0 Loss: 0.023496682656260176
Epoch: 45 Idx: 5000 Loss: 0.024803901810354523
Epoch: 46 Idx: 0 Loss: 0.019445118224982856
Epoch: 46 Idx: 5000 Loss: 0.015282657464457549
Epoch: 47 Idx: 0 Loss: 0.024006328321986473
Epoch: 47 Idx: 5000 Loss: 0.03024761515443123
Epoch: 48 Idx: 0 Loss: 0.027803738105848436
Epoch: 48 Idx: 5000 Loss: 0.033768288550009906
Epoch: 49 Idx: 0 Loss: 0.009885111203205255
Epoch: 49 Idx: 5000 Loss: 0.013502321424043836
Len (direct inputs):  642
Final Results: [0.93669065 0.87206966 0.90322581 0.88427058 0.92301148]
Threshold:  0.8446796498192247
s.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
th Disk quota exceeded

-------------------------
Sender: LSF System <rer@dccxc254>
Subject: Job 3289874: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs_wtpath1_4.pkl Models/anatomy_aml_bagofnbrs_wtpath1_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs_wtpath1_4.pkl Models/anatomy_aml_bagofnbrs_wtpath1_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc254>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Tue Sep  1 20:42:03 2020
Results reported at Tue Sep  1 20:42:03 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs_wtpath1_4.pkl Models/anatomy_aml_bagofnbrs_wtpath1_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24335.67 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2723.00 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   24437 sec.
    Turnaround time :                            24493 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc255>
Subject: Job 3289876: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs_wtpath2_4.pkl Models/anatomy_aml_bagofnbrs_wtpath2_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs_wtpath2_4.pkl Models/anatomy_aml_bagofnbrs_wtpath2_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc255>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:07:37 2020
Results reported at Tue Sep  1 21:07:37 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs_wtpath2_4.pkl Models/anatomy_aml_bagofnbrs_wtpath2_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25965.07 sec.
    Max Memory :                                 2918 MB
    Average Memory :                             2699.26 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40499.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25996 sec.
    Turnaround time :                            26027 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc236>
Subject: Job 3289878: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs_wtpath3_4.pkl Models/anatomy_aml_bagofnbrs_wtpath3_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs_wtpath3_4.pkl Models/anatomy_aml_bagofnbrs_wtpath3_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc236>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:25:29 2020
Results reported at Tue Sep  1 21:25:29 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs_wtpath3_4.pkl Models/anatomy_aml_bagofnbrs_wtpath3_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26981.45 sec.
    Max Memory :                                 2925 MB
    Average Memory :                             2714.09 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40492.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27041 sec.
    Turnaround time :                            27099 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc258>
Subject: Job 3289880: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs_wtpath4_4.pkl Models/anatomy_aml_bagofnbrs_wtpath4_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs_wtpath4_4.pkl Models/anatomy_aml_bagofnbrs_wtpath4_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc258>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:40:48 2020
Results reported at Tue Sep  1 21:40:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs_wtpath4_4.pkl Models/anatomy_aml_bagofnbrs_wtpath4_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27891.24 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2712.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27986 sec.
    Turnaround time :                            28018 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc273>
Subject: Job 3289882: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs_wtpath5_4.pkl Models/anatomy_aml_bagofnbrs_wtpath5_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs_wtpath5_4.pkl Models/anatomy_aml_bagofnbrs_wtpath5_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc273>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 22:26:58 2020
Results reported at Tue Sep  1 22:26:58 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs_wtpath5_4.pkl Models/anatomy_aml_bagofnbrs_wtpath5_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30639.92 sec.
    Max Memory :                                 2934 MB
    Average Memory :                             2726.53 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40483.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30730 sec.
    Turnaround time :                            30788 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc238>
Subject: Job 3289884: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs_wtpath7_4.pkl Models/anatomy_aml_bagofnbrs_wtpath7_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs_wtpath7_4.pkl Models/anatomy_aml_bagofnbrs_wtpath7_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc238>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 22:49:55 2020
Results reported at Tue Sep  1 22:49:55 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs_wtpath7_4.pkl Models/anatomy_aml_bagofnbrs_wtpath7_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   32058.21 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2710.76 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   32134 sec.
    Turnaround time :                            32165 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc243>
Subject: Job 3289904: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 4 Output/test_anatomy_aml_bagofnbrs_wtpath30_4.pkl Models/anatomy_aml_bagofnbrs_wtpath30_4.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 4 Output/test_anatomy_aml_bagofnbrs_wtpath30_4.pkl Models/anatomy_aml_bagofnbrs_wtpath30_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:51 2020
Job was executed on host(s) <dccxc243>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:55:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:55:47 2020
Terminated at Wed Sep  2 11:46:58 2020
Results reported at Wed Sep  2 11:46:58 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 4 Output/test_anatomy_aml_bagofnbrs_wtpath30_4.pkl Models/anatomy_aml_bagofnbrs_wtpath30_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   78655.63 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2714.10 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   78671 sec.
    Turnaround time :                            78787 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc278>
Subject: Job 3289912: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 4 Output/test_anatomy_aml_bagofnbrs_wtpath152_4.pkl Models/anatomy_aml_bagofnbrs_wtpath152_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 4 Output/test_anatomy_aml_bagofnbrs_wtpath152_4.pkl Models/anatomy_aml_bagofnbrs_wtpath152_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc278>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 4 Output/test_anatomy_aml_bagofnbrs_wtpath152_4.pkl Models/anatomy_aml_bagofnbrs_wtpath152_4.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80368.53 sec.
    Max Memory :                                 2724 MB
    Average Memory :                             2637.29 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40693.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80520 sec.
    Turnaround time :                            80696 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc260>
Subject: Job 3289910: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 4 Output/test_anatomy_aml_bagofnbrs_wtpath80_4.pkl Models/anatomy_aml_bagofnbrs_wtpath80_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 4 Output/test_anatomy_aml_bagofnbrs_wtpath80_4.pkl Models/anatomy_aml_bagofnbrs_wtpath80_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc260>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 4 Output/test_anatomy_aml_bagofnbrs_wtpath80_4.pkl Models/anatomy_aml_bagofnbrs_wtpath80_4.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80504.02 sec.
    Max Memory :                                 2733 MB
    Average Memory :                             2662.72 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40684.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80521 sec.
    Turnaround time :                            80697 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc271>
Subject: Job 3289908: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 4 Output/test_anatomy_aml_bagofnbrs_wtpath40_4.pkl Models/anatomy_aml_bagofnbrs_wtpath40_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 4 Output/test_anatomy_aml_bagofnbrs_wtpath40_4.pkl Models/anatomy_aml_bagofnbrs_wtpath40_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc271>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 4 Output/test_anatomy_aml_bagofnbrs_wtpath40_4.pkl Models/anatomy_aml_bagofnbrs_wtpath40_4.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80519.12 sec.
    Max Memory :                                 2743 MB
    Average Memory :                             2673.92 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40674.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80522 sec.
    Turnaround time :                            80697 sec.

The output (if any) is above this job summary.

