Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.23578977248846172
Epoch: 0 Idx: 5000 Loss: 0.015414800297779634
Epoch: 1 Idx: 0 Loss: 0.03342456132115745
Epoch: 1 Idx: 5000 Loss: 0.009219894321848249
Epoch: 2 Idx: 0 Loss: 0.014775097814587478
Epoch: 2 Idx: 5000 Loss: 0.011599755412547774
Epoch: 3 Idx: 0 Loss: 0.020186421119453263
Epoch: 3 Idx: 5000 Loss: 0.02933975432314532
Epoch: 4 Idx: 0 Loss: 0.017250154667326226
Epoch: 4 Idx: 5000 Loss: 0.016838468005413533
Epoch: 5 Idx: 0 Loss: 0.03662502909276083
Epoch: 5 Idx: 5000 Loss: 0.01194440104029379
Epoch: 6 Idx: 0 Loss: 0.01696305327935068
Epoch: 6 Idx: 5000 Loss: 0.041479965688794605
Epoch: 7 Idx: 0 Loss: 0.016875532502396585
Epoch: 7 Idx: 5000 Loss: 0.01740252206484273
Epoch: 8 Idx: 0 Loss: 0.020080939751363467
Epoch: 8 Idx: 5000 Loss: 0.023574622388592176
Epoch: 9 Idx: 0 Loss: 0.01646282489611285
Epoch: 9 Idx: 5000 Loss: 0.015185812426782627
Epoch: 10 Idx: 0 Loss: 0.02090329899144105
Epoch: 10 Idx: 5000 Loss: 0.017233102469018404
Epoch: 11 Idx: 0 Loss: 0.034899294040622766
Epoch: 11 Idx: 5000 Loss: 0.03009299649348782
Epoch: 12 Idx: 0 Loss: 0.016249490940530995
Epoch: 12 Idx: 5000 Loss: 0.017258204276348207
Epoch: 13 Idx: 0 Loss: 0.025469156858348233
Epoch: 13 Idx: 5000 Loss: 0.018643937838944177
Epoch: 14 Idx: 0 Loss: 0.02601422632272829
Epoch: 14 Idx: 5000 Loss: 0.01875069072116961
Epoch: 15 Idx: 0 Loss: 0.023057092679846836
Epoch: 15 Idx: 5000 Loss: 0.019842835714606464
Epoch: 16 Idx: 0 Loss: 0.03372257931504853
Epoch: 16 Idx: 5000 Loss: 0.017496535307324382
Epoch: 17 Idx: 0 Loss: 0.022114007265948872
Epoch: 17 Idx: 5000 Loss: 0.02095146922080859
Epoch: 18 Idx: 0 Loss: 0.012913058946512592
Epoch: 18 Idx: 5000 Loss: 0.022414624091627876
Epoch: 19 Idx: 0 Loss: 0.013957839264577644
Epoch: 19 Idx: 5000 Loss: 0.022868395446089176
Epoch: 20 Idx: 0 Loss: 0.02125298099153266
Epoch: 20 Idx: 5000 Loss: 0.015862211733486974
Epoch: 21 Idx: 0 Loss: 0.023568095551553286
Epoch: 21 Idx: 5000 Loss: 0.017443511288765187
Epoch: 22 Idx: 0 Loss: 0.036790816409854515
Epoch: 22 Idx: 5000 Loss: 0.025476037430322382
Epoch: 23 Idx: 0 Loss: 0.028465850608109664
Epoch: 23 Idx: 5000 Loss: 0.023626048664384672
Epoch: 24 Idx: 0 Loss: 0.029768326911500893
Epoch: 24 Idx: 5000 Loss: 0.022570511530941965
Epoch: 25 Idx: 0 Loss: 0.024222016848015917
Epoch: 25 Idx: 5000 Loss: 0.015799465815313617
Epoch: 26 Idx: 0 Loss: 0.014861621329205316
Epoch: 26 Idx: 5000 Loss: 0.029728803018868592
Epoch: 27 Idx: 0 Loss: 0.024580238693802204
Epoch: 27 Idx: 5000 Loss: 0.03453720932849879
Epoch: 28 Idx: 0 Loss: 0.018915158516359096
Epoch: 28 Idx: 5000 Loss: 0.028297970873593002
Epoch: 29 Idx: 0 Loss: 0.027931134806021662
Epoch: 29 Idx: 5000 Loss: 0.014703726838316084
Epoch: 30 Idx: 0 Loss: 0.017977131255721035
Epoch: 30 Idx: 5000 Loss: 0.017248717563356226
Epoch: 31 Idx: 0 Loss: 0.012761967500044543
Epoch: 31 Idx: 5000 Loss: 0.018500344476109147
Epoch: 32 Idx: 0 Loss: 0.009150298769429953
Epoch: 32 Idx: 5000 Loss: 0.01655097505593267
Epoch: 33 Idx: 0 Loss: 0.016984013280075632
Epoch: 33 Idx: 5000 Loss: 0.011890959495536067
Epoch: 34 Idx: 0 Loss: 0.024362488650407847
Epoch: 34 Idx: 5000 Loss: 0.024060141410754625
Epoch: 35 Idx: 0 Loss: 0.021940911007034478
Epoch: 35 Idx: 5000 Loss: 0.015498623662235833
Epoch: 36 Idx: 0 Loss: 0.02069072780458569
Epoch: 36 Idx: 5000 Loss: 0.03263790201753228
Epoch: 37 Idx: 0 Loss: 0.013922480835845526
Epoch: 37 Idx: 5000 Loss: 0.02657182427713049
Epoch: 38 Idx: 0 Loss: 0.035412921554950114
Epoch: 38 Idx: 5000 Loss: 0.012358466784686125
Epoch: 39 Idx: 0 Loss: 0.015204042513856281
Epoch: 39 Idx: 5000 Loss: 0.03628686859342618
Epoch: 40 Idx: 0 Loss: 0.013242842199908778
Epoch: 40 Idx: 5000 Loss: 0.04744753946930501
Epoch: 41 Idx: 0 Loss: 0.03742656349031942
Epoch: 41 Idx: 5000 Loss: 0.03294751098932977
Epoch: 42 Idx: 0 Loss: 0.012758316409672975
Epoch: 42 Idx: 5000 Loss: 0.016338600074515444
Epoch: 43 Idx: 0 Loss: 0.01775270041726385
Epoch: 43 Idx: 5000 Loss: 0.031552604421319144
Epoch: 44 Idx: 0 Loss: 0.011637580592062373
Epoch: 44 Idx: 5000 Loss: 0.022645706580971037
Epoch: 45 Idx: 0 Loss: 0.02088949523887633
Epoch: 45 Idx: 5000 Loss: 0.034184108085354055
Epoch: 46 Idx: 0 Loss: 0.026561491446617617
Epoch: 46 Idx: 5000 Loss: 0.022428108181536344
Epoch: 47 Idx: 0 Loss: 0.014014959947403711
Epoch: 47 Idx: 5000 Loss: 0.01996787293966044
Epoch: 48 Idx: 0 Loss: 0.01364473935766762
Epoch: 48 Idx: 5000 Loss: 0.021541111740334067
Epoch: 49 Idx: 0 Loss: 0.02150687849950402
Epoch: 49 Idx: 5000 Loss: 0.011562782362270818
Len (direct inputs):  83
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisionTraining Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2552982564050011
Epoch: 0 Idx: 5000 Loss: 0.035249251198909426
Epoch: 1 Idx: 0 Loss: 0.029996031560437664
Epoch: 1 Idx: 5000 Loss: 0.014941400871629549
Epoch: 2 Idx: 0 Loss: 0.017930886621354315
Epoch: 2 Idx: 5000 Loss: 0.03079475922767224
Epoch: 3 Idx: 0 Loss: 0.013662724047681263
Epoch: 3 Idx: 5000 Loss: 0.021069699772615476
Epoch: 4 Idx: 0 Loss: 0.020149322543471133
Epoch: 4 Idx: 5000 Loss: 0.02023761685880049
Epoch: 5 Idx: 0 Loss: 0.014583672901086997
Epoch: 5 Idx: 5000 Loss: 0.03935894845843514
Epoch: 6 Idx: 0 Loss: 0.023844164972695453
Epoch: 6 Idx: 5000 Loss: 0.03294870116550137
Epoch: 7 Idx: 0 Loss: 0.023547586658212938
Epoch: 7 Idx: 5000 Loss: 0.020875460198687294
Epoch: 8 Idx: 0 Loss: 0.02808683764274814
Epoch: 8 Idx: 5000 Loss: 0.016661964784887406
Epoch: 9 Idx: 0 Loss: 0.019812649229737953
Epoch: 9 Idx: 5000 Loss: 0.04301607282034234
Epoch: 10 Idx: 0 Loss: 0.017083624790668044
Epoch: 10 Idx: 5000 Loss: 0.03168197805992802
Epoch: 11 Idx: 0 Loss: 0.01464027700018371
Epoch: 11 Idx: 5000 Loss: 0.021730670515258024
Epoch: 12 Idx: 0 Loss: 0.014608105313947631
Epoch: 12 Idx: 5000 Loss: 0.02157669850513462
Epoch: 13 Idx: 0 Loss: 0.029014514715724894
Epoch: 13 Idx: 5000 Loss: 0.01758350870156946
Epoch: 14 Idx: 0 Loss: 0.03343478080652787
Epoch: 14 Idx: 5000 Loss: 0.01572813755753641
Epoch: 15 Idx: 0 Loss: 0.059368646484211966
Epoch: 15 Idx: 5000 Loss: 0.038545067525546754
Epoch: 16 Idx: 0 Loss: 0.01063659459958606
Epoch: 16 Idx: 5000 Loss: 0.01934293735089246
Epoch: 17 Idx: 0 Loss: 0.01887633337783994
Epoch: 17 Idx: 5000 Loss: 0.011426513596795859
Epoch: 18 Idx: 0 Loss: 0.02710238661188307
Epoch: 18 Idx: 5000 Loss: 0.020785010955125815
Epoch: 19 Idx: 0 Loss: 0.024524509461401337
Epoch: 19 Idx: 5000 Loss: 0.02754483696784364
Epoch: 20 Idx: 0 Loss: 0.01541892124007079
Epoch: 20 Idx: 5000 Loss: 0.027368938175788168
Epoch: 21 Idx: 0 Loss: 0.038341869603425074
Epoch: 21 Idx: 5000 Loss: 0.010128123534152735
Epoch: 22 Idx: 0 Loss: 0.021856876667725938
Epoch: 22 Idx: 5000 Loss: 0.021485389218177796
Epoch: 23 Idx: 0 Loss: 0.020916497757847652
Epoch: 23 Idx: 5000 Loss: 0.011549776134536666
Epoch: 24 Idx: 0 Loss: 0.025265215019685844
Epoch: 24 Idx: 5000 Loss: 0.013149216924121942
Epoch: 25 Idx: 0 Loss: 0.006754399729706215
Epoch: 25 Idx: 5000 Loss: 0.028683619132004422
Epoch: 26 Idx: 0 Loss: 0.024450189234598477
Epoch: 26 Idx: 5000 Loss: 0.028751430218629213
Epoch: 27 Idx: 0 Loss: 0.04026120259495834
Epoch: 27 Idx: 5000 Loss: 0.013818846357988732
Epoch: 28 Idx: 0 Loss: 0.02326495748383872
Epoch: 28 Idx: 5000 Loss: 0.01747600258500783
Epoch: 29 Idx: 0 Loss: 0.026735624983697442
Epoch: 29 Idx: 5000 Loss: 0.012081328818084373
Epoch: 30 Idx: 0 Loss: 0.02314304074729228
Epoch: 30 Idx: 5000 Loss: 0.04203474930195938
Epoch: 31 Idx: 0 Loss: 0.019579752940830277
Epoch: 31 Idx: 5000 Loss: 0.028287479376267816
Epoch: 32 Idx: 0 Loss: 0.02881133025340437
Epoch: 32 Idx: 5000 Loss: 0.026992527040306406
Epoch: 33 Idx: 0 Loss: 0.018150018679840743
Epoch: 33 Idx: 5000 Loss: 0.0271209778631937
Epoch: 34 Idx: 0 Loss: 0.03234083265763442
Epoch: 34 Idx: 5000 Loss: 0.023264567842887236
Epoch: 35 Idx: 0 Loss: 0.02316749158989455
Epoch: 35 Idx: 5000 Loss: 0.026026828194054524
Epoch: 36 Idx: 0 Loss: 0.022732712399742602
Epoch: 36 Idx: 5000 Loss: 0.019504234982861822
Epoch: 37 Idx: 0 Loss: 0.026958846057348787
Epoch: 37 Idx: 5000 Loss: 0.022963426731170064
Epoch: 38 Idx: 0 Loss: 0.02024693835645174
Epoch: 38 Idx: 5000 Loss: 0.02913577687435486
Epoch: 39 Idx: 0 Loss: 0.015475670142841062
Epoch: 39 Idx: 5000 Loss: 0.018559274607533045
Epoch: 40 Idx: 0 Loss: 0.023727162339720012
Epoch: 40 Idx: 5000 Loss: 0.030533453493571214
Epoch: 41 Idx: 0 Loss: 0.013485402569830917
Epoch: 41 Idx: 5000 Loss: 0.017559058682559255
Epoch: 42 Idx: 0 Loss: 0.011241794613380884
Epoch: 42 Idx: 5000 Loss: 0.040643755469573654
Epoch: 43 Idx: 0 Loss: 0.012516062077236143
Epoch: 43 Idx: 5000 Loss: 0.03131659036328806
Epoch: 44 Idx: 0 Loss: 0.013070034155596802
Epoch: 44 Idx: 5000 Loss: 0.019935468639055303
Epoch: 45 Idx: 0 Loss: 0.017835414648598016
Epoch: 45 Idx: 5000 Loss: 0.007838431099169266
Epoch: 46 Idx: 0 Loss: 0.012711435322249102
Epoch: 46 Idx: 5000 Loss: 0.0150613511954392
Epoch: 47 Idx: 0 Loss: 0.02405770081104209
Epoch: 47 Idx: 5000 Loss: 0.016730484412442233
Epoch: 48 Idx: 0 Loss: 0.019418797826396328
Epoch: 48 Idx: 5000 Loss: 0.011833882331388886
Epoch: 49 Idx: 0 Loss: 0.020722964607095593
Epoch: 49 Idx: 5000 Loss: 0.036401567362981044
Len (direct inputs):  88
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.23088536280145391
Epoch: 0 Idx: 5000 Loss: 0.014139423054067507
Epoch: 1 Idx: 0 Loss: 0.02584020670226827
Epoch: 1 Idx: 5000 Loss: 0.0466440090600512
Epoch: 2 Idx: 0 Loss: 0.016431106961737715
Epoch: 2 Idx: 5000 Loss: 0.029970427005132715
Epoch: 3 Idx: 0 Loss: 0.021109409048702513
Epoch: 3 Idx: 5000 Loss: 0.012436154044531344
Epoch: 4 Idx: 0 Loss: 0.02730630756413121
Epoch: 4 Idx: 5000 Loss: 0.017871990935461764
Epoch: 5 Idx: 0 Loss: 0.035768484252836866
Epoch: 5 Idx: 5000 Loss: 0.01829354227233395
Epoch: 6 Idx: 0 Loss: 0.02325799090937202
Epoch: 6 Idx: 5000 Loss: 0.022698148924153585
Epoch: 7 Idx: 0 Loss: 0.008517341139272447
Epoch: 7 Idx: 5000 Loss: 0.018342604458516657
Epoch: 8 Idx: 0 Loss: 0.01194786261898184
Epoch: 8 Idx: 5000 Loss: 0.017600870135395578
Epoch: 9 Idx: 0 Loss: 0.03879609823753108
Epoch: 9 Idx: 5000 Loss: 0.023988435458806884
Epoch: 10 Idx: 0 Loss: 0.01926580850170325
Epoch: 10 Idx: 5000 Loss: 0.033421948113271815
Epoch: 11 Idx: 0 Loss: 0.010194949596542893
Epoch: 11 Idx: 5000 Loss: 0.024554379139818866
Epoch: 12 Idx: 0 Loss: 0.01890698022794542
Epoch: 12 Idx: 5000 Loss: 0.016991512942803814
Epoch: 13 Idx: 0 Loss: 0.012951219928873365
Epoch: 13 Idx: 5000 Loss: 0.013797324226007152
Epoch: 14 Idx: 0 Loss: 0.0246415737599597
Epoch: 14 Idx: 5000 Loss: 0.03549871406914705
Epoch: 15 Idx: 0 Loss: 0.017996305019425757
Epoch: 15 Idx: 5000 Loss: 0.03317460997565322
Epoch: 16 Idx: 0 Loss: 0.026255531891512834
Epoch: 16 Idx: 5000 Loss: 0.033006961249784667
Epoch: 17 Idx: 0 Loss: 0.01972129421632504
Epoch: 17 Idx: 5000 Loss: 0.01568822983117187
Epoch: 18 Idx: 0 Loss: 0.02849521000594331
Epoch: 18 Idx: 5000 Loss: 0.019597828070056535
Epoch: 19 Idx: 0 Loss: 0.02236757970716706
Epoch: 19 Idx: 5000 Loss: 0.007240950143002999
Epoch: 20 Idx: 0 Loss: 0.013580605463218803
Epoch: 20 Idx: 5000 Loss: 0.01767477700724887
Epoch: 21 Idx: 0 Loss: 0.017702297780315
Epoch: 21 Idx: 5000 Loss: 0.017894000051951478
Epoch: 22 Idx: 0 Loss: 0.01831272400126352
Epoch: 22 Idx: 5000 Loss: 0.016125885297777483
Epoch: 23 Idx: 0 Loss: 0.018030392237793468
Epoch: 23 Idx: 5000 Loss: 0.04462693044992343
Epoch: 24 Idx: 0 Loss: 0.00824450887172436
Epoch: 24 Idx: 5000 Loss: 0.03504743929141714
Epoch: 25 Idx: 0 Loss: 0.025496412492392338
Epoch: 25 Idx: 5000 Loss: 0.018777470761916434
Epoch: 26 Idx: 0 Loss: 0.04211388208034746
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
000 Loss: 0.020069389843572337
Epoch: 29 Idx: 0 Loss: 0.024515214232606528
Epoch: 29 Idx: 5000 Loss: 0.02684629944522333
Epoch: 30 Idx: 0 Loss: 0.011662773420340215
Epoch: 30 Idx: 5000 Loss: 0.014990750581228722
Epoch: 31 Idx: 0 Loss: 0.016965444218005724
Epoch: 31 Idx: 5000 Loss: 0.03818698404939526
Epoch: 32 Idx: 0 Loss: 0.021750173118438854
Epoch: 32 Idx: 5000 Loss: 0.018607589150632903
Epoch: 33 Idx: 0 Loss: 0.02853861116858741
Epoch: 33 Idx: 5000 Loss: 0.015670698687731546
Epoch: 34 Idx: 0 Loss: 0.04121759116955726
Epoch: 34 Idx: 5000 Loss: 0.01357261071998726
Epoch: 35 Idx: 0 Loss: 0.011401196636126635
Epoch: 35 Idx: 5000 Loss: 0.019368432316442908
Epoch: 36 Idx: 0 Loss: 0.028046712085227955
Epoch: 36 Idx: 5000 Loss: 0.024667647469774543
Epoch: 37 Idx: 0 Loss: 0.019751161587801495
Epoch: 37 Idx: 5000 Loss: 0.012373571135583506
Epoch: 38 Idx: 0 Loss: 0.019140615547543205
Epoch: 38 Idx: 5000 Loss: 0.027491691383116523
Epoch: 39 Idx: 0 Loss: 0.008387364181861508
Epoch: 39 Idx: 5000 Loss: 0.0260350635937577
Epoch: 40 Idx: 0 Loss: 0.02808450467201837
Epoch: 40 Idx: 5000 Loss: 0.02188521088462952
Epoch: 41 Idx: 0 Loss: 0.015434518047652895
Epoch: 41 Idx: 5000 Loss: 0.010792986162012727
Epoch: 42 Idx: 0 Loss: 0.027662144721274068
Epoch: 42 Idx: 5000 Loss: 0.01993802568426021
Epoch: 43 Idx: 0 Loss: 0.010707286109551586
Epoch: 43 Idx: 5000 Loss: 0.03142055995687427
Epoch: 44 Idx: 0 Loss: 0.017022224234208454
Epoch: 44 Idx: 5000 Loss: 0.012621376444460593
Epoch: 45 Idx: 0 Loss: 0.042948394070609866
Epoch: 45 Idx: 5000 Loss: 0.03457157058697163
Epoch: 46 Idx: 0 Loss: 0.0291764363626418
Epoch: 46 Idx: 5000 Loss: 0.018457760910455465
Epoch: 47 Idx: 0 Loss: 0.03056909218724317
Epoch: 47 Idx: 5000 Loss: 0.012236946169418467
Epoch: 48 Idx: 0 Loss: 0.034407375574915994
Epoch: 48 Idx: 5000 Loss: 0.017190297079036708
Epoch: 49 Idx: 0 Loss: 0.010179788323610404
Epoch: 49 Idx: 5000 Loss: 0.023701504760117946
Len (direct inputs):  110
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21099979881871067
Epoch: 0 Idx: 5000 Loss: 0.018349087603033422
Epoch: 1 Idx: 0 Loss: 0.022022275717325082
Epoch: 1 Idx: 5000 Loss: 0.015343340373828125
Epoch: 2 Idx: 0 Loss: 0.02336137549615751
Epoch: 2 Idx: 5000 Loss: 0.022834264498711596
Epoch: 3 Idx: 0 Loss: 0.02923062561326742
Epoch: 3 Idx: 5000 Loss: 0.02415851261106397
Epoch: 4 Idx: 0 Loss: 0.03463983709195812
Epoch: 4 Idx: 5000 Loss: 0.01674542916333158
Epoch: 5 Idx: 0 Loss: 0.01810714298565388
Epoch: 5 Idx: 5000 Loss: 0.01588896281266249
Epoch: 6 Idx: 0 Loss: 0.032596227921661114
Epoch: 6 Idx: 5000 Loss: 0.0546211090305507
Epoch: 7 Idx: 0 Loss: 0.020099772667653865
Epoch: 7 Idx: 5000 Loss: 0.009915399142749966
Epoch: 8 Idx: 0 Loss: 0.02481891573484938
Epoch: 8 Idx: 5000 Loss: 0.029174807994574146
Epoch: 9 Idx: 0 Loss: 0.029391101515537005
Epoch: 9 Idx: 5000 Loss: 0.013276311631768328
Epoch: 10 Idx: 0 Loss: 0.014211474312014746
Epoch: 10 Idx: 5000 Loss: 0.026115834024136322
Epoch: 11 Idx: 0 Loss: 0.016141804372233788
Epoch: 11 Idx: 5000 Loss: 0.043519037404125174
Epoch: 12 Idx: 0 Loss: 0.021185585362660547
Epoch: 12 Idx: 5000 Loss: 0.03518462203422494
Epoch: 13 Idx: 0 Loss: 0.01656304570546343
Epoch: 13 Idx: 5000 Loss: 0.023939126394257916
Epoch: 14 Idx: 0 Loss: 0.017175613300187825
Epoch: 14 Idx: 5000 Loss: 0.022472992093776963
Epoch: 15 Idx: 0 Loss: 0.035002652850336005
Epoch: 15 Idx: 5000 Loss: 0.016646584897901513
Epoch: 16 Idx: 0 Loss: 0.02475249185338929
Epoch: 16 Idx: 5000 Loss: 0.038879955438864605
Epoch: 17 Idx: 0 Loss: 0.03532370874223706
Epoch: 17 Idx: 5000 Loss: 0.007119406133302605
Epoch: 18 Idx: 0 Loss: 0.030558554707569033
Epoch: 18 Idx: 5000 Loss: 0.019117538540182822
Epoch: 19 Idx: 0 Loss: 0.032240023945466975
Epoch: 19 Idx: 5000 Loss: 0.006894820320481345
Epoch: 20 Idx: 0 Loss: 0.019222207636383038
Epoch: 20 Idx: 5000 Loss: 0.03118122216413415
Epoch: 21 Idx: 0 Loss: 0.017855869845504788
Epoch: 21 Idx: 5000 Loss: 0.021912217594947853
Epoch: 22 Idx: 0 Loss: 0.026461192109031934
Epoch: 22 Idx: 5000 Loss: 0.02028552702621712
Epoch: 23 Idx: 0 Loss: 0.019342232269855708
Epoch: 23 Idx: 5000 Loss: 0.017649895624377653
Epoch: 24 Idx: 0 Loss: 0.024275827862599568
Epoch: 24 Idx: 5000 Loss: 0.017212203840934054
Epoch: 25 Idx: 0 Loss: 0.021075320970415867
Epoch: 25 Idx: 5000 Loss: 0.014044814914531302
Epoch: 26 Idx: 0 Loss: 0.03668846818252351
Epoch: 26 Idx: 5000 Loss: 0.017127680031757186
Epoch: 27 Idx: 0 Loss: 0.008135539884823503
Epoch: 27 Idx: 5000 Loss: 0.021372789154056108
Epoch: 28 Idx: 0 Loss: 0.024237542047362007
Epoch: 28 Idx: 5000 Loss: 0.013894759626645844
Epoch: 29 Idx: 0 Loss: 0.020214326257061592
Epoch: 29 Idx: 5000 Loss: 0.026595571065994254
Epoch: 30 Idx: 0 Loss: 0.02115174327744296
Epoch: 30 Idx: 5000 Loss: 0.016233572621050435
Epoch: 31 Idx: 0 Loss: 0.021280557808047905
Epoch: 31 Idx: 5000 Loss: 0.0246677497363711
Epoch: 32 Idx: 0 Loss: 0.01915479403378595
Epoch: 32 Idx: 5000 Loss: 0.011140520641711954
Epoch: 33 Idx: 0 Loss: 0.01491505871928964
Epoch: 33 Idx: 5000 Loss: 0.03802544123536451
Epoch: 34 Idx: 0 Loss: 0.007848811803052143
Epoch: 34 Idx: 5000 Loss: 0.029356156938492063
Epoch: 35 Idx: 0 Loss: 0.01682513698033149
Epoch: 35 Idx: 5000 Loss: 0.007154479211495778
Epoch: 36 Idx: 0 Loss: 0.04691647103548423
Epoch: 36 Idx: 5000 Loss: 0.020256244656310898
Epoch: 37 Idx: 0 Loss: 0.016724470287173727
Epoch: 37 Idx: 5000 Loss: 0.012095961756744742
Epoch: 38 Idx: 0 Loss: 0.018252350962358548
Epoch: 38 Idx: 5000 Loss: 0.00971595888349661
Epoch: 39 Idx: 0 Loss: 0.017948910650939093
Epoch: 39 Idx: 5000 Loss: 0.023985904821515542
Epoch: 40 Idx: 0 Loss: 0.024399082996854306
Epoch: 40 Idx: 5000 Loss: 0.030669859933593068
Epoch: 41 Idx: 0 Loss: 0.011768711494763197
Epoch: 41 Idx: 5000 Loss: 0.016174191151964855
Epoch: 42 Idx: 0 Loss: 0.040560670831253774
Epoch: 42 Idx: 5000 Loss: 0.024648610727685455
Epoch: 43 Idx: 0 Loss: 0.003683215621162594
Epoch: 43 Idx: 5000 Loss: 0.030104389315975456
Epoch: 44 Idx: 0 Loss: 0.020470176665123043
Epoch: 44 Idx: 5000 Loss: 0.020627013836327146
Epoch: 45 Idx: 0 Loss: 0.019554476839255916
Epoch: 45 Idx: 5000 Loss: 0.014898749617590374
Epoch: 46 Idx: 0 Loss: 0.02347032017830715
Epoch: 46 Idx: 5000 Loss: 0.01750829334264227
Epoch: 47 Idx: 0 Loss: 0.006865484921497092
Epoch: 47 Idx: 5000 Loss: 0.016295592364788735
Epoch: 48 Idx: 0 Loss: 0.013023544517190845
Epoch: 48 Idx: 5000 Loss: 0.04201328777778144
Epoch: 49 Idx: 0 Loss: 0.027570437598852297
Epoch: 49 Idx: 5000 Loss: 0.01720443331796877
Len (direct inputs):  110
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2502797417264023
Epoch: 0 Idx: 5000 Loss: 0.03257372225286861
Epoch: 1 Idx: 0 Loss: 0.017475144321487196
Epoch: 1 Idx: 5000 Loss: 0.013437341384658155
Epoch: 2 Idx: 0 Loss: 0.02269375712685423
Epoch: 2 Idx: 5000 Loss: 0.02232685407006195
Epoch: 3 Idx: 0 Loss: 0.02237033615919653
Epoch: 3 Idx: 5000 Loss: 0.02373862215907505
Epoch: 4 Idx: 0 Loss: 0.014323897220384074
Epoch: 4 Idx: 5000 Loss: 0.017925715322818836
Epoch: 5 Idx: 0 Loss: 0.028194710775654667
Epoch: 5 Idx: 5000 Loss: 0.010566917127639873
Epoch: 6 Idx: 0 Loss: 0.02637942004112474
Epoch: 6 Idx: 5000 Loss: 0.032611719204006705
Epoch: 7 Idx: 0 Loss: 0.011854488005585372
Epoch: 7 Idx: 5000 Loss: 0.020572312283888165
Epoch: 8 Idx: 0 Loss: 0.03188740893926959
Epoch: 8 Idx: 5000 Loss: 0.02125160559154095
Epoch: 9 Idx: 0 Loss: 0.01923082111778561
Epoch: 9 Idx: 5000 Loss: 0.01115045569089593
Epoch: 10 Idx: 0 Loss: 0.021608955515352652
Epoch: 10 Idx: 5000 Loss: 0.030675532458681036
Epoch: 11 Idx: 0 Loss: 0.015452604112150894
Epoch: 11 Idx: 5000 Loss: 0.029127893214580022
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml.py", line 314, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 314, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 313, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml.py", line 312, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml.py", line 311, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
Epoch: 21 Idx: 0 Loss: 0.024294854603290714
Epoch: 21 Idx: 5000 Loss: 0.027309115677913495
Epoch: 22 Idx: 0 Loss: 0.027040617968271615
Epoch: 22 Idx: 5000 Loss: 0.015884904852539912
Epoch: 23 Idx: 0 Loss: 0.030549149318502705
Epoch: 23 Idx: 5000 Loss: 0.02927737004714466
Epoch: 24 Idx: 0 Loss: 0.01799995843844979
Epoch: 24 Idx: 5000 Loss: 0.020844537695635094
Epoch: 25 Idx: 0 Loss: 0.014954466765347885
Epoch: 25 Idx: 5000 Loss: 0.016768604156194425
Epoch: 26 Idx: 0 Loss: 0.014088997597189169
Epoch: 26 Idx: 5000 Loss: 0.018342196591182032
Epoch: 27 Idx: 0 Loss: 0.012198143719131752
Epoch: 27 Idx: 5000 Loss: 0.02252382294497668
Epoch: 28 Idx: 0 Loss: 0.034833742699217046
Epoch: 28 Idx: 5000 Loss: 0.010781618028162392
Epoch: 29 Idx: 0 Loss: 0.03363115121893552
Epoch: 29 Idx: 5000 Loss: 0.01303529088509008
Epoch: 30 Idx: 0 Loss: 0.01597923562307195
Epoch: 30 Idx: 5000 Loss: 0.0266314388786249
Epoch: 31 Idx: 0 Loss: 0.015338543196937072
Epoch: 31 Idx: 5000 Loss: 0.024955100224959344
Epoch: 32 Idx: 0 Loss: 0.016708095953799487
Epoch: 32 Idx: 5000 Loss: 0.016088411903049094
Epoch: 33 Idx: 0 Loss: 0.014455666105996662
Epoch: 33 Idx: 5000 Loss: 0.012979404202849477
Epoch: 34 Idx: 0 Loss: 0.025399628108482654
Epoch: 34 Idx: 5000 Loss: 0.017706020595021528
Epoch: 35 Idx: 0 Loss: 0.020231264121466995
Epoch: 35 Idx: 5000 Loss: 0.012531440629755633
Epoch: 36 Idx: 0 Loss: 0.013324010837257186
Epoch: 36 Idx: 5000 Loss: 0.013801937864677932
Epoch: 37 Idx: 0 Loss: 0.02350418517812873
Epoch: 37 Idx: 5000 Loss: 0.018080845804134462
Epoch: 38 Idx: 0 Loss: 0.015242061063177843
Epoch: 38 Idx: 5000 Loss: 0.033275803605262264
Epoch: 39 Idx: 0 Loss: 0.01815474094342972
Epoch: 39 Idx: 5000 Loss: 0.035520748522956246
Epoch: 40 Idx: 0 Loss: 0.022196813743387837
Epoch: 40 Idx: 5000 Loss: 0.011976230607279124
Epoch: 41 Idx: 0 Loss: 0.030324321438331393
Epoch: 41 Idx: 5000 Loss: 0.035428538060513
Epoch: 42 Idx: 0 Loss: 0.016007160981701907
Epoch: 42 Idx: 5000 Loss: 0.011826694853563866
Epoch: 43 Idx: 0 Loss: 0.022310455935866245
Epoch: 43 Idx: 5000 Loss: 0.013566632083298363
Epoch: 44 Idx: 0 Loss: 0.016797147792883355
Epoch: 44 Idx: 5000 Loss: 0.026288693204530478
Epoch: 45 Idx: 0 Loss: 0.026125737614923226
Epoch: 45 Idx: 5000 Loss: 0.014449740852210695
Epoch: 46 Idx: 0 Loss: 0.01541689678110103
Epoch: 46 Idx: 5000 Loss: 0.032361338794241926
Epoch: 47 Idx: 0 Loss: 0.02292118171661804
Epoch: 47 Idx: 5000 Loss: 0.029468547016655862
Epoch: 48 Idx: 0 Loss: 0.025604775531378486
Epoch: 48 Idx: 5000 Loss: 0.03196450418659123
Epoch: 49 Idx: 0 Loss: 0.016230393330543214
Epoch: 49 Idx: 5000 Loss: 0.031102841612664675
Len (direct inputs):  86
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18348906979321555
Epoch: 0 Idx: 5000 Loss: 0.02198338469138934
Epoch: 1 Idx: 0 Loss: 0.04482481108889294
Epoch: 1 Idx: 5000 Loss: 0.015709972613494792
Epoch: 2 Idx: 0 Loss: 0.018766111741990277
Epoch: 2 Idx: 5000 Loss: 0.031939418645916866
Epoch: 3 Idx: 0 Loss: 0.010111161472683806
Epoch: 3 Idx: 5000 Loss: 0.026050955984219602
Epoch: 4 Idx: 0 Loss: 0.010184310880736257
Epoch: 4 Idx: 5000 Loss: 0.013398899416078535
Epoch: 5 Idx: 0 Loss: 0.013349939252706836
Epoch: 5 Idx: 5000 Loss: 0.017486997397578752
Epoch: 6 Idx: 0 Loss: 0.05108984588431025
Epoch: 6 Idx: 5000 Loss: 0.00995345164363105
Epoch: 7 Idx: 0 Loss: 0.0112843449553046
Epoch: 7 Idx: 5000 Loss: 0.023520873493021996
Epoch: 8 Idx: 0 Loss: 0.01138491827605204
Epoch: 8 Idx: 5000 Loss: 0.016300654511628602
Epoch: 9 Idx: 0 Loss: 0.03247640715489271
Epoch: 9 Idx: 5000 Loss: 0.02482150016701076
Epoch: 10 Idx: 0 Loss: 0.014210120364138677
Epoch: 10 Idx: 5000 Loss: 0.027072680605691447
Epoch: 11 Idx: 0 Loss: 0.03169217218956061
Epoch: 11 Idx: 5000 Loss: 0.02689433463691382
Epoch: 12 Idx: 0 Loss: 0.031344832780413036
Epoch: 12 Idx: 5000 Loss: 0.03995731195654996
Epoch: 13 Idx: 0 Loss: 0.01090735044703455
Epoch: 13 Idx: 5000 Loss: 0.06464508155773269
Epoch: 14 Idx: 0 Loss: 0.031670340367088254
Epoch: 14 Idx: 5000 Loss: 0.031149766710218206
Epoch: 15 Idx: 0 Loss: 0.011406394445536885
Epoch: 15 Idx: 5000 Loss: 0.026701175647568538
Epoch: 16 Idx: 0 Loss: 0.033749835334894285
Epoch: 16 Idx: 5000 Loss: 0.01927296257707338
Epoch: 17 Idx: 0 Loss: 0.023376979287038295
Epoch: 17 Idx: 5000 Loss: 0.015052667208248412
Epoch: 18 Idx: 0 Loss: 0.0268087141681045
Epoch: 18 Idx: 5000 Loss: 0.021878424003537914
Epoch: 19 Idx: 0 Loss: 0.03179914648720563
Epoch: 19 Idx: 5000 Loss: 0.015500031064347754
Epoch: 20 Idx: 0 Loss: 0.009327563697880039
Epoch: 20 Idx: 5000 Loss: 0.016777244596818036
Epoch: 21 Idx: 0 Loss: 0.035765502806944564
Epoch: 21 Idx: 5000 Loss: 0.03631342456094837
Epoch: 22 Idx: 0 Loss: 0.01875354952220234
Epoch: 22 Idx: 5000 Loss: 0.021325922785767795
Epoch: 23 Idx: 0 Loss: 0.02758257694607295
Epoch: 23 Idx: 5000 Loss: 0.01837900446948017
Epoch: 24 Idx: 0 Loss: 0.018072152431403678
Epoch: 24 Idx: 5000 Loss: 0.014427387612398639
Epoch: 25 Idx: 0 Loss: 0.021867508869571117
Epoch: 25 Idx: 5000 Loss: 0.011374845900737986
Epoch: 26 Idx: 0 Loss: 0.04084620046805754
Epoch: 26 Idx: 5000 Loss: 0.029839100880878863
Epoch: 27 Idx: 0 Loss: 0.031681113366224756
Epoch: 27 Idx: 5000 Loss: 0.01864788823196662
Epoch: 28 Idx: 0 Loss: 0.016550618830570477
Epoch: 28 Idx: 5000 Loss: 0.015278873653667953
Epoch: 29 Idx: 0 Loss: 0.03574454432560938
Epoch: 29 Idx: 5000 Loss: 0.01606680545883729
Epoch: 30 Idx: 0 Loss: 0.02272671321958859
Epoch: 30 Idx: 5000 Loss: 0.0187568807824144
Epoch: 31 Idx: 0 Loss: 0.011771480741322395
Epoch: 31 Idx: 5000 Loss: 0.02023837592474715
Epoch: 32 Idx: 0 Loss: 0.025139147869191785
Epoch: 32 Idx: 5000 Loss: 0.027948184062393036
Epoch: 33 Idx: 0 Loss: 0.027735424015463056
Epoch: 33 Idx: 5000 Loss: 0.03409880583652809
Epoch: 34 Idx: 0 Loss: 0.013888677214105227
Epoch: 34 Idx: 5000 Loss: 0.021879829783447245
Epoch: 35 Idx: 0 Loss: 0.020227548483365027
Epoch: 35 Idx: 5000 Loss: 0.011732754226960924
Epoch: 36 Idx: 0 Loss: 0.013215502675370566
Epoch: 36 Idx: 5000 Loss: 0.02157547493713015
Epoch: 37 Idx: 0 Loss: 0.023889940109209928
Epoch: 37 Idx: 5000 Loss: 0.027032485938778047
Epoch: 38 Idx: 0 Loss: 0.013089147397639913
Epoch: 38 Idx: 5000 Loss: 0.02070802649856355
Epoch: 39 Idx: 0 Loss: 0.02125166222019037
Epoch: 39 Idx: 5000 Loss: 0.015175596320568445
Epoch: 40 Idx: 0 Loss: 0.01728067468633248
Epoch: 40 Idx: 5000 Loss: 0.013340909523170442
Epoch: 41 Idx: 0 Loss: 0.017089445526668065
Epoch: 41 Idx: 5000 Loss: 0.012360332328238244
Epoch: 42 Idx: 0 Loss: 0.014302703754232406
Epoch: 42 Idx: 5000 Loss: 0.01676165347883428
Epoch: 43 Idx: 0 Loss: 0.02067251921231153
Epoch: 43 Idx: 5000 Loss: 0.0187111710591183
Epoch: 44 Idx: 0 Loss: 0.008245345476338983
Epoch: 44 Idx: 5000 Loss: 0.02448985419557915
Epoch: 45 Idx: 0 Loss: 0.026521224112630366
Epoch: 45 Idx: 5000 Loss: 0.019871765455407905
Epoch: 46 Idx: 0 Loss: 0.022285579432063923
Epoch: 46 Idx: 5000 Loss: 0.016969964512728898
Epoch: 47 Idx: 0 Loss: 0.03650335842853886
Epoch: 47 Idx: 5000 Loss: 0.03073065595935512
Epoch: 48 Idx: 0 Loss: 0.022660173677625055
Epoch: 48 Idx: 5000 Loss: 0.038386746872327196
Epoch: 49 Idx: 0 Loss: 0.018096264476484113
Epoch: 49 Idx: 5000 Loss: 0.023483965467232498
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
0.808773122792671
Parameter containing:
tensor([0.8088], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.21525529059217227
Epoch: 0 Idx: 5000 Loss: 0.027525945431151225
Epoch: 1 Idx: 0 Loss: 0.02600838728246204
Epoch: 1 Idx: 5000 Loss: 0.015190956688715743
Epoch: 2 Idx: 0 Loss: 0.021441528652314572
Epoch: 2 Idx: 5000 Loss: 0.014029966320184467
Epoch: 3 Idx: 0 Loss: 0.02477506352882987
Epoch: 3 Idx: 5000 Loss: 0.031098152277096357
Epoch: 4 Idx: 0 Loss: 0.025726303374339954
Epoch: 4 Idx: 5000 Loss: 0.028417641848656222
Epoch: 5 Idx: 0 Loss: 0.012823513278658546
Epoch: 5 Idx: 5000 Loss: 0.025160617936990697
Epoch: 6 Idx: 0 Loss: 0.01821880468887567
Epoch: 6 Idx: 5000 Loss: 0.03202055630161424
Epoch: 7 Idx: 0 Loss: 0.0273836167091122
Epoch: 7 Idx: 5000 Loss: 0.04051507395565536
Epoch: 8 Idx: 0 Loss: 0.020183932692986906
Epoch: 8 Idx: 5000 Loss: 0.01847089177892633
Epoch: 9 Idx: 0 Loss: 0.024802479865522926
Epoch: 9 Idx: 5000 Loss: 0.01421933356551527
Epoch: 10 Idx: 0 Loss: 0.0186671655524665
Epoch: 10 Idx: 5000 Loss: 0.020247777620059276
Epoch: 11 Idx: 0 Loss: 0.01990590930278744
Epoch: 11 Idx: 5000 Loss: 0.030783184028851662
Epoch: 12 Idx: 0 Loss: 0.025144659341989922
Epoch: 12 Idx: 5000 Loss: 0.034033365150944656
Epoch: 13 Idx: 0 Loss: 0.03882360202560882
Epoch: 13 Idx: 5000 Loss: 0.03388752429420637
Epoch: 14 Idx: 0 Loss: 0.013719014310560931
Epoch: 14 Idx: 5000 Loss: 0.013894302369748274
Epoch: 15 Idx: 0 Loss: 0.032398908343895444
Epoch: 15 Idx: 5000 Loss: 0.029206649245583235
Epoch: 16 Idx: 0 Loss: 0.017653509305461088
Epoch: 16 Idx: 5000 Loss: 0.021559847067485375
Epoch: 17 Idx: 0 Loss: 0.02264107663287529
Epoch: 17 Idx: 5000 Loss: 0.025169559225198535
Epoch: 18 Idx: 0 Loss: 0.01216358241670922
Epoch: 18 Idx: 5000 Loss: 0.014000630417468518
Epoch: 19 Idx: 0 Loss: 0.023070587422955518
Epoch: 19 Idx: 5000 Loss: 0.010564294185146962
Epoch: 20 Idx: 0 Loss: 0.02140275551212678
Epoch: 20 Idx: 5000 Loss: 0.02514876922225865
Epoch: 21 Idx: 0 Loss: 0.009683808249726564
Epoch: 21 Idx: 5000 Loss: 0.01549199347601687
Epoch: 22 Idx: 0 Loss: 0.012081857111183637
Epoch: 22 Idx: 5000 Loss: 0.03234924862201081
Epoch: 23 Idx: 0 Loss: 0.019741056692455607
Epoch: 23 Idx: 5000 Loss: 0.012131294995004711
Epoch: 24 Idx: 0 Loss: 0.022222299442397467
Epoch: 24 Idx: 5000 Loss: 0.02915164654673398
Epoch: 25 Idx: 0 Loss: 0.029776898019544928
Epoch: 25 Idx: 5000 Loss: 0.01552472324943526
Epoch: 26 Idx: 0 Loss: 0.019896196919979632
Epoch: 26 Idx: 5000 Loss: 0.023905798653962397
Epoch: 27 Idx: 0 Loss: 0.018193623295635074
Epoch: 27 Idx: 5000 Loss: 0.03998371863458758
Epoch: 28 Idx: 0 Loss: 0.011224449388766017
Epoch: 28 Idx: 5000 Loss: 0.03194285221829586
Epoch: 29 Idx: 0 Loss: 0.015823873460419573
Epoch: 29 Idx: 5000 Loss: 0.015869874867200782
Epoch: 30 Idx: 0 Loss: 0.023219524050700463
Epoch: 30 Idx: 5000 Loss: 0.012524718677668094
Epoch: 31 Idx: 0 Loss: 0.02221177489625458
Epoch: 31 Idx: 5000 Loss: 0.01815596293553568
Epoch: 32 Idx: 0 Loss: 0.012290413779240141
Epoch: 32 Idx: 5000 Loss: 0.026387704243309155
Epoch: 33 Idx: 0 Loss: 0.016205454534395577
Epoch: 33 Idx: 5000 Loss: 0.041541368465516385
Epoch: 34 Idx: 0 Loss: 0.022901106341960423
Epoch: 34 Idx: 5000 Loss: 0.021097489713106134
Epoch: 35 Idx: 0 Loss: 0.029666892240153918
Epoch: 35 Idx: 5000 Loss: 0.01259062796257651
Epoch: 36 Idx: 0 Loss: 0.017548755428760124
Epoch: 36 Idx: 5000 Loss: 0.038412748799115076
Epoch: 37 Idx: 0 Loss: 0.017254493162828954
Epoch: 37 Idx: 5000 Loss: 0.015215401031758582
Epoch: 38 Idx: 0 Loss: 0.01645206005661286
Epoch: 38 Idx: 5000 Loss: 0.01118546308200187
Epoch: 39 Idx: 0 Loss: 0.014203639671641694
Epoch: 39 Idx: 5000 Loss: 0.02670919568523986
Epoch: 40 Idx: 0 Loss: 0.027839976018552606
Epoch: 40 Idx: 5000 Loss: 0.029795987968147107
Epoch: 41 Idx: 0 Loss: 0.03240299629509255
Epoch: 41 Idx: 5000 Loss: 0.023814104755046667
Epoch: 42 Idx: 0 Loss: 0.03849083313579073
Epoch: 42 Idx: 5000 Loss: 0.024669300906394076
Epoch: 43 Idx: 0 Loss: 0.017197909050385395
Epoch: 43 Idx: 5000 Loss: 0.02668473541198043
Epoch: 44 Idx: 0 Loss: 0.025517910943639966
Epoch: 44 Idx: 5000 Loss: 0.026353231051114416
Epoch: 45 Idx: 0 Loss: 0.018452763791196765
Epoch: 45 Idx: 5000 Loss: 0.024336746560441838
Epoch: 46 Idx: 0 Loss: 0.03398199889648233
Epoch: 46 Idx: 5000 Loss: 0.01818277392680745
Epoch: 47 Idx: 0 Loss: 0.01543842553929142
Epoch: 47 Idx: 5000 Loss: 0.02570857612600334
Epoch: 48 Idx: 0 Loss: 0.024356147487643667
Epoch: 48 Idx: 5000 Loss: 0.015209101259659092
Epoch: 49 Idx: 0 Loss: 0.032875605771168714
Epoch: 49 Idx: 5000 Loss: 0.027664080634269554
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 475, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
ded
eded
 Disk quota exceeded
ed as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 19:19:34 2020
Results reported at Tue Sep  1 19:19:34 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 2 Output/test_anatomy_aml_bagofnbrs1_2.pkl Models/anatomy_aml_bagofnbrs1_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   19538.83 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2718.12 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   19549 sec.
    Turnaround time :                            19550 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3289795: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs2_2.pkl Models/anatomy_aml_bagofnbrs2_2.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs2_2.pkl Models/anatomy_aml_bagofnbrs2_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 19:36:18 2020
Results reported at Tue Sep  1 19:36:18 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs2_2.pkl Models/anatomy_aml_bagofnbrs2_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   20547.37 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2719.02 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   20569 sec.
    Turnaround time :                            20554 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc271>
Subject: Job 3289797: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs3_2.pkl Models/anatomy_aml_bagofnbrs3_2.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs3_2.pkl Models/anatomy_aml_bagofnbrs3_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc271>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 19:50:01 2020
Results reported at Tue Sep  1 19:50:01 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs3_2.pkl Models/anatomy_aml_bagofnbrs3_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21337.22 sec.
    Max Memory :                                 2911 MB
    Average Memory :                             2718.52 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40506.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21383 sec.
    Turnaround time :                            21377 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc268>
Subject: Job 3289799: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs4_2.pkl Models/anatomy_aml_bagofnbrs4_2.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs4_2.pkl Models/anatomy_aml_bagofnbrs4_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc268>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:10:03 2020
Results reported at Tue Sep  1 20:10:03 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs4_2.pkl Models/anatomy_aml_bagofnbrs4_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22481.76 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2720.42 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22580 sec.
    Turnaround time :                            22579 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc274>
Subject: Job 3289801: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs5_2.pkl Models/anatomy_aml_bagofnbrs5_2.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs5_2.pkl Models/anatomy_aml_bagofnbrs5_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc274>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:19:31 2020
Results reported at Tue Sep  1 20:19:31 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs5_2.pkl Models/anatomy_aml_bagofnbrs5_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23109.71 sec.
    Max Memory :                                 2925 MB
    Average Memory :                             2723.19 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40492.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23146 sec.
    Turnaround time :                            23147 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc250>
Subject: Job 3289805: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs8_2.pkl Models/anatomy_aml_bagofnbrs8_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs8_2.pkl Models/anatomy_aml_bagofnbrs8_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc250>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:25:07 2020
Results reported at Tue Sep  1 21:25:07 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs8_2.pkl Models/anatomy_aml_bagofnbrs8_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27030.00 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2716.04 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27082 sec.
    Turnaround time :                            27083 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc214>
Subject: Job 3289803: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs7_2.pkl Models/anatomy_aml_bagofnbrs7_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs7_2.pkl Models/anatomy_aml_bagofnbrs7_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc214>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:46:05 2020
Results reported at Tue Sep  1 21:46:05 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs7_2.pkl Models/anatomy_aml_bagofnbrs7_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28118.24 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2718.92 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28340 sec.
    Turnaround time :                            28341 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc237>
Subject: Job 3289831: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 2 Output/test_anatomy_aml_bagofnbrs152_2.pkl Models/anatomy_aml_bagofnbrs152_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 2 Output/test_anatomy_aml_bagofnbrs152_2.pkl Models/anatomy_aml_bagofnbrs152_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc237>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 2 Output/test_anatomy_aml_bagofnbrs152_2.pkl Models/anatomy_aml_bagofnbrs152_2.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80671.38 sec.
    Max Memory :                                 2731 MB
    Average Memory :                             2653.69 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40686.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80701 sec.
    Turnaround time :                            80703 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc258>
Subject: Job 3289829: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 2 Output/test_anatomy_aml_bagofnbrs80_2.pkl Models/anatomy_aml_bagofnbrs80_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 2 Output/test_anatomy_aml_bagofnbrs80_2.pkl Models/anatomy_aml_bagofnbrs80_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc258>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 2 Output/test_anatomy_aml_bagofnbrs80_2.pkl Models/anatomy_aml_bagofnbrs80_2.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   78521.38 sec.
    Max Memory :                                 2736 MB
    Average Memory :                             2664.23 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40681.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80702 sec.
    Turnaround time :                            80703 sec.

The output (if any) is above this job summary.

