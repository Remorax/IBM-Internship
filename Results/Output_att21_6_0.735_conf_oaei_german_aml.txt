Max number of nodes in a path: Input/data_conf_oaei_german_aml_thresh.pkl
Number of entities: 122893
Training size: 109284 Validation size: 13609
Epoch: 0 Idx: 0 Loss: 0.1895248421752986
Epoch: 0 Idx: 5000 Loss: 0.037317346483098646
Epoch: 1 Idx: 0 Loss: 0.017058545532816164
Epoch: 1 Idx: 5000 Loss: 0.016118487564689515
Epoch: 2 Idx: 0 Loss: 0.011021764699689473
Epoch: 2 Idx: 5000 Loss: 0.016413238404797154
Epoch: 3 Idx: 0 Loss: 0.012451178646235461
Epoch: 3 Idx: 5000 Loss: 0.010219416780720848
Epoch: 4 Idx: 0 Loss: 0.011905566937181157
Epoch: 4 Idx: 5000 Loss: 0.012315968914449273
Epoch: 5 Idx: 0 Loss: 0.015621089285078014
Epoch: 5 Idx: 5000 Loss: 0.007858444739828925
Epoch: 6 Idx: 0 Loss: 0.014022136226489857
Epoch: 6 Idx: 5000 Loss: 0.0022493958905748642
Epoch: 7 Idx: 0 Loss: 0.005749614932224116
Epoch: 7 Idx: 5000 Loss: 0.008722104842809629
Epoch: 8 Idx: 0 Loss: 0.010089581306159365
Epoch: 8 Idx: 5000 Loss: 0.010217227794583917
Epoch: 9 Idx: 0 Loss: 0.020357493853270577
Epoch: 9 Idx: 5000 Loss: 0.007826219555263528
Epoch: 10 Idx: 0 Loss: 0.013091824329300013
Epoch: 10 Idx: 5000 Loss: 0.006306211503150824
Epoch: 11 Idx: 0 Loss: 0.023119029842612552
Epoch: 11 Idx: 5000 Loss: 0.013451973794267895
Epoch: 12 Idx: 0 Loss: 0.015998705760483536
Epoch: 12 Idx: 5000 Loss: 0.005587572185930123
Epoch: 13 Idx: 0 Loss: 0.006452067347278905
Epoch: 13 Idx: 5000 Loss: 0.015794882130384786
Epoch: 14 Idx: 0 Loss: 0.009071691799566253
Epoch: 14 Idx: 5000 Loss: 0.022477645306651275
Epoch: 15 Idx: 0 Loss: 0.010838123030681091
Epoch: 15 Idx: 5000 Loss: 0.012664360325884368
Epoch: 16 Idx: 0 Loss: 0.004375915302332954
Epoch: 16 Idx: 5000 Loss: 0.01318210524302894
Epoch: 17 Idx: 0 Loss: 0.01335095126110217
Epoch: 17 Idx: 5000 Loss: 0.02079900046318162
Epoch: 18 Idx: 0 Loss: 0.018304011776111226
Epoch: 18 Idx: 5000 Loss: 0.007515089477975316
Epoch: 19 Idx: 0 Loss: 0.02469581003353695
Epoch: 19 Idx: 5000 Loss: 0.01742269245401678
Epoch: 20 Idx: 0 Loss: 0.007269729820679832
Epoch: 20 Idx: 5000 Loss: 0.019504917613263385
Epoch: 21 Idx: 0 Loss: 0.012763523285987852
Epoch: 21 Idx: 5000 Loss: 0.004991172192286023
Epoch: 22 Idx: 0 Loss: 0.014373119727309275
Epoch: 22 Idx: 5000 Loss: 0.011169537349576271
Epoch: 23 Idx: 0 Loss: 0.004895176825105044
Epoch: 23 Idx: 5000 Loss: 0.014828456532264193
Epoch: 24 Idx: 0 Loss: 0.0069081093495849625
Epoch: 24 Idx: 5000 Loss: 0.00845544302639381
Epoch: 25 Idx: 0 Loss: 0.009307329711137425
Epoch: 25 Idx: 5000 Loss: 0.007598089031077568
Epoch: 26 Idx: 0 Loss: 0.007296771663292492
Epoch: 26 Idx: 5000 Loss: 0.004655456336278221
Epoch: 27 Idx: 0 Loss: 0.010823390755824442
Epoch: 27 Idx: 5000 Loss: 0.011184694522655875
Epoch: 28 Idx: 0 Loss: 0.017620218483215905
Epoch: 28 Idx: 5000 Loss: 0.01575721420696627
Epoch: 29 Idx: 0 Loss: 0.012466633214503656
Epoch: 29 Idx: 5000 Loss: 0.01387632303905791
Epoch: 30 Idx: 0 Loss: 0.01012125576632092
Epoch: 30 Idx: 5000 Loss: 0.014889316469958124
Epoch: 31 Idx: 0 Loss: 0.03082340162094763
Epoch: 31 Idx: 5000 Loss: 0.007234071295949454
Epoch: 32 Idx: 0 Loss: 0.028776256880249325
Epoch: 32 Idx: 5000 Loss: 0.03328376551276966
Epoch: 33 Idx: 0 Loss: 0.035240519408828656
Epoch: 33 Idx: 5000 Loss: 0.013317833483299754
Epoch: 34 Idx: 0 Loss: 0.008259302857913382
Epoch: 34 Idx: 5000 Loss: 0.004201605270331256
Epoch: 35 Idx: 0 Loss: 0.006019981432967824
Epoch: 35 Idx: 5000 Loss: 0.0080849269395936
Epoch: 36 Idx: 0 Loss: 0.01163358162245514
Epoch: 36 Idx: 5000 Loss: 0.011349554911639703
Epoch: 37 Idx: 0 Loss: 0.00942959205127703
Epoch: 37 Idx: 5000 Loss: 0.010997466517261863
Epoch: 38 Idx: 0 Loss: 0.014429211293572998
Epoch: 38 Idx: 5000 Loss: 0.018867630439218446
Epoch: 39 Idx: 0 Loss: 0.008883742253283231
Epoch: 39 Idx: 5000 Loss: 0.01165515232875695
Epoch: 40 Idx: 0 Loss: 0.010019985679014295
Epoch: 40 Idx: 5000 Loss: 0.027180598085889584
Epoch: 41 Idx: 0 Loss: 0.03453811235574509
Epoch: 41 Idx: 5000 Loss: 0.009079010475299916
Epoch: 42 Idx: 0 Loss: 0.01404253859746171
Epoch: 42 Idx: 5000 Loss: 0.026815042409701784
Epoch: 43 Idx: 0 Loss: 0.012978990469545646
Epoch: 43 Idx: 5000 Loss: 0.02136464668428841
Epoch: 44 Idx: 0 Loss: 0.015177817697142004
Epoch: 44 Idx: 5000 Loss: 0.013940765644891351
Epoch: 45 Idx: 0 Loss: 0.014694698259120191
Epoch: 45 Idx: 5000 Loss: 0.01747524351871621
Epoch: 46 Idx: 0 Loss: 0.014201839737411528
Epoch: 46 Idx: 5000 Loss: 0.00854685837603249
Epoch: 47 Idx: 0 Loss: 0.011796185417887009
Epoch: 47 Idx: 5000 Loss: 0.007118645519324679
Epoch: 48 Idx: 0 Loss: 0.019142369799860715
Epoch: 48 Idx: 5000 Loss: 0.016213263079724805
Epoch: 49 Idx: 0 Loss: 0.01735279820476161
Epoch: 49 Idx: 5000 Loss: 0.01591900383796334
Len (direct inputs):  2941
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 106829 Validation size: 16064
Epoch: 0 Idx: 0 Loss: 0.2032932725598125
Epoch: 0 Idx: 5000 Loss: 0.03279369614311685
Epoch: 1 Idx: 0 Loss: 0.014571401947632763
Epoch: 1 Idx: 5000 Loss: 0.0070280994152990784
Epoch: 2 Idx: 0 Loss: 0.03697488242158799
Epoch: 2 Idx: 5000 Loss: 0.004757467470196189
Epoch: 3 Idx: 0 Loss: 0.01383519700246502
Epoch: 3 Idx: 5000 Loss: 0.010039098512401028
Epoch: 4 Idx: 0 Loss: 0.007208700147805242
Epoch: 4 Idx: 5000 Loss: 0.010371533627004428
Epoch: 5 Idx: 0 Loss: 0.03170663403972251
Epoch: 5 Idx: 5000 Loss: 0.0037693569503725642
Epoch: 6 Idx: 0 Loss: 0.011874539970414846
Epoch: 6 Idx: 5000 Loss: 0.0113124445803447
Epoch: 7 Idx: 0 Loss: 0.00967346466265522
Epoch: 7 Idx: 5000 Loss: 0.003546706272624777
Epoch: 8 Idx: 0 Loss: 0.011354241485472654
Epoch: 8 Idx: 5000 Loss: 0.010535489652672342
Epoch: 9 Idx: 0 Loss: 0.0073100466695353115
Epoch: 9 Idx: 5000 Loss: 0.00905340948276355
Epoch: 10 Idx: 0 Loss: 0.009569554297244147
Epoch: 10 Idx: 5000 Loss: 0.006059625588959547
Epoch: 11 Idx: 0 Loss: 0.01990097787922192
Epoch: 11 Idx: 5000 Loss: 0.010292084936492309
Epoch: 12 Idx: 0 Loss: 0.032822621078757906
Epoch: 12 Idx: 5000 Loss: 0.008276888622032755
Epoch: 13 Idx: 0 Loss: 0.008963384237470059
Epoch: 13 Idx: 5000 Loss: 0.009739188743886719
Epoch: 14 Idx: 0 Loss: 0.008772618096920525
Epoch: 14 Idx: 5000 Loss: 0.010568799947789559
Epoch: 15 Idx: 0 Loss: 0.014326136166266523
Epoch: 15 Idx: 5000 Loss: 0.005514145054885782
Epoch: 16 Idx: 0 Loss: 0.010221398542927605
Epoch: 16 Idx: 5000 Loss: 0.005683027840330618
Epoch: 17 Idx: 0 Loss: 0.020964173446843558
Epoch: 17 Idx: 5000 Loss: 0.008177981701736184
Epoch: 18 Idx: 0 Loss: 0.0060875658853791065
Epoch: 18 Idx: 5000 Loss: 0.015047690947485764
Epoch: 19 Idx: 0 Loss: 0.01224131910145186
Epoch: 19 Idx: 5000 Loss: 0.013577923858601324
Epoch: 20 Idx: 0 Loss: 0.020147578590037612
Epoch: 20 Idx: 5000 Loss: 0.013775916964095136
Epoch: 21 Idx: 0 Loss: 0.008662822095966452
Epoch: 21 Idx: 5000 Loss: 0.03524770399570605
Epoch: 22 Idx: 0 Loss: 0.014076081584547515
Epoch: 22 Idx: 5000 Loss: 0.01940768978495776
Epoch: 23 Idx: 0 Loss: 0.005743012123077963
Traceback (most recent call last):
  File "Attention_german_amlconf_oaei.py", line 401, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "Attention_german_amlconf_oaei.py", line 258, in forward
    feature_emb = self.name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 66060288 bytes. Error code 12 (Cannot allocate memory)

------------------------------------------------------------
Sender: LSF System <rer@dccxc247>
Subject: Job 4053681: <python Attention_german_amlconf_oaei.py Input/data_conf_oaei_german_aml_thresh.pkl 21 6 0.735 Output/test_conf_oaei_german_aml_21_6_0.735.pkl Models/conf_oaei_german_aml_21_6_0.735.pt> in cluster <dcc> Exited

Job <python Attention_german_amlconf_oaei.py Input/data_conf_oaei_german_aml_thresh.pkl 21 6 0.735 Output/test_conf_oaei_german_aml_21_6_0.735.pkl Models/conf_oaei_german_aml_21_6_0.735.pt> was submitted from host <dccxl009> by user <vmunig10> in cluster <dcc> at Tue Sep 15 06:48:53 2020
Job was executed on host(s) <dccxc247>, in queue <x86_24h>, as user <vmunig10> in cluster <dcc> at Tue Sep 15 06:49:51 2020
</u/vmunig10> was used as the home directory.
</dccstor/cogfin/aaai2021-vitobha/IBM-Internship> was used as the working directory.
Started at Tue Sep 15 06:49:51 2020
Terminated at Tue Sep 15 16:17:25 2020
Results reported at Tue Sep 15 16:17:25 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python Attention_german_amlconf_oaei.py Input/data_conf_oaei_german_aml_thresh.pkl 21 6 0.735 Output/test_conf_oaei_german_aml_21_6_0.735.pkl Models/conf_oaei_german_aml_21_6_0.735.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   33872.64 sec.
    Max Memory :                                 1382 MB
    Average Memory :                             1165.36 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               42035.00 MB
    Max Swap :                                   934 MB
    Max Processes :                              3
    Max Threads :                                5
    Run time :                                   34080 sec.
    Turnaround time :                            34112 sec.

The output (if any) is above this job summary.

