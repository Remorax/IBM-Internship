Max number of nodes in a path: Input/data_conf_oaei_german_aml_thresh.pkl
Number of entities: 122893
Training size: 109284 Validation size: 13609
Epoch: 0 Idx: 0 Loss: 0.14472601650316225
Epoch: 0 Idx: 5000 Loss: 0.010351178545651464
Epoch: 1 Idx: 0 Loss: 0.005788809518137561
Epoch: 1 Idx: 5000 Loss: 0.01094764181866118
Epoch: 2 Idx: 0 Loss: 0.008270077447783169
Epoch: 2 Idx: 5000 Loss: 0.007344239858401186
Epoch: 3 Idx: 0 Loss: 0.009835979070230847
Epoch: 3 Idx: 5000 Loss: 0.014340375132916399
Epoch: 4 Idx: 0 Loss: 0.005094580032432616
Epoch: 4 Idx: 5000 Loss: 0.02999645830259725
Epoch: 5 Idx: 0 Loss: 0.019181452809633494
Epoch: 5 Idx: 5000 Loss: 0.012891608975720751
Epoch: 6 Idx: 0 Loss: 0.04387119485011026
Epoch: 6 Idx: 5000 Loss: 0.003709322993000455
Epoch: 7 Idx: 0 Loss: 0.020885427082647463
Epoch: 7 Idx: 5000 Loss: 0.010953987645321187
Epoch: 8 Idx: 0 Loss: 0.01074027021421494
Epoch: 8 Idx: 5000 Loss: 0.010179862733568112
Epoch: 9 Idx: 0 Loss: 0.011734625820989431
Epoch: 9 Idx: 5000 Loss: 0.014231379170014833
Epoch: 10 Idx: 0 Loss: 0.01692002535676971
Epoch: 10 Idx: 5000 Loss: 0.027431757154729
Epoch: 11 Idx: 0 Loss: 0.016747915791176987
Epoch: 11 Idx: 5000 Loss: 0.013101829802696245
Epoch: 12 Idx: 0 Loss: 0.006260040373781058
Epoch: 12 Idx: 5000 Loss: 0.019030658172184398
Epoch: 13 Idx: 0 Loss: 0.024664277389096115
Epoch: 13 Idx: 5000 Loss: 0.03792111068507758
Epoch: 14 Idx: 0 Loss: 0.011343749064729968
Epoch: 14 Idx: 5000 Loss: 0.014715473902592126
Epoch: 15 Idx: 0 Loss: 0.014939003924138692
Epoch: 15 Idx: 5000 Loss: 0.012482578314337883
Epoch: 16 Idx: 0 Loss: 0.008364496731492517
Epoch: 16 Idx: 5000 Loss: 0.012212832948631413
Epoch: 17 Idx: 0 Loss: 0.024714291511208093
Epoch: 17 Idx: 5000 Loss: 0.03498971238246172
Epoch: 18 Idx: 0 Loss: 0.005100879305407518
Epoch: 18 Idx: 5000 Loss: 0.006654480197069782
Epoch: 19 Idx: 0 Loss: 0.019332434259655647
Epoch: 19 Idx: 5000 Loss: 0.007504978422680587
Epoch: 20 Idx: 0 Loss: 0.008892809810211162
Epoch: 20 Idx: 5000 Loss: 0.018734492864513855
Epoch: 21 Idx: 0 Loss: 0.016343140480188343
Epoch: 21 Idx: 5000 Loss: 0.010423326291379497
Epoch: 22 Idx: 0 Loss: 0.01908985098708916
Epoch: 22 Idx: 5000 Loss: 0.01459094986542174
Epoch: 23 Idx: 0 Loss: 0.007047093144940638
Epoch: 23 Idx: 5000 Loss: 0.008828313827957523
Epoch: 24 Idx: 0 Loss: 0.03024295959430445
Epoch: 24 Idx: 5000 Loss: 0.011144009844761504
Epoch: 25 Idx: 0 Loss: 0.00490508653665502
Epoch: 25 Idx: 5000 Loss: 0.023356063456062337
Epoch: 26 Idx: 0 Loss: 0.013770327938566713
Epoch: 26 Idx: 5000 Loss: 0.012755563142981999
Epoch: 27 Idx: 0 Loss: 0.024983936366035568
Epoch: 27 Idx: 5000 Loss: 0.01621038606230399
Epoch: 28 Idx: 0 Loss: 0.014577145399287061
Epoch: 28 Idx: 5000 Loss: 0.019500552992925937
Epoch: 29 Idx: 0 Loss: 0.005101084599019235
Epoch: 29 Idx: 5000 Loss: 0.008720725019262684
Epoch: 30 Idx: 0 Loss: 0.005462669453291568
Epoch: 30 Idx: 5000 Loss: 0.006999811058992713
Epoch: 31 Idx: 0 Loss: 0.009830317728534968
Epoch: 31 Idx: 5000 Loss: 0.01904557313310245
Epoch: 32 Idx: 0 Loss: 0.034925840286074136
Epoch: 32 Idx: 5000 Loss: 0.011094302313354365
Epoch: 33 Idx: 0 Loss: 0.011170025838858313
Epoch: 33 Idx: 5000 Loss: 0.009465190748548348
Epoch: 34 Idx: 0 Loss: 0.01866730088401898
Epoch: 34 Idx: 5000 Loss: 0.026778873001330538
Epoch: 35 Idx: 0 Loss: 0.00839900712461782
Epoch: 35 Idx: 5000 Loss: 0.009366755459745326
Epoch: 36 Idx: 0 Loss: 0.010126104129271049
Epoch: 36 Idx: 5000 Loss: 0.019685942807955468
Epoch: 37 Idx: 0 Loss: 0.014850287503256602
Epoch: 37 Idx: 5000 Loss: 0.004094823733254813
Epoch: 38 Idx: 0 Loss: 0.030821633618922532
Epoch: 38 Idx: 5000 Loss: 0.027219829194475135
Epoch: 39 Idx: 0 Loss: 0.009842370642005514
Epoch: 39 Idx: 5000 Loss: 0.030137396454668407
Epoch: 40 Idx: 0 Loss: 0.009634235387217756
Epoch: 40 Idx: 5000 Loss: 0.018763363695807098
Epoch: 41 Idx: 0 Loss: 0.013794508635740752
Epoch: 41 Idx: 5000 Loss: 0.007996186656657828
Epoch: 42 Idx: 0 Loss: 0.01607751638847392
Epoch: 42 Idx: 5000 Loss: 0.01310597794393482
Epoch: 43 Idx: 0 Loss: 0.00796002177170891
Epoch: 43 Idx: 5000 Loss: 0.008358180233083257
Epoch: 44 Idx: 0 Loss: 0.002748828923077703
Epoch: 44 Idx: 5000 Loss: 0.006265965940300832
Epoch: 45 Idx: 0 Loss: 0.02823133077735414
Epoch: 45 Idx: 5000 Loss: 0.03771631371009566
Epoch: 46 Idx: 0 Loss: 0.020498414071180877
Epoch: 46 Idx: 5000 Loss: 0.010897666733737677
Epoch: 47 Idx: 0 Loss: 0.01158122285640634
Epoch: 47 Idx: 5000 Loss: 0.014935142025947892
Epoch: 48 Idx: 0 Loss: 0.007378642123683318
Epoch: 48 Idx: 5000 Loss: 0.0073657146939187955
Epoch: 49 Idx: 0 Loss: 0.006905479790195878
Epoch: 49 Idx: 5000 Loss: 0.020508788147002136
Len (direct inputs):  2941
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 106829 Validation size: 16064
Epoch: 0 Idx: 0 Loss: 0.17856533651457235
Epoch: 0 Idx: 5000 Loss: 0.02115564627899282
Epoch: 1 Idx: 0 Loss: 0.02032357925027456
Epoch: 1 Idx: 5000 Loss: 0.009524714280717824
Epoch: 2 Idx: 0 Loss: 0.03408283196175153
Epoch: 2 Idx: 5000 Loss: 0.031158993492336837
Epoch: 3 Idx: 0 Loss: 0.025840047004002352
Epoch: 3 Idx: 5000 Loss: 0.015493304327419211
Epoch: 4 Idx: 0 Loss: 0.011360505029995306
Epoch: 4 Idx: 5000 Loss: 0.01900199841183369
Epoch: 5 Idx: 0 Loss: 0.014315623151224513
Epoch: 5 Idx: 5000 Loss: 0.00991404070580829
Epoch: 6 Idx: 0 Loss: 0.019757048818912704
Epoch: 6 Idx: 5000 Loss: 0.011834977991172448
Epoch: 7 Idx: 0 Loss: 0.007144551730008421
Epoch: 7 Idx: 5000 Loss: 0.010273263998746762
Epoch: 8 Idx: 0 Loss: 0.027010379351047654
Epoch: 8 Idx: 5000 Loss: 0.00939005261868854
Epoch: 9 Idx: 0 Loss: 0.010484661406905477
Epoch: 9 Idx: 5000 Loss: 0.0322846710709786
Epoch: 10 Idx: 0 Loss: 0.01607271360688413
Epoch: 10 Idx: 5000 Loss: 0.016512563443423556
Epoch: 11 Idx: 0 Loss: 0.007156704276779071
Epoch: 11 Idx: 5000 Loss: 0.01772928810917801
Epoch: 12 Idx: 0 Loss: 0.010651327896636517
Epoch: 12 Idx: 5000 Loss: 0.021645861341151658
Epoch: 13 Idx: 0 Loss: 0.008338910259797456
Epoch: 13 Idx: 5000 Loss: 0.012649085680616852
Epoch: 14 Idx: 0 Loss: 0.004522142488179869
Epoch: 14 Idx: 5000 Loss: 0.023594629463430026
Epoch: 15 Idx: 0 Loss: 0.009788971065594277
Epoch: 15 Idx: 5000 Loss: 0.01029767708865091
Epoch: 16 Idx: 0 Loss: 0.005838610241812065
Epoch: 16 Idx: 5000 Loss: 0.02456096799501484
Epoch: 17 Idx: 0 Loss: 0.011678266308187205
Epoch: 17 Idx: 5000 Loss: 0.012922047203554088
Epoch: 18 Idx: 0 Loss: 0.009373524573660686
Epoch: 18 Idx: 5000 Loss: 0.010752964426042726
Epoch: 19 Idx: 0 Loss: 0.014137617498693775
Epoch: 19 Idx: 5000 Loss: 0.03515822727887359
Epoch: 20 Idx: 0 Loss: 0.028487179023108645
Epoch: 20 Idx: 5000 Loss: 0.007773656343422362
Epoch: 21 Idx: 0 Loss: 0.008067337912659213
Epoch: 21 Idx: 5000 Loss: 0.009088203814405048
Epoch: 22 Idx: 0 Loss: 0.017914872591261066
Epoch: 22 Idx: 5000 Loss: 0.012895534382773084
Epoch: 23 Idx: 0 Loss: 0.006037233127543065
Epoch: 23 Idx: 5000 Loss: 0.015258462775117231
Epoch: 24 Idx: 0 Loss: 0.01972680669741173
Epoch: 24 Idx: 5000 Loss: 0.03560604033984009
Epoch: 25 Idx: 0 Loss: 0.015338633590523452
Epoch: 25 Idx: 5000 Loss: 0.01878225483567781
Epoch: 26 Idx: 0 Loss: 0.002951247977288079
Epoch: 26 Idx: 5000 Loss: 0.013013433800554357
Epoch: 27 Idx: 0 Loss: 0.03080189977967886
Epoch: 27 Idx: 5000 Loss: 0.012627606135727672
Epoch: 28 Idx: 0 Loss: 0.005878408324615269
Epoch: 28 Idx: 5000 Loss: 0.020550740247912182
Epoch: 29 Idx: 0 Loss: 0.0356880885940192
Epoch: 29 Idx: 5000 Loss: 0.011054127497034192
Epoch: 30 Idx: 0 Loss: 0.013247305912546357
Epoch: 30 Idx: 5000 Loss: 0.017624145107801543
Epoch: 31 Idx: 0 Loss: 0.018977327331313913
Epoch: 31 Idx: 5000 Loss: 0.011395685628347517
Epoch: 32 Idx: 0 Loss: 0.00749914311437711
Epoch: 32 Idx: 5000 Loss: 0.004854080853847623
Epoch: 33 Idx: 0 Loss: 0.006094397718968737
Epoch: 33 Idx: 5000 Loss: 0.007547076431450063
Epoch: 34 Idx: 0 Loss: 0.008153674747060139
Epoch: 34 Idx: 5000 Loss: 0.008696344490047282
Epoch: 35 Idx: 0 Loss: 0.0023068683721561367
Epoch: 35 Idx: 5000 Loss: 0.009396807440875857
Epoch: 36 Idx: 0 Loss: 0.02043689451386537
Epoch: 36 Idx: 5000 Loss: 0.007834962233349497
Epoch: 37 Idx: 0 Loss: 0.01273582783243897
Epoch: 37 Idx: 5000 Loss: 0.00742913576036655
Epoch: 38 Idx: 0 Loss: 0.009186386987874624
Epoch: 38 Idx: 5000 Loss: 0.012231039330062348
Epoch: 39 Idx: 0 Loss: 0.017564487214781294
Epoch: 39 Idx: 5000 Loss: 0.024928145963529498
Epoch: 40 Idx: 0 Loss: 0.006462419750476158
Epoch: 40 Idx: 5000 Loss: 0.013534129233001914
Epoch: 41 Idx: 0 Loss: 0.020818779531762532
Epoch: 41 Idx: 5000 Loss: 0.014299883314776562
Epoch: 42 Idx: 0 Loss: 0.018211312987236133
Epoch: 42 Idx: 5000 Loss: 0.007792508338675475
Epoch: 43 Idx: 0 Loss: 0.028932463957876648
Epoch: 43 Idx: 5000 Loss: 0.015891844115729973
Epoch: 44 Idx: 0 Loss: 0.015901027870938902
Traceback (most recent call last):
  File "Attention_german_amlconf_oaei.py", line 401, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "Attention_german_amlconf_oaei.py", line 258, in forward
    feature_emb = self.name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/modules/sparse.py", line 126, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/u/vmunig10/miniconda3/envs/aaai2021/lib/python3.6/site-packages/torch/nn/functional.py", line 1814, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 44040192 bytes. Error code 12 (Cannot allocate memory)

------------------------------------------------------------
Sender: LSF System <rer@dccxc247>
Subject: Job 4053611: <python Attention_german_amlconf_oaei.py Input/data_conf_oaei_german_aml_thresh.pkl 21 4 0.6075 Output/test_conf_oaei_german_aml_21_4_0.6075.pkl Models/conf_oaei_german_aml_21_4_0.6075.pt> in cluster <dcc> Exited

Job <python Attention_german_amlconf_oaei.py Input/data_conf_oaei_german_aml_thresh.pkl 21 4 0.6075 Output/test_conf_oaei_german_aml_21_4_0.6075.pkl Models/conf_oaei_german_aml_21_4_0.6075.pt> was submitted from host <dccxl009> by user <vmunig10> in cluster <dcc> at Tue Sep 15 06:48:50 2020
Job was executed on host(s) <dccxc247>, in queue <x86_24h>, as user <vmunig10> in cluster <dcc> at Tue Sep 15 06:48:51 2020
</u/vmunig10> was used as the home directory.
</dccstor/cogfin/aaai2021-vitobha/IBM-Internship> was used as the working directory.
Started at Tue Sep 15 06:48:51 2020
Terminated at Tue Sep 15 16:17:28 2020
Results reported at Tue Sep 15 16:17:28 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python Attention_german_amlconf_oaei.py Input/data_conf_oaei_german_aml_thresh.pkl 21 4 0.6075 Output/test_conf_oaei_german_aml_21_4_0.6075.pkl Models/conf_oaei_german_aml_21_4_0.6075.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   33935.57 sec.
    Max Memory :                                 1320 MB
    Average Memory :                             1149.23 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               42097.00 MB
    Max Swap :                                   940 MB
    Max Processes :                              3
    Max Threads :                                5
    Run time :                                   34122 sec.
    Turnaround time :                            34118 sec.

The output (if any) is above this job summary.

