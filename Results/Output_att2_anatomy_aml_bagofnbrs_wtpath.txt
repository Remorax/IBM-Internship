Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22364357617092337
Epoch: 0 Idx: 5000 Loss: 0.03015932957141387
Epoch: 1 Idx: 0 Loss: 0.03421549941766191
Epoch: 1 Idx: 5000 Loss: 0.011288498096933785
Epoch: 2 Idx: 0 Loss: 0.02595961831971285
Epoch: 2 Idx: 5000 Loss: 0.021099186086120266
Epoch: 3 Idx: 0 Loss: 0.04095608377609228
Epoch: 3 Idx: 5000 Loss: 0.03562598146974658
Epoch: 4 Idx: 0 Loss: 0.012387716075559977
Epoch: 4 Idx: 5000 Loss: 0.021875856999639877
Epoch: 5 Idx: 0 Loss: 0.016103161291113086
Epoch: 5 Idx: 5000 Loss: 0.018151888637308558
Epoch: 6 Idx: 0 Loss: 0.007792126906297296
Epoch: 6 Idx: 5000 Loss: 0.012944421245131009
Epoch: 7 Idx: 0 Loss: 0.02192083341924199
Epoch: 7 Idx: 5000 Loss: 0.03084371951556792
Epoch: 8 Idx: 0 Loss: 0.021485783631673954
Epoch: 8 Idx: 5000 Loss: 0.026763699476761917
Epoch: 9 Idx: 0 Loss: 0.010910132007686965
Epoch: 9 Idx: 5000 Loss: 0.015019487130445801
Epoch: 10 Idx: 0 Loss: 0.017989675433364636
Epoch: 10 Idx: 5000 Loss: 0.016177267997931177
Epoch: 11 Idx: 0 Loss: 0.02156817268260294
Epoch: 11 Idx: 5000 Loss: 0.02376940525641157
Epoch: 12 Idx: 0 Loss: 0.016381866060236675
Epoch: 12 Idx: 5000 Loss: 0.03418528086216881
Epoch: 13 Idx: 0 Loss: 0.018477276912983738
Epoch: 13 Idx: 5000 Loss: 0.015262079624116038
Epoch: 14 Idx: 0 Loss: 0.01741642335697048
Epoch: 14 Idx: 5000 Loss: 0.018982039118295668
Epoch: 15 Idx: 0 Loss: 0.02103384875307594
Epoch: 15 Idx: 5000 Loss: 0.014238113820642988
Epoch: 16 Idx: 0 Loss: 0.017805104085099277
Epoch: 16 Idx: 5000 Loss: 0.01329327027952248
Epoch: 17 Idx: 0 Loss: 0.020647282962308378
Epoch: 17 Idx: 5000 Loss: 0.022617765790643476
Epoch: 18 Idx: 0 Loss: 0.014236997651373654
Epoch: 18 Idx: 5000 Loss: 0.017708131777503428
Epoch: 19 Idx: 0 Loss: 0.016937148570366874
Epoch: 19 Idx: 5000 Loss: 0.022549967933369258
Epoch: 20 Idx: 0 Loss: 0.017707518090603655
Epoch: 20 Idx: 5000 Loss: 0.018413682153574074
Epoch: 21 Idx: 0 Loss: 0.019774353185880177
Epoch: 21 Idx: 5000 Loss: 0.005442634777824779
Epoch: 22 Idx: 0 Loss: 0.018988509570046313
Epoch: 22 Idx: 5000 Loss: 0.013303034306446863
Epoch: 23 Idx: 0 Loss: 0.014507398932976074
Epoch: 23 Idx: 5000 Loss: 0.04661085282440615
Epoch: 24 Idx: 0 Loss: 0.018873139763747746
Epoch: 24 Idx: 5000 Loss: 0.010785342227473666
Epoch: 25 Idx: 0 Loss: 0.01794746344602238
Epoch: 25 Idx: 5000 Loss: 0.02684844906672721
Epoch: 26 Idx: 0 Loss: 0.025058313663145135
Epoch: 26 Idx: 5000 Loss: 0.016960068174442622
Epoch: 27 Idx: 0 Loss: 0.0205048049014833
Epoch: 27 Idx: 5000 Loss: 0.019033715081724203
Epoch: 28 Idx: 0 Loss: 0.021306595139263335
Epoch: 28 Idx: 5000 Loss: 0.020851818349430687
Epoch: 29 Idx: 0 Loss: 0.01204350202042951
Epoch: 29 Idx: 5000 Loss: 0.018398503197185632
Epoch: 30 Idx: 0 Loss: 0.026942273898405728
Epoch: 30 Idx: 5000 Loss: 0.03791376480563358
Epoch: 31 Idx: 0 Loss: 0.016947699161288222
Epoch: 31 Idx: 5000 Loss: 0.01931805555127719
Epoch: 32 Idx: 0 Loss: 0.0380883405156425
Epoch: 32 Idx: 5000 Loss: 0.022468304679676074
Epoch: 33 Idx: 0 Loss: 0.019404141531988908
Epoch: 33 Idx: 5000 Loss: 0.02062539078817431
Epoch: 34 Idx: 0 Loss: 0.02503879350520224
Epoch: 34 Idx: 5000 Loss: 0.01248565783432356
Epoch: 35 Idx: 0 Loss: 0.021491621319914122
Epoch: 35 Idx: 5000 Loss: 0.0361233576579922
Epoch: 36 Idx: 0 Loss: 0.016451296481518395
Epoch: 36 Idx: 5000 Loss: 0.013864165660006922
Epoch: 37 Idx: 0 Loss: 0.02451352438965858
Epoch: 37 Idx: 5000 Loss: 0.01580668490102491
Epoch: 38 Idx: 0 Loss: 0.01609355367297626
Epoch: 38 Idx: 5000 Loss: 0.022453675360626625
Epoch: 39 Idx: 0 Loss: 0.023524325511839916
Epoch: 39 Idx: 5000 Loss: 0.056895674551710886
Epoch: 40 Idx: 0 Loss: 0.017846447600971475
Epoch: 40 Idx: 5000 Loss: 0.017653039730651986
Epoch: 41 Idx: 0 Loss: 0.007315648173094618
Epoch: 41 Idx: 5000 Loss: 0.020105928508572003
Epoch: 42 Idx: 0 Loss: 0.012998320981231513
Epoch: 42 Idx: 5000 Loss: 0.0190489182443479
Epoch: 43 Idx: 0 Loss: 0.017881652856696156
Epoch: 43 Idx: 5000 Loss: 0.02566608083039661
Epoch: 44 Idx: 0 Loss: 0.021243088170025275
Epoch: 44 Idx: 5000 Loss: 0.019285672633130264
Epoch: 45 Idx: 0 Loss: 0.0318332278166587
Epoch: 45 Idx: 5000 Loss: 0.03251291619229487
Epoch: 46 Idx: 0 Loss: 0.01807107244569736
Epoch: 46 Idx: 5000 Loss: 0.019117811684502335
Epoch: 47 Idx: 0 Loss: 0.01755844377910972
Epoch: 47 Idx: 5000 Loss: 0.020837652759827956
Epoch: 48 Idx: 0 Loss: 0.016525942977434895
Epoch: 48 Idx: 5000 Loss: 0.027608978052573573
Epoch: 49 Idx: 0 Loss: 0.01792869420827591
Epoch: 49 Idx: 5000 Loss: 0.024128297200720357
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisiTraining siTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.16996388444935187
Epoch: 0 Idx: 5000 Loss: 0.01120528001888764
Epoch: 1 Idx: 0 Loss: 0.03937449623968817
Epoch: 1 Idx: 5000 Loss: 0.010045632554579475
Epoch: 2 Idx: 0 Loss: 0.020067331235838946
Epoch: 2 Idx: 5000 Loss: 0.017815097737215783
Epoch: 3 Idx: 0 Loss: 0.016006305458121643
Epoch: 3 Idx: 5000 Loss: 0.024791840330834626
Epoch: 4 Idx: 0 Loss: 0.019170104173466162
Epoch: 4 Idx: 5000 Loss: 0.03751895186868393
Epoch: 5 Idx: 0 Loss: 0.010843263566205621
Epoch: 5 Idx: 5000 Loss: 0.030308014215981584
Epoch: 6 Idx: 0 Loss: 0.03237854341101169
Epoch: 6 Idx: 5000 Loss: 0.013222399560190987
Epoch: 7 Idx: 0 Loss: 0.027725593685713666
Epoch: 7 Idx: 5000 Loss: 0.028069954666092932
Epoch: 8 Idx: 0 Loss: 0.03428177188940392
Epoch: 8 Idx: 5000 Loss: 0.02711137104739456
Epoch: 9 Idx: 0 Loss: 0.01934722736217954
Epoch: 9 Idx: 5000 Loss: 0.016791924236413847
Epoch: 10 Idx: 0 Loss: 0.008955202873470807
Epoch: 10 Idx: 5000 Loss: 0.0140275539090629
Epoch: 11 Idx: 0 Loss: 0.01538417609877934
Epoch: 11 Idx: 5000 Loss: 0.03671586654617419
Epoch: 12 Idx: 0 Loss: 0.013502288583323141
Epoch: 12 Idx: 5000 Loss: 0.021982139427772926
Epoch: 13 Idx: 0 Loss: 0.04145367976277146
Epoch: 13 Idx: 5000 Loss: 0.019966677758659207
Epoch: 14 Idx: 0 Loss: 0.02486978805499829
Epoch: 14 Idx: 5000 Loss: 0.014218072132678667
Epoch: 15 Idx: 0 Loss: 0.021594525412446545
Epoch: 15 Idx: 5000 Loss: 0.019552795111683743
Epoch: 16 Idx: 0 Loss: 0.01733427366333095
Epoch: 16 Idx: 5000 Loss: 0.028029154427676298
Epoch: 17 Idx: 0 Loss: 0.02413208780918502
Epoch: 17 Idx: 5000 Loss: 0.01895493310446572
Epoch: 18 Idx: 0 Loss: 0.028246723434794195
Epoch: 18 Idx: 5000 Loss: 0.02886762553226266
Epoch: 19 Idx: 0 Loss: 0.01612364623603431
Epoch: 19 Idx: 5000 Loss: 0.026030900663590567
Epoch: 20 Idx: 0 Loss: 0.02473819510612059
Epoch: 20 Idx: 5000 Loss: 0.023858855108697896
Epoch: 21 Idx: 0 Loss: 0.03059831846126535
Epoch: 21 Idx: 5000 Loss: 0.024991460115418175
Epoch: 22 Idx: 0 Loss: 0.017277945332322436
Epoch: 22 Idx: 5000 Loss: 0.01254544253757686
Epoch: 23 Idx: 0 Loss: 0.014464476286662104
Epoch: 23 Idx: 5000 Loss: 0.012271477955429743
Epoch: 24 Idx: 0 Loss: 0.023068699930680343
Epoch: 24 Idx: 5000 Loss: 0.01905304804111521
Epoch: 25 Idx: 0 Loss: 0.01161785305891041
Epoch: 25 Idx: 5000 Loss: 0.020992327709580563
Epoch: 26 Idx: 0 Loss: 0.022283119298006426
Epoch: 26 Idx: 5000 Loss: 0.041868368320863325
Epoch: 27 Idx: 0 Loss: 0.01937552220478493
Epoch: 27 Idx: 5000 Loss: 0.018737160335885787
Epoch: 28 Idx: 0 Loss: 0.029797623168372377
Epoch: 28 Idx: 5000 Loss: 0.02873065754497984
Epoch: 29 Idx: 0 Loss: 0.021352454017970236
Epoch: 29 Idx: 5000 Loss: 0.020405865179987298
Epoch: 30 Idx: 0 Loss: 0.023422076658813934
Epoch: 30 Idx: 5000 Loss: 0.032879058677770606
Epoch: 31 Idx: 0 Loss: 0.008124493243895362
Epoch: 31 Idx: 5000 Loss: 0.015437015415257918
Epoch: 32 Idx: 0 Loss: 0.019203899954022066
Epoch: 32 Idx: 5000 Loss: 0.04454618851553034
Epoch: 33 Idx: 0 Loss: 0.019760110031017894
Epoch: 33 Idx: 5000 Loss: 0.02696839949129721
Epoch: 34 Idx: 0 Loss: 0.021504309693330247
Epoch: 34 Idx: 5000 Loss: 0.021053341286921365
Epoch: 35 Idx: 0 Loss: 0.016096115416541063
Epoch: 35 Idx: 5000 Loss: 0.022981335559838917
Epoch: 36 Idx: 0 Loss: 0.017990412576837252
Epoch: 36 Idx: 5000 Loss: 0.016520613290043416
Epoch: 37 Idx: 0 Loss: 0.03359530605365764
Epoch: 37 Idx: 5000 Loss: 0.013169775264547733
Epoch: 38 Idx: 0 Loss: 0.022090789983338518
Epoch: 38 Idx: 5000 Loss: 0.016117052196112775
Epoch: 39 Idx: 0 Loss: 0.023583370687175607
Epoch: 39 Idx: 5000 Loss: 0.026725262392413407
Epoch: 40 Idx: 0 Loss: 0.02597716445129124
Epoch: 40 Idx: 5000 Loss: 0.032386790860536474
Epoch: 41 Idx: 0 Loss: 0.04304754385110489
Epoch: 41 Idx: 5000 Loss: 0.0125978707745409
Epoch: 42 Idx: 0 Loss: 0.02640941083713965
Epoch: 42 Idx: 5000 Loss: 0.016794214334398018
Epoch: 43 Idx: 0 Loss: 0.019975754349437366
Epoch: 43 Idx: 5000 Loss: 0.030917418206208747
Epoch: 44 Idx: 0 Loss: 0.011485591183269927
Epoch: 44 Idx: 5000 Loss: 0.021469659282658923
Epoch: 45 Idx: 0 Loss: 0.034602352495092886
Epoch: 45 Idx: 5000 Loss: 0.016779009492055255
Epoch: 46 Idx: 0 Loss: 0.01995461541280339
Epoch: 46 Idx: 5000 Loss: 0.017017917301610497
Epoch: 47 Idx: 0 Loss: 0.020269711758567843
Epoch: 47 Idx: 5000 Loss: 0.011482005618410816
Epoch: 48 Idx: 0 Loss: 0.029717108976055152
Epoch: 48 Idx: 5000 Loss: 0.029926946140509936
Epoch: 49 Idx: 0 Loss: 0.022994795409956827
Epoch: 49 Idx: 5000 Loss: 0.02321807852372045
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2755249519434957
Epoch: 0 Idx: 5000 Loss: 0.030418779012855304
Epoch: 1 Idx: 0 Loss: 0.01958251263091872
Epoch: 1 Idx: 5000 Loss: 0.013516354992111679
Epoch: 2 Idx: 0 Loss: 0.025704347713176694
Epoch: 2 Idx: 5000 Loss: 0.029675108066853258
Epoch: 3 Idx: 0 Loss: 0.02345093716009506
Epoch: 3 Idx: 5000 Loss: 0.021651791467203623
Epoch: 4 Idx: 0 Loss: 0.024431242734571573
Epoch: 4 Idx: 5000 Loss: 0.014301075832210769
Epoch: 5 Idx: 0 Loss: 0.014657202749024635
Epoch: 5 Idx: 5000 Loss: 0.024431400763270356
Epoch: 6 Idx: 0 Loss: 0.02009913541998066
Epoch: 6 Idx: 5000 Loss: 0.01232685926172342
Epoch: 7 Idx: 0 Loss: 0.01513814733343305
Epoch: 7 Idx: 5000 Loss: 0.022051858183829803
Epoch: 8 Idx: 0 Loss: 0.015055229929532121
Epoch: 8 Idx: 5000 Loss: 0.025226072948141734
Epoch: 9 Idx: 0 Loss: 0.030423341146453065
Epoch: 9 Idx: 5000 Loss: 0.03259974664218654
Epoch: 10 Idx: 0 Loss: 0.01992400742900164
Epoch: 10 Idx: 5000 Loss: 0.025019708817393665
Epoch: 11 Idx: 0 Loss: 0.012655935129711913
Epoch: 11 Idx: 5000 Loss: 0.046206682717292794
Epoch: 12 Idx: 0 Loss: 0.0374590616019047
Epoch: 12 Idx: 5000 Loss: 0.024954692737200006
Epoch: 13 Idx: 0 Loss: 0.015041961112844383
Epoch: 13 Idx: 5000 Loss: 0.025877261190915015
Epoch: 14 Idx: 0 Loss: 0.013493367552805427
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
 18 Idx: 5000 Loss: 0.033410807482196284
Epoch: 19 Idx: 0 Loss: 0.01783109870895601
Epoch: 19 Idx: 5000 Loss: 0.012423421524982622
Epoch: 20 Idx: 0 Loss: 0.04128497400855143
Epoch: 20 Idx: 5000 Loss: 0.024195322405598685
Epoch: 21 Idx: 0 Loss: 0.01958423246787492
Epoch: 21 Idx: 5000 Loss: 0.024232384991490573
Epoch: 22 Idx: 0 Loss: 0.031409348483581005
Epoch: 22 Idx: 5000 Loss: 0.008363146411667487
Epoch: 23 Idx: 0 Loss: 0.018127114509836374
Epoch: 23 Idx: 5000 Loss: 0.013280140822490901
Epoch: 24 Idx: 0 Loss: 0.03543313250586138
Epoch: 24 Idx: 5000 Loss: 0.009258879693960794
Epoch: 25 Idx: 0 Loss: 0.028302418400199952
Epoch: 25 Idx: 5000 Loss: 0.01741417548463462
Epoch: 26 Idx: 0 Loss: 0.014580875468258961
Epoch: 26 Idx: 5000 Loss: 0.010042153242236123
Epoch: 27 Idx: 0 Loss: 0.031174985758648854
Epoch: 27 Idx: 5000 Loss: 0.03880322089772547
Epoch: 28 Idx: 0 Loss: 0.02882507644627022
Epoch: 28 Idx: 5000 Loss: 0.02451315387564919
Epoch: 29 Idx: 0 Loss: 0.020342374845340946
Epoch: 29 Idx: 5000 Loss: 0.02671944056401441
Epoch: 30 Idx: 0 Loss: 0.023286856026536663
Epoch: 30 Idx: 5000 Loss: 0.017084441739557442
Epoch: 31 Idx: 0 Loss: 0.027068215516788886
Epoch: 31 Idx: 5000 Loss: 0.01829000715309772
Epoch: 32 Idx: 0 Loss: 0.02274058197908736
Epoch: 32 Idx: 5000 Loss: 0.018681299940195316
Epoch: 33 Idx: 0 Loss: 0.03629420902225029
Epoch: 33 Idx: 5000 Loss: 0.014535169510877842
Epoch: 34 Idx: 0 Loss: 0.013107083842986117
Epoch: 34 Idx: 5000 Loss: 0.008867355374141475
Epoch: 35 Idx: 0 Loss: 0.024881395610550805
Epoch: 35 Idx: 5000 Loss: 0.024433142790246522
Epoch: 36 Idx: 0 Loss: 0.026955401899869603
Epoch: 36 Idx: 5000 Loss: 0.02306694878439437
Epoch: 37 Idx: 0 Loss: 0.015381964683417775
Epoch: 37 Idx: 5000 Loss: 0.011323366539562185
Epoch: 38 Idx: 0 Loss: 0.008507083547338888
Epoch: 38 Idx: 5000 Loss: 0.019010473141051178
Epoch: 39 Idx: 0 Loss: 0.013029602964080539
Epoch: 39 Idx: 5000 Loss: 0.018648178422467875
Epoch: 40 Idx: 0 Loss: 0.015167307019206338
Epoch: 40 Idx: 5000 Loss: 0.012395076384039338
Epoch: 41 Idx: 0 Loss: 0.020381198891032855
Epoch: 41 Idx: 5000 Loss: 0.012559644931581244
Epoch: 42 Idx: 0 Loss: 0.024544413448439998
Epoch: 42 Idx: 5000 Loss: 0.030322832788923223
Epoch: 43 Idx: 0 Loss: 0.029758402590495982
Epoch: 43 Idx: 5000 Loss: 0.022353922487492153
Epoch: 44 Idx: 0 Loss: 0.01822833474655582
Epoch: 44 Idx: 5000 Loss: 0.01640135615310188
Epoch: 45 Idx: 0 Loss: 0.028368501609124398
Epoch: 45 Idx: 5000 Loss: 0.010936753007484461
Epoch: 46 Idx: 0 Loss: 0.024546927489414453
Epoch: 46 Idx: 5000 Loss: 0.011694143452216605
Epoch: 47 Idx: 0 Loss: 0.021895834457606407
Epoch: 47 Idx: 5000 Loss: 0.025637826558994145
Epoch: 48 Idx: 0 Loss: 0.02114465623299943
Epoch: 48 Idx: 5000 Loss: 0.021396081352986908
Epoch: 49 Idx: 0 Loss: 0.013994108335924564
Epoch: 49 Idx: 5000 Loss: 0.030853257844484398
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17775909785505734
Epoch: 0 Idx: 5000 Loss: 0.011678973529499334
Epoch: 1 Idx: 0 Loss: 0.02159131436395091
Epoch: 1 Idx: 5000 Loss: 0.018346943449674157
Epoch: 2 Idx: 0 Loss: 0.017275476545412845
Epoch: 2 Idx: 5000 Loss: 0.018907333200485998
Epoch: 3 Idx: 0 Loss: 0.023657762234593288
Epoch: 3 Idx: 5000 Loss: 0.009874323627017291
Epoch: 4 Idx: 0 Loss: 0.016356795485849448
Epoch: 4 Idx: 5000 Loss: 0.02439274664955174
Epoch: 5 Idx: 0 Loss: 0.006752769017129578
Epoch: 5 Idx: 5000 Loss: 0.020996412788288864
Epoch: 6 Idx: 0 Loss: 0.03530467156206935
Epoch: 6 Idx: 5000 Loss: 0.026465355806459964
Epoch: 7 Idx: 0 Loss: 0.008742815894791114
Epoch: 7 Idx: 5000 Loss: 0.043926064666474904
Epoch: 8 Idx: 0 Loss: 0.020254198097241254
Epoch: 8 Idx: 5000 Loss: 0.01889936157288062
Epoch: 9 Idx: 0 Loss: 0.030795540122949723
Epoch: 9 Idx: 5000 Loss: 0.017468015083468968
Epoch: 10 Idx: 0 Loss: 0.014306962993727194
Epoch: 10 Idx: 5000 Loss: 0.010380183896441303
Epoch: 11 Idx: 0 Loss: 0.027833663699158122
Epoch: 11 Idx: 5000 Loss: 0.0362939373277253
Epoch: 12 Idx: 0 Loss: 0.022804893480596056
Epoch: 12 Idx: 5000 Loss: 0.021504065082824923
Epoch: 13 Idx: 0 Loss: 0.008880393635489696
Epoch: 13 Idx: 5000 Loss: 0.01482137897156631
Epoch: 14 Idx: 0 Loss: 0.03650607147662267
Epoch: 14 Idx: 5000 Loss: 0.020066165817898884
Epoch: 15 Idx: 0 Loss: 0.018879247648024358
Epoch: 15 Idx: 5000 Loss: 0.018528245067832277
Epoch: 16 Idx: 0 Loss: 0.016967588500213307
Epoch: 16 Idx: 5000 Loss: 0.019098093045135895
Epoch: 17 Idx: 0 Loss: 0.03453157006725565
Epoch: 17 Idx: 5000 Loss: 0.01796509821624626
Epoch: 18 Idx: 0 Loss: 0.015644415397212633
Epoch: 18 Idx: 5000 Loss: 0.014986175360069628
Epoch: 19 Idx: 0 Loss: 0.025477010594960214
Epoch: 19 Idx: 5000 Loss: 0.01888676566396444
Epoch: 20 Idx: 0 Loss: 0.008395097273165238
Epoch: 20 Idx: 5000 Loss: 0.02233900688461306
Epoch: 21 Idx: 0 Loss: 0.045794837886501724
Epoch: 21 Idx: 5000 Loss: 0.02016579053314141
Epoch: 22 Idx: 0 Loss: 0.018538350318009718
Epoch: 22 Idx: 5000 Loss: 0.015524973661370048
Epoch: 23 Idx: 0 Loss: 0.020985571174966813
Epoch: 23 Idx: 5000 Loss: 0.02939152737579807
Epoch: 24 Idx: 0 Loss: 0.019264052765999233
Epoch: 24 Idx: 5000 Loss: 0.02063880940685581
Epoch: 25 Idx: 0 Loss: 0.01927328151412183
Epoch: 25 Idx: 5000 Loss: 0.014558694759223249
Epoch: 26 Idx: 0 Loss: 0.03036927319386502
Epoch: 26 Idx: 5000 Loss: 0.029764027145376636
Epoch: 27 Idx: 0 Loss: 0.02041511823100441
Epoch: 27 Idx: 5000 Loss: 0.02146543942650752
Epoch: 28 Idx: 0 Loss: 0.017111902805700484
Epoch: 28 Idx: 5000 Loss: 0.017988655624423602
Epoch: 29 Idx: 0 Loss: 0.016369811660974252
Epoch: 29 Idx: 5000 Loss: 0.019636491217485685
Epoch: 30 Idx: 0 Loss: 0.02174560546711344
Epoch: 30 Idx: 5000 Loss: 0.02097943212733462
Epoch: 31 Idx: 0 Loss: 0.01576482212774121
Epoch: 31 Idx: 5000 Loss: 0.010771430082663597
Epoch: 32 Idx: 0 Loss: 0.05265656618567046
Epoch: 32 Idx: 5000 Loss: 0.019530831724650817
Epoch: 33 Idx: 0 Loss: 0.015211797958565799
Epoch: 33 Idx: 5000 Loss: 0.03192005741937172
Epoch: 34 Idx: 0 Loss: 0.015638656657164823
Epoch: 34 Idx: 5000 Loss: 0.04380840689021629
Epoch: 35 Idx: 0 Loss: 0.019370149540658237
Epoch: 35 Idx: 5000 Loss: 0.03147496377272854
Epoch: 36 Idx: 0 Loss: 0.0366098585958884
Epoch: 36 Idx: 5000 Loss: 0.01387565770965027
Epoch: 37 Idx: 0 Loss: 0.017240987882361343
Epoch: 37 Idx: 5000 Loss: 0.029655944142152058
Epoch: 38 Idx: 0 Loss: 0.02182396063575616
Epoch: 38 Idx: 5000 Loss: 0.01842767018661865
Epoch: 39 Idx: 0 Loss: 0.014713919160240424
Epoch: 39 Idx: 5000 Loss: 0.015781515928529704
Epoch: 40 Idx: 0 Loss: 0.026465688748376837
Epoch: 40 Idx: 5000 Loss: 0.01922491248520131
Epoch: 41 Idx: 0 Loss: 0.030193318223947274
Epoch: 41 Idx: 5000 Loss: 0.017732475365993487
Epoch: 42 Idx: 0 Loss: 0.019168077526674175
Epoch: 42 Idx: 5000 Loss: 0.029529705051933725
Epoch: 43 Idx: 0 Loss: 0.05615368249599065
Epoch: 43 Idx: 5000 Loss: 0.02211090657629516
Epoch: 44 Idx: 0 Loss: 0.013030235542381304
Epoch: 44 Idx: 5000 Loss: 0.02766449593924719
Epoch: 45 Idx: 0 Loss: 0.01860612436320389
Epoch: 45 Idx: 5000 Loss: 0.01681548007741864
Epoch: 46 Idx: 0 Loss: 0.017024745168949473
Epoch: 46 Idx: 5000 Loss: 0.01821047843105249
Epoch: 47 Idx: 0 Loss: 0.01392112688498082
Epoch: 47 Idx: 5000 Loss: 0.030668631306881487
Epoch: 48 Idx: 0 Loss: 0.030584122161675442
Epoch: 48 Idx: 5000 Loss: 0.021232195135848557
Epoch: 49 Idx: 0 Loss: 0.03840918963861382
Epoch: 49 Idx: 5000 Loss: 0.022406872138214344
Len (direct inputs):  104
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.20837881632629673
Epoch: 0 Idx: 5000 Loss: 0.037660752317449406
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 397, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "Attention_anatomy_aml_weighted.py", line 263, in forward
    path_weights = masked_softmax(torch.sum(path_weights, dim=-1))
  File "Attention_anatomy_aml_weighted.py", line 219, in masked_softmax
    mask = ((inp != 0).double() - 1) * 9999  # for -inf
KeyboardInterrupt
Loss: 0.012389243115005836
Epoch: 8 Idx: 0 Loss: 0.019794241801448537
Epoch: 8 Idx: 5000 Loss: 0.043223300126852854
Epoch: 9 Idx: 0 Loss: 0.010242377445402882
Epoch: 9 Idx: 5000 Loss: 0.04530577555796693
Epoch: 10 Idx: 0 Loss: 0.01213414426214799
Epoch: 10 Idx: 5000 Loss: 0.04123864791697869
Epoch: 11 Idx: 0 Loss: 0.009720217195449531
Epoch: 11 Idx: 5000 Loss: 0.01570624034877789
Epoch: 12 Idx: 0 Loss: 0.020191888585730386
Epoch: 12 Idx: 5000 Loss: 0.018073213143587626
Epoch: 13 Idx: 0 Loss: 0.013726386082648624
Epoch: 13 Idx: 5000 Loss: 0.020184483758363268
Epoch: 14 Idx: 0 Loss: 0.023359160311443708
Epoch: 14 Idx: 5000 Loss: 0.02842220753633006
Epoch: 15 Idx: 0 Loss: 0.02411728128594367
Epoch: 15 Idx: 5000 Loss: 0.024631670291840442
Epoch: 16 Idx: 0 Loss: 0.03149899826882664
Epoch: 16 Idx: 5000 Loss: 0.029405334417696015
Epoch: 17 Idx: 0 Loss: 0.03443248536445876
Epoch: 17 Idx: 5000 Loss: 0.012493742848905727
Epoch: 18 Idx: 0 Loss: 0.024093322855891684
Epoch: 18 Idx: 5000 Loss: 0.017006097212822716
Epoch: 19 Idx: 0 Loss: 0.011544878925012833
Epoch: 19 Idx: 5000 Loss: 0.021868033712830602
Epoch: 20 Idx: 0 Loss: 0.027844336234277907
Epoch: 20 Idx: 5000 Loss: 0.010661636906691268
Epoch: 21 Idx: 0 Loss: 0.01220069328471096
Epoch: 21 Idx: 5000 Loss: 0.012430040195691227
Epoch: 22 Idx: 0 Loss: 0.011849937629648926
Epoch: 22 Idx: 5000 Loss: 0.01797072079374951
Epoch: 23 Idx: 0 Loss: 0.04044192664330809
Epoch: 23 Idx: 5000 Loss: 0.032785916427083256
Epoch: 24 Idx: 0 Loss: 0.021833122739655396
Epoch: 24 Idx: 5000 Loss: 0.017083777077302437
Epoch: 25 Idx: 0 Loss: 0.02105253683148583
Epoch: 25 Idx: 5000 Loss: 0.028330458235027455
Epoch: 26 Idx: 0 Loss: 0.018912933170649994
Epoch: 26 Idx: 5000 Loss: 0.012463975226905842
Epoch: 27 Idx: 0 Loss: 0.03720857922778746
Epoch: 27 Idx: 5000 Loss: 0.01615671024781533
Epoch: 28 Idx: 0 Loss: 0.016073164460347806
Epoch: 28 Idx: 5000 Loss: 0.031509460187902724
Epoch: 29 Idx: 0 Loss: 0.01048158664431647
Epoch: 29 Idx: 5000 Loss: 0.014691378981318025
Epoch: 30 Idx: 0 Loss: 0.0293455559621484
Epoch: 30 Idx: 5000 Loss: 0.0170802098360366
Epoch: 31 Idx: 0 Loss: 0.005495986897435166
Epoch: 31 Idx: 5000 Loss: 0.016646724182159824
Epoch: 32 Idx: 0 Loss: 0.01729357849119065
Epoch: 32 Idx: 5000 Loss: 0.012540273501121411
Epoch: 33 Idx: 0 Loss: 0.006840059808492655
Epoch: 33 Idx: 5000 Loss: 0.020712241010265767
Epoch: 34 Idx: 0 Loss: 0.02062684758095428
Epoch: 34 Idx: 5000 Loss: 0.018741470690410652
Epoch: 35 Idx: 0 Loss: 0.02505091590362056
Epoch: 35 Idx: 5000 Loss: 0.020189772120914252
Epoch: 36 Idx: 0 Loss: 0.026972020721157536
Epoch: 36 Idx: 5000 Loss: 0.014343929970424225
Epoch: 37 Idx: 0 Loss: 0.018932536071577636
Epoch: 37 Idx: 5000 Loss: 0.017062151112157507
Epoch: 38 Idx: 0 Loss: 0.019196168216193226
Epoch: 38 Idx: 5000 Loss: 0.027250654487120454
Epoch: 39 Idx: 0 Loss: 0.013617889777344051
Epoch: 39 Idx: 5000 Loss: 0.025994423700337166
Epoch: 40 Idx: 0 Loss: 0.02186225160240407
Epoch: 40 Idx: 5000 Loss: 0.0147718678315574
Epoch: 41 Idx: 0 Loss: 0.018443785841517214
Epoch: 41 Idx: 5000 Loss: 0.016653372523959546
Epoch: 42 Idx: 0 Loss: 0.025396018381000387
Epoch: 42 Idx: 5000 Loss: 0.01955548401706476
Epoch: 43 Idx: 0 Loss: 0.020496711483898512
Epoch: 43 Idx: 5000 Loss: 0.022131977859541095
Epoch: 44 Idx: 0 Loss: 0.012185412028707248
Epoch: 44 Idx: 5000 Loss: 0.039106627320134035
Epoch: 45 Idx: 0 Loss: 0.010847775873970998
Epoch: 45 Idx: 5000 Loss: 0.027562942704853904
Epoch: 46 Idx: 0 Loss: 0.02107149579174426
Epoch: 46 Idx: 5000 Loss: 0.022508177948544508
Epoch: 47 Idx: 0 Loss: 0.02224819286516118
Epoch: 47 Idx: 5000 Loss: 0.01723137060148166
Epoch: 48 Idx: 0 Loss: 0.020728835606153156
Epoch: 48 Idx: 5000 Loss: 0.011613104250554028
Epoch: 49 Idx: 0 Loss: 0.015357212585589754
Epoch: 49 Idx: 5000 Loss: 0.013255346995355985
Len (direct inputs):  102
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17828574493424346
Epoch: 0 Idx: 5000 Loss: 0.028696519851407424
Epoch: 1 Idx: 0 Loss: 0.0070384823817360494
Epoch: 1 Idx: 5000 Loss: 0.008135907848565552
Epoch: 2 Idx: 0 Loss: 0.025016929562994658
Epoch: 2 Idx: 5000 Loss: 0.04209928629034303
Epoch: 3 Idx: 0 Loss: 0.012539033933596788
Epoch: 3 Idx: 5000 Loss: 0.01977476347494344
Epoch: 4 Idx: 0 Loss: 0.016449790464049048
Epoch: 4 Idx: 5000 Loss: 0.010216470225408055
Epoch: 5 Idx: 0 Loss: 0.016877303250213905
Epoch: 5 Idx: 5000 Loss: 0.026300846130450858
Epoch: 6 Idx: 0 Loss: 0.019120908871433653
Epoch: 6 Idx: 5000 Loss: 0.01629986724034011
Epoch: 7 Idx: 0 Loss: 0.01964757781295131
Epoch: 7 Idx: 5000 Loss: 0.017598290501262607
Epoch: 8 Idx: 0 Loss: 0.015258246488505147
Epoch: 8 Idx: 5000 Loss: 0.023892894634520195
Epoch: 9 Idx: 0 Loss: 0.02285580342390709
Epoch: 9 Idx: 5000 Loss: 0.01402276616277752
Epoch: 10 Idx: 0 Loss: 0.024595111858227624
Epoch: 10 Idx: 5000 Loss: 0.012419482215052902
Epoch: 11 Idx: 0 Loss: 0.03141806280500187
Epoch: 11 Idx: 5000 Loss: 0.01947188039811846
Epoch: 12 Idx: 0 Loss: 0.022215681929893445
Epoch: 12 Idx: 5000 Loss: 0.04037055685135586
Epoch: 13 Idx: 0 Loss: 0.020533107734688145
Epoch: 13 Idx: 5000 Loss: 0.014260448628106295
Epoch: 14 Idx: 0 Loss: 0.0271392629970024
Epoch: 14 Idx: 5000 Loss: 0.01858621335898961
Epoch: 15 Idx: 0 Loss: 0.02494899404489389
Epoch: 15 Idx: 5000 Loss: 0.03812020428409445
Epoch: 16 Idx: 0 Loss: 0.02337370242477195
Epoch: 16 Idx: 5000 Loss: 0.03818754512934672
Epoch: 17 Idx: 0 Loss: 0.012164486441464988
Epoch: 17 Idx: 5000 Loss: 0.014645209831599925
Epoch: 18 Idx: 0 Loss: 0.014590330297558376
Epoch: 18 Idx: 5000 Loss: 0.017462791571524347
Epoch: 19 Idx: 0 Loss: 0.020323338155212166
Epoch: 19 Idx: 5000 Loss: 0.017885143937868456
Epoch: 20 Idx: 0 Loss: 0.03437749842174885
Epoch: 20 Idx: 5000 Loss: 0.01695245709251221
Epoch: 21 Idx: 0 Loss: 0.044387552943874246
Epoch: 21 Idx: 5000 Loss: 0.014690477526194134
Epoch: 22 Idx: 0 Loss: 0.022831500096824844
Epoch: 22 Idx: 5000 Loss: 0.0328026656355295
Epoch: 23 Idx: 0 Loss: 0.010788924921399646
Epoch: 23 Idx: 5000 Loss: 0.019559060998830414
Epoch: 24 Idx: 0 Loss: 0.01967967944216982
Epoch: 24 Idx: 5000 Loss: 0.010564172238479014
Epoch: 25 Idx: 0 Loss: 0.01789232444810326
Epoch: 25 Idx: 5000 Loss: 0.019579300548214024
Epoch: 26 Idx: 0 Loss: 0.022326016080036912
Epoch: 26 Idx: 5000 Loss: 0.022225282883780798
Epoch: 27 Idx: 0 Loss: 0.028941812766072886
Epoch: 27 Idx: 5000 Loss: 0.013532353494653454
Epoch: 28 Idx: 0 Loss: 0.027771305624946264
Epoch: 28 Idx: 5000 Loss: 0.03452441208885339
Epoch: 29 Idx: 0 Loss: 0.011043895772968298
Epoch: 29 Idx: 5000 Loss: 0.011860214346122572
Epoch: 30 Idx: 0 Loss: 0.012004750594665676
Epoch: 30 Idx: 5000 Loss: 0.020481968950627386
Epoch: 31 Idx: 0 Loss: 0.02479760470085325
Epoch: 31 Idx: 5000 Loss: 0.030755882732759796
Epoch: 32 Idx: 0 Loss: 0.011704138001136961
Epoch: 32 Idx: 5000 Loss: 0.014557987449932301
Epoch: 33 Idx: 0 Loss: 0.03885014765814767
Epoch: 33 Idx: 5000 Loss: 0.026011972030919484
Epoch: 34 Idx: 0 Loss: 0.01533869746709841
Epoch: 34 Idx: 5000 Loss: 0.029134870821404705
Epoch: 35 Idx: 0 Loss: 0.017853185171099697
Epoch: 35 Idx: 5000 Loss: 0.018268177481013344
Epoch: 36 Idx: 0 Loss: 0.019539577826757498
Epoch: 36 Idx: 5000 Loss: 0.011496407965292083
Epoch: 37 Idx: 0 Loss: 0.014627198134774453
Epoch: 37 Idx: 5000 Loss: 0.01601185940707983
Epoch: 38 Idx: 0 Loss: 0.04231925393171283
Epoch: 38 Idx: 5000 Loss: 0.04821713287389921
Epoch: 39 Idx: 0 Loss: 0.02824162868509164
Epoch: 39 Idx: 5000 Loss: 0.039270487944437375
Epoch: 40 Idx: 0 Loss: 0.025541293850953703
Epoch: 40 Idx: 5000 Loss: 0.014470294886702014
Epoch: 41 Idx: 0 Loss: 0.02887058895187089
Epoch: 41 Idx: 5000 Loss: 0.014768287492627073
Epoch: 42 Idx: 0 Loss: 0.020346027149069325
Epoch: 42 Idx: 5000 Loss: 0.030823860059271124
Epoch: 43 Idx: 0 Loss: 0.012309500305766337
Epoch: 43 Idx: 5000 Loss: 0.025855443808482242
Epoch: 44 Idx: 0 Loss: 0.013106140288675327
Epoch: 44 Idx: 5000 Loss: 0.019513312802915177
Epoch: 45 Idx: 0 Loss: 0.013648834449089518
Epoch: 45 Idx: 5000 Loss: 0.03855713184200372
Epoch: 46 Idx: 0 Loss: 0.02614341946291189
Epoch: 46 Idx: 5000 Loss: 0.013478203704379792
Epoch: 47 Idx: 0 Loss: 0.02684269233576947
Epoch: 47 Idx: 5000 Loss: 0.022907133464817667
Epoch: 48 Idx: 0 Loss: 0.014028313396653534
Epoch: 48 Idx: 5000 Loss: 0.014093472489539231
Epoch: 49 Idx: 0 Loss: 0.02362501043520758
Epoch: 49 Idx: 5000 Loss: 0.02382266161399063
Len (direct inputs):  109
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
0.7674723338656021
Parameter containing:
tensor([0.7675], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.18955475621418166
Epoch: 0 Idx: 5000 Loss: 0.014665694391679255
Epoch: 1 Idx: 0 Loss: 0.011925708841824634
Epoch: 1 Idx: 5000 Loss: 0.018501454527293466
Epoch: 2 Idx: 0 Loss: 0.035177986039353654
Epoch: 2 Idx: 5000 Loss: 0.010963271451439688
Epoch: 3 Idx: 0 Loss: 0.009671248539604393
Epoch: 3 Idx: 5000 Loss: 0.018454471640329102
Epoch: 4 Idx: 0 Loss: 0.017668198358944783
Epoch: 4 Idx: 5000 Loss: 0.013894918702747356
Epoch: 5 Idx: 0 Loss: 0.019259416936835694
Epoch: 5 Idx: 5000 Loss: 0.03439389618947543
Epoch: 6 Idx: 0 Loss: 0.02106051875611891
Epoch: 6 Idx: 5000 Loss: 0.02464797850797612
Epoch: 7 Idx: 0 Loss: 0.026524127889980834
Epoch: 7 Idx: 5000 Loss: 0.018626646376675444
Epoch: 8 Idx: 0 Loss: 0.02872390304898193
Epoch: 8 Idx: 5000 Loss: 0.03501336874471299
Epoch: 9 Idx: 0 Loss: 0.03863344207738402
Epoch: 9 Idx: 5000 Loss: 0.022455583912995493
Epoch: 10 Idx: 0 Loss: 0.021802847448803425
Epoch: 10 Idx: 5000 Loss: 0.011839541270742118
Epoch: 11 Idx: 0 Loss: 0.018190864852730082
Epoch: 11 Idx: 5000 Loss: 0.02006185513002791
Epoch: 12 Idx: 0 Loss: 0.02516861288793874
Epoch: 12 Idx: 5000 Loss: 0.02458209099034794
Epoch: 13 Idx: 0 Loss: 0.024124073043729717
Epoch: 13 Idx: 5000 Loss: 0.026254635154374314
Epoch: 14 Idx: 0 Loss: 0.012294760286142542
Epoch: 14 Idx: 5000 Loss: 0.019883765421423123
Epoch: 15 Idx: 0 Loss: 0.012421650403912425
Epoch: 15 Idx: 5000 Loss: 0.03784929576579378
Epoch: 16 Idx: 0 Loss: 0.015124116366499393
Epoch: 16 Idx: 5000 Loss: 0.020199609205378406
Epoch: 17 Idx: 0 Loss: 0.014420852375786098
Epoch: 17 Idx: 5000 Loss: 0.012541410749400997
Epoch: 18 Idx: 0 Loss: 0.016666133978152642
Epoch: 18 Idx: 5000 Loss: 0.025296024823095706
Epoch: 19 Idx: 0 Loss: 0.037164703162892804
Epoch: 19 Idx: 5000 Loss: 0.011002771768002843
Epoch: 20 Idx: 0 Loss: 0.028764482027265146
Epoch: 20 Idx: 5000 Loss: 0.028098144823612795
Epoch: 21 Idx: 0 Loss: 0.03432439739221345
Epoch: 21 Idx: 5000 Loss: 0.020546521645173947
Epoch: 22 Idx: 0 Loss: 0.022399535129984948
Epoch: 22 Idx: 5000 Loss: 0.008958430594294874
Epoch: 23 Idx: 0 Loss: 0.017731124129551034
Epoch: 23 Idx: 5000 Loss: 0.01869987803189126
Epoch: 24 Idx: 0 Loss: 0.019781298714834385
Epoch: 24 Idx: 5000 Loss: 0.02451734432453534
Epoch: 25 Idx: 0 Loss: 0.014584193020956402
Epoch: 25 Idx: 5000 Loss: 0.012176687187158511
Epoch: 26 Idx: 0 Loss: 0.020164609725869448
Epoch: 26 Idx: 5000 Loss: 0.019139865400392513
Epoch: 27 Idx: 0 Loss: 0.02461957143002911
Epoch: 27 Idx: 5000 Loss: 0.009464656994790457
Epoch: 28 Idx: 0 Loss: 0.020008599189476825
Epoch: 28 Idx: 5000 Loss: 0.019555014403789924
Epoch: 29 Idx: 0 Loss: 0.024940707004446637
Epoch: 29 Idx: 5000 Loss: 0.01926557478149792
Epoch: 30 Idx: 0 Loss: 0.01841214342264961
Epoch: 30 Idx: 5000 Loss: 0.01461442234778786
Epoch: 31 Idx: 0 Loss: 0.01511507512197953
Epoch: 31 Idx: 5000 Loss: 0.033573915629438386
Epoch: 32 Idx: 0 Loss: 0.01619093023863987
Epoch: 32 Idx: 5000 Loss: 0.010586589117723408
Epoch: 33 Idx: 0 Loss: 0.021398385561077614
Epoch: 33 Idx: 5000 Loss: 0.01984837103825307
Epoch: 34 Idx: 0 Loss: 0.02517126668520558
Epoch: 34 Idx: 5000 Loss: 0.01925366647779143
Epoch: 35 Idx: 0 Loss: 0.009292646883450927
Epoch: 35 Idx: 5000 Loss: 0.02003830614917529
Epoch: 36 Idx: 0 Loss: 0.047484655730005085
Epoch: 36 Idx: 5000 Loss: 0.012290739240309637
Epoch: 37 Idx: 0 Loss: 0.025411676721847805
Epoch: 37 Idx: 5000 Loss: 0.011547608124510157
Epoch: 38 Idx: 0 Loss: 0.013801158941858967
Epoch: 38 Idx: 5000 Loss: 0.03642361034047946
Epoch: 39 Idx: 0 Loss: 0.011787875828579
Epoch: 39 Idx: 5000 Loss: 0.02219090067587035
Epoch: 40 Idx: 0 Loss: 0.021063596620831695
Epoch: 40 Idx: 5000 Loss: 0.022708028007861508
Epoch: 41 Idx: 0 Loss: 0.019433750763154463
Epoch: 41 Idx: 5000 Loss: 0.01821999871767046
Epoch: 42 Idx: 0 Loss: 0.03608198305793185
Epoch: 42 Idx: 5000 Loss: 0.03290316900254561
Epoch: 43 Idx: 0 Loss: 0.022297380409136508
Epoch: 43 Idx: 5000 Loss: 0.016583651547073856
Epoch: 44 Idx: 0 Loss: 0.02713892411092633
Epoch: 44 Idx: 5000 Loss: 0.010837493329018063
Epoch: 45 Idx: 0 Loss: 0.022265422412712114
Epoch: 45 Idx: 5000 Loss: 0.019712818845657024
Epoch: 46 Idx: 0 Loss: 0.028715615019438312
Epoch: 46 Idx: 5000 Loss: 0.023836096734897345
Epoch: 47 Idx: 0 Loss: 0.017812895687889095
Epoch: 47 Idx: 5000 Loss: 0.01760482753026698
Epoch: 48 Idx: 0 Loss: 0.01551286665970777
Epoch: 48 Idx: 5000 Loss: 0.025490346336003986
Epoch: 49 Idx: 0 Loss: 0.014778988682248067
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 456, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 313, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
KeyboardInterrupt
thon3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
h Disk quota exceeded

xceeded
 Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:03:30 2020
Results reported at Tue Sep  1 20:03:30 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 2 Output/test_anatomy_aml_bagofnbrs_wtpath1_2.pkl Models/anatomy_aml_bagofnbrs_wtpath1_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22138.13 sec.
    Max Memory :                                 2915 MB
    Average Memory :                             2721.40 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40502.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22204 sec.
    Turnaround time :                            22186 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc233>
Subject: Job 3289796: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs_wtpath2_2.pkl Models/anatomy_aml_bagofnbrs_wtpath2_2.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs_wtpath2_2.pkl Models/anatomy_aml_bagofnbrs_wtpath2_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc233>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:05:56 2020
Results reported at Tue Sep  1 20:05:56 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs_wtpath2_2.pkl Models/anatomy_aml_bagofnbrs_wtpath2_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22324.88 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2712.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22355 sec.
    Turnaround time :                            22332 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc241>
Subject: Job 3289798: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs_wtpath3_2.pkl Models/anatomy_aml_bagofnbrs_wtpath3_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs_wtpath3_2.pkl Models/anatomy_aml_bagofnbrs_wtpath3_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc241>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:34:12 2020
Results reported at Tue Sep  1 20:34:12 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs_wtpath3_2.pkl Models/anatomy_aml_bagofnbrs_wtpath3_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24021.92 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2727.12 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   24043 sec.
    Turnaround time :                            24028 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc265>
Subject: Job 3289800: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs_wtpath4_2.pkl Models/anatomy_aml_bagofnbrs_wtpath4_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs_wtpath4_2.pkl Models/anatomy_aml_bagofnbrs_wtpath4_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc265>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:03:44 2020
Results reported at Tue Sep  1 21:03:44 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs_wtpath4_2.pkl Models/anatomy_aml_bagofnbrs_wtpath4_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25576.10 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2726.98 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25807 sec.
    Turnaround time :                            25800 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc207>
Subject: Job 3289802: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs_wtpath5_2.pkl Models/anatomy_aml_bagofnbrs_wtpath5_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs_wtpath5_2.pkl Models/anatomy_aml_bagofnbrs_wtpath5_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc207>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:34:30 2020
Results reported at Tue Sep  1 21:34:30 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs_wtpath5_2.pkl Models/anatomy_aml_bagofnbrs_wtpath5_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27641.78 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2724.72 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27653 sec.
    Turnaround time :                            27646 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc204>
Subject: Job 3289804: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs_wtpath7_2.pkl Models/anatomy_aml_bagofnbrs_wtpath7_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs_wtpath7_2.pkl Models/anatomy_aml_bagofnbrs_wtpath7_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc204>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:54:00 2020
Results reported at Tue Sep  1 21:54:00 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs_wtpath7_2.pkl Models/anatomy_aml_bagofnbrs_wtpath7_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28752.39 sec.
    Max Memory :                                 2916 MB
    Average Memory :                             2717.47 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40501.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28815 sec.
    Turnaround time :                            28816 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc222>
Subject: Job 3289806: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs_wtpath8_2.pkl Models/anatomy_aml_bagofnbrs_wtpath8_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs_wtpath8_2.pkl Models/anatomy_aml_bagofnbrs_wtpath8_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc222>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 22:21:28 2020
Results reported at Tue Sep  1 22:21:28 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs_wtpath8_2.pkl Models/anatomy_aml_bagofnbrs_wtpath8_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30452.30 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2720.00 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30463 sec.
    Turnaround time :                            30464 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc262>
Subject: Job 3289832: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 2 Output/test_anatomy_aml_bagofnbrs_wtpath152_2.pkl Models/anatomy_aml_bagofnbrs_wtpath152_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 2 Output/test_anatomy_aml_bagofnbrs_wtpath152_2.pkl Models/anatomy_aml_bagofnbrs_wtpath152_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc262>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 2 Output/test_anatomy_aml_bagofnbrs_wtpath152_2.pkl Models/anatomy_aml_bagofnbrs_wtpath152_2.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80468.74 sec.
    Max Memory :                                 2728 MB
    Average Memory :                             2647.10 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40689.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80701 sec.
    Turnaround time :                            80702 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc264>
Subject: Job 3289830: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 2 Output/test_anatomy_aml_bagofnbrs_wtpath80_2.pkl Models/anatomy_aml_bagofnbrs_wtpath80_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 2 Output/test_anatomy_aml_bagofnbrs_wtpath80_2.pkl Models/anatomy_aml_bagofnbrs_wtpath80_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc264>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 2 Output/test_anatomy_aml_bagofnbrs_wtpath80_2.pkl Models/anatomy_aml_bagofnbrs_wtpath80_2.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80689.45 sec.
    Max Memory :                                 2738 MB
    Average Memory :                             2663.48 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40679.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80701 sec.
    Turnaround time :                            80702 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc273>
Subject: Job 3289828: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 2 Output/test_anatomy_aml_bagofnbrs_wtpath40_2.pkl Models/anatomy_aml_bagofnbrs_wtpath40_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 2 Output/test_anatomy_aml_bagofnbrs_wtpath40_2.pkl Models/anatomy_aml_bagofnbrs_wtpath40_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc273>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 2 Output/test_anatomy_aml_bagofnbrs_wtpath40_2.pkl Models/anatomy_aml_bagofnbrs_wtpath40_2.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80470.89 sec.
    Max Memory :                                 2921 MB
    Average Memory :                             2713.21 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40496.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80702 sec.
    Turnaround time :                            80703 sec.

The output (if any) is above this job summary.

