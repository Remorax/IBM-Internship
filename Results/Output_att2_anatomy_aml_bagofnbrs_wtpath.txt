Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22364357617092337
Epoch: 0 Idx: 5000 Loss: 0.03015932957141387
Epoch: 1 Idx: 0 Loss: 0.03421549941766191
Epoch: 1 Idx: 5000 Loss: 0.011288498096933785
Epoch: 2 Idx: 0 Loss: 0.02595961831971285
Epoch: 2 Idx: 5000 Loss: 0.021099186086120266
Epoch: 3 Idx: 0 Loss: 0.04095608377609228
Epoch: 3 Idx: 5000 Loss: 0.03562598146974658
Epoch: 4 Idx: 0 Loss: 0.012387716075559977
Epoch: 4 Idx: 5000 Loss: 0.021875856999639877
Epoch: 5 Idx: 0 Loss: 0.016103161291113086
Epoch: 5 Idx: 5000 Loss: 0.018151888637308558
Epoch: 6 Idx: 0 Loss: 0.007792126906297296
Epoch: 6 Idx: 5000 Loss: 0.012944421245131009
Epoch: 7 Idx: 0 Loss: 0.02192083341924199
Epoch: 7 Idx: 5000 Loss: 0.03084371951556792
Epoch: 8 Idx: 0 Loss: 0.021485783631673954
Epoch: 8 Idx: 5000 Loss: 0.026763699476761917
Epoch: 9 Idx: 0 Loss: 0.010910132007686965
Epoch: 9 Idx: 5000 Loss: 0.015019487130445801
Epoch: 10 Idx: 0 Loss: 0.017989675433364636
Epoch: 10 Idx: 5000 Loss: 0.016177267997931177
Epoch: 11 Idx: 0 Loss: 0.02156817268260294
Epoch: 11 Idx: 5000 Loss: 0.02376940525641157
Epoch: 12 Idx: 0 Loss: 0.016381866060236675
Epoch: 12 Idx: 5000 Loss: 0.03418528086216881
Epoch: 13 Idx: 0 Loss: 0.018477276912983738
Epoch: 13 Idx: 5000 Loss: 0.015262079624116038
Epoch: 14 Idx: 0 Loss: 0.01741642335697048
Epoch: 14 Idx: 5000 Loss: 0.018982039118295668
Epoch: 15 Idx: 0 Loss: 0.02103384875307594
Epoch: 15 Idx: 5000 Loss: 0.014238113820642988
Epoch: 16 Idx: 0 Loss: 0.017805104085099277
Epoch: 16 Idx: 5000 Loss: 0.01329327027952248
Epoch: 17 Idx: 0 Loss: 0.020647282962308378
Epoch: 17 Idx: 5000 Loss: 0.022617765790643476
Epoch: 18 Idx: 0 Loss: 0.014236997651373654
Epoch: 18 Idx: 5000 Loss: 0.017708131777503428
Epoch: 19 Idx: 0 Loss: 0.016937148570366874
Epoch: 19 Idx: 5000 Loss: 0.022549967933369258
Epoch: 20 Idx: 0 Loss: 0.017707518090603655
Epoch: 20 Idx: 5000 Loss: 0.018413682153574074
Epoch: 21 Idx: 0 Loss: 0.019774353185880177
Epoch: 21 Idx: 5000 Loss: 0.005442634777824779
Epoch: 22 Idx: 0 Loss: 0.018988509570046313
Epoch: 22 Idx: 5000 Loss: 0.013303034306446863
Epoch: 23 Idx: 0 Loss: 0.014507398932976074
Epoch: 23 Idx: 5000 Loss: 0.04661085282440615
Epoch: 24 Idx: 0 Loss: 0.018873139763747746
Epoch: 24 Idx: 5000 Loss: 0.010785342227473666
Epoch: 25 Idx: 0 Loss: 0.01794746344602238
Epoch: 25 Idx: 5000 Loss: 0.02684844906672721
Epoch: 26 Idx: 0 Loss: 0.025058313663145135
Epoch: 26 Idx: 5000 Loss: 0.016960068174442622
Epoch: 27 Idx: 0 Loss: 0.0205048049014833
Epoch: 27 Idx: 5000 Loss: 0.019033715081724203
Epoch: 28 Idx: 0 Loss: 0.021306595139263335
Epoch: 28 Idx: 5000 Loss: 0.020851818349430687
Epoch: 29 Idx: 0 Loss: 0.01204350202042951
Epoch: 29 Idx: 5000 Loss: 0.018398503197185632
Epoch: 30 Idx: 0 Loss: 0.026942273898405728
Epoch: 30 Idx: 5000 Loss: 0.03791376480563358
Epoch: 31 Idx: 0 Loss: 0.016947699161288222
Epoch: 31 Idx: 5000 Loss: 0.01931805555127719
Epoch: 32 Idx: 0 Loss: 0.0380883405156425
Epoch: 32 Idx: 5000 Loss: 0.022468304679676074
Epoch: 33 Idx: 0 Loss: 0.019404141531988908
Epoch: 33 Idx: 5000 Loss: 0.02062539078817431
Epoch: 34 Idx: 0 Loss: 0.02503879350520224
Epoch: 34 Idx: 5000 Loss: 0.01248565783432356
Epoch: 35 Idx: 0 Loss: 0.021491621319914122
Epoch: 35 Idx: 5000 Loss: 0.0361233576579922
Epoch: 36 Idx: 0 Loss: 0.016451296481518395
Epoch: 36 Idx: 5000 Loss: 0.013864165660006922
Epoch: 37 Idx: 0 Loss: 0.02451352438965858
Epoch: 37 Idx: 5000 Loss: 0.01580668490102491
Epoch: 38 Idx: 0 Loss: 0.01609355367297626
Epoch: 38 Idx: 5000 Loss: 0.022453675360626625
Epoch: 39 Idx: 0 Loss: 0.023524325511839916
Epoch: 39 Idx: 5000 Loss: 0.056895674551710886
Epoch: 40 Idx: 0 Loss: 0.017846447600971475
Epoch: 40 Idx: 5000 Loss: 0.017653039730651986
Epoch: 41 Idx: 0 Loss: 0.007315648173094618
Epoch: 41 Idx: 5000 Loss: 0.020105928508572003
Epoch: 42 Idx: 0 Loss: 0.012998320981231513
Epoch: 42 Idx: 5000 Loss: 0.0190489182443479
Epoch: 43 Idx: 0 Loss: 0.017881652856696156
Epoch: 43 Idx: 5000 Loss: 0.02566608083039661
Epoch: 44 Idx: 0 Loss: 0.021243088170025275
Epoch: 44 Idx: 5000 Loss: 0.019285672633130264
Epoch: 45 Idx: 0 Loss: 0.0318332278166587
Epoch: 45 Idx: 5000 Loss: 0.03251291619229487
Epoch: 46 Idx: 0 Loss: 0.01807107244569736
Epoch: 46 Idx: 5000 Loss: 0.019117811684502335
Epoch: 47 Idx: 0 Loss: 0.01755844377910972
Epoch: 47 Idx: 5000 Loss: 0.020837652759827956
Epoch: 48 Idx: 0 Loss: 0.016525942977434895
Epoch: 48 Idx: 5000 Loss: 0.027608978052573573
Epoch: 49 Idx: 0 Loss: 0.01792869420827591
Epoch: 49 Idx: 5000 Loss: 0.024128297200720357
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisiTraining siTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.16996388444935187
Epoch: 0 Idx: 5000 Loss: 0.01120528001888764
Epoch: 1 Idx: 0 Loss: 0.03937449623968817
Epoch: 1 Idx: 5000 Loss: 0.010045632554579475
Epoch: 2 Idx: 0 Loss: 0.020067331235838946
Epoch: 2 Idx: 5000 Loss: 0.017815097737215783
Epoch: 3 Idx: 0 Loss: 0.016006305458121643
Epoch: 3 Idx: 5000 Loss: 0.024791840330834626
Epoch: 4 Idx: 0 Loss: 0.019170104173466162
Epoch: 4 Idx: 5000 Loss: 0.03751895186868393
Epoch: 5 Idx: 0 Loss: 0.010843263566205621
Epoch: 5 Idx: 5000 Loss: 0.030308014215981584
Epoch: 6 Idx: 0 Loss: 0.03237854341101169
Epoch: 6 Idx: 5000 Loss: 0.013222399560190987
Epoch: 7 Idx: 0 Loss: 0.027725593685713666
Epoch: 7 Idx: 5000 Loss: 0.028069954666092932
Epoch: 8 Idx: 0 Loss: 0.03428177188940392
Epoch: 8 Idx: 5000 Loss: 0.02711137104739456
Epoch: 9 Idx: 0 Loss: 0.01934722736217954
Epoch: 9 Idx: 5000 Loss: 0.016791924236413847
Epoch: 10 Idx: 0 Loss: 0.008955202873470807
Epoch: 10 Idx: 5000 Loss: 0.0140275539090629
Epoch: 11 Idx: 0 Loss: 0.01538417609877934
Epoch: 11 Idx: 5000 Loss: 0.03671586654617419
Epoch: 12 Idx: 0 Loss: 0.013502288583323141
Epoch: 12 Idx: 5000 Loss: 0.021982139427772926
Epoch: 13 Idx: 0 Loss: 0.04145367976277146
Epoch: 13 Idx: 5000 Loss: 0.019966677758659207
Epoch: 14 Idx: 0 Loss: 0.02486978805499829
Epoch: 14 Idx: 5000 Loss: 0.014218072132678667
Epoch: 15 Idx: 0 Loss: 0.021594525412446545
Epoch: 15 Idx: 5000 Loss: 0.019552795111683743
Epoch: 16 Idx: 0 Loss: 0.01733427366333095
Epoch: 16 Idx: 5000 Loss: 0.028029154427676298
Epoch: 17 Idx: 0 Loss: 0.02413208780918502
Epoch: 17 Idx: 5000 Loss: 0.01895493310446572
Epoch: 18 Idx: 0 Loss: 0.028246723434794195
Epoch: 18 Idx: 5000 Loss: 0.02886762553226266
Epoch: 19 Idx: 0 Loss: 0.01612364623603431
Epoch: 19 Idx: 5000 Loss: 0.026030900663590567
Epoch: 20 Idx: 0 Loss: 0.02473819510612059
Epoch: 20 Idx: 5000 Loss: 0.023858855108697896
Epoch: 21 Idx: 0 Loss: 0.03059831846126535
Epoch: 21 Idx: 5000 Loss: 0.024991460115418175
Epoch: 22 Idx: 0 Loss: 0.017277945332322436
Epoch: 22 Idx: 5000 Loss: 0.01254544253757686
Epoch: 23 Idx: 0 Loss: 0.014464476286662104
Epoch: 23 Idx: 5000 Loss: 0.012271477955429743
Epoch: 24 Idx: 0 Loss: 0.023068699930680343
Epoch: 24 Idx: 5000 Loss: 0.01905304804111521
Epoch: 25 Idx: 0 Loss: 0.01161785305891041
Epoch: 25 Idx: 5000 Loss: 0.020992327709580563
Epoch: 26 Idx: 0 Loss: 0.022283119298006426
Epoch: 26 Idx: 5000 Loss: 0.041868368320863325
Epoch: 27 Idx: 0 Loss: 0.01937552220478493
Epoch: 27 Idx: 5000 Loss: 0.018737160335885787
Epoch: 28 Idx: 0 Loss: 0.029797623168372377
Epoch: 28 Idx: 5000 Loss: 0.02873065754497984
Epoch: 29 Idx: 0 Loss: 0.021352454017970236
Epoch: 29 Idx: 5000 Loss: 0.020405865179987298
Epoch: 30 Idx: 0 Loss: 0.023422076658813934
Epoch: 30 Idx: 5000 Loss: 0.032879058677770606
Epoch: 31 Idx: 0 Loss: 0.008124493243895362
Epoch: 31 Idx: 5000 Loss: 0.015437015415257918
Epoch: 32 Idx: 0 Loss: 0.019203899954022066
Epoch: 32 Idx: 5000 Loss: 0.04454618851553034
Epoch: 33 Idx: 0 Loss: 0.019760110031017894
Epoch: 33 Idx: 5000 Loss: 0.02696839949129721
Epoch: 34 Idx: 0 Loss: 0.021504309693330247
Epoch: 34 Idx: 5000 Loss: 0.021053341286921365
Epoch: 35 Idx: 0 Loss: 0.016096115416541063
Epoch: 35 Idx: 5000 Loss: 0.022981335559838917
Epoch: 36 Idx: 0 Loss: 0.017990412576837252
Epoch: 36 Idx: 5000 Loss: 0.016520613290043416
Epoch: 37 Idx: 0 Loss: 0.03359530605365764
Epoch: 37 Idx: 5000 Loss: 0.013169775264547733
Epoch: 38 Idx: 0 Loss: 0.022090789983338518
Epoch: 38 Idx: 5000 Loss: 0.016117052196112775
Epoch: 39 Idx: 0 Loss: 0.023583370687175607
Epoch: 39 Idx: 5000 Loss: 0.026725262392413407
Epoch: 40 Idx: 0 Loss: 0.02597716445129124
Epoch: 40 Idx: 5000 Loss: 0.032386790860536474
Epoch: 41 Idx: 0 Loss: 0.04304754385110489
Epoch: 41 Idx: 5000 Loss: 0.0125978707745409
Epoch: 42 Idx: 0 Loss: 0.02640941083713965
Epoch: 42 Idx: 5000 Loss: 0.016794214334398018
Epoch: 43 Idx: 0 Loss: 0.019975754349437366
Epoch: 43 Idx: 5000 Loss: 0.030917418206208747
Epoch: 44 Idx: 0 Loss: 0.011485591183269927
Epoch: 44 Idx: 5000 Loss: 0.021469659282658923
Epoch: 45 Idx: 0 Loss: 0.034602352495092886
Epoch: 45 Idx: 5000 Loss: 0.016779009492055255
Epoch: 46 Idx: 0 Loss: 0.01995461541280339
Epoch: 46 Idx: 5000 Loss: 0.017017917301610497
Epoch: 47 Idx: 0 Loss: 0.020269711758567843
Epoch: 47 Idx: 5000 Loss: 0.011482005618410816
Epoch: 48 Idx: 0 Loss: 0.029717108976055152
Epoch: 48 Idx: 5000 Loss: 0.029926946140509936
Epoch: 49 Idx: 0 Loss: 0.022994795409956827
Epoch: 49 Idx: 5000 Loss: 0.02321807852372045
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
ze: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21028738828880333
Epoch: 0 Idx: 5000 Loss: 0.015855176248696777
Epoch: 1 Idx: 0 Loss: 0.020519633608230895
Epoch: 1 Idx: 5000 Loss: 0.012402625909098935
Epoch: 2 Idx: 0 Loss: 0.02456463341697706
Epoch: 2 Idx: 5000 Loss: 0.024890815677964938
Epoch: 3 Idx: 0 Loss: 0.020469626057166908
Epoch: 3 Idx: 5000 Loss: 0.012851748270285474
Epoch: 4 Idx: 0 Loss: 0.03609988945340527
Epoch: 4 Idx: 5000 Loss: 0.017622454761864755
Epoch: 5 Idx: 0 Loss: 0.025165772601880056
Epoch: 5 Idx: 5000 Loss: 0.020414642850011797
Epoch: 6 Idx: 0 Loss: 0.01806321454347389
Epoch: 6 Idx: 5000 Loss: 0.015616044490129126
Epoch: 7 Idx: 0 Loss: 0.023281689025253013
Epoch: 7 Idx: 5000 Loss: 0.020057532198623572
Epoch: 8 Idx: 0 Loss: 0.025906518671402333
Epoch: 8 Idx: 5000 Loss: 0.013572673047755954
Epoch: 9 Idx: 0 Loss: 0.041474319554030886
Epoch: 9 Idx: 5000 Loss: 0.01086409749334849
Epoch: 10 Idx: 0 Loss: 0.025847823734107492
Epoch: 10 Idx: 5000 Loss: 0.01714613277394629
Epoch: 11 Idx: 0 Loss: 0.02765418375952631
Epoch: 11 Idx: 5000 Loss: 0.014990622622427546
Epoch: 12 Idx: 0 Loss: 0.022935762291458897
Epoch: 12 Idx: 5000 Loss: 0.031422415649741214
Epoch: 13 Idx: 0 Loss: 0.03677378368683126
Epoch: 13 Idx: 5000 Loss: 0.021932333877903082
Epoch: 14 Idx: 0 Loss: 0.012467032700019691
Epoch: 14 Idx: 5000 Loss: 0.019772802005041203
Epoch: 15 Idx: 0 Loss: 0.027986050877582175
Epoch: 15 Idx: 5000 Loss: 0.042922190760386156
Epoch: 16 Idx: 0 Loss: 0.022737259904814584
Epoch: 16 Idx: 5000 Loss: 0.018560826892214007
Epoch: 17 Idx: 0 Loss: 0.014234250079721802
Epoch: 17 Idx: 5000 Loss: 0.024149665079959023
Epoch: 18 Idx: 0 Loss: 0.020677206891292398
Epoch: 18 Idx: 5000 Loss: 0.033410807482196284
Epoch: 19 Idx: 0 Loss: 0.01783109870895601
Epoch: 19 Idx: 5000 Loss: 0.012423421524982622
Epoch: 20 Idx: 0 Loss: 0.04128497400855143
Epoch: 20 Idx: 5000 Loss: 0.024195322405598685
Epoch: 21 Idx: 0 Loss: 0.01958423246787492
Epoch: 21 Idx: 5000 Loss: 0.024232384991490573
Epoch: 22 Idx: 0 Loss: 0.031409348483581005
Epoch: 22 Idx: 5000 Loss: 0.008363146411667487
Epoch: 23 Idx: 0 Loss: 0.018127114509836374
Epoch: 23 Idx: 5000 Loss: 0.013280140822490901
Epoch: 24 Idx: 0 Loss: 0.03543313250586138
Epoch: 24 Idx: 5000 Loss: 0.009258879693960794
Epoch: 25 Idx: 0 Loss: 0.028302418400199952
Epoch: 25 Idx: 5000 Loss: 0.01741417548463462
Epoch: 26 Idx: 0 Loss: 0.014580875468258961
Epoch: 26 Idx: 5000 Loss: 0.010042153242236123
Epoch: 27 Idx: 0 Loss: 0.031174985758648854
Epoch: 27 Idx: 5000 Loss: 0.03880322089772547
Epoch: 28 Idx: 0 Loss: 0.02882507644627022
Epoch: 28 Idx: 5000 Loss: 0.02451315387564919
Epoch: 29 Idx: 0 Loss: 0.020342374845340946
Epoch: 29 Idx: 5000 Loss: 0.02671944056401441
Epoch: 30 Idx: 0 Loss: 0.023286856026536663
Epoch: 30 Idx: 5000 Loss: 0.017084441739557442
Epoch: 31 Idx: 0 Loss: 0.027068215516788886
Epoch: 31 Idx: 5000 Loss: 0.01829000715309772
Epoch: 32 Idx: 0 Loss: 0.02274058197908736
Epoch: 32 Idx: 5000 Loss: 0.018681299940195316
Epoch: 33 Idx: 0 Loss: 0.03629420902225029
Epoch: 33 Idx: 5000 Loss: 0.014535169510877842
Epoch: 34 Idx: 0 Loss: 0.013107083842986117
Epoch: 34 Idx: 5000 Loss: 0.008867355374141475
Epoch: 35 Idx: 0 Loss: 0.024881395610550805
Epoch: 35 Idx: 5000 Loss: 0.024433142790246522
Epoch: 36 Idx: 0 Loss: 0.026955401899869603
Epoch: 36 Idx: 5000 Loss: 0.02306694878439437
Epoch: 37 Idx: 0 Loss: 0.015381964683417775
Epoch: 37 Idx: 5000 Loss: 0.011323366539562185
Epoch: 38 Idx: 0 Loss: 0.008507083547338888
Epoch: 38 Idx: 5000 Loss: 0.019010473141051178
Epoch: 39 Idx: 0 Loss: 0.013029602964080539
Epoch: 39 Idx: 5000 Loss: 0.018648178422467875
Epoch: 40 Idx: 0 Loss: 0.015167307019206338
Epoch: 40 Idx: 5000 Loss: 0.012395076384039338
Epoch: 41 Idx: 0 Loss: 0.020381198891032855
Epoch: 41 Idx: 5000 Loss: 0.012559644931581244
Epoch: 42 Idx: 0 Loss: 0.024544413448439998
Epoch: 42 Idx: 5000 Loss: 0.030322832788923223
Epoch: 43 Idx: 0 Loss: 0.029758402590495982
Epoch: 43 Idx: 5000 Loss: 0.022353922487492153
Epoch: 44 Idx: 0 Loss: 0.01822833474655582
Epoch: 44 Idx: 5000 Loss: 0.01640135615310188
Epoch: 45 Idx: 0 Loss: 0.028368501609124398
Epoch: 45 Idx: 5000 Loss: 0.010936753007484461
Epoch: 46 Idx: 0 Loss: 0.024546927489414453
Epoch: 46 Idx: 5000 Loss: 0.011694143452216605
Epoch: 47 Idx: 0 Loss: 0.021895834457606407
Epoch: 47 Idx: 5000 Loss: 0.025637826558994145
Epoch: 48 Idx: 0 Loss: 0.02114465623299943
Epoch: 48 Idx: 5000 Loss: 0.021396081352986908
Epoch: 49 Idx: 0 Loss: 0.013994108335924564
Epoch: 49 Idx: 5000 Loss: 0.030853257844484398
Len (direct inputs):  101
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
7500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21564269322711038
Epoch: 0 Idx: 5000 Loss: 0.04394660596988708
Epoch: 1 Idx: 0 Loss: 0.018102431452493146
Epoch: 1 Idx: 5000 Loss: 0.021959301394486184
Epoch: 2 Idx: 0 Loss: 0.026029606271231763
Epoch: 2 Idx: 5000 Loss: 0.012171796816046937
Epoch: 3 Idx: 0 Loss: 0.026692756604592697
Epoch: 3 Idx: 5000 Loss: 0.015641093727062426
Epoch: 4 Idx: 0 Loss: 0.025212621075280925
Epoch: 4 Idx: 5000 Loss: 0.013016292231328885
Epoch: 5 Idx: 0 Loss: 0.013392045223202056
Epoch: 5 Idx: 5000 Loss: 0.038353283008191866
Epoch: 6 Idx: 0 Loss: 0.023874942244610425
Epoch: 6 Idx: 5000 Loss: 0.022099096282277384
Epoch: 7 Idx: 0 Loss: 0.02973669408824105
Epoch: 7 Idx: 5000 Loss: 0.017935562485742965
Epoch: 8 Idx: 0 Loss: 0.013231953782516983
Epoch: 8 Idx: 5000 Loss: 0.0225677742189946
Epoch: 9 Idx: 0 Loss: 0.02735183159870361
Epoch: 9 Idx: 5000 Loss: 0.01759679419639092
Epoch: 10 Idx: 0 Loss: 0.014865381688977022
Epoch: 10 Idx: 5000 Loss: 0.02254969448509747
Epoch: 11 Idx: 0 Loss: 0.035218376720011095
Epoch: 11 Idx: 5000 Loss: 0.013602203264943484
Epoch: 12 Idx: 0 Loss: 0.01964375438416534
Epoch: 12 Idx: 5000 Loss: 0.02052552189935887
Epoch: 13 Idx: 0 Loss: 0.014258665644739578
Epoch: 13 Idx: 5000 Loss: 0.02340369566397229
Epoch: 14 Idx: 0 Loss: 0.025127784033656828
Epoch: 14 Idx: 5000 Loss: 0.03922920832863006
Epoch: 15 Idx: 0 Loss: 0.02936115074835424
Epoch: 15 Idx: 5000 Loss: 0.03255242507018302
Epoch: 16 Idx: 0 Loss: 0.052371429502852035
Epoch: 16 Idx: 5000 Loss: 0.019113319853979738
Epoch: 17 Idx: 0 Loss: 0.01263857121049769
Epoch: 17 Idx: 5000 Loss: 0.018925445121784498
Epoch: 18 Idx: 0 Loss: 0.016327631479297753
Epoch: 18 Idx: 5000 Loss: 0.022507327566479815
Epoch: 19 Idx: 0 Loss: 0.02095331745805438
Epoch: 19 Idx: 5000 Loss: 0.02365796645957552
Epoch: 20 Idx: 0 Loss: 0.01519554810452315
Epoch: 20 Idx: 5000 Loss: 0.021661141389495348
Epoch: 21 Idx: 0 Loss: 0.027297758643286284
Epoch: 21 Idx: 5000 Loss: 0.018063849074234077
Epoch: 22 Idx: 0 Loss: 0.0314851291639062
Epoch: 22 Idx: 5000 Loss: 0.016808879458555026
Epoch: 23 Idx: 0 Loss: 0.009854945335839804
Epoch: 23 Idx: 5000 Loss: 0.008910318261666638
Epoch: 24 Idx: 0 Loss: 0.02612281080209148
Epoch: 24 Idx: 5000 Loss: 0.01884231779542318
Epoch: 25 Idx: 0 Loss: 0.0437821252028298
Epoch: 25 Idx: 5000 Loss: 0.019466733760114832
Epoch: 26 Idx: 0 Loss: 0.02538055808200374
Epoch: 26 Idx: 5000 Loss: 0.03631081667866671
Epoch: 27 Idx: 0 Loss: 0.021206595287545232
Epoch: 27 Idx: 5000 Loss: 0.028117069114746512
Epoch: 28 Idx: 0 Loss: 0.00972526718113961
Epoch: 28 Idx: 5000 Loss: 0.040308802391835576
Epoch: 29 Idx: 0 Loss: 0.014383352379807455
Epoch: 29 Idx: 5000 Loss: 0.020809335263082516
Epoch: 30 Idx: 0 Loss: 0.043306903315944195
Epoch: 30 Idx: 5000 Loss: 0.023229611384838297
Epoch: 31 Idx: 0 Loss: 0.022423788132316352
Epoch: 31 Idx: 5000 Loss: 0.019498151340819243
Epoch: 32 Idx: 0 Loss: 0.024453974120036907
Epoch: 32 Idx: 5000 Loss: 0.009834773217787968
Epoch: 33 Idx: 0 Loss: 0.02244364674092369
Epoch: 33 Idx: 5000 Loss: 0.017602729956650798
Epoch: 34 Idx: 0 Loss: 0.020083594924879784
Epoch: 34 Idx: 5000 Loss: 0.009356753560968255
Epoch: 35 Idx: 0 Loss: 0.015570911989243014
Epoch: 35 Idx: 5000 Loss: 0.02247603841241547
Epoch: 36 Idx: 0 Loss: 0.019325942883897287
Epoch: 36 Idx: 5000 Loss: 0.01676726130357187
Epoch: 37 Idx: 0 Loss: 0.036769827226540146
Epoch: 37 Idx: 5000 Loss: 0.034141940480558984
Epoch: 38 Idx: 0 Loss: 0.01736606460960058
Epoch: 38 Idx: 5000 Loss: 0.03518229035564303
Epoch: 39 Idx: 0 Loss: 0.023772336846784217
Epoch: 39 Idx: 5000 Loss: 0.018532824981016077
Epoch: 40 Idx: 0 Loss: 0.02370623219853762
Epoch: 40 Idx: 5000 Loss: 0.006354099496971212
Epoch: 41 Idx: 0 Loss: 0.013286429777681071
Epoch: 41 Idx: 5000 Loss: 0.0256928425429974
Epoch: 42 Idx: 0 Loss: 0.023371330177563047
Epoch: 42 Idx: 5000 Loss: 0.0227232784943319
Epoch: 43 Idx: 0 Loss: 0.018654696205554665
Epoch: 43 Idx: 5000 Loss: 0.02762061281193991
Epoch: 44 Idx: 0 Loss: 0.017086150991187654
Epoch: 44 Idx: 5000 Loss: 0.01730962884393341
Epoch: 45 Idx: 0 Loss: 0.017065990377397075
Epoch: 45 Idx: 5000 Loss: 0.029243551688873094
Epoch: 46 Idx: 0 Loss: 0.0263293581505293
Epoch: 46 Idx: 5000 Loss: 0.012439005137081934
Epoch: 47 Idx: 0 Loss: 0.031032378001461962
Epoch: 47 Idx: 5000 Loss: 0.018794986748917844
Epoch: 48 Idx: 0 Loss: 0.03499930320096868
Epoch: 48 Idx: 5000 Loss: 0.014975779249633536
Epoch: 49 Idx: 0 Loss: 0.021036883640343618
Epoch: 49 Idx: 5000 Loss: 0.017747862388877163
Len (direct inputs):  91
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18767964527980094
Epoch: 0 Idx: 5000 Loss: 0.014079827917833423
Epoch: 1 Idx: 0 Loss: 0.02263312525750708
Epoch: 1 Idx: 5000 Loss: 0.03443750412562825
Epoch: 2 Idx: 0 Loss: 0.014660694549641043
Epoch: 2 Idx: 5000 Loss: 0.022931661886730716
Epoch: 3 Idx: 0 Loss: 0.017149453560564482
Epoch: 3 Idx: 5000 Loss: 0.01494429133430509
Epoch: 4 Idx: 0 Loss: 0.02199685711075955
Epoch: 4 Idx: 5000 Loss: 0.032803081532232145
Epoch: 5 Idx: 0 Loss: 0.014848389606024647
Epoch: 5 Idx: 5000 Loss: 0.023909603117170795
Epoch: 6 Idx: 0 Loss: 0.01910391757697169
Epoch: 6 Idx: 5000 Loss: 0.005995709639875172
Epoch: 7 Idx: 0 Loss: 0.022144895556191563
Epoch: 7 Idx: 5000 Loss: 0.012389243115005836
Epoch: 8 Idx: 0 Loss: 0.019794241801448537
Epoch: 8 Idx: 5000 Loss: 0.043223300126852854
Epoch: 9 Idx: 0 Loss: 0.010242377445402882
Epoch: 9 Idx: 5000 Loss: 0.04530577555796693
Epoch: 10 Idx: 0 Loss: 0.01213414426214799
Epoch: 10 Idx: 5000 Loss: 0.04123864791697869
Epoch: 11 Idx: 0 Loss: 0.009720217195449531
Epoch: 11 Idx: 5000 Loss: 0.01570624034877789
Epoch: 12 Idx: 0 Loss: 0.020191888585730386
Epoch: 12 Idx: 5000 Loss: 0.018073213143587626
Epoch: 13 Idx: 0 Loss: 0.013726386082648624
Epoch: 13 Idx: 5000 Loss: 0.020184483758363268
Epoch: 14 Idx: 0 Loss: 0.023359160311443708
Epoch: 14 Idx: 5000 Loss: 0.02842220753633006
Epoch: 15 Idx: 0 Loss: 0.02411728128594367
Epoch: 15 Idx: 5000 Loss: 0.024631670291840442
Epoch: 16 Idx: 0 Loss: 0.03149899826882664
Epoch: 16 Idx: 5000 Loss: 0.029405334417696015
Epoch: 17 Idx: 0 Loss: 0.03443248536445876
Epoch: 17 Idx: 5000 Loss: 0.012493742848905727
Epoch: 18 Idx: 0 Loss: 0.024093322855891684
Epoch: 18 Idx: 5000 Loss: 0.017006097212822716
Epoch: 19 Idx: 0 Loss: 0.011544878925012833
Epoch: 19 Idx: 5000 Loss: 0.021868033712830602
Epoch: 20 Idx: 0 Loss: 0.027844336234277907
Epoch: 20 Idx: 5000 Loss: 0.010661636906691268
Epoch: 21 Idx: 0 Loss: 0.01220069328471096
Epoch: 21 Idx: 5000 Loss: 0.012430040195691227
Epoch: 22 Idx: 0 Loss: 0.011849937629648926
Epoch: 22 Idx: 5000 Loss: 0.01797072079374951
Epoch: 23 Idx: 0 Loss: 0.04044192664330809
Epoch: 23 Idx: 5000 Loss: 0.032785916427083256
Epoch: 24 Idx: 0 Loss: 0.021833122739655396
Epoch: 24 Idx: 5000 Loss: 0.017083777077302437
Epoch: 25 Idx: 0 Loss: 0.02105253683148583
Epoch: 25 Idx: 5000 Loss: 0.028330458235027455
Epoch: 26 Idx: 0 Loss: 0.018912933170649994
Epoch: 26 Idx: 5000 Loss: 0.012463975226905842
Epoch: 27 Idx: 0 Loss: 0.03720857922778746
Epoch: 27 Idx: 5000 Loss: 0.01615671024781533
Epoch: 28 Idx: 0 Loss: 0.016073164460347806
Epoch: 28 Idx: 5000 Loss: 0.031509460187902724
Epoch: 29 Idx: 0 Loss: 0.01048158664431647
Epoch: 29 Idx: 5000 Loss: 0.014691378981318025
Epoch: 30 Idx: 0 Loss: 0.0293455559621484
Epoch: 30 Idx: 5000 Loss: 0.0170802098360366
Epoch: 31 Idx: 0 Loss: 0.005495986897435166
Epoch: 31 Idx: 5000 Loss: 0.016646724182159824
Epoch: 32 Idx: 0 Loss: 0.01729357849119065
Epoch: 32 Idx: 5000 Loss: 0.012540273501121411
Epoch: 33 Idx: 0 Loss: 0.006840059808492655
Epoch: 33 Idx: 5000 Loss: 0.020712241010265767
Epoch: 34 Idx: 0 Loss: 0.02062684758095428
Epoch: 34 Idx: 5000 Loss: 0.018741470690410652
Epoch: 35 Idx: 0 Loss: 0.02505091590362056
Epoch: 35 Idx: 5000 Loss: 0.020189772120914252
Epoch: 36 Idx: 0 Loss: 0.026972020721157536
Epoch: 36 Idx: 5000 Loss: 0.014343929970424225
Epoch: 37 Idx: 0 Loss: 0.018932536071577636
Epoch: 37 Idx: 5000 Loss: 0.017062151112157507
Epoch: 38 Idx: 0 Loss: 0.019196168216193226
Epoch: 38 Idx: 5000 Loss: 0.027250654487120454
Epoch: 39 Idx: 0 Loss: 0.013617889777344051
Epoch: 39 Idx: 5000 Loss: 0.025994423700337166
Epoch: 40 Idx: 0 Loss: 0.02186225160240407
Epoch: 40 Idx: 5000 Loss: 0.0147718678315574
Epoch: 41 Idx: 0 Loss: 0.018443785841517214
Epoch: 41 Idx: 5000 Loss: 0.016653372523959546
Epoch: 42 Idx: 0 Loss: 0.025396018381000387
Epoch: 42 Idx: 5000 Loss: 0.01955548401706476
Epoch: 43 Idx: 0 Loss: 0.020496711483898512
Epoch: 43 Idx: 5000 Loss: 0.022131977859541095
Epoch: 44 Idx: 0 Loss: 0.012185412028707248
Epoch: 44 Idx: 5000 Loss: 0.039106627320134035
Epoch: 45 Idx: 0 Loss: 0.010847775873970998
Epoch: 45 Idx: 5000 Loss: 0.027562942704853904
Epoch: 46 Idx: 0 Loss: 0.02107149579174426
Epoch: 46 Idx: 5000 Loss: 0.022508177948544508
Epoch: 47 Idx: 0 Loss: 0.02224819286516118
Epoch: 47 Idx: 5000 Loss: 0.01723137060148166
Epoch: 48 Idx: 0 Loss: 0.020728835606153156
Epoch: 48 Idx: 5000 Loss: 0.011613104250554028
Epoch: 49 Idx: 0 Loss: 0.015357212585589754
Epoch: 49 Idx: 5000 Loss: 0.013255346995355985
Len (direct inputs):  102
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17828574493424346
Epoch: 0 Idx: 5000 Loss: 0.028696519851407424
Epoch: 1 Idx: 0 Loss: 0.0070384823817360494
Epoch: 1 Idx: 5000 Loss: 0.008135907848565552
Epoch: 2 Idx: 0 Loss: 0.025016929562994658
Epoch: 2 Idx: 5000 Loss: 0.04209928629034303
Epoch: 3 Idx: 0 Loss: 0.012539033933596788
Epoch: 3 Idx: 5000 Loss: 0.01977476347494344
Epoch: 4 Idx: 0 Loss: 0.016449790464049048
Epoch: 4 Idx: 5000 Loss: 0.010216470225408055
Epoch: 5 Idx: 0 Loss: 0.016877303250213905
Epoch: 5 Idx: 5000 Loss: 0.026300846130450858
Epoch: 6 Idx: 0 Loss: 0.019120908871433653
Epoch: 6 Idx: 5000 Loss: 0.01629986724034011
Epoch: 7 Idx: 0 Loss: 0.01964757781295131
Epoch: 7 Idx: 5000 Loss: 0.017598290501262607
Epoch: 8 Idx: 0 Loss: 0.015258246488505147
Epoch: 8 Idx: 5000 Loss: 0.023892894634520195
Epoch: 9 Idx: 0 Loss: 0.02285580342390709
Epoch: 9 Idx: 5000 Loss: 0.01402276616277752
Epoch: 10 Idx: 0 Loss: 0.024595111858227624
Epoch: 10 Idx: 5000 Loss: 0.012419482215052902
Epoch: 11 Idx: 0 Loss: 0.03141806280500187
Epoch: 11 Idx: 5000 Loss: 0.01947188039811846
Epoch: 12 Idx: 0 Loss: 0.022215681929893445
Epoch: 12 Idx: 5000 Loss: 0.04037055685135586
Epoch: 13 Idx: 0 Loss: 0.020533107734688145
Epoch: 13 Idx: 5000 Loss: 0.014260448628106295
Epoch: 14 Idx: 0 Loss: 0.0271392629970024
Epoch: 14 Idx: 5000 Loss: 0.01858621335898961
Epoch: 15 Idx: 0 Loss: 0.02494899404489389
Epoch: 15 Idx: 5000 Loss: 0.03812020428409445
Epoch: 16 Idx: 0 Loss: 0.02337370242477195
Epoch: 16 Idx: 5000 Loss: 0.03818754512934672
Epoch: 17 Idx: 0 Loss: 0.012164486441464988
Epoch: 17 Idx: 5000 Loss: 0.014645209831599925
Epoch: 18 Idx: 0 Loss: 0.014590330297558376
Epoch: 18 Idx: 5000 Loss: 0.017462791571524347
Epoch: 19 Idx: 0 Loss: 0.020323338155212166
Epoch: 19 Idx: 5000 Loss: 0.017885143937868456
Epoch: 20 Idx: 0 Loss: 0.03437749842174885
Epoch: 20 Idx: 5000 Loss: 0.01695245709251221
Epoch: 21 Idx: 0 Loss: 0.044387552943874246
Epoch: 21 Idx: 5000 Loss: 0.014690477526194134
Epoch: 22 Idx: 0 Loss: 0.022831500096824844
Epoch: 22 Idx: 5000 Loss: 0.0328026656355295
Epoch: 23 Idx: 0 Loss: 0.010788924921399646
Epoch: 23 Idx: 5000 Loss: 0.019559060998830414
Epoch: 24 Idx: 0 Loss: 0.01967967944216982
Epoch: 24 Idx: 5000 Loss: 0.010564172238479014
Epoch: 25 Idx: 0 Loss: 0.01789232444810326
Epoch: 25 Idx: 5000 Loss: 0.019579300548214024
Epoch: 26 Idx: 0 Loss: 0.022326016080036912
Epoch: 26 Idx: 5000 Loss: 0.022225282883780798
Epoch: 27 Idx: 0 Loss: 0.028941812766072886
Epoch: 27 Idx: 5000 Loss: 0.013532353494653454
Epoch: 28 Idx: 0 Loss: 0.027771305624946264
Epoch: 28 Idx: 5000 Loss: 0.03452441208885339
Epoch: 29 Idx: 0 Loss: 0.011043895772968298
Epoch: 29 Idx: 5000 Loss: 0.011860214346122572
Epoch: 30 Idx: 0 Loss: 0.012004750594665676
Epoch: 30 Idx: 5000 Loss: 0.020481968950627386
Epoch: 31 Idx: 0 Loss: 0.02479760470085325
Epoch: 31 Idx: 5000 Loss: 0.030755882732759796
Epoch: 32 Idx: 0 Loss: 0.011704138001136961
Epoch: 32 Idx: 5000 Loss: 0.014557987449932301
Epoch: 33 Idx: 0 Loss: 0.03885014765814767
Epoch: 33 Idx: 5000 Loss: 0.026011972030919484
Epoch: 34 Idx: 0 Loss: 0.01533869746709841
Epoch: 34 Idx: 5000 Loss: 0.029134870821404705
Epoch: 35 Idx: 0 Loss: 0.017853185171099697
Epoch: 35 Idx: 5000 Loss: 0.018268177481013344
Epoch: 36 Idx: 0 Loss: 0.019539577826757498
Epoch: 36 Idx: 5000 Loss: 0.011496407965292083
Epoch: 37 Idx: 0 Loss: 0.014627198134774453
Epoch: 37 Idx: 5000 Loss: 0.01601185940707983
Epoch: 38 Idx: 0 Loss: 0.04231925393171283
Epoch: 38 Idx: 5000 Loss: 0.04821713287389921
Epoch: 39 Idx: 0 Loss: 0.02824162868509164
Epoch: 39 Idx: 5000 Loss: 0.039270487944437375
Epoch: 40 Idx: 0 Loss: 0.025541293850953703
Epoch: 40 Idx: 5000 Loss: 0.014470294886702014
Epoch: 41 Idx: 0 Loss: 0.02887058895187089
Epoch: 41 Idx: 5000 Loss: 0.014768287492627073
Epoch: 42 Idx: 0 Loss: 0.020346027149069325
Epoch: 42 Idx: 5000 Loss: 0.030823860059271124
Epoch: 43 Idx: 0 Loss: 0.012309500305766337
Epoch: 43 Idx: 5000 Loss: 0.025855443808482242
Epoch: 44 Idx: 0 Loss: 0.013106140288675327
Epoch: 44 Idx: 5000 Loss: 0.019513312802915177
Epoch: 45 Idx: 0 Loss: 0.013648834449089518
Epoch: 45 Idx: 5000 Loss: 0.03855713184200372
Epoch: 46 Idx: 0 Loss: 0.02614341946291189
Epoch: 46 Idx: 5000 Loss: 0.013478203704379792
Epoch: 47 Idx: 0 Loss: 0.02684269233576947
Epoch: 47 Idx: 5000 Loss: 0.022907133464817667
Epoch: 48 Idx: 0 Loss: 0.014028313396653534
Epoch: 48 Idx: 5000 Loss: 0.014093472489539231
Epoch: 49 Idx: 0 Loss: 0.02362501043520758
Epoch: 49 Idx: 5000 Loss: 0.02382266161399063
Len (direct inputs):  109
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
90716
Parameter containing:
tensor([0.8065], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.26094875124888983
Epoch: 0 Idx: 5000 Loss: 0.024519792036581504
Epoch: 1 Idx: 0 Loss: 0.034399802808783034
Epoch: 1 Idx: 5000 Loss: 0.03629100895547898
Epoch: 2 Idx: 0 Loss: 0.030435781606959825
Epoch: 2 Idx: 5000 Loss: 0.016820835027736125
Epoch: 3 Idx: 0 Loss: 0.018116585302416346
Epoch: 3 Idx: 5000 Loss: 0.014728617224606427
Epoch: 4 Idx: 0 Loss: 0.019798791870498916
Epoch: 4 Idx: 5000 Loss: 0.013377736209605277
Epoch: 5 Idx: 0 Loss: 0.02513068381786274
Epoch: 5 Idx: 5000 Loss: 0.014346177133808903
Epoch: 6 Idx: 0 Loss: 0.019259894501629463
Epoch: 6 Idx: 5000 Loss: 0.02108352759410674
Epoch: 7 Idx: 0 Loss: 0.0174209909510722
Epoch: 7 Idx: 5000 Loss: 0.015601983278666534
Epoch: 8 Idx: 0 Loss: 0.03445896001802176
Epoch: 8 Idx: 5000 Loss: 0.018680036799211774
Epoch: 9 Idx: 0 Loss: 0.015515356735564645
Epoch: 9 Idx: 5000 Loss: 0.011026689870988983
Epoch: 10 Idx: 0 Loss: 0.03667932198419058
Epoch: 10 Idx: 5000 Loss: 0.013967673050835188
Epoch: 11 Idx: 0 Loss: 0.03026301713965584
Epoch: 11 Idx: 5000 Loss: 0.018203320165476176
Epoch: 12 Idx: 0 Loss: 0.029040066173982776
Epoch: 12 Idx: 5000 Loss: 0.008278076962596248
Epoch: 13 Idx: 0 Loss: 0.019847293040981685
Epoch: 13 Idx: 5000 Loss: 0.03376679351587902
Epoch: 14 Idx: 0 Loss: 0.019772191728939348
Epoch: 14 Idx: 5000 Loss: 0.023974711793117283
Epoch: 15 Idx: 0 Loss: 0.013771554235204113
Epoch: 15 Idx: 5000 Loss: 0.02873348581134961
Epoch: 16 Idx: 0 Loss: 0.023608683626019377
Epoch: 16 Idx: 5000 Loss: 0.018880209245362073
Epoch: 17 Idx: 0 Loss: 0.02260943669194899
Epoch: 17 Idx: 5000 Loss: 0.01671288611039381
Epoch: 18 Idx: 0 Loss: 0.02575387463037796
Epoch: 18 Idx: 5000 Loss: 0.02993696522774368
Epoch: 19 Idx: 0 Loss: 0.03677630381778427
Epoch: 19 Idx: 5000 Loss: 0.01584087498640526
Epoch: 20 Idx: 0 Loss: 0.016224481816609137
Epoch: 20 Idx: 5000 Loss: 0.03542766719543104
Epoch: 21 Idx: 0 Loss: 0.025339293886911952
Epoch: 21 Idx: 5000 Loss: 0.017050272317942945
Epoch: 22 Idx: 0 Loss: 0.019246229938566606
Epoch: 22 Idx: 5000 Loss: 0.017536195787763033
Epoch: 23 Idx: 0 Loss: 0.009263712524310004
Epoch: 23 Idx: 5000 Loss: 0.025367805427947657
Epoch: 24 Idx: 0 Loss: 0.018518120244961397
Epoch: 24 Idx: 5000 Loss: 0.02699502163968694
Epoch: 25 Idx: 0 Loss: 0.0336511161854781
Epoch: 25 Idx: 5000 Loss: 0.0356949440291254
Epoch: 26 Idx: 0 Loss: 0.034720619589246084
Epoch: 26 Idx: 5000 Loss: 0.02601133670588212
Epoch: 27 Idx: 0 Loss: 0.021386470220052193
Epoch: 27 Idx: 5000 Loss: 0.010530813975451584
Epoch: 28 Idx: 0 Loss: 0.03682320173311937
Epoch: 28 Idx: 5000 Loss: 0.03839375590742882
Epoch: 29 Idx: 0 Loss: 0.029759033957319332
Epoch: 29 Idx: 5000 Loss: 0.029193009038696212
Epoch: 30 Idx: 0 Loss: 0.02880519778484505
Epoch: 30 Idx: 5000 Loss: 0.01829482163018629
Epoch: 31 Idx: 0 Loss: 0.011777347649772143
Epoch: 31 Idx: 5000 Loss: 0.019284455108771137
Epoch: 32 Idx: 0 Loss: 0.019877182091559392
Epoch: 32 Idx: 5000 Loss: 0.02159436279360947
Epoch: 33 Idx: 0 Loss: 0.013236654266529814
Epoch: 33 Idx: 5000 Loss: 0.037385036687336934
Epoch: 34 Idx: 0 Loss: 0.01760136649171884
Epoch: 34 Idx: 5000 Loss: 0.023325874444159452
Epoch: 35 Idx: 0 Loss: 0.017301848387164212
Epoch: 35 Idx: 5000 Loss: 0.02256204400264978
Epoch: 36 Idx: 0 Loss: 0.008860332553439298
Epoch: 36 Idx: 5000 Loss: 0.018399173088308463
Epoch: 37 Idx: 0 Loss: 0.016541728148333074
Epoch: 37 Idx: 5000 Loss: 0.017184923457821626
Epoch: 38 Idx: 0 Loss: 0.022430096673304353
Epoch: 38 Idx: 5000 Loss: 0.014566202084864529
Epoch: 39 Idx: 0 Loss: 0.03974107755055614
Epoch: 39 Idx: 5000 Loss: 0.01833513288966747
Epoch: 40 Idx: 0 Loss: 0.021863628576827228
Epoch: 40 Idx: 5000 Loss: 0.017487930982007693
Epoch: 41 Idx: 0 Loss: 0.023963863391385974
Epoch: 41 Idx: 5000 Loss: 0.029138328694163385
Epoch: 42 Idx: 0 Loss: 0.01859136828935631
Epoch: 42 Idx: 5000 Loss: 0.032188953760319175
Epoch: 43 Idx: 0 Loss: 0.01773320108893095
Epoch: 43 Idx: 5000 Loss: 0.027695694778222597
Epoch: 44 Idx: 0 Loss: 0.016610853407773202
Epoch: 44 Idx: 5000 Loss: 0.038168180583494936
Epoch: 45 Idx: 0 Loss: 0.013359544313018782
Epoch: 45 Idx: 5000 Loss: 0.03423025480401533
Epoch: 46 Idx: 0 Loss: 0.018939359634940575
Epoch: 46 Idx: 5000 Loss: 0.026949591506716813
Epoch: 47 Idx: 0 Loss: 0.042136357538597354
Epoch: 47 Idx: 5000 Loss: 0.01573026886949759
Epoch: 48 Idx: 0 Loss: 0.027316881862959358
Epoch: 48 Idx: 5000 Loss: 0.030986097451002816
Epoch: 49 Idx: 0 Loss: 0.022135529794840188
Epoch: 49 Idx: 5000 Loss: 0.029890515865534553
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 476, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
h Disk quota exceeded

xceeded
 Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:03:30 2020
Results reported at Tue Sep  1 20:03:30 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 2 Output/test_anatomy_aml_bagofnbrs_wtpath1_2.pkl Models/anatomy_aml_bagofnbrs_wtpath1_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22138.13 sec.
    Max Memory :                                 2915 MB
    Average Memory :                             2721.40 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40502.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22204 sec.
    Turnaround time :                            22186 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc233>
Subject: Job 3289796: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs_wtpath2_2.pkl Models/anatomy_aml_bagofnbrs_wtpath2_2.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs_wtpath2_2.pkl Models/anatomy_aml_bagofnbrs_wtpath2_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc233>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:05:56 2020
Results reported at Tue Sep  1 20:05:56 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 2 Output/test_anatomy_aml_bagofnbrs_wtpath2_2.pkl Models/anatomy_aml_bagofnbrs_wtpath2_2.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22324.88 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2712.83 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22355 sec.
    Turnaround time :                            22332 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc241>
Subject: Job 3289798: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs_wtpath3_2.pkl Models/anatomy_aml_bagofnbrs_wtpath3_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs_wtpath3_2.pkl Models/anatomy_aml_bagofnbrs_wtpath3_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc241>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 20:34:12 2020
Results reported at Tue Sep  1 20:34:12 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 2 Output/test_anatomy_aml_bagofnbrs_wtpath3_2.pkl Models/anatomy_aml_bagofnbrs_wtpath3_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24021.92 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2727.12 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   24043 sec.
    Turnaround time :                            24028 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc265>
Subject: Job 3289800: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs_wtpath4_2.pkl Models/anatomy_aml_bagofnbrs_wtpath4_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs_wtpath4_2.pkl Models/anatomy_aml_bagofnbrs_wtpath4_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc265>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:03:44 2020
Results reported at Tue Sep  1 21:03:44 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 2 Output/test_anatomy_aml_bagofnbrs_wtpath4_2.pkl Models/anatomy_aml_bagofnbrs_wtpath4_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25576.10 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2726.98 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25807 sec.
    Turnaround time :                            25800 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc207>
Subject: Job 3289802: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs_wtpath5_2.pkl Models/anatomy_aml_bagofnbrs_wtpath5_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs_wtpath5_2.pkl Models/anatomy_aml_bagofnbrs_wtpath5_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc207>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:34:30 2020
Results reported at Tue Sep  1 21:34:30 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 2 Output/test_anatomy_aml_bagofnbrs_wtpath5_2.pkl Models/anatomy_aml_bagofnbrs_wtpath5_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27641.78 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2724.72 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27653 sec.
    Turnaround time :                            27646 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc204>
Subject: Job 3289804: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs_wtpath7_2.pkl Models/anatomy_aml_bagofnbrs_wtpath7_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs_wtpath7_2.pkl Models/anatomy_aml_bagofnbrs_wtpath7_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc204>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 21:54:00 2020
Results reported at Tue Sep  1 21:54:00 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 2 Output/test_anatomy_aml_bagofnbrs_wtpath7_2.pkl Models/anatomy_aml_bagofnbrs_wtpath7_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28752.39 sec.
    Max Memory :                                 2916 MB
    Average Memory :                             2717.47 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40501.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28815 sec.
    Turnaround time :                            28816 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc222>
Subject: Job 3289806: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs_wtpath8_2.pkl Models/anatomy_aml_bagofnbrs_wtpath8_2.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs_wtpath8_2.pkl Models/anatomy_aml_bagofnbrs_wtpath8_2.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:44 2020
Job was executed on host(s) <dccxc222>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:45 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:45 2020
Terminated at Tue Sep  1 22:21:28 2020
Results reported at Tue Sep  1 22:21:28 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 2 Output/test_anatomy_aml_bagofnbrs_wtpath8_2.pkl Models/anatomy_aml_bagofnbrs_wtpath8_2.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30452.30 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2720.00 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30463 sec.
    Turnaround time :                            30464 sec.

The output (if any) is above this job summary.

