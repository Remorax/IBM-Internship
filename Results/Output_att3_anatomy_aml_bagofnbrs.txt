Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19872947259224605
Epoch: 0 Idx: 5000 Loss: 0.02652601090853326
Epoch: 1 Idx: 0 Loss: 0.013550927927833177
Epoch: 1 Idx: 5000 Loss: 0.03334802473782244
Epoch: 2 Idx: 0 Loss: 0.03351269810425207
Epoch: 2 Idx: 5000 Loss: 0.01934083920270074
Epoch: 3 Idx: 0 Loss: 0.02126480051102718
Epoch: 3 Idx: 5000 Loss: 0.019972869204676547
Epoch: 4 Idx: 0 Loss: 0.021011018778602796
Epoch: 4 Idx: 5000 Loss: 0.03882247814930975
Epoch: 5 Idx: 0 Loss: 0.02407598847599593
Epoch: 5 Idx: 5000 Loss: 0.035614784611096253
Epoch: 6 Idx: 0 Loss: 0.01996772573770913
Epoch: 6 Idx: 5000 Loss: 0.05043052765533158
Epoch: 7 Idx: 0 Loss: 0.014395449892744407
Epoch: 7 Idx: 5000 Loss: 0.007838370134148957
Epoch: 8 Idx: 0 Loss: 0.02853077374612454
Epoch: 8 Idx: 5000 Loss: 0.027213384857946693
Epoch: 9 Idx: 0 Loss: 0.0225635885958373
Epoch: 9 Idx: 5000 Loss: 0.026691139635134926
Epoch: 10 Idx: 0 Loss: 0.015175521979633542
Epoch: 10 Idx: 5000 Loss: 0.011302979556680644
Epoch: 11 Idx: 0 Loss: 0.016209120994945925
Epoch: 11 Idx: 5000 Loss: 0.0163572830571025
Epoch: 12 Idx: 0 Loss: 0.020894246420768856
Epoch: 12 Idx: 5000 Loss: 0.025836767087100816
Epoch: 13 Idx: 0 Loss: 0.023072892517164496
Epoch: 13 Idx: 5000 Loss: 0.01472186955649549
Epoch: 14 Idx: 0 Loss: 0.023016153409797818
Epoch: 14 Idx: 5000 Loss: 0.04321291398242251
Epoch: 15 Idx: 0 Loss: 0.01820051567990457
Epoch: 15 Idx: 5000 Loss: 0.028395026400434047
Epoch: 16 Idx: 0 Loss: 0.02379782953494228
Epoch: 16 Idx: 5000 Loss: 0.0151282205564876
Epoch: 17 Idx: 0 Loss: 0.028408311608713568
Epoch: 17 Idx: 5000 Loss: 0.017718270913342753
Epoch: 18 Idx: 0 Loss: 0.023210660162280076
Epoch: 18 Idx: 5000 Loss: 0.02226843079524755
Epoch: 19 Idx: 0 Loss: 0.024464265620979433
Epoch: 19 Idx: 5000 Loss: 0.020913385963457008
Epoch: 20 Idx: 0 Loss: 0.027213168805068595
Epoch: 20 Idx: 5000 Loss: 0.030561523780832718
Epoch: 21 Idx: 0 Loss: 0.023168743499540633
Epoch: 21 Idx: 5000 Loss: 0.015501978502353238
Epoch: 22 Idx: 0 Loss: 0.02691842395325806
Epoch: 22 Idx: 5000 Loss: 0.028859419364288968
Epoch: 23 Idx: 0 Loss: 0.0303981068978127
Epoch: 23 Idx: 5000 Loss: 0.04999178277200753
Epoch: 24 Idx: 0 Loss: 0.03800898815914565
Epoch: 24 Idx: 5000 Loss: 0.03210976910612108
Epoch: 25 Idx: 0 Loss: 0.01659832860834754
Epoch: 25 Idx: 5000 Loss: 0.014852135124624823
Epoch: 26 Idx: 0 Loss: 0.016900800432735157
Epoch: 26 Idx: 5000 Loss: 0.027347604568453983
Epoch: 27 Idx: 0 Loss: 0.03141919019607334
Epoch: 27 Idx: 5000 Loss: 0.015954041186043467
Epoch: 28 Idx: 0 Loss: 0.020257766624831627
Epoch: 28 Idx: 5000 Loss: 0.02004789631756984
Epoch: 29 Idx: 0 Loss: 0.015937550538183597
Epoch: 29 Idx: 5000 Loss: 0.022142656390590324
Epoch: 30 Idx: 0 Loss: 0.011993240771718892
Epoch: 30 Idx: 5000 Loss: 0.02087669664005349
Epoch: 31 Idx: 0 Loss: 0.011179915810915098
Epoch: 31 Idx: 5000 Loss: 0.023097037841792657
Epoch: 32 Idx: 0 Loss: 0.0212197178801086
Epoch: 32 Idx: 5000 Loss: 0.024568817533933425
Epoch: 33 Idx: 0 Loss: 0.009219623786906071
Epoch: 33 Idx: 5000 Loss: 0.020263738226327113
Epoch: 34 Idx: 0 Loss: 0.014574195288662446
Epoch: 34 Idx: 5000 Loss: 0.012641350159879217
Epoch: 35 Idx: 0 Loss: 0.018152373819177666
Epoch: 35 Idx: 5000 Loss: 0.025022739180857047
Epoch: 36 Idx: 0 Loss: 0.012305357204473243
Epoch: 36 Idx: 5000 Loss: 0.021319044156495274
Epoch: 37 Idx: 0 Loss: 0.01046123218719337
Epoch: 37 Idx: 5000 Loss: 0.02086327559527953
Epoch: 38 Idx: 0 Loss: 0.028265959357555482
Epoch: 38 Idx: 5000 Loss: 0.009576595539268418
Epoch: 39 Idx: 0 Loss: 0.025450597942762943
Epoch: 39 Idx: 5000 Loss: 0.017691460152785
Epoch: 40 Idx: 0 Loss: 0.024275959971118796
Epoch: 40 Idx: 5000 Loss: 0.01794054846240167
Epoch: 41 Idx: 0 Loss: 0.042111087247331466
Epoch: 41 Idx: 5000 Loss: 0.015437945530902563
Epoch: 42 Idx: 0 Loss: 0.03078072623155787
Epoch: 42 Idx: 5000 Loss: 0.029688673674054956
Epoch: 43 Idx: 0 Loss: 0.033766846659194355
Epoch: 43 Idx: 5000 Loss: 0.024149250837652573
Epoch: 44 Idx: 0 Loss: 0.023994981904640767
Epoch: 44 Idx: 5000 Loss: 0.01691729491265584
Epoch: 45 Idx: 0 Loss: 0.016081526512993112
Epoch: 45 Idx: 5000 Loss: 0.021276614154743637
Epoch: 46 Idx: 0 Loss: 0.012657237613948814
Epoch: 46 Idx: 5000 Loss: 0.028447984688472212
Epoch: 47 Idx: 0 Loss: 0.021780606665318544
Epoch: 47 Idx: 5000 Loss: 0.02145697920883631
Epoch: 48 Idx: 0 Loss: 0.008820329628797537
Epoch: 48 Idx: 5000 Loss: 0.02125776423204877
Epoch: 49 Idx: 0 Loss: 0.024805909015220562
Epoch: 49 Idx: 5000 Loss: 0.04706712256770314
Len (direct inputs):  116
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.26365718754335205
Epoch: 0 Idx: 5000 Loss: 0.03299091462332643
Epoch: 1 Idx: 0 Loss: 0.017409649356251174
Epoch: 1 Idx: 5000 Loss: 0.028714686174711672
Epoch: 2 Idx: 0 Loss: 0.02052793881508788
Epoch: 2 Idx: 5000 Loss: 0.011299775598920448
Epoch: 3 Idx: 0 Loss: 0.03277077931827632
Epoch: 3 Idx: 5000 Loss: 0.020821699973279942
Epoch: 4 Idx: 0 Loss: 0.01369437175823584
Epoch: 4 Idx: 5000 Loss: 0.017910762313060317
Epoch: 5 Idx: 0 Loss: 0.019261063909703842
Epoch: 5 Idx: 5000 Loss: 0.01197906283059742
Epoch: 6 Idx: 0 Loss: 0.018018303558481386
Epoch: 6 Idx: 5000 Loss: 0.014363418784956533
Epoch: 7 Idx: 0 Loss: 0.008520939630633264
Epoch: 7 Idx: 5000 Loss: 0.01385641763525788
Epoch: 8 Idx: 0 Loss: 0.02176144329603953
Epoch: 8 Idx: 5000 Loss: 0.02049161653536857
Epoch: 9 Idx: 0 Loss: 0.020467478577213056
Epoch: 9 Idx: 5000 Loss: 0.010802096278904912
Epoch: 10 Idx: 0 Loss: 0.016592389186984365
Epoch: 10 Idx: 5000 Loss: 0.023409943929117202
Epoch: 11 Idx: 0 Loss: 0.02219935368981857
Epoch: 11 Idx: 5000 Loss: 0.0313474238895373
Epoch: 12 Idx: 0 Loss: 0.019567316558883305
Epoch: 12 Idx: 5000 Loss: 0.04130758203119469
Epoch: 13 Idx: 0 Loss: 0.034076813226667564
Epoch: 13 Idx: 5000 Loss: 0.03997553623610178
Epoch: 14 Idx: 0 Loss: 0.016749899379788834
Epoch: 14 Idx: 5000 Loss: 0.034621891268677346
Epoch: 15 Idx: 0 Loss: 0.017104663573156327
Epoch: 15 Idx: 5000 Loss: 0.009922718441714138
Epoch: 16 Idx: 0 Loss: 0.035574776554439885
Epoch: 16 Idx: 5000 Loss: 0.012898961649885387
Epoch: 17 Idx: 0 Loss: 0.009712750153042585
Epoch: 17 Idx: 5000 Loss: 0.02548673657186607
Epoch: 18 Idx: 0 Loss: 0.01519111805594453
Epoch: 18 Idx: 5000 Loss: 0.027102783831399996
Epoch: 19 Idx: 0 Loss: 0.013800203099158538
Epoch: 19 Idx: 5000 Loss: 0.01886095843695196
Epoch: 20 Idx: 0 Loss: 0.019919213599247088
Epoch: 20 Idx: 5000 Loss: 0.014210681176029995
Epoch: 21 Idx: 0 Loss: 0.024357379594960944
Epoch: 21 Idx: 5000 Loss: 0.023841820488955397
Epoch: 22 Idx: 0 Loss: 0.0128907519835182
Epoch: 22 Idx: 5000 Loss: 0.03398173647274804
Epoch: 23 Idx: 0 Loss: 0.029917663758497647
Epoch: 23 Idx: 5000 Loss: 0.01990831315214394
Epoch: 24 Idx: 0 Loss: 0.029405576050984195
Epoch: 24 Idx: 5000 Loss: 0.015213619050923395
Epoch: 25 Idx: 0 Loss: 0.01712360106787786
Epoch: 25 Idx: 5000 Loss: 0.019788091906367725
Epoch: 26 Idx: 0 Loss: 0.012106728133484098
Epoch: 26 Idx: 5000 Loss: 0.014310308932893909
Epoch: 27 Idx: 0 Loss: 0.011945590520599937
Epoch: 27 Idx: 5000 Loss: 0.014343661018848467
Epoch: 28 Idx: 0 Loss: 0.04143062840976798
Epoch: 28 Idx: 5000 Loss: 0.011644693170430553
Epoch: 29 Idx: 0 Loss: 0.040374332582143416
Epoch: 29 Idx: 5000 Loss: 0.013545808890904681
Epoch: 30 Idx: 0 Loss: 0.02049374438373132
Epoch: 30 Idx: 5000 Loss: 0.013143234485643628
Epoch: 31 Idx: 0 Loss: 0.032870193757978415
Epoch: 31 Idx: 5000 Loss: 0.021295753672639528
Epoch: 32 Idx: 0 Loss: 0.02626611834004986
Epoch: 32 Idx: 5000 Loss: 0.015863151654990502
Epoch: 33 Idx: 0 Loss: 0.021615969909454205
Epoch: 33 Idx: 5000 Loss: 0.025849779023660455
Epoch: 34 Idx: 0 Loss: 0.03058351408080509
Epoch: 34 Idx: 5000 Loss: 0.017465675519990703
Epoch: 35 Idx: 0 Loss: 0.02386654760698949
Epoch: 35 Idx: 5000 Loss: 0.026688253489743692
Epoch: 36 Idx: 0 Loss: 0.056071438237884005
Epoch: 36 Idx: 5000 Loss: 0.02497855789583969
Epoch: 37 Idx: 0 Loss: 0.01981871790272353
Epoch: 37 Idx: 5000 Loss: 0.022846292018556785
Epoch: 38 Idx: 0 Loss: 0.024505736776975654
Epoch: 38 Idx: 5000 Loss: 0.017340181716923985
Epoch: 39 Idx: 0 Loss: 0.02176331223887235
Epoch: 39 Idx: 5000 Loss: 0.03934581907652771
Epoch: 40 Idx: 0 Loss: 0.020774081071957815
Epoch: 40 Idx: 5000 Loss: 0.02565637579530438
Epoch: 41 Idx: 0 Loss: 0.018459742621168226
Epoch: 41 Idx: 5000 Loss: 0.019138824222593408
Epoch: 42 Idx: 0 Loss: 0.022655739897568184
Epoch: 42 Idx: 5000 Loss: 0.021228948448599936
Epoch: 43 Idx: 0 Loss: 0.03184827762700062
Epoch: 43 Idx: 5000 Loss: 0.016593811193086182
Epoch: 44 Idx: 0 Loss: 0.03730477085218645
Epoch: 44 Idx: 5000 Loss: 0.03322384757194298
Epoch: 45 Idx: 0 Loss: 0.029216907500516743
Epoch: 45 Idx: 5000 Loss: 0.031795283703602635
Epoch: 46 Idx: 0 Loss: 0.023704992084567285
Epoch: 46 Idx: 5000 Loss: 0.03605579685035125
Epoch: 47 Idx: 0 Loss: 0.014290489599769694
Epoch: 47 Idx: 5000 Loss: 0.017459410681392805
Epoch: 48 Idx: 0 Loss: 0.013179038987080054
Epoch: 48 Idx: 5000 Loss: 0.030508538669651884
Epoch: 49 Idx: 0 Loss: 0.023548232273417624
Epoch: 49 Idx: 5000 Loss: 0.022833741656923975
Len (direct inputs):  97
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.22303113710079797
Epoch: 0 Idx: 5000 Loss: 0.028497793576367426
Epoch: 1 Idx: 0 Loss: 0.030700178735979557
Epoch: 1 Idx: 5000 Loss: 0.011816954493344595
Epoch: 2 Idx: 0 Loss: 0.03783131888161181
Epoch: 2 Idx: 5000 Loss: 0.020664035098845495
Epoch: 3 Idx: 0 Loss: 0.02241302444514315
Epoch: 3 Idx: 5000 Loss: 0.029104028838000725
Epoch: 4 Idx: 0 Loss: 0.02464886970281098
Epoch: 4 Idx: 5000 Loss: 0.029894327830552306
Epoch: 5 Idx: 0 Loss: 0.012739164672821103
Epoch: 5 Idx: 5000 Loss: 0.026188958619927885
Epoch: 6 Idx: 0 Loss: 0.016983030381544365
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 398, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
dx: 5000 Loss: 0.016002770202550677
Epoch: 11 Idx: 0 Loss: 0.017589934245087695
Epoch: 11 Idx: 5000 Loss: 0.03697243256668839
Epoch: 12 Idx: 0 Loss: 0.026440316891328336
Epoch: 12 Idx: 5000 Loss: 0.023858825810475718
Epoch: 13 Idx: 0 Loss: 0.02992671577074311
Epoch: 13 Idx: 5000 Loss: 0.027072699408788863
Epoch: 14 Idx: 0 Loss: 0.02419637841200222
Epoch: 14 Idx: 5000 Loss: 0.015172396610697431
Epoch: 15 Idx: 0 Loss: 0.026156329796334817
Epoch: 15 Idx: 5000 Loss: 0.02782484281277546
Epoch: 16 Idx: 0 Loss: 0.017214139596198753
Epoch: 16 Idx: 5000 Loss: 0.014730802541012502
Epoch: 17 Idx: 0 Loss: 0.013428650463407955
Epoch: 17 Idx: 5000 Loss: 0.013958537263272907
Epoch: 18 Idx: 0 Loss: 0.017450317755346167
Epoch: 18 Idx: 5000 Loss: 0.02076826742107253
Epoch: 19 Idx: 0 Loss: 0.014377678070018187
Epoch: 19 Idx: 5000 Loss: 0.013501502345571386
Epoch: 20 Idx: 0 Loss: 0.03341083120555165
Epoch: 20 Idx: 5000 Loss: 0.022844039958081673
Epoch: 21 Idx: 0 Loss: 0.019063253030226324
Epoch: 21 Idx: 5000 Loss: 0.03603475783067507
Epoch: 22 Idx: 0 Loss: 0.02209341610658136
Epoch: 22 Idx: 5000 Loss: 0.014328090621521334
Epoch: 23 Idx: 0 Loss: 0.023007729566276936
Epoch: 23 Idx: 5000 Loss: 0.020441988336831286
Epoch: 24 Idx: 0 Loss: 0.014464859424658945
Epoch: 24 Idx: 5000 Loss: 0.02979627313553658
Epoch: 25 Idx: 0 Loss: 0.019506428660298912
Epoch: 25 Idx: 5000 Loss: 0.03155786070656495
Epoch: 26 Idx: 0 Loss: 0.023324898019292287
Epoch: 26 Idx: 5000 Loss: 0.03326243633744155
Epoch: 27 Idx: 0 Loss: 0.01837842691122601
Epoch: 27 Idx: 5000 Loss: 0.02860612837530392
Epoch: 28 Idx: 0 Loss: 0.023663298188288058
Epoch: 28 Idx: 5000 Loss: 0.01834335130970317
Epoch: 29 Idx: 0 Loss: 0.01769261564664376
Epoch: 29 Idx: 5000 Loss: 0.029494944474097333
Epoch: 30 Idx: 0 Loss: 0.024563463187516357
Epoch: 30 Idx: 5000 Loss: 0.03517013521947088
Epoch: 31 Idx: 0 Loss: 0.015340474786095949
Epoch: 31 Idx: 5000 Loss: 0.019621945297418994
Epoch: 32 Idx: 0 Loss: 0.03752023951125721
Epoch: 32 Idx: 5000 Loss: 0.016234023646057787
Epoch: 33 Idx: 0 Loss: 0.0227863139640319
Epoch: 33 Idx: 5000 Loss: 0.01842435181725775
Epoch: 34 Idx: 0 Loss: 0.026245051474572848
Epoch: 34 Idx: 5000 Loss: 0.02204558089510463
Epoch: 35 Idx: 0 Loss: 0.025691776747850496
Epoch: 35 Idx: 5000 Loss: 0.020677320202773256
Epoch: 36 Idx: 0 Loss: 0.021899933811468507
Epoch: 36 Idx: 5000 Loss: 0.029496738825433397
Epoch: 37 Idx: 0 Loss: 0.015933861417991167
Epoch: 37 Idx: 5000 Loss: 0.011443432670107185
Epoch: 38 Idx: 0 Loss: 0.030546520527326586
Epoch: 38 Idx: 5000 Loss: 0.02280779464422392
Epoch: 39 Idx: 0 Loss: 0.028632115220826673
Epoch: 39 Idx: 5000 Loss: 0.01236453335960636
Epoch: 40 Idx: 0 Loss: 0.015486684498808008
Epoch: 40 Idx: 5000 Loss: 0.020779578954194782
Epoch: 41 Idx: 0 Loss: 0.02692945680577151
Epoch: 41 Idx: 5000 Loss: 0.011953202872148582
Epoch: 42 Idx: 0 Loss: 0.018612832900966644
Epoch: 42 Idx: 5000 Loss: 0.019844995366095654
Epoch: 43 Idx: 0 Loss: 0.03358798776882354
Epoch: 43 Idx: 5000 Loss: 0.010523095965347784
Epoch: 44 Idx: 0 Loss: 0.021262011359033543
Epoch: 44 Idx: 5000 Loss: 0.017473243281143507
Epoch: 45 Idx: 0 Loss: 0.020742233704556884
Epoch: 45 Idx: 5000 Loss: 0.015055442423798247
Epoch: 46 Idx: 0 Loss: 0.016719430663834058
Epoch: 46 Idx: 5000 Loss: 0.010225149593068115
Epoch: 47 Idx: 0 Loss: 0.016026791636373633
Epoch: 47 Idx: 5000 Loss: 0.011395934839833598
Epoch: 48 Idx: 0 Loss: 0.009572614784886994
Epoch: 48 Idx: 5000 Loss: 0.01710685367154295
Epoch: 49 Idx: 0 Loss: 0.01634777582242442
Epoch: 49 Idx: 5000 Loss: 0.022477207768886343
Len (direct inputs):  113
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.24027903791418037
Epoch: 0 Idx: 5000 Loss: 0.01466320848624407
Epoch: 1 Idx: 0 Loss: 0.02333533458584173
Epoch: 1 Idx: 5000 Loss: 0.006990260141085749
Epoch: 2 Idx: 0 Loss: 0.026617436555109633
Epoch: 2 Idx: 5000 Loss: 0.027942953425091295
Epoch: 3 Idx: 0 Loss: 0.02325590740925025
Epoch: 3 Idx: 5000 Loss: 0.023890334337809276
Epoch: 4 Idx: 0 Loss: 0.023362224867316088
Epoch: 4 Idx: 5000 Loss: 0.011467052532438134
Epoch: 5 Idx: 0 Loss: 0.0221491129240136
Epoch: 5 Idx: 5000 Loss: 0.016310076941692387
Epoch: 6 Idx: 0 Loss: 0.012180015066754906
Epoch: 6 Idx: 5000 Loss: 0.025167816806059545
Epoch: 7 Idx: 0 Loss: 0.021164488415033003
Epoch: 7 Idx: 5000 Loss: 0.03179588360160955
Epoch: 8 Idx: 0 Loss: 0.019995890503348823
Epoch: 8 Idx: 5000 Loss: 0.013030588544340286
Epoch: 9 Idx: 0 Loss: 0.012023728637840075
Epoch: 9 Idx: 5000 Loss: 0.02224430631916986
Epoch: 10 Idx: 0 Loss: 0.027111984751066832
Epoch: 10 Idx: 5000 Loss: 0.010134378453314732
Epoch: 11 Idx: 0 Loss: 0.02950710373825224
Epoch: 11 Idx: 5000 Loss: 0.014457057303442185
Epoch: 12 Idx: 0 Loss: 0.02096566921459559
Epoch: 12 Idx: 5000 Loss: 0.015919589411762886
Epoch: 13 Idx: 0 Loss: 0.01091432626223399
Epoch: 13 Idx: 5000 Loss: 0.008087753324734968
Epoch: 14 Idx: 0 Loss: 0.0472373165293102
Epoch: 14 Idx: 5000 Loss: 0.029930359882542112
Epoch: 15 Idx: 0 Loss: 0.022676925597966434
Epoch: 15 Idx: 5000 Loss: 0.04064240251770865
Epoch: 16 Idx: 0 Loss: 0.026719015766143182
Epoch: 16 Idx: 5000 Loss: 0.015244093469096897
Epoch: 17 Idx: 0 Loss: 0.03161337014317009
Epoch: 17 Idx: 5000 Loss: 0.017858933563401236
Epoch: 18 Idx: 0 Loss: 0.010488065888574602
Epoch: 18 Idx: 5000 Loss: 0.014724425035790259
Epoch: 19 Idx: 0 Loss: 0.030156148354577854
Epoch: 19 Idx: 5000 Loss: 0.011265441726896388
Epoch: 20 Idx: 0 Loss: 0.016216801801682296
Epoch: 20 Idx: 5000 Loss: 0.01983235545868454
Epoch: 21 Idx: 0 Loss: 0.025309735773967952
Epoch: 21 Idx: 5000 Loss: 0.01599476880298683
Epoch: 22 Idx: 0 Loss: 0.021940178100434363
Epoch: 22 Idx: 5000 Loss: 0.020568607275528478
Epoch: 23 Idx: 0 Loss: 0.028846490424059033
Epoch: 23 Idx: 5000 Loss: 0.027021449210653727
Epoch: 24 Idx: 0 Loss: 0.03242121243139575
Epoch: 24 Idx: 5000 Loss: 0.0164944696983293
Epoch: 25 Idx: 0 Loss: 0.021015229402625932
Epoch: 25 Idx: 5000 Loss: 0.016772215682299814
Epoch: 26 Idx: 0 Loss: 0.021513904006175488
Epoch: 26 Idx: 5000 Loss: 0.023909928711136168
Epoch: 27 Idx: 0 Loss: 0.007433457548387974
Epoch: 27 Idx: 5000 Loss: 0.020916953719185533
Epoch: 28 Idx: 0 Loss: 0.02729131053381924
Epoch: 28 Idx: 5000 Loss: 0.02304722684498293
Epoch: 29 Idx: 0 Loss: 0.014816376891061715
Epoch: 29 Idx: 5000 Loss: 0.012023194781765452
Epoch: 30 Idx: 0 Loss: 0.01129959209052549
Epoch: 30 Idx: 5000 Loss: 0.020493702760539387
Epoch: 31 Idx: 0 Loss: 0.02156127378398716
Epoch: 31 Idx: 5000 Loss: 0.02033332655948614
Epoch: 32 Idx: 0 Loss: 0.0257810854453579
Epoch: 32 Idx: 5000 Loss: 0.018212698974154975
Epoch: 33 Idx: 0 Loss: 0.023904368425939168
Epoch: 33 Idx: 5000 Loss: 0.024742643647936567
Epoch: 34 Idx: 0 Loss: 0.02233200704511563
Epoch: 34 Idx: 5000 Loss: 0.01477060973983831
Epoch: 35 Idx: 0 Loss: 0.019222440091543634
Epoch: 35 Idx: 5000 Loss: 0.032513476356125455
Epoch: 36 Idx: 0 Loss: 0.01011680330078866
Epoch: 36 Idx: 5000 Loss: 0.023286935142810352
Epoch: 37 Idx: 0 Loss: 0.03963942271760508
Epoch: 37 Idx: 5000 Loss: 0.013659346687547647
Epoch: 38 Idx: 0 Loss: 0.023539247548325076
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 398, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt

Epoch: 42 Idx: 5000 Loss: 0.0454128456952059
Epoch: 43 Idx: 0 Loss: 0.01902198114645464
Epoch: 43 Idx: 5000 Loss: 0.010970234245083007
Epoch: 44 Idx: 0 Loss: 0.02136850576568876
Epoch: 44 Idx: 5000 Loss: 0.015097534800539558
Epoch: 45 Idx: 0 Loss: 0.022055641737292162
Epoch: 45 Idx: 5000 Loss: 0.01938969506046727
Epoch: 46 Idx: 0 Loss: 0.01664936019780191
Epoch: 46 Idx: 5000 Loss: 0.034356953767922915
Epoch: 47 Idx: 0 Loss: 0.030786790966797907
Epoch: 47 Idx: 5000 Loss: 0.0330977431949003
Epoch: 48 Idx: 0 Loss: 0.018004422942013064
Epoch: 48 Idx: 5000 Loss: 0.02359674921329743
Epoch: 49 Idx: 0 Loss: 0.02255985609174884
Epoch: 49 Idx: 5000 Loss: 0.023176686443080394
Len (direct inputs):  111
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.25225633617556087
Epoch: 0 Idx: 5000 Loss: 0.022899102385638893
Epoch: 1 Idx: 0 Loss: 0.011101832134757048
Epoch: 1 Idx: 5000 Loss: 0.038260395473794
Epoch: 2 Idx: 0 Loss: 0.024137415301905073
Epoch: 2 Idx: 5000 Loss: 0.04043092588064947
Epoch: 3 Idx: 0 Loss: 0.01571680190817203
Epoch: 3 Idx: 5000 Loss: 0.020014477637509104
Epoch: 4 Idx: 0 Loss: 0.031546672516195855
Epoch: 4 Idx: 5000 Loss: 0.02814775537922151
Epoch: 5 Idx: 0 Loss: 0.011829426437329557
Epoch: 5 Idx: 5000 Loss: 0.018624794889332212
Epoch: 6 Idx: 0 Loss: 0.016122056517301496
Epoch: 6 Idx: 5000 Loss: 0.024993661087214528
Epoch: 7 Idx: 0 Loss: 0.04768893142574191
Epoch: 7 Idx: 5000 Loss: 0.016615202247926994
Epoch: 8 Idx: 0 Loss: 0.011139772505766848
Epoch: 8 Idx: 5000 Loss: 0.01712513668662828
Epoch: 9 Idx: 0 Loss: 0.010057904339797164
Epoch: 9 Idx: 5000 Loss: 0.02598032340406017
Epoch: 10 Idx: 0 Loss: 0.028937851589267236
Epoch: 10 Idx: 5000 Loss: 0.015645021663420324
Epoch: 11 Idx: 0 Loss: 0.02207605735030111
Epoch: 11 Idx: 5000 Loss: 0.03090178564110592
Epoch: 12 Idx: 0 Loss: 0.024281530799601485
Epoch: 12 Idx: 5000 Loss: 0.020984950876121712
Epoch: 13 Idx: 0 Loss: 0.01353940283543253
Epoch: 13 Idx: 5000 Loss: 0.016799808995211128
Epoch: 14 Idx: 0 Loss: 0.015682490275724305
Epoch: 14 Idx: 5000 Loss: 0.022657571002683986
Epoch: 15 Idx: 0 Loss: 0.03431212032762172
Epoch: 15 Idx: 5000 Loss: 0.018953098573138347
Epoch: 16 Idx: 0 Loss: 0.020808239131642176
Epoch: 16 Idx: 5000 Loss: 0.028150985635642597
Epoch: 17 Idx: 0 Loss: 0.034837390706133634
Epoch: 17 Idx: 5000 Loss: 0.03707781872865337
Epoch: 18 Idx: 0 Loss: 0.036051105366196476
Epoch: 18 Idx: 5000 Loss: 0.010146101104794397
Epoch: 19 Idx: 0 Loss: 0.024529309078579673
Epoch: 19 Idx: 5000 Loss: 0.01636470021529531
Epoch: 20 Idx: 0 Loss: 0.006744135992651966
Epoch: 20 Idx: 5000 Loss: 0.03160352701487148
Epoch: 21 Idx: 0 Loss: 0.02043833139077113
Epoch: 21 Idx: 5000 Loss: 0.03803673860627345
Epoch: 22 Idx: 0 Loss: 0.033601142717072804
Epoch: 22 Idx: 5000 Loss: 0.01158610320015453
Epoch: 23 Idx: 0 Loss: 0.03241792402351501
Epoch: 23 Idx: 5000 Loss: 0.018150391682284484
Epoch: 24 Idx: 0 Loss: 0.015563725254521973
Epoch: 24 Idx: 5000 Loss: 0.020114418288704167
Epoch: 25 Idx: 0 Loss: 0.0153908991533834
Epoch: 25 Idx: 5000 Loss: 0.013052039396834521
Epoch: 26 Idx: 0 Loss: 0.012892608590376302
Epoch: 26 Idx: 5000 Loss: 0.030362948643593977
Epoch: 27 Idx: 0 Loss: 0.013720735626755524
Epoch: 27 Idx: 5000 Loss: 0.02507859440868805
Epoch: 28 Idx: 0 Loss: 0.021436672889164444
Epoch: 28 Idx: 5000 Loss: 0.02117250383730212
Epoch: 29 Idx: 0 Loss: 0.030295035975568006
Epoch: 29 Idx: 5000 Loss: 0.00970489126323644
Epoch: 30 Idx: 0 Loss: 0.031660971000545976
Epoch: 30 Idx: 5000 Loss: 0.01762022786051646
Epoch: 31 Idx: 0 Loss: 0.034333722571842355
Epoch: 31 Idx: 5000 Loss: 0.009801217405374815
Epoch: 32 Idx: 0 Loss: 0.015043627035943963
Epoch: 32 Idx: 5000 Loss: 0.02485777472755406
Epoch: 33 Idx: 0 Loss: 0.02983673325600495
Epoch: 33 Idx: 5000 Loss: 0.018514016370248073
Epoch: 34 Idx: 0 Loss: 0.012257972773862681
Epoch: 34 Idx: 5000 Loss: 0.02391403175090482
Epoch: 35 Idx: 0 Loss: 0.02207876512008866
Epoch: 35 Idx: 5000 Loss: 0.020605682759344237
Epoch: 36 Idx: 0 Loss: 0.03237587100292999
Epoch: 36 Idx: 5000 Loss: 0.0210264300579628
Epoch: 37 Idx: 0 Loss: 0.021124590521409168
Epoch: 37 Idx: 5000 Loss: 0.019550370247281023
Epoch: 38 Idx: 0 Loss: 0.032178694861774895
Epoch: 38 Idx: 5000 Loss: 0.01599093774158116
Epoch: 39 Idx: 0 Loss: 0.005078476282051724
Epoch: 39 Idx: 5000 Loss: 0.03719207705502567
Epoch: 40 Idx: 0 Loss: 0.03865584885197615
Epoch: 40 Idx: 5000 Loss: 0.015773139148491565
Epoch: 41 Idx: 0 Loss: 0.017251147498448235
Epoch: 41 Idx: 5000 Loss: 0.014533674516749948
Epoch: 42 Idx: 0 Loss: 0.01728220738509544
Epoch: 42 Idx: 5000 Loss: 0.03128142256572683
Epoch: 43 Idx: 0 Loss: 0.01610169141171408
Epoch: 43 Idx: 5000 Loss: 0.032255962934096556
Epoch: 44 Idx: 0 Loss: 0.020726481635168364
Epoch: 44 Idx: 5000 Loss: 0.01675071114424091
Epoch: 45 Idx: 0 Loss: 0.024248471356113584
Epoch: 45 Idx: 5000 Loss: 0.021853510924907976
Epoch: 46 Idx: 0 Loss: 0.018186258709583618
Epoch: 46 Idx: 5000 Loss: 0.02583681148532621
Epoch: 47 Idx: 0 Loss: 0.03529652374194899
Epoch: 47 Idx: 5000 Loss: 0.024865345374262826
Epoch: 48 Idx: 0 Loss: 0.02333501535326508
Epoch: 48 Idx: 5000 Loss: 0.010784059004515765
Epoch: 49 Idx: 0 Loss: 0.024096435517002107
Epoch: 49 Idx: 5000 Loss: 0.02871801498869053
Len (direct inputs):  109
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17733100722462558
Epoch: 0 Idx: 5000 Loss: 0.008159448371523726
Epoch: 1 Idx: 0 Loss: 0.020624314458133827
Epoch: 1 Idx: 5000 Loss: 0.02778419041384856
Epoch: 2 Idx: 0 Loss: 0.024001693283592685
Epoch: 2 Idx: 5000 Loss: 0.026919006845289792
Epoch: 3 Idx: 0 Loss: 0.014434736189998638
Epoch: 3 Idx: 5000 Loss: 0.01579908607909101
Epoch: 4 Idx: 0 Loss: 0.022005547493791498
Epoch: 4 Idx: 5000 Loss: 0.03566390375260096
Epoch: 5 Idx: 0 Loss: 0.029889203168822792
Epoch: 5 Idx: 5000 Loss: 0.012232644590014042
Epoch: 6 Idx: 0 Loss: 0.015001476276388039
Epoch: 6 Idx: 5000 Loss: 0.030740331214658616
Epoch: 7 Idx: 0 Loss: 0.016271225896150224
Epoch: 7 Idx: 5000 Loss: 0.03956521687876499
Epoch: 8 Idx: 0 Loss: 0.03857349768142029
Epoch: 8 Idx: 5000 Loss: 0.017560879830564837
Epoch: 9 Idx: 0 Loss: 0.016156635684878635
Epoch: 9 Idx: 5000 Loss: 0.009705079689714381
Epoch: 10 Idx: 0 Loss: 0.021839698880858363
Epoch: 10 Idx: 5000 Loss: 0.02167508420200817
Epoch: 11 Idx: 0 Loss: 0.020204998697610423
Epoch: 11 Idx: 5000 Loss: 0.03140084854282703
Epoch: 12 Idx: 0 Loss: 0.02283955410650774
Epoch: 12 Idx: 5000 Loss: 0.02054372321520328
Epoch: 13 Idx: 0 Loss: 0.01622069790819429
Epoch: 13 Idx: 5000 Loss: 0.03411336838482881
Epoch: 14 Idx: 0 Loss: 0.02886117663823346
Epoch: 14 Idx: 5000 Loss: 0.02442658363116297
Epoch: 15 Idx: 0 Loss: 0.01758187354944927
Epoch: 15 Idx: 5000 Loss: 0.011394152603978061
Epoch: 16 Idx: 0 Loss: 0.038330576826302615
Epoch: 16 Idx: 5000 Loss: 0.01865879330222986
Epoch: 17 Idx: 0 Loss: 0.019211849307803664
Epoch: 17 Idx: 5000 Loss: 0.019930125200923104
Epoch: 18 Idx: 0 Loss: 0.021736794283968977
Epoch: 18 Idx: 5000 Loss: 0.013554113579975638
Epoch: 19 Idx: 0 Loss: 0.022608358559809607
Epoch: 19 Idx: 5000 Loss: 0.03308190834508866
Epoch: 20 Idx: 0 Loss: 0.01840653226387746
Epoch: 20 Idx: 5000 Loss: 0.030431284433453228
Epoch: 21 Idx: 0 Loss: 0.030582388193744838
Epoch: 21 Idx: 5000 Loss: 0.00943441762300875
Epoch: 22 Idx: 0 Loss: 0.03703308941075799
Epoch: 22 Idx: 5000 Loss: 0.021441629418575525
Epoch: 23 Idx: 0 Loss: 0.028071133775447413
Epoch: 23 Idx: 5000 Loss: 0.021453981650007045
Epoch: 24 Idx: 0 Loss: 0.02325941109405073
Epoch: 24 Idx: 5000 Loss: 0.05947548397901915
Epoch: 25 Idx: 0 Loss: 0.01688540110858957
Epoch: 25 Idx: 5000 Loss: 0.014364988533791421
Epoch: 26 Idx: 0 Loss: 0.034098197225447345
Epoch: 26 Idx: 5000 Loss: 0.019564110953451782
Epoch: 27 Idx: 0 Loss: 0.02255993195366355
Epoch: 27 Idx: 5000 Loss: 0.019736995370004562
Epoch: 28 Idx: 0 Loss: 0.01587599680473855
Epoch: 28 Idx: 5000 Loss: 0.03833302523040465
Epoch: 29 Idx: 0 Loss: 0.016790565613107267
Epoch: 29 Idx: 5000 Loss: 0.00850935231118072
Epoch: 30 Idx: 0 Loss: 0.016378921550096206
Epoch: 30 Idx: 5000 Loss: 0.027743931159104958
Epoch: 31 Idx: 0 Loss: 0.02283768259247669
Epoch: 31 Idx: 5000 Loss: 0.023426925237503254
Epoch: 32 Idx: 0 Loss: 0.02723522477524129
Epoch: 32 Idx: 5000 Loss: 0.04811265655148485
Epoch: 33 Idx: 0 Loss: 0.019238016373880645
Epoch: 33 Idx: 5000 Loss: 0.01790150346955422
Epoch: 34 Idx: 0 Loss: 0.019582678742052356
Epoch: 34 Idx: 5000 Loss: 0.012520983788681883
Epoch: 35 Idx: 0 Loss: 0.020584278204509632
Epoch: 35 Idx: 5000 Loss: 0.03678579070256502
Epoch: 36 Idx: 0 Loss: 0.013044435725355713
Epoch: 36 Idx: 5000 Loss: 0.022790671231120302
Epoch: 37 Idx: 0 Loss: 0.02525390288367324
Epoch: 37 Idx: 5000 Loss: 0.031961893325748274
Epoch: 38 Idx: 0 Loss: 0.016321602834701677
Epoch: 38 Idx: 5000 Loss: 0.023658708634908716
Epoch: 39 Idx: 0 Loss: 0.022985795111151237
Epoch: 39 Idx: 5000 Loss: 0.019056388148819763
Epoch: 40 Idx: 0 Loss: 0.0177622415140023
Epoch: 40 Idx: 5000 Loss: 0.02959151490138992
Epoch: 41 Idx: 0 Loss: 0.027143266305267037
Epoch: 41 Idx: 5000 Loss: 0.016185568365269076
Epoch: 42 Idx: 0 Loss: 0.022485877836706972
Epoch: 42 Idx: 5000 Loss: 0.010109525948470505
Epoch: 43 Idx: 0 Loss: 0.015049965580883725
Epoch: 43 Idx: 5000 Loss: 0.02672097394733853
Epoch: 44 Idx: 0 Loss: 0.027067832879701045
Epoch: 44 Idx: 5000 Loss: 0.01836476823594701
Epoch: 45 Idx: 0 Loss: 0.02663369791600161
Epoch: 45 Idx: 5000 Loss: 0.016523040345237362
Epoch: 46 Idx: 0 Loss: 0.02614458228836057
Epoch: 46 Idx: 5000 Loss: 0.014835251515743966
Epoch: 47 Idx: 0 Loss: 0.018341713309202066
Epoch: 47 Idx: 5000 Loss: 0.023094515848039475
Epoch: 48 Idx: 0 Loss: 0.02139859587216532
Epoch: 48 Idx: 5000 Loss: 0.016187747531112136
Epoch: 49 Idx: 0 Loss: 0.02367989378271071
Epoch: 49 Idx: 5000 Loss: 0.020878748376075673
Len (direct inputs):  87
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
0.8347043311696052
Parameter containing:
tensor([0.8347], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.18074576664544179
Epoch: 0 Idx: 5000 Loss: 0.015622154569161498
Epoch: 1 Idx: 0 Loss: 0.022102961323717876
Epoch: 1 Idx: 5000 Loss: 0.030600330321414265
Epoch: 2 Idx: 0 Loss: 0.026611642095770133
Epoch: 2 Idx: 5000 Loss: 0.017911834941148
Epoch: 3 Idx: 0 Loss: 0.030912546098025476
Epoch: 3 Idx: 5000 Loss: 0.025051145722073134
Epoch: 4 Idx: 0 Loss: 0.029891648448325744
Epoch: 4 Idx: 5000 Loss: 0.014138706635284857
Epoch: 5 Idx: 0 Loss: 0.018324205744539537
Epoch: 5 Idx: 5000 Loss: 0.025415918052086112
Epoch: 6 Idx: 0 Loss: 0.022809332514909457
Epoch: 6 Idx: 5000 Loss: 0.016756594905096317
Epoch: 7 Idx: 0 Loss: 0.01527748683606648
Epoch: 7 Idx: 5000 Loss: 0.030180524946692273
Epoch: 8 Idx: 0 Loss: 0.01827424089726101
Epoch: 8 Idx: 5000 Loss: 0.020734292586997827
Epoch: 9 Idx: 0 Loss: 0.027767726185597395
Epoch: 9 Idx: 5000 Loss: 0.013024761990912175
Epoch: 10 Idx: 0 Loss: 0.03514923534874058
Epoch: 10 Idx: 5000 Loss: 0.03783479661515817
Epoch: 11 Idx: 0 Loss: 0.029427407915194624
Epoch: 11 Idx: 5000 Loss: 0.014547808409746085
Epoch: 12 Idx: 0 Loss: 0.015146661503228406
Epoch: 12 Idx: 5000 Loss: 0.016017702022317507
Epoch: 13 Idx: 0 Loss: 0.009800750872815654
Epoch: 13 Idx: 5000 Loss: 0.023675887234117707
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 466, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
0.02549730906789362
Epoch: 18 Idx: 0 Loss: 0.0400109070984447
Epoch: 18 Idx: 5000 Loss: 0.018578530653465295
Epoch: 19 Idx: 0 Loss: 0.01786454162427357
Epoch: 19 Idx: 5000 Loss: 0.01661842552669738
Epoch: 20 Idx: 0 Loss: 0.019721321521500967
Epoch: 20 Idx: 5000 Loss: 0.028114886832195145
Epoch: 21 Idx: 0 Loss: 0.017539540295296776
Epoch: 21 Idx: 5000 Loss: 0.028407054223581182
Epoch: 22 Idx: 0 Loss: 0.02666310205465513
Epoch: 22 Idx: 5000 Loss: 0.01794484910506964
Epoch: 23 Idx: 0 Loss: 0.02265286767720575
Epoch: 23 Idx: 5000 Loss: 0.01476875838053713
Epoch: 24 Idx: 0 Loss: 0.02273393801001373
Epoch: 24 Idx: 5000 Loss: 0.031350715948341185
Epoch: 25 Idx: 0 Loss: 0.01762272237261507
Epoch: 25 Idx: 5000 Loss: 0.03796825421435251
Epoch: 26 Idx: 0 Loss: 0.012576925711236035
Epoch: 26 Idx: 5000 Loss: 0.03128287164607976
Epoch: 27 Idx: 0 Loss: 0.034940998452745425
Epoch: 27 Idx: 5000 Loss: 0.027369428731660664
Epoch: 28 Idx: 0 Loss: 0.01521647569782858
Epoch: 28 Idx: 5000 Loss: 0.012209370538402275
Epoch: 29 Idx: 0 Loss: 0.018986040142522753
Epoch: 29 Idx: 5000 Loss: 0.031489791304315506
Epoch: 30 Idx: 0 Loss: 0.015803519661394684
Epoch: 30 Idx: 5000 Loss: 0.012347124590489974
Epoch: 31 Idx: 0 Loss: 0.032365718410385276
Epoch: 31 Idx: 5000 Loss: 0.028573988361499526
Epoch: 32 Idx: 0 Loss: 0.02051192077550751
Epoch: 32 Idx: 5000 Loss: 0.017880906318756773
Epoch: 33 Idx: 0 Loss: 0.021680946321961776
Epoch: 33 Idx: 5000 Loss: 0.006219405727346816
Epoch: 34 Idx: 0 Loss: 0.022784849315676044
Epoch: 34 Idx: 5000 Loss: 0.01447585668700594
Epoch: 35 Idx: 0 Loss: 0.020238837956113743
Epoch: 35 Idx: 5000 Loss: 0.022343069982483726
Epoch: 36 Idx: 0 Loss: 0.014021272262451355
Epoch: 36 Idx: 5000 Loss: 0.020177563486982124
Epoch: 37 Idx: 0 Loss: 0.012456702227353143
Epoch: 37 Idx: 5000 Loss: 0.013574126638858678
Epoch: 38 Idx: 0 Loss: 0.015317494609195913
Epoch: 38 Idx: 5000 Loss: 0.013948585335980562
Epoch: 39 Idx: 0 Loss: 0.023353907767526327
Epoch: 39 Idx: 5000 Loss: 0.023376483772414025
Epoch: 40 Idx: 0 Loss: 0.012721392032990862
Epoch: 40 Idx: 5000 Loss: 0.026670281471972306
Epoch: 41 Idx: 0 Loss: 0.026135426468587866
Epoch: 41 Idx: 5000 Loss: 0.0372127275784577
Epoch: 42 Idx: 0 Loss: 0.013863954024631718
Epoch: 42 Idx: 5000 Loss: 0.015670459078130958
Epoch: 43 Idx: 0 Loss: 0.009134250325452906
Epoch: 43 Idx: 5000 Loss: 0.05314909759321772
Epoch: 44 Idx: 0 Loss: 0.016244802579681326
Epoch: 44 Idx: 5000 Loss: 0.013389648250675795
Epoch: 45 Idx: 0 Loss: 0.02212591164314942
Epoch: 45 Idx: 5000 Loss: 0.02213757066274542
Epoch: 46 Idx: 0 Loss: 0.035922112001883055
Epoch: 46 Idx: 5000 Loss: 0.0185955393281216
Epoch: 47 Idx: 0 Loss: 0.014503530199376596
Epoch: 47 Idx: 5000 Loss: 0.00601331748015128
Epoch: 48 Idx: 0 Loss: 0.020422046076830395
Epoch: 48 Idx: 5000 Loss: 0.03409348873126468
Epoch: 49 Idx: 0 Loss: 0.021522204938880136
Epoch: 49 Idx: 5000 Loss: 0.04174342448548544
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 475, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
xceeded
ded
d
quota exceeded
as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 19:35:31 2020
Results reported at Tue Sep  1 19:35:31 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs1_3.pkl Models/anatomy_aml_bagofnbrs1_3.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   20435.57 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2720.46 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   20504 sec.
    Turnaround time :                            20505 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc255>
Subject: Job 3289835: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs2_3.pkl Models/anatomy_aml_bagofnbrs2_3.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs2_3.pkl Models/anatomy_aml_bagofnbrs2_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc255>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 19:52:47 2020
Results reported at Tue Sep  1 19:52:47 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs2_3.pkl Models/anatomy_aml_bagofnbrs2_3.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21528.28 sec.
    Max Memory :                                 2922 MB
    Average Memory :                             2714.41 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40495.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21540 sec.
    Turnaround time :                            21541 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc240>
Subject: Job 3289837: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs3_3.pkl Models/anatomy_aml_bagofnbrs3_3.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs3_3.pkl Models/anatomy_aml_bagofnbrs3_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc240>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 20:11:08 2020
Results reported at Tue Sep  1 20:11:08 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs3_3.pkl Models/anatomy_aml_bagofnbrs3_3.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   22564.45 sec.
    Max Memory :                                 2939 MB
    Average Memory :                             2721.29 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40478.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   22641 sec.
    Turnaround time :                            22642 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc234>
Subject: Job 3289839: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs4_3.pkl Models/anatomy_aml_bagofnbrs4_3.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs4_3.pkl Models/anatomy_aml_bagofnbrs4_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc234>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 20:19:58 2020
Results reported at Tue Sep  1 20:19:58 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs4_3.pkl Models/anatomy_aml_bagofnbrs4_3.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23096.43 sec.
    Max Memory :                                 2932 MB
    Average Memory :                             2725.59 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40485.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23181 sec.
    Turnaround time :                            23171 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc226>
Subject: Job 3289841: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs5_3.pkl Models/anatomy_aml_bagofnbrs5_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs5_3.pkl Models/anatomy_aml_bagofnbrs5_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc226>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 21:06:46 2020
Results reported at Tue Sep  1 21:06:46 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs5_3.pkl Models/anatomy_aml_bagofnbrs5_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25899.02 sec.
    Max Memory :                                 2930 MB
    Average Memory :                             2718.79 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40487.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25978 sec.
    Turnaround time :                            25979 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc225>
Subject: Job 3289843: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 3 Output/test_anatomy_aml_bagofnbrs7_3.pkl Models/anatomy_aml_bagofnbrs7_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 3 Output/test_anatomy_aml_bagofnbrs7_3.pkl Models/anatomy_aml_bagofnbrs7_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc225>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 22:15:32 2020
Results reported at Tue Sep  1 22:15:32 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 3 Output/test_anatomy_aml_bagofnbrs7_3.pkl Models/anatomy_aml_bagofnbrs7_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30009.67 sec.
    Max Memory :                                 2925 MB
    Average Memory :                             2709.62 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40492.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30104 sec.
    Turnaround time :                            30105 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc228>
Subject: Job 3289845: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs8_3.pkl Models/anatomy_aml_bagofnbrs8_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs8_3.pkl Models/anatomy_aml_bagofnbrs8_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc228>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 23:06:23 2020
Results reported at Tue Sep  1 23:06:23 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs8_3.pkl Models/anatomy_aml_bagofnbrs8_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   31706.72 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2720.63 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   33178 sec.
    Turnaround time :                            33156 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc267>
Subject: Job 3289871: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 3 Output/test_anatomy_aml_bagofnbrs152_3.pkl Models/anatomy_aml_bagofnbrs152_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 3 Output/test_anatomy_aml_bagofnbrs152_3.pkl Models/anatomy_aml_bagofnbrs152_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:49 2020
Job was executed on host(s) <dccxc267>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 3 Output/test_anatomy_aml_bagofnbrs152_3.pkl Models/anatomy_aml_bagofnbrs152_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80436.55 sec.
    Max Memory :                                 2730 MB
    Average Memory :                             2642.16 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40687.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80642 sec.
    Turnaround time :                            80699 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc278>
Subject: Job 3289869: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 3 Output/test_anatomy_aml_bagofnbrs80_3.pkl Models/anatomy_aml_bagofnbrs80_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 3 Output/test_anatomy_aml_bagofnbrs80_3.pkl Models/anatomy_aml_bagofnbrs80_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:49 2020
Job was executed on host(s) <dccxc278>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 3 Output/test_anatomy_aml_bagofnbrs80_3.pkl Models/anatomy_aml_bagofnbrs80_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80534.52 sec.
    Max Memory :                                 2735 MB
    Average Memory :                             2668.48 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40682.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80668 sec.
    Turnaround time :                            80700 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc208>
Subject: Job 3289867: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 3 Output/test_anatomy_aml_bagofnbrs40_3.pkl Models/anatomy_aml_bagofnbrs40_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 3 Output/test_anatomy_aml_bagofnbrs40_3.pkl Models/anatomy_aml_bagofnbrs40_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
Job was executed on host(s) <dccxc208>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 3 Output/test_anatomy_aml_bagofnbrs40_3.pkl Models/anatomy_aml_bagofnbrs40_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80483.93 sec.
    Max Memory :                                 2921 MB
    Average Memory :                             2692.03 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40496.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80643 sec.
    Turnaround time :                            80701 sec.

The output (if any) is above this job summary.

