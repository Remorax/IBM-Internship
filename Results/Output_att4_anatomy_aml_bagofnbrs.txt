Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2177216712533136
Epoch: 0 Idx: 5000 Loss: 0.017723186831185362
Epoch: 1 Idx: 0 Loss: 0.03907929040854541
Epoch: 1 Idx: 5000 Loss: 0.02763463127232316
Epoch: 2 Idx: 0 Loss: 0.02858991829235101
Epoch: 2 Idx: 5000 Loss: 0.027293130362424374
Epoch: 3 Idx: 0 Loss: 0.023386086393317546
Epoch: 3 Idx: 5000 Loss: 0.012515514113706744
Epoch: 4 Idx: 0 Loss: 0.01719812575645072
Epoch: 4 Idx: 5000 Loss: 0.026097632891451436
Epoch: 5 Idx: 0 Loss: 0.026443208530561327
Epoch: 5 Idx: 5000 Loss: 0.037671088631979315
Epoch: 6 Idx: 0 Loss: 0.017653402702660057
Epoch: 6 Idx: 5000 Loss: 0.01694081811337865
Epoch: 7 Idx: 0 Loss: 0.006703090355173379
Epoch: 7 Idx: 5000 Loss: 0.04012594684647951
Epoch: 8 Idx: 0 Loss: 0.045730895599664825
Epoch: 8 Idx: 5000 Loss: 0.022065740693664772
Epoch: 9 Idx: 0 Loss: 0.02431618115818166
Epoch: 9 Idx: 5000 Loss: 0.013180820602406801
Epoch: 10 Idx: 0 Loss: 0.031133546748664073
Epoch: 10 Idx: 5000 Loss: 0.025853741419125824
Epoch: 11 Idx: 0 Loss: 0.022432889351447896
Epoch: 11 Idx: 5000 Loss: 0.028369284480499274
Epoch: 12 Idx: 0 Loss: 0.027629995947545574
Epoch: 12 Idx: 5000 Loss: 0.012779422369470744
Epoch: 13 Idx: 0 Loss: 0.024887570028424934
Epoch: 13 Idx: 5000 Loss: 0.020651395245620853
Epoch: 14 Idx: 0 Loss: 0.023383996177087002
Epoch: 14 Idx: 5000 Loss: 0.0332946479826826
Epoch: 15 Idx: 0 Loss: 0.03048116519314395
Epoch: 15 Idx: 5000 Loss: 0.024297417865255733
Epoch: 16 Idx: 0 Loss: 0.0171078205564728
Epoch: 16 Idx: 5000 Loss: 0.018637055892335802
Epoch: 17 Idx: 0 Loss: 0.022332791970191706
Epoch: 17 Idx: 5000 Loss: 0.013074939635401628
Epoch: 18 Idx: 0 Loss: 0.026737221755724183
Epoch: 18 Idx: 5000 Loss: 0.019288098481816493
Epoch: 19 Idx: 0 Loss: 0.009968848781921658
Epoch: 19 Idx: 5000 Loss: 0.02092823803422862
Epoch: 20 Idx: 0 Loss: 0.011901141786418917
Epoch: 20 Idx: 5000 Loss: 0.01888745453881964
Epoch: 21 Idx: 0 Loss: 0.024637430583589172
Epoch: 21 Idx: 5000 Loss: 0.018114659681771506
Epoch: 22 Idx: 0 Loss: 0.012155518208814296
Epoch: 22 Idx: 5000 Loss: 0.016856498546883265
Epoch: 23 Idx: 0 Loss: 0.02851612655422088
Epoch: 23 Idx: 5000 Loss: 0.028460632822048238
Epoch: 24 Idx: 0 Loss: 0.023644086988943706
Epoch: 24 Idx: 5000 Loss: 0.008688713501203253
Epoch: 25 Idx: 0 Loss: 0.049438930978880954
Epoch: 25 Idx: 5000 Loss: 0.01144299707246954
Epoch: 26 Idx: 0 Loss: 0.015060834760770402
Epoch: 26 Idx: 5000 Loss: 0.02538460456421236
Epoch: 27 Idx: 0 Loss: 0.030721751303055912
Epoch: 27 Idx: 5000 Loss: 0.024825220995222712
Epoch: 28 Idx: 0 Loss: 0.02613728483011861
Epoch: 28 Idx: 5000 Loss: 0.02527065663163435
Epoch: 29 Idx: 0 Loss: 0.028455920822815818
Epoch: 29 Idx: 5000 Loss: 0.02041921741421711
Epoch: 30 Idx: 0 Loss: 0.011919919229376744
Epoch: 30 Idx: 5000 Loss: 0.01438142910501043
Epoch: 31 Idx: 0 Loss: 0.02225705267271587
Epoch: 31 Idx: 5000 Loss: 0.01737235845484817
Epoch: 32 Idx: 0 Loss: 0.02472560534322716
Epoch: 32 Idx: 5000 Loss: 0.028096332669553418
Epoch: 33 Idx: 0 Loss: 0.024135579058867304
Epoch: 33 Idx: 5000 Loss: 0.008773289581740878
Epoch: 34 Idx: 0 Loss: 0.018507651342998428
Epoch: 34 Idx: 5000 Loss: 0.02860720861680534
Epoch: 35 Idx: 0 Loss: 0.033078414540691756
Epoch: 35 Idx: 5000 Loss: 0.011266023088624899
Epoch: 36 Idx: 0 Loss: 0.024077984851433747
Epoch: 36 Idx: 5000 Loss: 0.014581774521175173
Epoch: 37 Idx: 0 Loss: 0.016005296284972704
Epoch: 37 Idx: 5000 Loss: 0.03703965557960934
Epoch: 38 Idx: 0 Loss: 0.01980056826024171
Epoch: 38 Idx: 5000 Loss: 0.01688301716559838
Epoch: 39 Idx: 0 Loss: 0.02129821215355972
Epoch: 39 Idx: 5000 Loss: 0.014072046455498161
Epoch: 40 Idx: 0 Loss: 0.019571185435577357
Epoch: 40 Idx: 5000 Loss: 0.013850850540132426
Epoch: 41 Idx: 0 Loss: 0.04509870412038617
Epoch: 41 Idx: 5000 Loss: 0.015025467594123137
Epoch: 42 Idx: 0 Loss: 0.027216991614688962
Epoch: 42 Idx: 5000 Loss: 0.035217186161480056
Epoch: 43 Idx: 0 Loss: 0.015002880308413055
Epoch: 43 Idx: 5000 Loss: 0.027121779507284314
Epoch: 44 Idx: 0 Loss: 0.014739447319624219
Epoch: 44 Idx: 5000 Loss: 0.023878516399766556
Epoch: 45 Idx: 0 Loss: 0.027159220734800864
Epoch: 45 Idx: 5000 Loss: 0.02144361283379431
Epoch: 46 Idx: 0 Loss: 0.03835327205765557
Epoch: 46 Idx: 5000 Loss: 0.014832772363895677
Epoch: 47 Idx: 0 Loss: 0.022696528296176417
Epoch: 47 Idx: 5000 Loss: 0.019726461676074877
Epoch: 48 Idx: 0 Loss: 0.012659009342087137
Epoch: 48 Idx: 5000 Loss: 0.023660479465281804
Epoch: 49 Idx: 0 Loss: 0.04127348191475486
Epoch: 49 Idx: 5000 Loss: 0.038408102610584596
Len (direct inputs):  113
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division TrainingTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2394560470792766
Epoch: 0 Idx: 5000 Loss: 0.016781194303610004
Epoch: 1 Idx: 0 Loss: 0.017699700525305583
Epoch: 1 Idx: 5000 Loss: 0.02906772827711356
Epoch: 2 Idx: 0 Loss: 0.020762944874105627
Epoch: 2 Idx: 5000 Loss: 0.022115299062043695
Epoch: 3 Idx: 0 Loss: 0.02127416168637434
Epoch: 3 Idx: 5000 Loss: 0.012384024032330194
Epoch: 4 Idx: 0 Loss: 0.011995285339609852
Epoch: 4 Idx: 5000 Loss: 0.021798330585910942
Epoch: 5 Idx: 0 Loss: 0.021374440071256103
Epoch: 5 Idx: 5000 Loss: 0.03829982456522894
Epoch: 6 Idx: 0 Loss: 0.018062428554405287
Epoch: 6 Idx: 5000 Loss: 0.008240046256678565
Epoch: 7 Idx: 0 Loss: 0.011052170677969271
Epoch: 7 Idx: 5000 Loss: 0.02502451173579654
Epoch: 8 Idx: 0 Loss: 0.03876675406672523
Epoch: 8 Idx: 5000 Loss: 0.03567870852194894
Epoch: 9 Idx: 0 Loss: 0.017584178700224315
Epoch: 9 Idx: 5000 Loss: 0.018396172770190237
Epoch: 10 Idx: 0 Loss: 0.018722553229081465
Epoch: 10 Idx: 5000 Loss: 0.015535467332678906
Epoch: 11 Idx: 0 Loss: 0.01722852015949295
Epoch: 11 Idx: 5000 Loss: 0.023967545573990508
Epoch: 12 Idx: 0 Loss: 0.02815073775378737
Epoch: 12 Idx: 5000 Loss: 0.022813181440183374
Epoch: 13 Idx: 0 Loss: 0.02827191212961698
Epoch: 13 Idx: 5000 Loss: 0.00991613105435871
Epoch: 14 Idx: 0 Loss: 0.024970648305113675
Epoch: 14 Idx: 5000 Loss: 0.018983407257714212
Epoch: 15 Idx: 0 Loss: 0.040897652126914506
Epoch: 15 Idx: 5000 Loss: 0.014190312708590625
Epoch: 16 Idx: 0 Loss: 0.018285500870798835
Epoch: 16 Idx: 5000 Loss: 0.03600457323230435
Epoch: 17 Idx: 0 Loss: 0.028358409196971062
Epoch: 17 Idx: 5000 Loss: 0.026230293883087237
Epoch: 18 Idx: 0 Loss: 0.016051204237707145
Epoch: 18 Idx: 5000 Loss: 0.020694152694459345
Epoch: 19 Idx: 0 Loss: 0.015996589809031677
Epoch: 19 Idx: 5000 Loss: 0.023418799822136894
Epoch: 20 Idx: 0 Loss: 0.023007322174091113
Epoch: 20 Idx: 5000 Loss: 0.01957746688145705
Epoch: 21 Idx: 0 Loss: 0.021983536620927814
Epoch: 21 Idx: 5000 Loss: 0.02295324302054409
Epoch: 22 Idx: 0 Loss: 0.017146897776520592
Epoch: 22 Idx: 5000 Loss: 0.011395561111198244
Epoch: 23 Idx: 0 Loss: 0.010441704378409722
Epoch: 23 Idx: 5000 Loss: 0.014599476195214681
Epoch: 24 Idx: 0 Loss: 0.02085679985872423
Epoch: 24 Idx: 5000 Loss: 0.05107026545346929
Epoch: 25 Idx: 0 Loss: 0.02092324515914415
Epoch: 25 Idx: 5000 Loss: 0.020741109264657068
Epoch: 26 Idx: 0 Loss: 0.014744915052183874
Epoch: 26 Idx: 5000 Loss: 0.03415990700489081
Epoch: 27 Idx: 0 Loss: 0.006186274737832365
Epoch: 27 Idx: 5000 Loss: 0.02969730382856781
Epoch: 28 Idx: 0 Loss: 0.029600411951603622
Epoch: 28 Idx: 5000 Loss: 0.027301014373317348
Epoch: 29 Idx: 0 Loss: 0.01707032867338638
Epoch: 29 Idx: 5000 Loss: 0.025286751210535252
Epoch: 30 Idx: 0 Loss: 0.025349327016437967
Epoch: 30 Idx: 5000 Loss: 0.019614069914413446
Epoch: 31 Idx: 0 Loss: 0.015087158355822318
Epoch: 31 Idx: 5000 Loss: 0.011475695793714871
Epoch: 32 Idx: 0 Loss: 0.038876492421719484
Epoch: 32 Idx: 5000 Loss: 0.022263008223424075
Epoch: 33 Idx: 0 Loss: 0.026292018269069323
Epoch: 33 Idx: 5000 Loss: 0.01707378464778465
Epoch: 34 Idx: 0 Loss: 0.01955513633577509
Epoch: 34 Idx: 5000 Loss: 0.017428630404425924
Epoch: 35 Idx: 0 Loss: 0.015597379353705445
Epoch: 35 Idx: 5000 Loss: 0.018549104400396026
Epoch: 36 Idx: 0 Loss: 0.042276349577791736
Epoch: 36 Idx: 5000 Loss: 0.01362210452962972
Epoch: 37 Idx: 0 Loss: 0.035221227882005865
Epoch: 37 Idx: 5000 Loss: 0.04049268563617248
Epoch: 38 Idx: 0 Loss: 0.03890866430587907
Epoch: 38 Idx: 5000 Loss: 0.03218813673518463
Epoch: 39 Idx: 0 Loss: 0.014408879393144279
Epoch: 39 Idx: 5000 Loss: 0.020469309576707916
Epoch: 40 Idx: 0 Loss: 0.023419140236197206
Epoch: 40 Idx: 5000 Loss: 0.025218057122885065
Epoch: 41 Idx: 0 Loss: 0.02166285142652058
Epoch: 41 Idx: 5000 Loss: 0.02427693479706667
Epoch: 42 Idx: 0 Loss: 0.021676028353719297
Epoch: 42 Idx: 5000 Loss: 0.016517483831437314
Epoch: 43 Idx: 0 Loss: 0.02549575261381709
Epoch: 43 Idx: 5000 Loss: 0.018919864747462302
Epoch: 44 Idx: 0 Loss: 0.012815361640830934
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 387, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
 Idx: 5000 Loss: 0.02997017219196767
Epoch: 47 Idx: 0 Loss: 0.016774342848415093
Epoch: 47 Idx: 5000 Loss: 0.01570885922114132
Epoch: 48 Idx: 0 Loss: 0.03388707941241126
Epoch: 48 Idx: 5000 Loss: 0.013624389210626125
Epoch: 49 Idx: 0 Loss: 0.020349130940964272
Epoch: 49 Idx: 5000 Loss: 0.034764854131385234
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.20482529930817564
Epoch: 0 Idx: 5000 Loss: 0.02505011096886543
Epoch: 1 Idx: 0 Loss: 0.012920316654443799
Epoch: 1 Idx: 5000 Loss: 0.028913991339502627
Epoch: 2 Idx: 0 Loss: 0.014627839656023872
Epoch: 2 Idx: 5000 Loss: 0.008247676182864051
Epoch: 3 Idx: 0 Loss: 0.01686499349662183
Epoch: 3 Idx: 5000 Loss: 0.016107276904949522
Epoch: 4 Idx: 0 Loss: 0.010239341230024663
Epoch: 4 Idx: 5000 Loss: 0.03813467056042187
Epoch: 5 Idx: 0 Loss: 0.016388494615936534
Epoch: 5 Idx: 5000 Loss: 0.021983163340993758
Epoch: 6 Idx: 0 Loss: 0.017217757641604804
Epoch: 6 Idx: 5000 Loss: 0.019789707544398027
Epoch: 7 Idx: 0 Loss: 0.01770625740174009
Epoch: 7 Idx: 5000 Loss: 0.012913731085555743
Epoch: 8 Idx: 0 Loss: 0.01904842072631837
Epoch: 8 Idx: 5000 Loss: 0.0165360907258234
Epoch: 9 Idx: 0 Loss: 0.01799488530722018
Epoch: 9 Idx: 5000 Loss: 0.014943121937260814
Epoch: 10 Idx: 0 Loss: 0.02973549299863075
Epoch: 10 Idx: 5000 Loss: 0.020558125637439698
Epoch: 11 Idx: 0 Loss: 0.023604140461777598
Epoch: 11 Idx: 5000 Loss: 0.018536465996966137
Epoch: 12 Idx: 0 Loss: 0.04728688059594466
Epoch: 12 Idx: 5000 Loss: 0.04003522736670449
Epoch: 13 Idx: 0 Loss: 0.020368690271079477
Epoch: 13 Idx: 5000 Loss: 0.017773880728781215
Epoch: 14 Idx: 0 Loss: 0.03131205598998435
Epoch: 14 Idx: 5000 Loss: 0.021241073998188358
Epoch: 15 Idx: 0 Loss: 0.023757374085566576
Epoch: 15 Idx: 5000 Loss: 0.04059487846793625
Epoch: 16 Idx: 0 Loss: 0.021237760108322228
Epoch: 16 Idx: 5000 Loss: 0.036276095376227835
Epoch: 17 Idx: 0 Loss: 0.013892830156136567
Epoch: 17 Idx: 5000 Loss: 0.017274819121902013
Epoch: 18 Idx: 0 Loss: 0.02252692196547891
Epoch: 18 Idx: 5000 Loss: 0.04205826854991822
Epoch: 19 Idx: 0 Loss: 0.010643025886393105
Epoch: 19 Idx: 5000 Loss: 0.015930540640994452
Epoch: 20 Idx: 0 Loss: 0.02405795627238665
Epoch: 20 Idx: 5000 Loss: 0.022159182849461556
Epoch: 21 Idx: 0 Loss: 0.01404303003651516
Epoch: 21 Idx: 5000 Loss: 0.02371556654830377
Epoch: 22 Idx: 0 Loss: 0.01301589917480436
Epoch: 22 Idx: 5000 Loss: 0.015700540977815036
Epoch: 23 Idx: 0 Loss: 0.022400912564031213
Epoch: 23 Idx: 5000 Loss: 0.012159934188568469
Epoch: 24 Idx: 0 Loss: 0.017543924324126416
Epoch: 24 Idx: 5000 Loss: 0.017215058681481564
Epoch: 25 Idx: 0 Loss: 0.018931933434902473
Epoch: 25 Idx: 5000 Loss: 0.02699951068047069
Epoch: 26 Idx: 0 Loss: 0.014169003144147796
Epoch: 26 Idx: 5000 Loss: 0.021207754073088288
Epoch: 27 Idx: 0 Loss: 0.013003002361521385
Epoch: 27 Idx: 5000 Loss: 0.023768653618728226
Epoch: 28 Idx: 0 Loss: 0.02755282201671542
Epoch: 28 Idx: 5000 Loss: 0.01639712791821048
Epoch: 29 Idx: 0 Loss: 0.03490055626507854
Epoch: 29 Idx: 5000 Loss: 0.03057505022622935
Epoch: 30 Idx: 0 Loss: 0.041233772708876994
Epoch: 30 Idx: 5000 Loss: 0.05298735416571039
Epoch: 31 Idx: 0 Loss: 0.01880331141932412
Epoch: 31 Idx: 5000 Loss: 0.01595180418884314
Epoch: 32 Idx: 0 Loss: 0.011516994897938199
Epoch: 32 Idx: 5000 Loss: 0.018240991691974115
Epoch: 33 Idx: 0 Loss: 0.024036310561919794
Epoch: 33 Idx: 5000 Loss: 0.00870809207606246
Epoch: 34 Idx: 0 Loss: 0.012107020843627948
Epoch: 34 Idx: 5000 Loss: 0.016956932977982717
Epoch: 35 Idx: 0 Loss: 0.016138003181059755
Epoch: 35 Idx: 5000 Loss: 0.03155942169328252
Epoch: 36 Idx: 0 Loss: 0.02092686475518679
Epoch: 36 Idx: 5000 Loss: 0.023464544879134623
Epoch: 37 Idx: 0 Loss: 0.03220947689196024
Epoch: 37 Idx: 5000 Loss: 0.017246609209696895
Epoch: 38 Idx: 0 Loss: 0.024662397603324327
Epoch: 38 Idx: 5000 Loss: 0.03661629691459093
Epoch: 39 Idx: 0 Loss: 0.018293407638640343
Epoch: 39 Idx: 5000 Loss: 0.01833978807328769
Epoch: 40 Idx: 0 Loss: 0.016671756288739494
Epoch: 40 Idx: 5000 Loss: 0.015038618463274471
Epoch: 41 Idx: 0 Loss: 0.0332280451520436
Epoch: 41 Idx: 5000 Loss: 0.02118494269679551
Epoch: 42 Idx: 0 Loss: 0.01779682401803706
Epoch: 42 Idx: 5000 Loss: 0.013319492462483441
Epoch: 43 Idx: 0 Loss: 0.015477982671005221
Epoch: 43 Idx: 5000 Loss: 0.035830830417045725
Epoch: 44 Idx: 0 Loss: 0.016474460510086154
Epoch: 44 Idx: 5000 Loss: 0.02794693052171576
Epoch: 45 Idx: 0 Loss: 0.02241735247186305
Epoch: 45 Idx: 5000 Loss: 0.01862176591165164
Epoch: 46 Idx: 0 Loss: 0.006800153332390089
Epoch: 46 Idx: 5000 Loss: 0.023272677474042623
Epoch: 47 Idx: 0 Loss: 0.021994679083693767
Epoch: 47 Idx: 5000 Loss: 0.02565680386890891
Epoch: 48 Idx: 0 Loss: 0.03753731369687405
Epoch: 48 Idx: 5000 Loss: 0.02091929105692306
Epoch: 49 Idx: 0 Loss: 0.013958198185832625
Epoch: 49 Idx: 5000 Loss: 0.0240559843466598
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.3213630436949091
Epoch: 0 Idx: 5000 Loss: 0.010524415985740538
Epoch: 1 Idx: 0 Loss: 0.04310562252647074
Epoch: 1 Idx: 5000 Loss: 0.01201119553535255
Epoch: 2 Idx: 0 Loss: 0.018212721367328225
Epoch: 2 Idx: 5000 Loss: 0.015935356768062098
Epoch: 3 Idx: 0 Loss: 0.012614992173946462
Epoch: 3 Idx: 5000 Loss: 0.02051071788305091
Epoch: 4 Idx: 0 Loss: 0.022095379935134787
Epoch: 4 Idx: 5000 Loss: 0.027506806893867
Epoch: 5 Idx: 0 Loss: 0.01231972882714688
Epoch: 5 Idx: 5000 Loss: 0.012017978022572054
Epoch: 6 Idx: 0 Loss: 0.021827382131099103
Epoch: 6 Idx: 5000 Loss: 0.007782570950358646
Epoch: 7 Idx: 0 Loss: 0.010356177710921937
Epoch: 7 Idx: 5000 Loss: 0.009528945498072375
Epoch: 8 Idx: 0 Loss: 0.021174743473692897
Epoch: 8 Idx: 5000 Loss: 0.025405362668809224
Epoch: 9 Idx: 0 Loss: 0.020913554287389214
Epoch: 9 Idx: 5000 Loss: 0.016551067861993275
Epoch: 10 Idx: 0 Loss: 0.012347470477965437
Epoch: 10 Idx: 5000 Loss: 0.036559894979656386
Epoch: 11 Idx: 0 Loss: 0.029683849237659106
Epoch: 11 Idx: 5000 Loss: 0.014952043585854708
Epoch: 12 Idx: 0 Loss: 0.011812535332611675
Epoch: 12 Idx: 5000 Loss: 0.022804711619308974
Epoch: 13 Idx: 0 Loss: 0.02789195946994992
Epoch: 13 Idx: 5000 Loss: 0.01037913351943146
Epoch: 14 Idx: 0 Loss: 0.028570204233905463
Epoch: 14 Idx: 5000 Loss: 0.014555182813444697
Epoch: 15 Idx: 0 Loss: 0.02946383816477112
Epoch: 15 Idx: 5000 Loss: 0.01982841934437972
Epoch: 16 Idx: 0 Loss: 0.01318129390827076
Epoch: 16 Idx: 5000 Loss: 0.024749388500685976
Epoch: 17 Idx: 0 Loss: 0.019214710574843013
Epoch: 17 Idx: 5000 Loss: 0.017235560153656872
Epoch: 18 Idx: 0 Loss: 0.02566281749725671
Epoch: 18 Idx: 5000 Loss: 0.03414225432213031
Epoch: 19 Idx: 0 Loss: 0.012974761241852878
Epoch: 19 Idx: 5000 Loss: 0.020188702104470168
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 398, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
.0321244742217489
Epoch: 24 Idx: 0 Loss: 0.03204416503527473
Epoch: 24 Idx: 5000 Loss: 0.04163114805553535
Epoch: 25 Idx: 0 Loss: 0.012882024058284848
Epoch: 25 Idx: 5000 Loss: 0.027566020297190522
Epoch: 26 Idx: 0 Loss: 0.015117681320161582
Epoch: 26 Idx: 5000 Loss: 0.0330848918221313
Epoch: 27 Idx: 0 Loss: 0.020260357042513992
Epoch: 27 Idx: 5000 Loss: 0.02191975890757099
Epoch: 28 Idx: 0 Loss: 0.022432269147834007
Epoch: 28 Idx: 5000 Loss: 0.02745116375056007
Epoch: 29 Idx: 0 Loss: 0.02598374874596132
Epoch: 29 Idx: 5000 Loss: 0.02058649162915086
Epoch: 30 Idx: 0 Loss: 0.020977263940497613
Epoch: 30 Idx: 5000 Loss: 0.010384797565794402
Epoch: 31 Idx: 0 Loss: 0.026741628460372667
Epoch: 31 Idx: 5000 Loss: 0.03365157172145854
Epoch: 32 Idx: 0 Loss: 0.018937890596559342
Epoch: 32 Idx: 5000 Loss: 0.023906237751407343
Epoch: 33 Idx: 0 Loss: 0.02137347741043049
Epoch: 33 Idx: 5000 Loss: 0.01745346255599301
Epoch: 34 Idx: 0 Loss: 0.012219753143212872
Epoch: 34 Idx: 5000 Loss: 0.018826863860267135
Epoch: 35 Idx: 0 Loss: 0.029897444026709033
Epoch: 35 Idx: 5000 Loss: 0.01960425664255781
Epoch: 36 Idx: 0 Loss: 0.012539114064558186
Epoch: 36 Idx: 5000 Loss: 0.03457978075011861
Epoch: 37 Idx: 0 Loss: 0.030780474086190036
Epoch: 37 Idx: 5000 Loss: 0.013972578302567209
Epoch: 38 Idx: 0 Loss: 0.011455107715080669
Epoch: 38 Idx: 5000 Loss: 0.040703292573622665
Epoch: 39 Idx: 0 Loss: 0.018948698448353625
Epoch: 39 Idx: 5000 Loss: 0.020473561655591
Epoch: 40 Idx: 0 Loss: 0.025477899144901256
Epoch: 40 Idx: 5000 Loss: 0.02613063270588785
Epoch: 41 Idx: 0 Loss: 0.023049065446673445
Epoch: 41 Idx: 5000 Loss: 0.0063011938487635985
Epoch: 42 Idx: 0 Loss: 0.03629694493663657
Epoch: 42 Idx: 5000 Loss: 0.018010283508120326
Epoch: 43 Idx: 0 Loss: 0.015264016003575848
Epoch: 43 Idx: 5000 Loss: 0.026886861225173548
Epoch: 44 Idx: 0 Loss: 0.03225526034088904
Epoch: 44 Idx: 5000 Loss: 0.015247007912531517
Epoch: 45 Idx: 0 Loss: 0.024581112698630586
Epoch: 45 Idx: 5000 Loss: 0.029892335303804855
Epoch: 46 Idx: 0 Loss: 0.012443572152533187
Epoch: 46 Idx: 5000 Loss: 0.03107380162340196
Epoch: 47 Idx: 0 Loss: 0.015948553709888933
Epoch: 47 Idx: 5000 Loss: 0.04058003124273889
Epoch: 48 Idx: 0 Loss: 0.017675902925298734
Epoch: 48 Idx: 5000 Loss: 0.05209618306760914
Epoch: 49 Idx: 0 Loss: 0.026217842606496483
Epoch: 49 Idx: 5000 Loss: 0.01772980933970884
Len (direct inputs):  104
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
dTraining size: 1Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.16771893762725137
Epoch: 0 Idx: 5000 Loss: 0.013052672297890186
Epoch: 1 Idx: 0 Loss: 0.019575725642392496
Epoch: 1 Idx: 5000 Loss: 0.024277307594959295
Epoch: 2 Idx: 0 Loss: 0.031263231560770974
Epoch: 2 Idx: 5000 Loss: 0.022730807192113824
Epoch: 3 Idx: 0 Loss: 0.04459695715567266
Epoch: 3 Idx: 5000 Loss: 0.01985545346220573
Epoch: 4 Idx: 0 Loss: 0.020925468670169707
Epoch: 4 Idx: 5000 Loss: 0.030369178170310665
Epoch: 5 Idx: 0 Loss: 0.016394412760662253
Epoch: 5 Idx: 5000 Loss: 0.01351186919645649
Epoch: 6 Idx: 0 Loss: 0.03242412800029005
Epoch: 6 Idx: 5000 Loss: 0.035578325159344455
Epoch: 7 Idx: 0 Loss: 0.020132394699641135
Epoch: 7 Idx: 5000 Loss: 0.02257527699455132
Epoch: 8 Idx: 0 Loss: 0.015438616980803713
Epoch: 8 Idx: 5000 Loss: 0.013235290370933385
Epoch: 9 Idx: 0 Loss: 0.018002317373180386
Epoch: 9 Idx: 5000 Loss: 0.043441904840142626
Epoch: 10 Idx: 0 Loss: 0.030434514951816816
Epoch: 10 Idx: 5000 Loss: 0.02086156601317542
Epoch: 11 Idx: 0 Loss: 0.018609258923014564
Epoch: 11 Idx: 5000 Loss: 0.02054458413666556
Epoch: 12 Idx: 0 Loss: 0.03489575636236861
Epoch: 12 Idx: 5000 Loss: 0.015831072983132202
Epoch: 13 Idx: 0 Loss: 0.010936165179505627
Epoch: 13 Idx: 5000 Loss: 0.020474736428395455
Epoch: 14 Idx: 0 Loss: 0.011485129977889072
Epoch: 14 Idx: 5000 Loss: 0.01875494833835619
Epoch: 15 Idx: 0 Loss: 0.020358048494295653
Epoch: 15 Idx: 5000 Loss: 0.019001646421739724
Epoch: 16 Idx: 0 Loss: 0.031157530033898238
Epoch: 16 Idx: 5000 Loss: 0.03265638264117911
Epoch: 17 Idx: 0 Loss: 0.00745124365277622
Epoch: 17 Idx: 5000 Loss: 0.02001930309754509
Epoch: 18 Idx: 0 Loss: 0.01767638520205455
Epoch: 18 Idx: 5000 Loss: 0.023183570628923345
Epoch: 19 Idx: 0 Loss: 0.009050205930130012
Epoch: 19 Idx: 5000 Loss: 0.035524725654498206
Epoch: 20 Idx: 0 Loss: 0.009880745080926165
Epoch: 20 Idx: 5000 Loss: 0.019052611952626756
Epoch: 21 Idx: 0 Loss: 0.04993840201334473
Epoch: 21 Idx: 5000 Loss: 0.027071784377368666
Epoch: 22 Idx: 0 Loss: 0.03511667370709461
Epoch: 22 Idx: 5000 Loss: 0.016356135209775732
Epoch: 23 Idx: 0 Loss: 0.02260294396671029
Epoch: 23 Idx: 5000 Loss: 0.029031271878458855
Epoch: 24 Idx: 0 Loss: 0.015094436837773832
Epoch: 24 Idx: 5000 Loss: 0.02153567729058352
Epoch: 25 Idx: 0 Loss: 0.030604992292763106
Epoch: 25 Idx: 5000 Loss: 0.024823652205912934
Epoch: 26 Idx: 0 Loss: 0.0373398144699778
Epoch: 26 Idx: 5000 Loss: 0.030621982157826362
Epoch: 27 Idx: 0 Loss: 0.025876084295481702
Epoch: 27 Idx: 5000 Loss: 0.020648627902831285
Epoch: 28 Idx: 0 Loss: 0.03455628171844338
Epoch: 28 Idx: 5000 Loss: 0.02417346271635312
Epoch: 29 Idx: 0 Loss: 0.04950483112478056
Epoch: 29 Idx: 5000 Loss: 0.023396929688396278
Epoch: 30 Idx: 0 Loss: 0.01971519631012094
Epoch: 30 Idx: 5000 Loss: 0.016048021481595516
Epoch: 31 Idx: 0 Loss: 0.028361310107875518
Epoch: 31 Idx: 5000 Loss: 0.02611798916131112
Epoch: 32 Idx: 0 Loss: 0.034382109040364116
Epoch: 32 Idx: 5000 Loss: 0.012913400687084407
Epoch: 33 Idx: 0 Loss: 0.01237135402582083
Epoch: 33 Idx: 5000 Loss: 0.027939156971954482
Epoch: 34 Idx: 0 Loss: 0.04162862224491129
Epoch: 34 Idx: 5000 Loss: 0.01929612164698062
Epoch: 35 Idx: 0 Loss: 0.013984861650166474
Epoch: 35 Idx: 5000 Loss: 0.018933919627137436
Epoch: 36 Idx: 0 Loss: 0.01086828414088464
Epoch: 36 Idx: 5000 Loss: 0.031912956430897706
Epoch: 37 Idx: 0 Loss: 0.028849696309282344
Epoch: 37 Idx: 5000 Loss: 0.020032279084201368
Epoch: 38 Idx: 0 Loss: 0.009223149738955536
Epoch: 38 Idx: 5000 Loss: 0.02902418575918194
Epoch: 39 Idx: 0 Loss: 0.041661360782961954
Epoch: 39 Idx: 5000 Loss: 0.03825342274551996
Epoch: 40 Idx: 0 Loss: 0.013720590829235083
Epoch: 40 Idx: 5000 Loss: 0.029059879092907477
Epoch: 41 Idx: 0 Loss: 0.01680460463021759
Epoch: 41 Idx: 5000 Loss: 0.009589819896779803
Epoch: 42 Idx: 0 Loss: 0.02729145030807275
Epoch: 42 Idx: 5000 Loss: 0.02099383277082574
Epoch: 43 Idx: 0 Loss: 0.026075318712898178
Epoch: 43 Idx: 5000 Loss: 0.012149786715628606
Epoch: 44 Idx: 0 Loss: 0.024452077446129365
Epoch: 44 Idx: 5000 Loss: 0.0293418493581415
Epoch: 45 Idx: 0 Loss: 0.023680442799788363
Epoch: 45 Idx: 5000 Loss: 0.05304166405890559
Epoch: 46 Idx: 0 Loss: 0.016791843277517184
Epoch: 46 Idx: 5000 Loss: 0.033708195329844835
Epoch: 47 Idx: 0 Loss: 0.017280597380833276
Epoch: 47 Idx: 5000 Loss: 0.02849740465257792
Epoch: 48 Idx: 0 Loss: 0.014461372612771792
Epoch: 48 Idx: 5000 Loss: 0.014780174006696518
Epoch: 49 Idx: 0 Loss: 0.02636527934016592
Epoch: 49 Idx: 5000 Loss: 0.020034005355114577
Len (direct inputs):  95
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19793047286476204
Epoch: 0 Idx: 5000 Loss: 0.025548766314516608
Epoch: 1 Idx: 0 Loss: 0.010361976403437953
Epoch: 1 Idx: 5000 Loss: 0.019751755100530212
Epoch: 2 Idx: 0 Loss: 0.009148552277332016
Epoch: 2 Idx: 5000 Loss: 0.03893237309063921
Epoch: 3 Idx: 0 Loss: 0.023945150596777216
Epoch: 3 Idx: 5000 Loss: 0.020115903757575923
Epoch: 4 Idx: 0 Loss: 0.023276769734566834
Epoch: 4 Idx: 5000 Loss: 0.038108450468333784
Epoch: 5 Idx: 0 Loss: 0.022787241303394996
Epoch: 5 Idx: 5000 Loss: 0.023407673559721457
Epoch: 6 Idx: 0 Loss: 0.027457996394697714
Epoch: 6 Idx: 5000 Loss: 0.014179431668879429
Epoch: 7 Idx: 0 Loss: 0.020144717106199906
Epoch: 7 Idx: 5000 Loss: 0.009921909202532402
Epoch: 8 Idx: 0 Loss: 0.016217592881972116
Epoch: 8 Idx: 5000 Loss: 0.0369975033075362
Epoch: 9 Idx: 0 Loss: 0.059019487094434525
Epoch: 9 Idx: 5000 Loss: 0.012662054356717089
Epoch: 10 Idx: 0 Loss: 0.0066254906119761784
Epoch: 10 Idx: 5000 Loss: 0.016361649069409033
Epoch: 11 Idx: 0 Loss: 0.028371745793078773
Epoch: 11 Idx: 5000 Loss: 0.0164882045540108
Epoch: 12 Idx: 0 Loss: 0.018794790878715772
Epoch: 12 Idx: 5000 Loss: 0.013633026816850245
Epoch: 13 Idx: 0 Loss: 0.024580848082918726
Epoch: 13 Idx: 5000 Loss: 0.02926798842210191
Epoch: 14 Idx: 0 Loss: 0.03072087824504552
Epoch: 14 Idx: 5000 Loss: 0.013666212568974035
Epoch: 15 Idx: 0 Loss: 0.01628662031828379
Epoch: 15 Idx: 5000 Loss: 0.025746209700291497
Epoch: 16 Idx: 0 Loss: 0.05016265610991147
Epoch: 16 Idx: 5000 Loss: 0.01615057609892526
Epoch: 17 Idx: 0 Loss: 0.013302994785354658
Epoch: 17 Idx: 5000 Loss: 0.015205054096859516
Epoch: 18 Idx: 0 Loss: 0.016921405208206716
Epoch: 18 Idx: 5000 Loss: 0.0091835254766549
Epoch: 19 Idx: 0 Loss: 0.0278992678190203
Epoch: 19 Idx: 5000 Loss: 0.008947954088461538
Epoch: 20 Idx: 0 Loss: 0.008475246487915644
Epoch: 20 Idx: 5000 Loss: 0.012841314165441271
Epoch: 21 Idx: 0 Loss: 0.010930948117877239
Epoch: 21 Idx: 5000 Loss: 0.02984027764988706
Epoch: 22 Idx: 0 Loss: 0.02261328981025193
Epoch: 22 Idx: 5000 Loss: 0.025575708079690732
Epoch: 23 Idx: 0 Loss: 0.020093195565001318
Epoch: 23 Idx: 5000 Loss: 0.016406779631152876
Epoch: 24 Idx: 0 Loss: 0.01732076534919519
Epoch: 24 Idx: 5000 Loss: 0.02367129267511945
Epoch: 25 Idx: 0 Loss: 0.025503241279390992
Epoch: 25 Idx: 5000 Loss: 0.02962332933881713
Epoch: 26 Idx: 0 Loss: 0.02250089101544376
Epoch: 26 Idx: 5000 Loss: 0.01089218138959392
Epoch: 27 Idx: 0 Loss: 0.02403882538328718
Epoch: 27 Idx: 5000 Loss: 0.021413097218586467
Epoch: 28 Idx: 0 Loss: 0.013292500866163546
Epoch: 28 Idx: 5000 Loss: 0.012369672014162057
Epoch: 29 Idx: 0 Loss: 0.017464727569316045
Epoch: 29 Idx: 5000 Loss: 0.014058959108397402
Epoch: 30 Idx: 0 Loss: 0.016071175796423497
Epoch: 30 Idx: 5000 Loss: 0.022073388622959378
Epoch: 31 Idx: 0 Loss: 0.026568764607008508
Epoch: 31 Idx: 5000 Loss: 0.04352632152013509
Epoch: 32 Idx: 0 Loss: 0.025444186325115897
Epoch: 32 Idx: 5000 Loss: 0.008575045082827124
Epoch: 33 Idx: 0 Loss: 0.018098536833627728
Epoch: 33 Idx: 5000 Loss: 0.031599480252834336
Epoch: 34 Idx: 0 Loss: 0.041921104274543694
Epoch: 34 Idx: 5000 Loss: 0.023368301631262516
Epoch: 35 Idx: 0 Loss: 0.014304451021098142
Epoch: 35 Idx: 5000 Loss: 0.02102048744124575
Epoch: 36 Idx: 0 Loss: 0.015298493986097238
Epoch: 36 Idx: 5000 Loss: 0.017977267897199632
Epoch: 37 Idx: 0 Loss: 0.027261363132447584
Epoch: 37 Idx: 5000 Loss: 0.021809147181409194
Epoch: 38 Idx: 0 Loss: 0.02799989951674521
Epoch: 38 Idx: 5000 Loss: 0.020560229155692094
Epoch: 39 Idx: 0 Loss: 0.03367593543200238
Epoch: 39 Idx: 5000 Loss: 0.01694107441981767
Epoch: 40 Idx: 0 Loss: 0.022782580819861415
Epoch: 40 Idx: 5000 Loss: 0.013333907679933244
Epoch: 41 Idx: 0 Loss: 0.02450132006121259
Epoch: 41 Idx: 5000 Loss: 0.02218626996837738
Epoch: 42 Idx: 0 Loss: 0.031978693665425995
Epoch: 42 Idx: 5000 Loss: 0.02929952298694488
Epoch: 43 Idx: 0 Loss: 0.015734209719495112
Epoch: 43 Idx: 5000 Loss: 0.020522378424293324
Epoch: 44 Idx: 0 Loss: 0.008811362120211347
Epoch: 44 Idx: 5000 Loss: 0.022171308414770025
Epoch: 45 Idx: 0 Loss: 0.019181920855219282
Epoch: 45 Idx: 5000 Loss: 0.024884837531411064
Epoch: 46 Idx: 0 Loss: 0.028995954582037152
Epoch: 46 Idx: 5000 Loss: 0.0344926329615539
Epoch: 47 Idx: 0 Loss: 0.02486708733166517
Epoch: 47 Idx: 5000 Loss: 0.030327493126908082
Epoch: 48 Idx: 0 Loss: 0.02077686169058495
Epoch: 48 Idx: 5000 Loss: 0.03440349855304596
Epoch: 49 Idx: 0 Loss: 0.018140156596555265
Epoch: 49 Idx: 5000 Loss: 0.02140841818640314
Len (direct inputs):  92
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
0.8483451165614271
Parameter containing:
tensor([0.8483], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.2718928523670359
Epoch: 0 Idx: 5000 Loss: 0.01974135305557711
Epoch: 1 Idx: 0 Loss: 0.033112795172039876
Epoch: 1 Idx: 5000 Loss: 0.03666618774018196
Epoch: 2 Idx: 0 Loss: 0.015781735284353816
Epoch: 2 Idx: 5000 Loss: 0.014544335576736867
Epoch: 3 Idx: 0 Loss: 0.011338400884575897
Epoch: 3 Idx: 5000 Loss: 0.02243356582263626
Epoch: 4 Idx: 0 Loss: 0.015200826884370805
Epoch: 4 Idx: 5000 Loss: 0.034532597632799505
Epoch: 5 Idx: 0 Loss: 0.0206867834047202
Epoch: 5 Idx: 5000 Loss: 0.017436608185567207
Epoch: 6 Idx: 0 Loss: 0.02043060151248602
Epoch: 6 Idx: 5000 Loss: 0.025266787606193046
Epoch: 7 Idx: 0 Loss: 0.008234598742889357
Epoch: 7 Idx: 5000 Loss: 0.018528134623840852
Epoch: 8 Idx: 0 Loss: 0.043040070733587565
Epoch: 8 Idx: 5000 Loss: 0.03155906984172963
Epoch: 9 Idx: 0 Loss: 0.009287872673631079
Epoch: 9 Idx: 5000 Loss: 0.036841928714741456
Epoch: 10 Idx: 0 Loss: 0.013452509623152637
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 455, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
1
Epoch: 12 Idx: 5000 Loss: 0.03404192611031495
Epoch: 13 Idx: 0 Loss: 0.019600001931068117
Epoch: 13 Idx: 5000 Loss: 0.024717434408180534
Epoch: 14 Idx: 0 Loss: 0.02387696134005988
Epoch: 14 Idx: 5000 Loss: 0.020942649961001984
Epoch: 15 Idx: 0 Loss: 0.018475441046230414
Epoch: 15 Idx: 5000 Loss: 0.021239025189361743
Epoch: 16 Idx: 0 Loss: 0.0195226830882516
Epoch: 16 Idx: 5000 Loss: 0.01420758110467725
Epoch: 17 Idx: 0 Loss: 0.029940380498101497
Epoch: 17 Idx: 5000 Loss: 0.027631860945762296
Epoch: 18 Idx: 0 Loss: 0.01299851548844452
Epoch: 18 Idx: 5000 Loss: 0.01738432076394057
Epoch: 19 Idx: 0 Loss: 0.020092066611202178
Epoch: 19 Idx: 5000 Loss: 0.02032287417436537
Epoch: 20 Idx: 0 Loss: 0.015861913879214873
Epoch: 20 Idx: 5000 Loss: 0.031122308116298902
Epoch: 21 Idx: 0 Loss: 0.012749165163605383
Epoch: 21 Idx: 5000 Loss: 0.02847112830802756
Epoch: 22 Idx: 0 Loss: 0.016488189267427563
Epoch: 22 Idx: 5000 Loss: 0.040753270903120586
Epoch: 23 Idx: 0 Loss: 0.03217979770194957
Epoch: 23 Idx: 5000 Loss: 0.024521572837754673
Epoch: 24 Idx: 0 Loss: 0.015460833420123271
Epoch: 24 Idx: 5000 Loss: 0.012261829672290357
Epoch: 25 Idx: 0 Loss: 0.02071028727331403
Epoch: 25 Idx: 5000 Loss: 0.020313155465457235
Epoch: 26 Idx: 0 Loss: 0.017162522086529984
Epoch: 26 Idx: 5000 Loss: 0.020309644449148803
Epoch: 27 Idx: 0 Loss: 0.0254785441703156
Epoch: 27 Idx: 5000 Loss: 0.0374256087146434
Epoch: 28 Idx: 0 Loss: 0.021354854784285133
Epoch: 28 Idx: 5000 Loss: 0.021658679765218347
Epoch: 29 Idx: 0 Loss: 0.034053816210421715
Epoch: 29 Idx: 5000 Loss: 0.02024066847990856
Epoch: 30 Idx: 0 Loss: 0.017713606989553824
Epoch: 30 Idx: 5000 Loss: 0.02380522788436326
Epoch: 31 Idx: 0 Loss: 0.022255197632539685
Epoch: 31 Idx: 5000 Loss: 0.019480923120303188
Epoch: 32 Idx: 0 Loss: 0.017590048747798993
Epoch: 32 Idx: 5000 Loss: 0.013498526016218275
Epoch: 33 Idx: 0 Loss: 0.026087433505554987
Epoch: 33 Idx: 5000 Loss: 0.018805656435789363
Epoch: 34 Idx: 0 Loss: 0.023799263005270785
Epoch: 34 Idx: 5000 Loss: 0.010494844048784254
Epoch: 35 Idx: 0 Loss: 0.007880729531751021
Epoch: 35 Idx: 5000 Loss: 0.03315312044528234
Epoch: 36 Idx: 0 Loss: 0.016392311341225936
Epoch: 36 Idx: 5000 Loss: 0.007368868442659764
Epoch: 37 Idx: 0 Loss: 0.02217181932458963
Epoch: 37 Idx: 5000 Loss: 0.013879409007291439
Epoch: 38 Idx: 0 Loss: 0.023738179030572466
Epoch: 38 Idx: 5000 Loss: 0.02143480178235832
Epoch: 39 Idx: 0 Loss: 0.0211398427852067
Epoch: 39 Idx: 5000 Loss: 0.010475161051559179
Epoch: 40 Idx: 0 Loss: 0.012829305676071313
Epoch: 40 Idx: 5000 Loss: 0.03032938914197047
Epoch: 41 Idx: 0 Loss: 0.01973894563153892
Epoch: 41 Idx: 5000 Loss: 0.015621256008450778
Epoch: 42 Idx: 0 Loss: 0.020049763892328237
Epoch: 42 Idx: 5000 Loss: 0.0217956667643023
Epoch: 43 Idx: 0 Loss: 0.02255796602769331
Epoch: 43 Idx: 5000 Loss: 0.022962269156308168
Epoch: 44 Idx: 0 Loss: 0.01986789658990188
Epoch: 44 Idx: 5000 Loss: 0.015890914673895563
Epoch: 45 Idx: 0 Loss: 0.016485386834800286
Epoch: 45 Idx: 5000 Loss: 0.06542523678404683
Epoch: 46 Idx: 0 Loss: 0.022475510837247022
Epoch: 46 Idx: 5000 Loss: 0.03074169933921641
Epoch: 47 Idx: 0 Loss: 0.023181785097713136
Epoch: 47 Idx: 5000 Loss: 0.023649653884991965
Epoch: 48 Idx: 0 Loss: 0.04907920224949548
Epoch: 48 Idx: 5000 Loss: 0.0292323490938386
Epoch: 49 Idx: 0 Loss: 0.01943938702828797
Epoch: 49 Idx: 5000 Loss: 0.011515061517041637
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 475, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
eded
ailed with Disk quota exceeded
tory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Tue Sep  1 19:51:06 2020
Results reported at Tue Sep  1 19:51:06 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs1_4.pkl Models/anatomy_aml_bagofnbrs1_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   20874.03 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2715.26 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21380 sec.
    Turnaround time :                            21437 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc237>
Subject: Job 3289875: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs2_4.pkl Models/anatomy_aml_bagofnbrs2_4.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs2_4.pkl Models/anatomy_aml_bagofnbrs2_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc237>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 20:00:42 2020
Results reported at Tue Sep  1 20:00:42 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs2_4.pkl Models/anatomy_aml_bagofnbrs2_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21890.70 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2716.24 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21981 sec.
    Turnaround time :                            22012 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc248>
Subject: Job 3289877: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs3_4.pkl Models/anatomy_aml_bagofnbrs3_4.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs3_4.pkl Models/anatomy_aml_bagofnbrs3_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc248>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 20:19:29 2020
Results reported at Tue Sep  1 20:19:29 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs3_4.pkl Models/anatomy_aml_bagofnbrs3_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23013.71 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2719.80 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23108 sec.
    Turnaround time :                            23139 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc264>
Subject: Job 3289879: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs4_4.pkl Models/anatomy_aml_bagofnbrs4_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs4_4.pkl Models/anatomy_aml_bagofnbrs4_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc264>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 20:56:31 2020
Results reported at Tue Sep  1 20:56:31 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs4_4.pkl Models/anatomy_aml_bagofnbrs4_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24595.89 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2717.65 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25330 sec.
    Turnaround time :                            25361 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc262>
Subject: Job 3289881: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs5_4.pkl Models/anatomy_aml_bagofnbrs5_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs5_4.pkl Models/anatomy_aml_bagofnbrs5_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc262>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:15:34 2020
Results reported at Tue Sep  1 21:15:34 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs5_4.pkl Models/anatomy_aml_bagofnbrs5_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26403.82 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2719.58 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   26446 sec.
    Turnaround time :                            26504 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc234>
Subject: Job 3289883: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs7_4.pkl Models/anatomy_aml_bagofnbrs7_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs7_4.pkl Models/anatomy_aml_bagofnbrs7_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc234>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:49:17 2020
Results reported at Tue Sep  1 21:49:17 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs7_4.pkl Models/anatomy_aml_bagofnbrs7_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28466.39 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2716.52 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28496 sec.
    Turnaround time :                            28527 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc240>
Subject: Job 3289885: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 4 Output/test_anatomy_aml_bagofnbrs8_4.pkl Models/anatomy_aml_bagofnbrs8_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 4 Output/test_anatomy_aml_bagofnbrs8_4.pkl Models/anatomy_aml_bagofnbrs8_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc240>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 22:14:02 2020
Results reported at Tue Sep  1 22:14:02 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 4 Output/test_anatomy_aml_bagofnbrs8_4.pkl Models/anatomy_aml_bagofnbrs8_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   29903.65 sec.
    Max Memory :                                 2922 MB
    Average Memory :                             2722.10 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40495.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   29954 sec.
    Turnaround time :                            30012 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc269>
Subject: Job 3289909: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 4 Output/test_anatomy_aml_bagofnbrs80_4.pkl Models/anatomy_aml_bagofnbrs80_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 4 Output/test_anatomy_aml_bagofnbrs80_4.pkl Models/anatomy_aml_bagofnbrs80_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc269>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 4 Output/test_anatomy_aml_bagofnbrs80_4.pkl Models/anatomy_aml_bagofnbrs80_4.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80290.49 sec.
    Max Memory :                                 2738 MB
    Average Memory :                             2666.58 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40679.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80550 sec.
    Turnaround time :                            80697 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc232>
Subject: Job 3289911: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 4 Output/test_anatomy_aml_bagofnbrs152_4.pkl Models/anatomy_aml_bagofnbrs152_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 4 Output/test_anatomy_aml_bagofnbrs152_4.pkl Models/anatomy_aml_bagofnbrs152_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc232>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:48 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 4 Output/test_anatomy_aml_bagofnbrs152_4.pkl Models/anatomy_aml_bagofnbrs152_4.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80364.84 sec.
    Max Memory :                                 2684 MB
    Average Memory :                             2619.31 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40733.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80548 sec.
    Turnaround time :                            80697 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc235>
Subject: Job 3289907: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 4 Output/test_anatomy_aml_bagofnbrs40_4.pkl Models/anatomy_aml_bagofnbrs40_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 4 Output/test_anatomy_aml_bagofnbrs40_4.pkl Models/anatomy_aml_bagofnbrs40_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:52 2020
Job was executed on host(s) <dccxc235>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:56:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:56:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 4 Output/test_anatomy_aml_bagofnbrs40_4.pkl Models/anatomy_aml_bagofnbrs40_4.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80396.59 sec.
    Max Memory :                                 2917 MB
    Average Memory :                             2685.20 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40500.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80523 sec.
    Turnaround time :                            80697 sec.

The output (if any) is above this job summary.

