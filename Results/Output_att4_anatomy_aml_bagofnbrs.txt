Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2177216712533136
Epoch: 0 Idx: 5000 Loss: 0.017723186831185362
Epoch: 1 Idx: 0 Loss: 0.03907929040854541
Epoch: 1 Idx: 5000 Loss: 0.02763463127232316
Epoch: 2 Idx: 0 Loss: 0.02858991829235101
Epoch: 2 Idx: 5000 Loss: 0.027293130362424374
Epoch: 3 Idx: 0 Loss: 0.023386086393317546
Epoch: 3 Idx: 5000 Loss: 0.012515514113706744
Epoch: 4 Idx: 0 Loss: 0.01719812575645072
Epoch: 4 Idx: 5000 Loss: 0.026097632891451436
Epoch: 5 Idx: 0 Loss: 0.026443208530561327
Epoch: 5 Idx: 5000 Loss: 0.037671088631979315
Epoch: 6 Idx: 0 Loss: 0.017653402702660057
Epoch: 6 Idx: 5000 Loss: 0.01694081811337865
Epoch: 7 Idx: 0 Loss: 0.006703090355173379
Epoch: 7 Idx: 5000 Loss: 0.04012594684647951
Epoch: 8 Idx: 0 Loss: 0.045730895599664825
Epoch: 8 Idx: 5000 Loss: 0.022065740693664772
Epoch: 9 Idx: 0 Loss: 0.02431618115818166
Epoch: 9 Idx: 5000 Loss: 0.013180820602406801
Epoch: 10 Idx: 0 Loss: 0.031133546748664073
Epoch: 10 Idx: 5000 Loss: 0.025853741419125824
Epoch: 11 Idx: 0 Loss: 0.022432889351447896
Epoch: 11 Idx: 5000 Loss: 0.028369284480499274
Epoch: 12 Idx: 0 Loss: 0.027629995947545574
Epoch: 12 Idx: 5000 Loss: 0.012779422369470744
Epoch: 13 Idx: 0 Loss: 0.024887570028424934
Epoch: 13 Idx: 5000 Loss: 0.020651395245620853
Epoch: 14 Idx: 0 Loss: 0.023383996177087002
Epoch: 14 Idx: 5000 Loss: 0.0332946479826826
Epoch: 15 Idx: 0 Loss: 0.03048116519314395
Epoch: 15 Idx: 5000 Loss: 0.024297417865255733
Epoch: 16 Idx: 0 Loss: 0.0171078205564728
Epoch: 16 Idx: 5000 Loss: 0.018637055892335802
Epoch: 17 Idx: 0 Loss: 0.022332791970191706
Epoch: 17 Idx: 5000 Loss: 0.013074939635401628
Epoch: 18 Idx: 0 Loss: 0.026737221755724183
Epoch: 18 Idx: 5000 Loss: 0.019288098481816493
Epoch: 19 Idx: 0 Loss: 0.009968848781921658
Epoch: 19 Idx: 5000 Loss: 0.02092823803422862
Epoch: 20 Idx: 0 Loss: 0.011901141786418917
Epoch: 20 Idx: 5000 Loss: 0.01888745453881964
Epoch: 21 Idx: 0 Loss: 0.024637430583589172
Epoch: 21 Idx: 5000 Loss: 0.018114659681771506
Epoch: 22 Idx: 0 Loss: 0.012155518208814296
Epoch: 22 Idx: 5000 Loss: 0.016856498546883265
Epoch: 23 Idx: 0 Loss: 0.02851612655422088
Epoch: 23 Idx: 5000 Loss: 0.028460632822048238
Epoch: 24 Idx: 0 Loss: 0.023644086988943706
Epoch: 24 Idx: 5000 Loss: 0.008688713501203253
Epoch: 25 Idx: 0 Loss: 0.049438930978880954
Epoch: 25 Idx: 5000 Loss: 0.01144299707246954
Epoch: 26 Idx: 0 Loss: 0.015060834760770402
Epoch: 26 Idx: 5000 Loss: 0.02538460456421236
Epoch: 27 Idx: 0 Loss: 0.030721751303055912
Epoch: 27 Idx: 5000 Loss: 0.024825220995222712
Epoch: 28 Idx: 0 Loss: 0.02613728483011861
Epoch: 28 Idx: 5000 Loss: 0.02527065663163435
Epoch: 29 Idx: 0 Loss: 0.028455920822815818
Epoch: 29 Idx: 5000 Loss: 0.02041921741421711
Epoch: 30 Idx: 0 Loss: 0.011919919229376744
Epoch: 30 Idx: 5000 Loss: 0.01438142910501043
Epoch: 31 Idx: 0 Loss: 0.02225705267271587
Epoch: 31 Idx: 5000 Loss: 0.01737235845484817
Epoch: 32 Idx: 0 Loss: 0.02472560534322716
Epoch: 32 Idx: 5000 Loss: 0.028096332669553418
Epoch: 33 Idx: 0 Loss: 0.024135579058867304
Epoch: 33 Idx: 5000 Loss: 0.008773289581740878
Epoch: 34 Idx: 0 Loss: 0.018507651342998428
Epoch: 34 Idx: 5000 Loss: 0.02860720861680534
Epoch: 35 Idx: 0 Loss: 0.033078414540691756
Epoch: 35 Idx: 5000 Loss: 0.011266023088624899
Epoch: 36 Idx: 0 Loss: 0.024077984851433747
Epoch: 36 Idx: 5000 Loss: 0.014581774521175173
Epoch: 37 Idx: 0 Loss: 0.016005296284972704
Epoch: 37 Idx: 5000 Loss: 0.03703965557960934
Epoch: 38 Idx: 0 Loss: 0.01980056826024171
Epoch: 38 Idx: 5000 Loss: 0.01688301716559838
Epoch: 39 Idx: 0 Loss: 0.02129821215355972
Epoch: 39 Idx: 5000 Loss: 0.014072046455498161
Epoch: 40 Idx: 0 Loss: 0.019571185435577357
Epoch: 40 Idx: 5000 Loss: 0.013850850540132426
Epoch: 41 Idx: 0 Loss: 0.04509870412038617
Epoch: 41 Idx: 5000 Loss: 0.015025467594123137
Epoch: 42 Idx: 0 Loss: 0.027216991614688962
Epoch: 42 Idx: 5000 Loss: 0.035217186161480056
Epoch: 43 Idx: 0 Loss: 0.015002880308413055
Epoch: 43 Idx: 5000 Loss: 0.027121779507284314
Epoch: 44 Idx: 0 Loss: 0.014739447319624219
Epoch: 44 Idx: 5000 Loss: 0.023878516399766556
Epoch: 45 Idx: 0 Loss: 0.027159220734800864
Epoch: 45 Idx: 5000 Loss: 0.02144361283379431
Epoch: 46 Idx: 0 Loss: 0.03835327205765557
Epoch: 46 Idx: 5000 Loss: 0.014832772363895677
Epoch: 47 Idx: 0 Loss: 0.022696528296176417
Epoch: 47 Idx: 5000 Loss: 0.019726461676074877
Epoch: 48 Idx: 0 Loss: 0.012659009342087137
Epoch: 48 Idx: 5000 Loss: 0.023660479465281804
Epoch: 49 Idx: 0 Loss: 0.04127348191475486
Epoch: 49 Idx: 5000 Loss: 0.038408102610584596
Len (direct inputs):  113
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.24730001607109914
Epoch: 0 Idx: 5000 Loss: 0.006574428552526696
Epoch: 1 Idx: 0 Loss: 0.029081608546782663
Epoch: 1 Idx: 5000 Loss: 0.016135946926623464
Epoch: 2 Idx: 0 Loss: 0.01780254701183348
Epoch: 2 Idx: 5000 Loss: 0.030233094160408997
Epoch: 3 Idx: 0 Loss: 0.03348703456026616
Epoch: 3 Idx: 5000 Loss: 0.01856626708303311
Epoch: 4 Idx: 0 Loss: 0.03550803787346235
Epoch: 4 Idx: 5000 Loss: 0.031307282553076715
Epoch: 5 Idx: 0 Loss: 0.01834842741767071
Epoch: 5 Idx: 5000 Loss: 0.013602141264103917
Epoch: 6 Idx: 0 Loss: 0.02027024536589371
Epoch: 6 Idx: 5000 Loss: 0.021118756621694695
Epoch: 7 Idx: 0 Loss: 0.03867028227749014
Epoch: 7 Idx: 5000 Loss: 0.018890295790931193
Epoch: 8 Idx: 0 Loss: 0.014107380575989995
Epoch: 8 Idx: 5000 Loss: 0.025775482458806145
Epoch: 9 Idx: 0 Loss: 0.013962113827059797
Epoch: 9 Idx: 5000 Loss: 0.025502148141773682
Epoch: 10 Idx: 0 Loss: 0.012987039881716459
Epoch: 10 Idx: 5000 Loss: 0.03156664997517972
Epoch: 11 Idx: 0 Loss: 0.016860398778303323
Epoch: 11 Idx: 5000 Loss: 0.015244239680226469
Epoch: 12 Idx: 0 Loss: 0.019329555734657125
Epoch: 12 Idx: 5000 Loss: 0.02253792392603601
Epoch: 13 Idx: 0 Loss: 0.0463584132530797
Epoch: 13 Idx: 5000 Loss: 0.02714968811819541
Epoch: 14 Idx: 0 Loss: 0.030939139880857707
Epoch: 14 Idx: 5000 Loss: 0.024626959765033922
Epoch: 15 Idx: 0 Loss: 0.012262136243247474
Epoch: 15 Idx: 5000 Loss: 0.027685655599584878
Epoch: 16 Idx: 0 Loss: 0.03260502862669533
Epoch: 16 Idx: 5000 Loss: 0.01158762819180437
Epoch: 17 Idx: 0 Loss: 0.024149741527285117
Epoch: 17 Idx: 5000 Loss: 0.014304836532883353
Epoch: 18 Idx: 0 Loss: 0.011709551218094413
Epoch: 18 Idx: 5000 Loss: 0.01977962139004815
Epoch: 19 Idx: 0 Loss: 0.01199419575021795
Epoch: 19 Idx: 5000 Loss: 0.03435743979185969
Epoch: 20 Idx: 0 Loss: 0.019996413966405693
Epoch: 20 Idx: 5000 Loss: 0.017888859110399554
Epoch: 21 Idx: 0 Loss: 0.011511615875851117
Epoch: 21 Idx: 5000 Loss: 0.02049934968604257
Epoch: 22 Idx: 0 Loss: 0.017167864623194737
Epoch: 22 Idx: 5000 Loss: 0.042844587675629965
Epoch: 23 Idx: 0 Loss: 0.007493950281879156
Epoch: 23 Idx: 5000 Loss: 0.015820909189261854
Epoch: 24 Idx: 0 Loss: 0.014920113228284359
Epoch: 24 Idx: 5000 Loss: 0.02421902019562136
Epoch: 25 Idx: 0 Loss: 0.01783920818802247
Epoch: 25 Idx: 5000 Loss: 0.011219563620586199
Epoch: 26 Idx: 0 Loss: 0.019376687612240165
Epoch: 26 Idx: 5000 Loss: 0.019137953466736975
Epoch: 27 Idx: 0 Loss: 0.03380151477282273
Epoch: 27 Idx: 5000 Loss: 0.022883535067081538
Epoch: 28 Idx: 0 Loss: 0.0309844634940923
Epoch: 28 Idx: 5000 Loss: 0.015790064280080493
Epoch: 29 Idx: 0 Loss: 0.05786722654074808
Epoch: 29 Idx: 5000 Loss: 0.026021356612161838
Epoch: 30 Idx: 0 Loss: 0.034364265529678965
Epoch: 30 Idx: 5000 Loss: 0.013276304533902734
Epoch: 31 Idx: 0 Loss: 0.01246302271363764
Epoch: 31 Idx: 5000 Loss: 0.014749017264610486
Epoch: 32 Idx: 0 Loss: 0.018200888162782057
Epoch: 32 Idx: 5000 Loss: 0.02080713397841593
Epoch: 33 Idx: 0 Loss: 0.034032693625881336
Epoch: 33 Idx: 5000 Loss: 0.029866388232763802
Epoch: 34 Idx: 0 Loss: 0.013510498530507667
Epoch: 34 Idx: 5000 Loss: 0.02711817845939501
Epoch: 35 Idx: 0 Loss: 0.022486718768662567
Epoch: 35 Idx: 5000 Loss: 0.017867001486898716
Epoch: 36 Idx: 0 Loss: 0.033945534771651936
Epoch: 36 Idx: 5000 Loss: 0.014756102706284817
Epoch: 37 Idx: 0 Loss: 0.03490391386833632
Epoch: 37 Idx: 5000 Loss: 0.024953863176018096
Epoch: 38 Idx: 0 Loss: 0.053116469905167094
Epoch: 38 Idx: 5000 Loss: 0.026449942027601578
Epoch: 39 Idx: 0 Loss: 0.023564623727042745
Epoch: 39 Idx: 5000 Loss: 0.019418111970017363
Epoch: 40 Idx: 0 Loss: 0.024901242621029156
Epoch: 40 Idx: 5000 Loss: 0.021195002566870524
Epoch: 41 Idx: 0 Loss: 0.03723451165131472
Epoch: 41 Idx: 5000 Loss: 0.02730154569074432
Epoch: 42 Idx: 0 Loss: 0.014809101634979133
Epoch: 42 Idx: 5000 Loss: 0.018753152138215264
Epoch: 43 Idx: 0 Loss: 0.018703823713138175
Epoch: 43 Idx: 5000 Loss: 0.022036316811908437
Epoch: 44 Idx: 0 Loss: 0.021066128472828544
Epoch: 44 Idx: 5000 Loss: 0.02228916888430815
Epoch: 45 Idx: 0 Loss: 0.014555681415325827
Epoch: 45 Idx: 5000 Loss: 0.01950045374419497
Epoch: 46 Idx: 0 Loss: 0.022151154572898138
Epoch: 46 Idx: 5000 Loss: 0.02997017219196767
Epoch: 47 Idx: 0 Loss: 0.016774342848415093
Epoch: 47 Idx: 5000 Loss: 0.01570885922114132
Epoch: 48 Idx: 0 Loss: 0.03388707941241126
Epoch: 48 Idx: 5000 Loss: 0.013624389210626125
Epoch: 49 Idx: 0 Loss: 0.020349130940964272
Epoch: 49 Idx: 5000 Loss: 0.034764854131385234
Len (direct inputs):  98
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.20482529930817564
Epoch: 0 Idx: 5000 Loss: 0.02505011096886543
Epoch: 1 Idx: 0 Loss: 0.012920316654443799
Epoch: 1 Idx: 5000 Loss: 0.028913991339502627
Epoch: 2 Idx: 0 Loss: 0.014627839656023872
Epoch: 2 Idx: 5000 Loss: 0.008247676182864051
Epoch: 3 Idx: 0 Loss: 0.01686499349662183
Epoch: 3 Idx: 5000 Loss: 0.016107276904949522
Epoch: 4 Idx: 0 Loss: 0.010239341230024663
Epoch: 4 Idx: 5000 Loss: 0.03813467056042187
Epoch: 5 Idx: 0 Loss: 0.016388494615936534
Epoch: 5 Idx: 5000 Loss: 0.021983163340993758
Epoch: 6 Idx: 0 Loss: 0.017217757641604804
Epoch: 6 Idx: 5000 Loss: 0.019789707544398027
Epoch: 7 Idx: 0 Loss: 0.01770625740174009
Epoch: 7 Idx: 5000 Loss: 0.012913731085555743
Epoch: 8 Idx: 0 Loss: 0.01904842072631837
Epoch: 8 Idx: 5000 Loss: 0.0165360907258234
Epoch: 9 Idx: 0 Loss: 0.01799488530722018
Epoch: 9 Idx: 5000 Loss: 0.014943121937260814
Epoch: 10 Idx: 0 Loss: 0.02973549299863075
Epoch: 10 Idx: 5000 Loss: 0.020558125637439698
Epoch: 11 Idx: 0 Loss: 0.023604140461777598
Epoch: 11 Idx: 5000 Loss: 0.018536465996966137
Epoch: 12 Idx: 0 Loss: 0.04728688059594466
Epoch: 12 Idx: 5000 Loss: 0.04003522736670449
Epoch: 13 Idx: 0 Loss: 0.020368690271079477
Epoch: 13 Idx: 5000 Loss: 0.017773880728781215
Epoch: 14 Idx: 0 Loss: 0.03131205598998435
Epoch: 14 Idx: 5000 Loss: 0.021241073998188358
Epoch: 15 Idx: 0 Loss: 0.023757374085566576
Epoch: 15 Idx: 5000 Loss: 0.04059487846793625
Epoch: 16 Idx: 0 Loss: 0.021237760108322228
Epoch: 16 Idx: 5000 Loss: 0.036276095376227835
Epoch: 17 Idx: 0 Loss: 0.013892830156136567
Epoch: 17 Idx: 5000 Loss: 0.017274819121902013
Epoch: 18 Idx: 0 Loss: 0.02252692196547891
Epoch: 18 Idx: 5000 Loss: 0.04205826854991822
Epoch: 19 Idx: 0 Loss: 0.010643025886393105
Epoch: 19 Idx: 5000 Loss: 0.015930540640994452
Epoch: 20 Idx: 0 Loss: 0.02405795627238665
Epoch: 20 Idx: 5000 Loss: 0.022159182849461556
Epoch: 21 Idx: 0 Loss: 0.01404303003651516
Epoch: 21 Idx: 5000 Loss: 0.02371556654830377
Epoch: 22 Idx: 0 Loss: 0.01301589917480436
Epoch: 22 Idx: 5000 Loss: 0.015700540977815036
Epoch: 23 Idx: 0 Loss: 0.022400912564031213
Epoch: 23 Idx: 5000 Loss: 0.012159934188568469
Epoch: 24 Idx: 0 Loss: 0.017543924324126416
Epoch: 24 Idx: 5000 Loss: 0.017215058681481564
Epoch: 25 Idx: 0 Loss: 0.018931933434902473
Epoch: 25 Idx: 5000 Loss: 0.02699951068047069
Epoch: 26 Idx: 0 Loss: 0.014169003144147796
Epoch: 26 Idx: 5000 Loss: 0.021207754073088288
Epoch: 27 Idx: 0 Loss: 0.013003002361521385
Epoch: 27 Idx: 5000 Loss: 0.023768653618728226
Epoch: 28 Idx: 0 Loss: 0.02755282201671542
Epoch: 28 Idx: 5000 Loss: 0.01639712791821048
Epoch: 29 Idx: 0 Loss: 0.03490055626507854
Epoch: 29 Idx: 5000 Loss: 0.03057505022622935
Epoch: 30 Idx: 0 Loss: 0.041233772708876994
Epoch: 30 Idx: 5000 Loss: 0.05298735416571039
Epoch: 31 Idx: 0 Loss: 0.01880331141932412
Epoch: 31 Idx: 5000 Loss: 0.01595180418884314
Epoch: 32 Idx: 0 Loss: 0.011516994897938199
Epoch: 32 Idx: 5000 Loss: 0.018240991691974115
Epoch: 33 Idx: 0 Loss: 0.024036310561919794
Epoch: 33 Idx: 5000 Loss: 0.00870809207606246
Epoch: 34 Idx: 0 Loss: 0.012107020843627948
Epoch: 34 Idx: 5000 Loss: 0.016956932977982717
Epoch: 35 Idx: 0 Loss: 0.016138003181059755
Epoch: 35 Idx: 5000 Loss: 0.03155942169328252
Epoch: 36 Idx: 0 Loss: 0.02092686475518679
Epoch: 36 Idx: 5000 Loss: 0.023464544879134623
Epoch: 37 Idx: 0 Loss: 0.03220947689196024
Epoch: 37 Idx: 5000 Loss: 0.017246609209696895
Epoch: 38 Idx: 0 Loss: 0.024662397603324327
Epoch: 38 Idx: 5000 Loss: 0.03661629691459093
Epoch: 39 Idx: 0 Loss: 0.018293407638640343
Epoch: 39 Idx: 5000 Loss: 0.01833978807328769
Epoch: 40 Idx: 0 Loss: 0.016671756288739494
Epoch: 40 Idx: 5000 Loss: 0.015038618463274471
Epoch: 41 Idx: 0 Loss: 0.0332280451520436
Epoch: 41 Idx: 5000 Loss: 0.02118494269679551
Epoch: 42 Idx: 0 Loss: 0.01779682401803706
Epoch: 42 Idx: 5000 Loss: 0.013319492462483441
Epoch: 43 Idx: 0 Loss: 0.015477982671005221
Epoch: 43 Idx: 5000 Loss: 0.035830830417045725
Epoch: 44 Idx: 0 Loss: 0.016474460510086154
Epoch: 44 Idx: 5000 Loss: 0.02794693052171576
Epoch: 45 Idx: 0 Loss: 0.02241735247186305
Epoch: 45 Idx: 5000 Loss: 0.01862176591165164
Epoch: 46 Idx: 0 Loss: 0.006800153332390089
Epoch: 46 Idx: 5000 Loss: 0.023272677474042623
Epoch: 47 Idx: 0 Loss: 0.021994679083693767
Epoch: 47 Idx: 5000 Loss: 0.02565680386890891
Epoch: 48 Idx: 0 Loss: 0.03753731369687405
Epoch: 48 Idx: 5000 Loss: 0.02091929105692306
Epoch: 49 Idx: 0 Loss: 0.013958198185832625
Epoch: 49 Idx: 5000 Loss: 0.0240559843466598
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
ining TrainTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19893585924121804
Epoch: 0 Idx: 5000 Loss: 0.020784413984193072
Epoch: 1 Idx: 0 Loss: 0.023167608135000597
Epoch: 1 Idx: 5000 Loss: 0.03518454845121537
Epoch: 2 Idx: 0 Loss: 0.01994286535126332
Epoch: 2 Idx: 5000 Loss: 0.020858071852719803
Epoch: 3 Idx: 0 Loss: 0.019618936912218013
Epoch: 3 Idx: 5000 Loss: 0.016941667540649713
Epoch: 4 Idx: 0 Loss: 0.009326329933667984
Epoch: 4 Idx: 5000 Loss: 0.018212159388604477
Epoch: 5 Idx: 0 Loss: 0.017896865728666285
Epoch: 5 Idx: 5000 Loss: 0.018456242206753806
Epoch: 6 Idx: 0 Loss: 0.01801946824765089
Epoch: 6 Idx: 5000 Loss: 0.022047876858781887
Epoch: 7 Idx: 0 Loss: 0.014954108361842305
Epoch: 7 Idx: 5000 Loss: 0.018048982821365356
Epoch: 8 Idx: 0 Loss: 0.020587632888935094
Epoch: 8 Idx: 5000 Loss: 0.015993878680751965
Epoch: 9 Idx: 0 Loss: 0.026667119447722737
Epoch: 9 Idx: 5000 Loss: 0.024184656442076746
Epoch: 10 Idx: 0 Loss: 0.02182232430446507
Epoch: 10 Idx: 5000 Loss: 0.02652940319602267
Epoch: 11 Idx: 0 Loss: 0.01999643088145433
Epoch: 11 Idx: 5000 Loss: 0.02334347387950634
Epoch: 12 Idx: 0 Loss: 0.02198744865794621
Epoch: 12 Idx: 5000 Loss: 0.021151109336753668
Epoch: 13 Idx: 0 Loss: 0.012945492217503754
Epoch: 13 Idx: 5000 Loss: 0.0260876448124142
Epoch: 14 Idx: 0 Loss: 0.03112156667714103
Epoch: 14 Idx: 5000 Loss: 0.023287458059284893
Epoch: 15 Idx: 0 Loss: 0.01717226913976441
Epoch: 15 Idx: 5000 Loss: 0.024335749098887777
Epoch: 16 Idx: 0 Loss: 0.03776337936011186
Epoch: 16 Idx: 5000 Loss: 0.044223298953301494
Epoch: 17 Idx: 0 Loss: 0.013738601291109917
Epoch: 17 Idx: 5000 Loss: 0.020546492461006823
Epoch: 18 Idx: 0 Loss: 0.014569583897942849
Epoch: 18 Idx: 5000 Loss: 0.01632807111879384
Epoch: 19 Idx: 0 Loss: 0.019862730154680847
Epoch: 19 Idx: 5000 Loss: 0.03190913753049579
Epoch: 20 Idx: 0 Loss: 0.02190463850445499
Epoch: 20 Idx: 5000 Loss: 0.014282349871959773
Epoch: 21 Idx: 0 Loss: 0.020058479916453258
Epoch: 21 Idx: 5000 Loss: 0.021720688096346683
Epoch: 22 Idx: 0 Loss: 0.014795560187299227
Epoch: 22 Idx: 5000 Loss: 0.02621789181585158
Epoch: 23 Idx: 0 Loss: 0.013552053324284852
Epoch: 23 Idx: 5000 Loss: 0.0321244742217489
Epoch: 24 Idx: 0 Loss: 0.03204416503527473
Epoch: 24 Idx: 5000 Loss: 0.04163114805553535
Epoch: 25 Idx: 0 Loss: 0.012882024058284848
Epoch: 25 Idx: 5000 Loss: 0.027566020297190522
Epoch: 26 Idx: 0 Loss: 0.015117681320161582
Epoch: 26 Idx: 5000 Loss: 0.0330848918221313
Epoch: 27 Idx: 0 Loss: 0.020260357042513992
Epoch: 27 Idx: 5000 Loss: 0.02191975890757099
Epoch: 28 Idx: 0 Loss: 0.022432269147834007
Epoch: 28 Idx: 5000 Loss: 0.02745116375056007
Epoch: 29 Idx: 0 Loss: 0.02598374874596132
Epoch: 29 Idx: 5000 Loss: 0.02058649162915086
Epoch: 30 Idx: 0 Loss: 0.020977263940497613
Epoch: 30 Idx: 5000 Loss: 0.010384797565794402
Epoch: 31 Idx: 0 Loss: 0.026741628460372667
Epoch: 31 Idx: 5000 Loss: 0.03365157172145854
Epoch: 32 Idx: 0 Loss: 0.018937890596559342
Epoch: 32 Idx: 5000 Loss: 0.023906237751407343
Epoch: 33 Idx: 0 Loss: 0.02137347741043049
Epoch: 33 Idx: 5000 Loss: 0.01745346255599301
Epoch: 34 Idx: 0 Loss: 0.012219753143212872
Epoch: 34 Idx: 5000 Loss: 0.018826863860267135
Epoch: 35 Idx: 0 Loss: 0.029897444026709033
Epoch: 35 Idx: 5000 Loss: 0.01960425664255781
Epoch: 36 Idx: 0 Loss: 0.012539114064558186
Epoch: 36 Idx: 5000 Loss: 0.03457978075011861
Epoch: 37 Idx: 0 Loss: 0.030780474086190036
Epoch: 37 Idx: 5000 Loss: 0.013972578302567209
Epoch: 38 Idx: 0 Loss: 0.011455107715080669
Epoch: 38 Idx: 5000 Loss: 0.040703292573622665
Epoch: 39 Idx: 0 Loss: 0.018948698448353625
Epoch: 39 Idx: 5000 Loss: 0.020473561655591
Epoch: 40 Idx: 0 Loss: 0.025477899144901256
Epoch: 40 Idx: 5000 Loss: 0.02613063270588785
Epoch: 41 Idx: 0 Loss: 0.023049065446673445
Epoch: 41 Idx: 5000 Loss: 0.0063011938487635985
Epoch: 42 Idx: 0 Loss: 0.03629694493663657
Epoch: 42 Idx: 5000 Loss: 0.018010283508120326
Epoch: 43 Idx: 0 Loss: 0.015264016003575848
Epoch: 43 Idx: 5000 Loss: 0.026886861225173548
Epoch: 44 Idx: 0 Loss: 0.03225526034088904
Epoch: 44 Idx: 5000 Loss: 0.015247007912531517
Epoch: 45 Idx: 0 Loss: 0.024581112698630586
Epoch: 45 Idx: 5000 Loss: 0.029892335303804855
Epoch: 46 Idx: 0 Loss: 0.012443572152533187
Epoch: 46 Idx: 5000 Loss: 0.03107380162340196
Epoch: 47 Idx: 0 Loss: 0.015948553709888933
Epoch: 47 Idx: 5000 Loss: 0.04058003124273889
Epoch: 48 Idx: 0 Loss: 0.017675902925298734
Epoch: 48 Idx: 5000 Loss: 0.05209618306760914
Epoch: 49 Idx: 0 Loss: 0.026217842606496483
Epoch: 49 Idx: 5000 Loss: 0.01772980933970884
Len (direct inputs):  104
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
dTraining size: 1Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.16771893762725137
Epoch: 0 Idx: 5000 Loss: 0.013052672297890186
Epoch: 1 Idx: 0 Loss: 0.019575725642392496
Epoch: 1 Idx: 5000 Loss: 0.024277307594959295
Epoch: 2 Idx: 0 Loss: 0.031263231560770974
Epoch: 2 Idx: 5000 Loss: 0.022730807192113824
Epoch: 3 Idx: 0 Loss: 0.04459695715567266
Epoch: 3 Idx: 5000 Loss: 0.01985545346220573
Epoch: 4 Idx: 0 Loss: 0.020925468670169707
Epoch: 4 Idx: 5000 Loss: 0.030369178170310665
Epoch: 5 Idx: 0 Loss: 0.016394412760662253
Epoch: 5 Idx: 5000 Loss: 0.01351186919645649
Epoch: 6 Idx: 0 Loss: 0.03242412800029005
Epoch: 6 Idx: 5000 Loss: 0.035578325159344455
Epoch: 7 Idx: 0 Loss: 0.020132394699641135
Epoch: 7 Idx: 5000 Loss: 0.02257527699455132
Epoch: 8 Idx: 0 Loss: 0.015438616980803713
Epoch: 8 Idx: 5000 Loss: 0.013235290370933385
Epoch: 9 Idx: 0 Loss: 0.018002317373180386
Epoch: 9 Idx: 5000 Loss: 0.043441904840142626
Epoch: 10 Idx: 0 Loss: 0.030434514951816816
Epoch: 10 Idx: 5000 Loss: 0.02086156601317542
Epoch: 11 Idx: 0 Loss: 0.018609258923014564
Epoch: 11 Idx: 5000 Loss: 0.02054458413666556
Epoch: 12 Idx: 0 Loss: 0.03489575636236861
Epoch: 12 Idx: 5000 Loss: 0.015831072983132202
Epoch: 13 Idx: 0 Loss: 0.010936165179505627
Epoch: 13 Idx: 5000 Loss: 0.020474736428395455
Epoch: 14 Idx: 0 Loss: 0.011485129977889072
Epoch: 14 Idx: 5000 Loss: 0.01875494833835619
Epoch: 15 Idx: 0 Loss: 0.020358048494295653
Epoch: 15 Idx: 5000 Loss: 0.019001646421739724
Epoch: 16 Idx: 0 Loss: 0.031157530033898238
Epoch: 16 Idx: 5000 Loss: 0.03265638264117911
Epoch: 17 Idx: 0 Loss: 0.00745124365277622
Epoch: 17 Idx: 5000 Loss: 0.02001930309754509
Epoch: 18 Idx: 0 Loss: 0.01767638520205455
Epoch: 18 Idx: 5000 Loss: 0.023183570628923345
Epoch: 19 Idx: 0 Loss: 0.009050205930130012
Epoch: 19 Idx: 5000 Loss: 0.035524725654498206
Epoch: 20 Idx: 0 Loss: 0.009880745080926165
Epoch: 20 Idx: 5000 Loss: 0.019052611952626756
Epoch: 21 Idx: 0 Loss: 0.04993840201334473
Epoch: 21 Idx: 5000 Loss: 0.027071784377368666
Epoch: 22 Idx: 0 Loss: 0.03511667370709461
Epoch: 22 Idx: 5000 Loss: 0.016356135209775732
Epoch: 23 Idx: 0 Loss: 0.02260294396671029
Epoch: 23 Idx: 5000 Loss: 0.029031271878458855
Epoch: 24 Idx: 0 Loss: 0.015094436837773832
Epoch: 24 Idx: 5000 Loss: 0.02153567729058352
Epoch: 25 Idx: 0 Loss: 0.030604992292763106
Epoch: 25 Idx: 5000 Loss: 0.024823652205912934
Epoch: 26 Idx: 0 Loss: 0.0373398144699778
Epoch: 26 Idx: 5000 Loss: 0.030621982157826362
Epoch: 27 Idx: 0 Loss: 0.025876084295481702
Epoch: 27 Idx: 5000 Loss: 0.020648627902831285
Epoch: 28 Idx: 0 Loss: 0.03455628171844338
Epoch: 28 Idx: 5000 Loss: 0.02417346271635312
Epoch: 29 Idx: 0 Loss: 0.04950483112478056
Epoch: 29 Idx: 5000 Loss: 0.023396929688396278
Epoch: 30 Idx: 0 Loss: 0.01971519631012094
Epoch: 30 Idx: 5000 Loss: 0.016048021481595516
Epoch: 31 Idx: 0 Loss: 0.028361310107875518
Epoch: 31 Idx: 5000 Loss: 0.02611798916131112
Epoch: 32 Idx: 0 Loss: 0.034382109040364116
Epoch: 32 Idx: 5000 Loss: 0.012913400687084407
Epoch: 33 Idx: 0 Loss: 0.01237135402582083
Epoch: 33 Idx: 5000 Loss: 0.027939156971954482
Epoch: 34 Idx: 0 Loss: 0.04162862224491129
Epoch: 34 Idx: 5000 Loss: 0.01929612164698062
Epoch: 35 Idx: 0 Loss: 0.013984861650166474
Epoch: 35 Idx: 5000 Loss: 0.018933919627137436
Epoch: 36 Idx: 0 Loss: 0.01086828414088464
Epoch: 36 Idx: 5000 Loss: 0.031912956430897706
Epoch: 37 Idx: 0 Loss: 0.028849696309282344
Epoch: 37 Idx: 5000 Loss: 0.020032279084201368
Epoch: 38 Idx: 0 Loss: 0.009223149738955536
Epoch: 38 Idx: 5000 Loss: 0.02902418575918194
Epoch: 39 Idx: 0 Loss: 0.041661360782961954
Epoch: 39 Idx: 5000 Loss: 0.03825342274551996
Epoch: 40 Idx: 0 Loss: 0.013720590829235083
Epoch: 40 Idx: 5000 Loss: 0.029059879092907477
Epoch: 41 Idx: 0 Loss: 0.01680460463021759
Epoch: 41 Idx: 5000 Loss: 0.009589819896779803
Epoch: 42 Idx: 0 Loss: 0.02729145030807275
Epoch: 42 Idx: 5000 Loss: 0.02099383277082574
Epoch: 43 Idx: 0 Loss: 0.026075318712898178
Epoch: 43 Idx: 5000 Loss: 0.012149786715628606
Epoch: 44 Idx: 0 Loss: 0.024452077446129365
Epoch: 44 Idx: 5000 Loss: 0.0293418493581415
Epoch: 45 Idx: 0 Loss: 0.023680442799788363
Epoch: 45 Idx: 5000 Loss: 0.05304166405890559
Epoch: 46 Idx: 0 Loss: 0.016791843277517184
Epoch: 46 Idx: 5000 Loss: 0.033708195329844835
Epoch: 47 Idx: 0 Loss: 0.017280597380833276
Epoch: 47 Idx: 5000 Loss: 0.02849740465257792
Epoch: 48 Idx: 0 Loss: 0.014461372612771792
Epoch: 48 Idx: 5000 Loss: 0.014780174006696518
Epoch: 49 Idx: 0 Loss: 0.02636527934016592
Epoch: 49 Idx: 5000 Loss: 0.020034005355114577
Len (direct inputs):  95
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
7Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.23590453230658215
Epoch: 0 Idx: 5000 Loss: 0.02090332309513575
Epoch: 1 Idx: 0 Loss: 0.017833690913831274
Epoch: 1 Idx: 5000 Loss: 0.0354963311704634
Epoch: 2 Idx: 0 Loss: 0.02767157841998523
Epoch: 2 Idx: 5000 Loss: 0.019147562353172613
Epoch: 3 Idx: 0 Loss: 0.015454002597237221
Epoch: 3 Idx: 5000 Loss: 0.027262228968800963
Epoch: 4 Idx: 0 Loss: 0.014351173180529933
Epoch: 4 Idx: 5000 Loss: 0.016551502565811674
Epoch: 5 Idx: 0 Loss: 0.018512986789287283
Epoch: 5 Idx: 5000 Loss: 0.025815133472596487
Epoch: 6 Idx: 0 Loss: 0.02001520661707272
Epoch: 6 Idx: 5000 Loss: 0.01215023225767227
Epoch: 7 Idx: 0 Loss: 0.022649861278431917
Epoch: 7 Idx: 5000 Loss: 0.017413568995472428
Epoch: 8 Idx: 0 Loss: 0.03390924664591079
Epoch: 8 Idx: 5000 Loss: 0.013046771022048298
Epoch: 9 Idx: 0 Loss: 0.013802324309547958
Epoch: 9 Idx: 5000 Loss: 0.020020458213089103
Epoch: 10 Idx: 0 Loss: 0.013689223027728424
Epoch: 10 Idx: 5000 Loss: 0.013463113702165994
Epoch: 11 Idx: 0 Loss: 0.031191327584955714
Epoch: 11 Idx: 5000 Loss: 0.03935885773078217
Epoch: 12 Idx: 0 Loss: 0.03232021029042113
Epoch: 12 Idx: 5000 Loss: 0.019882249823290815
Epoch: 13 Idx: 0 Loss: 0.017841794821819607
Epoch: 13 Idx: 5000 Loss: 0.023505959796847088
Epoch: 14 Idx: 0 Loss: 0.02099650660536552
Epoch: 14 Idx: 5000 Loss: 0.0177228796093612
Epoch: 15 Idx: 0 Loss: 0.017482172264099916
Epoch: 15 Idx: 5000 Loss: 0.017016319973899442
Epoch: 16 Idx: 0 Loss: 0.024948813654194737
Epoch: 16 Idx: 5000 Loss: 0.017439880478740115
Epoch: 17 Idx: 0 Loss: 0.018997487269289133
Epoch: 17 Idx: 5000 Loss: 0.011005079176635457
Epoch: 18 Idx: 0 Loss: 0.011727808610948461
Epoch: 18 Idx: 5000 Loss: 0.014773013137436331
Epoch: 19 Idx: 0 Loss: 0.015229155682972867
Epoch: 19 Idx: 5000 Loss: 0.012953116971815027
Epoch: 20 Idx: 0 Loss: 0.016339485385528874
Epoch: 20 Idx: 5000 Loss: 0.02605153138631141
Epoch: 21 Idx: 0 Loss: 0.024569582256431896
Epoch: 21 Idx: 5000 Loss: 0.02440890353630209
Epoch: 22 Idx: 0 Loss: 0.017759255210462763
Epoch: 22 Idx: 5000 Loss: 0.024795182677993795
Epoch: 23 Idx: 0 Loss: 0.014107264324874764
Epoch: 23 Idx: 5000 Loss: 0.03005563491176634
Epoch: 24 Idx: 0 Loss: 0.027479875695749827
Epoch: 24 Idx: 5000 Loss: 0.025991342635201725
Epoch: 25 Idx: 0 Loss: 0.020891027319126985
Epoch: 25 Idx: 5000 Loss: 0.02040101248275258
Epoch: 26 Idx: 0 Loss: 0.00878185737824879
Epoch: 26 Idx: 5000 Loss: 0.03173862669068362
Epoch: 27 Idx: 0 Loss: 0.016513325772837914
Epoch: 27 Idx: 5000 Loss: 0.021334298277015495
Epoch: 28 Idx: 0 Loss: 0.016215017524074702
Epoch: 28 Idx: 5000 Loss: 0.03975456193019637
Epoch: 29 Idx: 0 Loss: 0.019356780734004105
Epoch: 29 Idx: 5000 Loss: 0.02000035344642926
Epoch: 30 Idx: 0 Loss: 0.02449428065640471
Epoch: 30 Idx: 5000 Loss: 0.02151920742770533
Epoch: 31 Idx: 0 Loss: 0.027791434126462775
Epoch: 31 Idx: 5000 Loss: 0.021585973456620555
Epoch: 32 Idx: 0 Loss: 0.03331673971844604
Epoch: 32 Idx: 5000 Loss: 0.02179271290003072
Epoch: 33 Idx: 0 Loss: 0.0260071794653336
Epoch: 33 Idx: 5000 Loss: 0.013842388770200695
Epoch: 34 Idx: 0 Loss: 0.01856778502900895
Epoch: 34 Idx: 5000 Loss: 0.010706139428815918
Epoch: 35 Idx: 0 Loss: 0.023523860437592612
Epoch: 35 Idx: 5000 Loss: 0.024953890296562347
Epoch: 36 Idx: 0 Loss: 0.009077029869081037
Epoch: 36 Idx: 5000 Loss: 0.027952759128351698
Epoch: 37 Idx: 0 Loss: 0.027792070667922476
Epoch: 37 Idx: 5000 Loss: 0.023031146754676723
Epoch: 38 Idx: 0 Loss: 0.029828583776991217
Epoch: 38 Idx: 5000 Loss: 0.020903988519295737
Epoch: 39 Idx: 0 Loss: 0.014844762714041854
Epoch: 39 Idx: 5000 Loss: 0.012527028650868239
Epoch: 40 Idx: 0 Loss: 0.012786937132934537
Epoch: 40 Idx: 5000 Loss: 0.007020474920619417
Epoch: 41 Idx: 0 Loss: 0.04596211938829535
Epoch: 41 Idx: 5000 Loss: 0.014417507150314594
Epoch: 42 Idx: 0 Loss: 0.022965657383163092
Epoch: 42 Idx: 5000 Loss: 0.028484643657001284
Epoch: 43 Idx: 0 Loss: 0.029593983652523914
Epoch: 43 Idx: 5000 Loss: 0.01644601430506911
Epoch: 44 Idx: 0 Loss: 0.028510294131292975
Epoch: 44 Idx: 5000 Loss: 0.03006684612161796
Epoch: 45 Idx: 0 Loss: 0.013066600970063826
Epoch: 45 Idx: 5000 Loss: 0.02010420725834775
Epoch: 46 Idx: 0 Loss: 0.018038582341994295
Epoch: 46 Idx: 5000 Loss: 0.01679620147538198
Epoch: 47 Idx: 0 Loss: 0.025310309036987732
Epoch: 47 Idx: 5000 Loss: 0.02763536071338068
Epoch: 48 Idx: 0 Loss: 0.01778151253752676
Epoch: 48 Idx: 5000 Loss: 0.01914923764737001
Epoch: 49 Idx: 0 Loss: 0.015237082500393169
Epoch: 49 Idx: 5000 Loss: 0.013329766354124318
Len (direct inputs):  100
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
di0.8559443285354765
Parameter con0.8510442353304241
Parameter containing:
tensor([0.8510], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.19126923764706844
Epoch: 0 Idx: 5000 Loss: 0.030978320696210933
Epoch: 1 Idx: 0 Loss: 0.019971141399477622
Epoch: 1 Idx: 5000 Loss: 0.030774820254821568
Epoch: 2 Idx: 0 Loss: 0.0190288082972946
Epoch: 2 Idx: 5000 Loss: 0.015832285771472297
Epoch: 3 Idx: 0 Loss: 0.021480187634390798
Epoch: 3 Idx: 5000 Loss: 0.019002875982085804
Epoch: 4 Idx: 0 Loss: 0.00958518698425682
Epoch: 4 Idx: 5000 Loss: 0.010135660444901533
Epoch: 5 Idx: 0 Loss: 0.02478764149003465
Epoch: 5 Idx: 5000 Loss: 0.01829220615316199
Epoch: 6 Idx: 0 Loss: 0.020343620706636992
Epoch: 6 Idx: 5000 Loss: 0.03023094242223425
Epoch: 7 Idx: 0 Loss: 0.027253018952569485
Epoch: 7 Idx: 5000 Loss: 0.02200778994044931
Epoch: 8 Idx: 0 Loss: 0.013870402859605842
Epoch: 8 Idx: 5000 Loss: 0.04173223311606599
Epoch: 9 Idx: 0 Loss: 0.02729239071190882
Epoch: 9 Idx: 5000 Loss: 0.015277333048324855
Epoch: 10 Idx: 0 Loss: 0.019594483774466723
Epoch: 10 Idx: 5000 Loss: 0.029885489256172362
Epoch: 11 Idx: 0 Loss: 0.027473830780680594
Epoch: 11 Idx: 5000 Loss: 0.044865940271492724
Epoch: 12 Idx: 0 Loss: 0.02217085518879431
Epoch: 12 Idx: 5000 Loss: 0.03404192611031495
Epoch: 13 Idx: 0 Loss: 0.019600001931068117
Epoch: 13 Idx: 5000 Loss: 0.024717434408180534
Epoch: 14 Idx: 0 Loss: 0.02387696134005988
Epoch: 14 Idx: 5000 Loss: 0.020942649961001984
Epoch: 15 Idx: 0 Loss: 0.018475441046230414
Epoch: 15 Idx: 5000 Loss: 0.021239025189361743
Epoch: 16 Idx: 0 Loss: 0.0195226830882516
Epoch: 16 Idx: 5000 Loss: 0.01420758110467725
Epoch: 17 Idx: 0 Loss: 0.029940380498101497
Epoch: 17 Idx: 5000 Loss: 0.027631860945762296
Epoch: 18 Idx: 0 Loss: 0.01299851548844452
Epoch: 18 Idx: 5000 Loss: 0.01738432076394057
Epoch: 19 Idx: 0 Loss: 0.020092066611202178
Epoch: 19 Idx: 5000 Loss: 0.02032287417436537
Epoch: 20 Idx: 0 Loss: 0.015861913879214873
Epoch: 20 Idx: 5000 Loss: 0.031122308116298902
Epoch: 21 Idx: 0 Loss: 0.012749165163605383
Epoch: 21 Idx: 5000 Loss: 0.02847112830802756
Epoch: 22 Idx: 0 Loss: 0.016488189267427563
Epoch: 22 Idx: 5000 Loss: 0.040753270903120586
Epoch: 23 Idx: 0 Loss: 0.03217979770194957
Epoch: 23 Idx: 5000 Loss: 0.024521572837754673
Epoch: 24 Idx: 0 Loss: 0.015460833420123271
Epoch: 24 Idx: 5000 Loss: 0.012261829672290357
Epoch: 25 Idx: 0 Loss: 0.02071028727331403
Epoch: 25 Idx: 5000 Loss: 0.020313155465457235
Epoch: 26 Idx: 0 Loss: 0.017162522086529984
Epoch: 26 Idx: 5000 Loss: 0.020309644449148803
Epoch: 27 Idx: 0 Loss: 0.0254785441703156
Epoch: 27 Idx: 5000 Loss: 0.0374256087146434
Epoch: 28 Idx: 0 Loss: 0.021354854784285133
Epoch: 28 Idx: 5000 Loss: 0.021658679765218347
Epoch: 29 Idx: 0 Loss: 0.034053816210421715
Epoch: 29 Idx: 5000 Loss: 0.02024066847990856
Epoch: 30 Idx: 0 Loss: 0.017713606989553824
Epoch: 30 Idx: 5000 Loss: 0.02380522788436326
Epoch: 31 Idx: 0 Loss: 0.022255197632539685
Epoch: 31 Idx: 5000 Loss: 0.019480923120303188
Epoch: 32 Idx: 0 Loss: 0.017590048747798993
Epoch: 32 Idx: 5000 Loss: 0.013498526016218275
Epoch: 33 Idx: 0 Loss: 0.026087433505554987
Epoch: 33 Idx: 5000 Loss: 0.018805656435789363
Epoch: 34 Idx: 0 Loss: 0.023799263005270785
Epoch: 34 Idx: 5000 Loss: 0.010494844048784254
Epoch: 35 Idx: 0 Loss: 0.007880729531751021
Epoch: 35 Idx: 5000 Loss: 0.03315312044528234
Epoch: 36 Idx: 0 Loss: 0.016392311341225936
Epoch: 36 Idx: 5000 Loss: 0.007368868442659764
Epoch: 37 Idx: 0 Loss: 0.02217181932458963
Epoch: 37 Idx: 5000 Loss: 0.013879409007291439
Epoch: 38 Idx: 0 Loss: 0.023738179030572466
Epoch: 38 Idx: 5000 Loss: 0.02143480178235832
Epoch: 39 Idx: 0 Loss: 0.0211398427852067
Epoch: 39 Idx: 5000 Loss: 0.010475161051559179
Epoch: 40 Idx: 0 Loss: 0.012829305676071313
Epoch: 40 Idx: 5000 Loss: 0.03032938914197047
Epoch: 41 Idx: 0 Loss: 0.01973894563153892
Epoch: 41 Idx: 5000 Loss: 0.015621256008450778
Epoch: 42 Idx: 0 Loss: 0.020049763892328237
Epoch: 42 Idx: 5000 Loss: 0.0217956667643023
Epoch: 43 Idx: 0 Loss: 0.02255796602769331
Epoch: 43 Idx: 5000 Loss: 0.022962269156308168
Epoch: 44 Idx: 0 Loss: 0.01986789658990188
Epoch: 44 Idx: 5000 Loss: 0.015890914673895563
Epoch: 45 Idx: 0 Loss: 0.016485386834800286
Epoch: 45 Idx: 5000 Loss: 0.06542523678404683
Epoch: 46 Idx: 0 Loss: 0.022475510837247022
Epoch: 46 Idx: 5000 Loss: 0.03074169933921641
Epoch: 47 Idx: 0 Loss: 0.023181785097713136
Epoch: 47 Idx: 5000 Loss: 0.023649653884991965
Epoch: 48 Idx: 0 Loss: 0.04907920224949548
Epoch: 48 Idx: 5000 Loss: 0.0292323490938386
Epoch: 49 Idx: 0 Loss: 0.01943938702828797
Epoch: 49 Idx: 5000 Loss: 0.011515061517041637
Traceback (most recent call last):
  File "Attention_anatomy_aml.py", line 475, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 22 failed with Disk quota exceeded
eded
ailed with Disk quota exceeded
tory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Tue Sep  1 19:51:06 2020
Results reported at Tue Sep  1 19:51:06 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 4 Output/test_anatomy_aml_bagofnbrs1_4.pkl Models/anatomy_aml_bagofnbrs1_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   20874.03 sec.
    Max Memory :                                 2926 MB
    Average Memory :                             2715.26 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40491.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21380 sec.
    Turnaround time :                            21437 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc237>
Subject: Job 3289875: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs2_4.pkl Models/anatomy_aml_bagofnbrs2_4.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs2_4.pkl Models/anatomy_aml_bagofnbrs2_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc237>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 20:00:42 2020
Results reported at Tue Sep  1 20:00:42 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 4 Output/test_anatomy_aml_bagofnbrs2_4.pkl Models/anatomy_aml_bagofnbrs2_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   21890.70 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2716.24 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   21981 sec.
    Turnaround time :                            22012 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc248>
Subject: Job 3289877: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs3_4.pkl Models/anatomy_aml_bagofnbrs3_4.pt> in cluster <dcc> Done

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs3_4.pkl Models/anatomy_aml_bagofnbrs3_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc248>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 20:19:29 2020
Results reported at Tue Sep  1 20:19:29 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 4 Output/test_anatomy_aml_bagofnbrs3_4.pkl Models/anatomy_aml_bagofnbrs3_4.pt
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   23013.71 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2719.80 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23108 sec.
    Turnaround time :                            23139 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc264>
Subject: Job 3289879: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs4_4.pkl Models/anatomy_aml_bagofnbrs4_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs4_4.pkl Models/anatomy_aml_bagofnbrs4_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc264>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 20:56:31 2020
Results reported at Tue Sep  1 20:56:31 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 4 Output/test_anatomy_aml_bagofnbrs4_4.pkl Models/anatomy_aml_bagofnbrs4_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24595.89 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2717.65 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25330 sec.
    Turnaround time :                            25361 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc262>
Subject: Job 3289881: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs5_4.pkl Models/anatomy_aml_bagofnbrs5_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs5_4.pkl Models/anatomy_aml_bagofnbrs5_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc262>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:15:34 2020
Results reported at Tue Sep  1 21:15:34 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 4 Output/test_anatomy_aml_bagofnbrs5_4.pkl Models/anatomy_aml_bagofnbrs5_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   26403.82 sec.
    Max Memory :                                 2928 MB
    Average Memory :                             2719.58 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40489.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   26446 sec.
    Turnaround time :                            26504 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc234>
Subject: Job 3289883: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs7_4.pkl Models/anatomy_aml_bagofnbrs7_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs7_4.pkl Models/anatomy_aml_bagofnbrs7_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc234>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 21:49:17 2020
Results reported at Tue Sep  1 21:49:17 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 7 4 Output/test_anatomy_aml_bagofnbrs7_4.pkl Models/anatomy_aml_bagofnbrs7_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   28466.39 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2716.52 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   28496 sec.
    Turnaround time :                            28527 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc240>
Subject: Job 3289885: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 4 Output/test_anatomy_aml_bagofnbrs8_4.pkl Models/anatomy_aml_bagofnbrs8_4.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 4 Output/test_anatomy_aml_bagofnbrs8_4.pkl Models/anatomy_aml_bagofnbrs8_4.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:50 2020
Job was executed on host(s) <dccxc240>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:48 2020
Terminated at Tue Sep  1 22:14:02 2020
Results reported at Tue Sep  1 22:14:02 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 4 Output/test_anatomy_aml_bagofnbrs8_4.pkl Models/anatomy_aml_bagofnbrs8_4.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   29903.65 sec.
    Max Memory :                                 2922 MB
    Average Memory :                             2722.10 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40495.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   29954 sec.
    Turnaround time :                            30012 sec.

The output (if any) is above this job summary.

