Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.23878544540866664
Epoch: 0 Idx: 5000 Loss: 0.03206202710870101
Epoch: 1 Idx: 0 Loss: 0.03840293756639006
Epoch: 1 Idx: 5000 Loss: 0.05365854537352463
Epoch: 2 Idx: 0 Loss: 0.021636900476161826
Epoch: 2 Idx: 5000 Loss: 0.03098422601754382
Epoch: 3 Idx: 0 Loss: 0.03903707343437331
Epoch: 3 Idx: 5000 Loss: 0.017259222722169853
Epoch: 4 Idx: 0 Loss: 0.0138614140357499
Epoch: 4 Idx: 5000 Loss: 0.012606131644449486
Epoch: 5 Idx: 0 Loss: 0.01464950357369916
Epoch: 5 Idx: 5000 Loss: 0.00957253706016603
Epoch: 6 Idx: 0 Loss: 0.031535729418091955
Epoch: 6 Idx: 5000 Loss: 0.030708515408850695
Epoch: 7 Idx: 0 Loss: 0.031863245181433957
Epoch: 7 Idx: 5000 Loss: 0.009977237644358646
Epoch: 8 Idx: 0 Loss: 0.021889489464992556
Epoch: 8 Idx: 5000 Loss: 0.031703972632879676
Epoch: 9 Idx: 0 Loss: 0.014007488399220667
Epoch: 9 Idx: 5000 Loss: 0.028290633748938844
Epoch: 10 Idx: 0 Loss: 0.008297127532390081
Epoch: 10 Idx: 5000 Loss: 0.02776459238247266
Epoch: 11 Idx: 0 Loss: 0.019405874041186055
Epoch: 11 Idx: 5000 Loss: 0.025829467886495008
Epoch: 12 Idx: 0 Loss: 0.014853996057538206
Epoch: 12 Idx: 5000 Loss: 0.03457198198582854
Epoch: 13 Idx: 0 Loss: 0.026218695395603446
Epoch: 13 Idx: 5000 Loss: 0.022795179338566517
Epoch: 14 Idx: 0 Loss: 0.05356619364716246
Epoch: 14 Idx: 5000 Loss: 0.019128848711103097
Epoch: 15 Idx: 0 Loss: 0.020409288968562263
Epoch: 15 Idx: 5000 Loss: 0.015898114791798274
Epoch: 16 Idx: 0 Loss: 0.015331118565207384
Epoch: 16 Idx: 5000 Loss: 0.014590375109332856
Epoch: 17 Idx: 0 Loss: 0.016296173226106263
Epoch: 17 Idx: 5000 Loss: 0.028196431768154846
Epoch: 18 Idx: 0 Loss: 0.017489820137048945
Epoch: 18 Idx: 5000 Loss: 0.05684690890734807
Epoch: 19 Idx: 0 Loss: 0.01898408654290516
Epoch: 19 Idx: 5000 Loss: 0.021389668934145345
Epoch: 20 Idx: 0 Loss: 0.03291893545891511
Epoch: 20 Idx: 5000 Loss: 0.040707167926536614
Epoch: 21 Idx: 0 Loss: 0.02229417250133372
Epoch: 21 Idx: 5000 Loss: 0.017267571919762727
Epoch: 22 Idx: 0 Loss: 0.025004832224665645
Epoch: 22 Idx: 5000 Loss: 0.025496611069013887
Epoch: 23 Idx: 0 Loss: 0.025087070911679374
Epoch: 23 Idx: 5000 Loss: 0.01034982619113976
Epoch: 24 Idx: 0 Loss: 0.030686495367287583
Epoch: 24 Idx: 5000 Loss: 0.01704847277531213
Epoch: 25 Idx: 0 Loss: 0.022614305094514
Epoch: 25 Idx: 5000 Loss: 0.022347201911626237
Epoch: 26 Idx: 0 Loss: 0.01207744293876435
Epoch: 26 Idx: 5000 Loss: 0.015878627824337438
Epoch: 27 Idx: 0 Loss: 0.013128398589991527
Epoch: 27 Idx: 5000 Loss: 0.022698820991066165
Epoch: 28 Idx: 0 Loss: 0.022979763053169764
Epoch: 28 Idx: 5000 Loss: 0.04122088706454895
Epoch: 29 Idx: 0 Loss: 0.027560978359554088
Epoch: 29 Idx: 5000 Loss: 0.01703459765516646
Epoch: 30 Idx: 0 Loss: 0.02132462747669694
Epoch: 30 Idx: 5000 Loss: 0.018829837163785793
Epoch: 31 Idx: 0 Loss: 0.025175891409572604
Epoch: 31 Idx: 5000 Loss: 0.023883646808816064
Epoch: 32 Idx: 0 Loss: 0.011815012292442473
Epoch: 32 Idx: 5000 Loss: 0.012938139742720792
Epoch: 33 Idx: 0 Loss: 0.03197036942630703
Epoch: 33 Idx: 5000 Loss: 0.02639241189536413
Epoch: 34 Idx: 0 Loss: 0.00886320549969237
Epoch: 34 Idx: 5000 Loss: 0.03691840223001138
Epoch: 35 Idx: 0 Loss: 0.01331559728975041
Epoch: 35 Idx: 5000 Loss: 0.015995720817208813
Epoch: 36 Idx: 0 Loss: 0.01620198836093464
Epoch: 36 Idx: 5000 Loss: 0.015977307446964767
Epoch: 37 Idx: 0 Loss: 0.03355571018556296
Epoch: 37 Idx: 5000 Loss: 0.020105568051058686
Epoch: 38 Idx: 0 Loss: 0.022762597058509134
Epoch: 38 Idx: 5000 Loss: 0.01895612852192248
Epoch: 39 Idx: 0 Loss: 0.011176745522962191
Epoch: 39 Idx: 5000 Loss: 0.019428393779507898
Epoch: 40 Idx: 0 Loss: 0.014540040638077945
Epoch: 40 Idx: 5000 Loss: 0.03457766784640191
Epoch: 41 Idx: 0 Loss: 0.012162426903570568
Epoch: 41 Idx: 5000 Loss: 0.016715097338422337
Epoch: 42 Idx: 0 Loss: 0.040882726967291116
Epoch: 42 Idx: 5000 Loss: 0.02632074740988348
Epoch: 43 Idx: 0 Loss: 0.01752051710084308
Epoch: 43 Idx: 5000 Loss: 0.023942464178266412
Epoch: 44 Idx: 0 Loss: 0.03491193010316179
Epoch: 44 Idx: 5000 Loss: 0.014387321506507268
Epoch: 45 Idx: 0 Loss: 0.024612795196972376
Epoch: 45 Idx: 5000 Loss: 0.02427718437660311
Epoch: 46 Idx: 0 Loss: 0.013603678594540387
Epoch: 46 Idx: 5000 Loss: 0.008859055593414754
Epoch: 47 Idx: 0 Loss: 0.02979935431632813
Epoch: 47 Idx: 5000 Loss: 0.026872902999275538
Epoch: 48 Idx: 0 Loss: 0.037750068347955607
Epoch: 48 Idx: 5000 Loss: 0.026427073411593262
Epoch: 49 Idx: 0 Loss: 0.02088215149548424
Epoch: 49 Idx: 5000 Loss: 0.012926102399127209
Len (direct inputs):  94
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division byTrainiTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1655144274160915
Epoch: 0 Idx: 5000 Loss: 0.017883395260455848
Epoch: 1 Idx: 0 Loss: 0.02053776263094664
Epoch: 1 Idx: 5000 Loss: 0.015335022438452313
Epoch: 2 Idx: 0 Loss: 0.020164154498257106
Epoch: 2 Idx: 5000 Loss: 0.019171139499148747
Epoch: 3 Idx: 0 Loss: 0.016095735443007568
Epoch: 3 Idx: 5000 Loss: 0.018938440495258068
Epoch: 4 Idx: 0 Loss: 0.022331716392422214
Epoch: 4 Idx: 5000 Loss: 0.014313940140419347
Epoch: 5 Idx: 0 Loss: 0.02149706846520176
Epoch: 5 Idx: 5000 Loss: 0.03157901221575153
Epoch: 6 Idx: 0 Loss: 0.021522797942275677
Epoch: 6 Idx: 5000 Loss: 0.02970330246293778
Epoch: 7 Idx: 0 Loss: 0.015686766999044532
Epoch: 7 Idx: 5000 Loss: 0.01910092499254192
Epoch: 8 Idx: 0 Loss: 0.02559379175774462
Epoch: 8 Idx: 5000 Loss: 0.016137616821326778
Epoch: 9 Idx: 0 Loss: 0.036732764966067956
Epoch: 9 Idx: 5000 Loss: 0.045305110393452724
Epoch: 10 Idx: 0 Loss: 0.022388012971699532
Epoch: 10 Idx: 5000 Loss: 0.021688032616268883
Epoch: 11 Idx: 0 Loss: 0.02122157816247993
Epoch: 11 Idx: 5000 Loss: 0.013785157610001148
Epoch: 12 Idx: 0 Loss: 0.03019651261732534
Epoch: 12 Idx: 5000 Loss: 0.03450146241071737
Epoch: 13 Idx: 0 Loss: 0.017873481309960047
Epoch: 13 Idx: 5000 Loss: 0.011820509646930868
Epoch: 14 Idx: 0 Loss: 0.022655267377268848
Epoch: 14 Idx: 5000 Loss: 0.039317679427904435
Epoch: 15 Idx: 0 Loss: 0.02405672781159076
Epoch: 15 Idx: 5000 Loss: 0.031898440564570224
Epoch: 16 Idx: 0 Loss: 0.03383476260664167
Epoch: 16 Idx: 5000 Loss: 0.02652911727076049
Epoch: 17 Idx: 0 Loss: 0.024276477378185585
Epoch: 17 Idx: 5000 Loss: 0.035147395242589835
Epoch: 18 Idx: 0 Loss: 0.025791538949704353
Epoch: 18 Idx: 5000 Loss: 0.021449813018722987
Epoch: 19 Idx: 0 Loss: 0.023298824334722974
Epoch: 19 Idx: 5000 Loss: 0.02325019384738161
Epoch: 20 Idx: 0 Loss: 0.02009081319238103
Epoch: 20 Idx: 5000 Loss: 0.031119695300853713
Epoch: 21 Idx: 0 Loss: 0.03399687541342128
Epoch: 21 Idx: 5000 Loss: 0.015894238417058927
Epoch: 22 Idx: 0 Loss: 0.016591808861829843
Epoch: 22 Idx: 5000 Loss: 0.016144320856689487
Epoch: 23 Idx: 0 Loss: 0.015334691269183204
Epoch: 23 Idx: 5000 Loss: 0.026783374490310846
Epoch: 24 Idx: 0 Loss: 0.013451227191126607
Epoch: 24 Idx: 5000 Loss: 0.021789686163411027
Epoch: 25 Idx: 0 Loss: 0.018631214756207982
Epoch: 25 Idx: 5000 Loss: 0.019093065704882346
Epoch: 26 Idx: 0 Loss: 0.02651630753464479
Epoch: 26 Idx: 5000 Loss: 0.013457784570864774
Epoch: 27 Idx: 0 Loss: 0.015750487760239892
Epoch: 27 Idx: 5000 Loss: 0.03776655183900286
Epoch: 28 Idx: 0 Loss: 0.028511277924786006
Epoch: 28 Idx: 5000 Loss: 0.01902915153283557
Epoch: 29 Idx: 0 Loss: 0.017991817892077537
Epoch: 29 Idx: 5000 Loss: 0.016174200152795144
Epoch: 30 Idx: 0 Loss: 0.02939284637666096
Epoch: 30 Idx: 5000 Loss: 0.011709340785286125
Epoch: 31 Idx: 0 Loss: 0.02598567821539473
Epoch: 31 Idx: 5000 Loss: 0.014868107536985361
Epoch: 32 Idx: 0 Loss: 0.028992354581599433
Epoch: 32 Idx: 5000 Loss: 0.017448443963323266
Epoch: 33 Idx: 0 Loss: 0.031277735643768574
Epoch: 33 Idx: 5000 Loss: 0.015778486202688888
Epoch: 34 Idx: 0 Loss: 0.02039083196901051
Epoch: 34 Idx: 5000 Loss: 0.034812193239815215
Epoch: 35 Idx: 0 Loss: 0.015535043622634392
Epoch: 35 Idx: 5000 Loss: 0.012577898845831076
Epoch: 36 Idx: 0 Loss: 0.019902592695154167
Epoch: 36 Idx: 5000 Loss: 0.024519887746340247
Epoch: 37 Idx: 0 Loss: 0.014676787240038184
Epoch: 37 Idx: 5000 Loss: 0.035965664978949374
Epoch: 38 Idx: 0 Loss: 0.022789606867187135
Epoch: 38 Idx: 5000 Loss: 0.016926941635281363
Epoch: 39 Idx: 0 Loss: 0.020795049886748043
Epoch: 39 Idx: 5000 Loss: 0.014875859040184338
Epoch: 40 Idx: 0 Loss: 0.01965206537919048
Epoch: 40 Idx: 5000 Loss: 0.010289856228363942
Epoch: 41 Idx: 0 Loss: 0.014876373179263791
Epoch: 41 Idx: 5000 Loss: 0.0348794212912235
Epoch: 42 Idx: 0 Loss: 0.02262523615425501
Epoch: 42 Idx: 5000 Loss: 0.01125453966313314
Epoch: 43 Idx: 0 Loss: 0.015577514606225775
Epoch: 43 Idx: 5000 Loss: 0.010402671376455766
Epoch: 44 Idx: 0 Loss: 0.014887304925034674
Epoch: 44 Idx: 5000 Loss: 0.021836153536815936
Epoch: 45 Idx: 0 Loss: 0.005787305325896238
Epoch: 45 Idx: 5000 Loss: 0.01862065923923056
Epoch: 46 Idx: 0 Loss: 0.02536816134951324
Epoch: 46 Idx: 5000 Loss: 0.020349121561812858
Epoch: 47 Idx: 0 Loss: 0.010937792606934959
Epoch: 47 Idx: 5000 Loss: 0.01540519053900769
Epoch: 48 Idx: 0 Loss: 0.027720196715627567
Epoch: 48 Idx: 5000 Loss: 0.03832500926651658
Epoch: 49 Idx: 0 Loss: 0.03463469810208204
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 400, in <module>
    loss.backward()
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
0.18772790541617435
Epoch: 0 Idx: 5000 Loss: 0.014813967239445223
Epoch: 1 Idx: 0 Loss: 0.023394894283293742
Epoch: 1 Idx: 5000 Loss: 0.014775839536635032
Epoch: 2 Idx: 0 Loss: 0.028505670969823944
Epoch: 2 Idx: 5000 Loss: 0.032961691124454906
Epoch: 3 Idx: 0 Loss: 0.03797761357937053
Epoch: 3 Idx: 5000 Loss: 0.030617531272031526
Epoch: 4 Idx: 0 Loss: 0.01584384429033223
Epoch: 4 Idx: 5000 Loss: 0.04148838062284131
Epoch: 5 Idx: 0 Loss: 0.01814669921074829
Epoch: 5 Idx: 5000 Loss: 0.024483100878543468
Epoch: 6 Idx: 0 Loss: 0.024328146860671623
Epoch: 6 Idx: 5000 Loss: 0.005986596893065695
Epoch: 7 Idx: 0 Loss: 0.02407949050014027
Epoch: 7 Idx: 5000 Loss: 0.025562745983239064
Epoch: 8 Idx: 0 Loss: 0.03720385031516964
Epoch: 8 Idx: 5000 Loss: 0.0330666794665925
Epoch: 9 Idx: 0 Loss: 0.022681655914153442
Epoch: 9 Idx: 5000 Loss: 0.03865495600828145
Epoch: 10 Idx: 0 Loss: 0.01865965870606598
Epoch: 10 Idx: 5000 Loss: 0.021921045160721235
Epoch: 11 Idx: 0 Loss: 0.017856536874098432
Epoch: 11 Idx: 5000 Loss: 0.018062868927876355
Epoch: 12 Idx: 0 Loss: 0.034122188662850744
Epoch: 12 Idx: 5000 Loss: 0.009420647163450768
Epoch: 13 Idx: 0 Loss: 0.027101232616073417
Epoch: 13 Idx: 5000 Loss: 0.024017708512518533
Epoch: 14 Idx: 0 Loss: 0.02100979226344152
Epoch: 14 Idx: 5000 Loss: 0.0254579549608509
Epoch: 15 Idx: 0 Loss: 0.023912751742040883
Epoch: 15 Idx: 5000 Loss: 0.0119547042553768
Epoch: 16 Idx: 0 Loss: 0.02945453313864868
Epoch: 16 Idx: 5000 Loss: 0.015909333032884495
Epoch: 17 Idx: 0 Loss: 0.026781830750142614
Epoch: 17 Idx: 5000 Loss: 0.022116200218328556
Epoch: 18 Idx: 0 Loss: 0.020293267680004938
Epoch: 18 Idx: 5000 Loss: 0.023271118843389595
Epoch: 19 Idx: 0 Loss: 0.018055944346409385
Epoch: 19 Idx: 5000 Loss: 0.03255605612807549
Epoch: 20 Idx: 0 Loss: 0.028546340547187125
Epoch: 20 Idx: 5000 Loss: 0.026592385150667017
Epoch: 21 Idx: 0 Loss: 0.019811232153945202
Epoch: 21 Idx: 5000 Loss: 0.018618995345692148
Epoch: 22 Idx: 0 Loss: 0.01591123198237832
Epoch: 22 Idx: 5000 Loss: 0.018239549133873587
Epoch: 23 Idx: 0 Loss: 0.01985609023286586
Epoch: 23 Idx: 5000 Loss: 0.018297294383348532
Epoch: 24 Idx: 0 Loss: 0.06081268451463723
Epoch: 24 Idx: 5000 Loss: 0.036307759353223126
Epoch: 25 Idx: 0 Loss: 0.021082261136065473
Epoch: 25 Idx: 5000 Loss: 0.03591619457124
Epoch: 26 Idx: 0 Loss: 0.018833215859007164
Epoch: 26 Idx: 5000 Loss: 0.013704545515936431
Epoch: 27 Idx: 0 Loss: 0.013991806178781844
Epoch: 27 Idx: 5000 Loss: 0.03148395128994529
Epoch: 28 Idx: 0 Loss: 0.03097839563837148
Epoch: 28 Idx: 5000 Loss: 0.008104731867657608
Epoch: 29 Idx: 0 Loss: 0.025109667521075355
Epoch: 29 Idx: 5000 Loss: 0.046742271726956025
Epoch: 30 Idx: 0 Loss: 0.020010704157034275
Epoch: 30 Idx: 5000 Loss: 0.027051049532629504
Epoch: 31 Idx: 0 Loss: 0.013102056460226587
Epoch: 31 Idx: 5000 Loss: 0.030140967751396523
Epoch: 32 Idx: 0 Loss: 0.01330932535564983
Epoch: 32 Idx: 5000 Loss: 0.023652654269151294
Epoch: 33 Idx: 0 Loss: 0.022455267344452752
Epoch: 33 Idx: 5000 Loss: 0.011039920308178266
Epoch: 34 Idx: 0 Loss: 0.019485975021218585
Epoch: 34 Idx: 5000 Loss: 0.0318106482097103
Epoch: 35 Idx: 0 Loss: 0.022279557937323266
Epoch: 35 Idx: 5000 Loss: 0.012777631240902042
Epoch: 36 Idx: 0 Loss: 0.03287399391587571
Epoch: 36 Idx: 5000 Loss: 0.016179551731869014
Epoch: 37 Idx: 0 Loss: 0.024088456565332313
Epoch: 37 Idx: 5000 Loss: 0.027240639762415905
Epoch: 38 Idx: 0 Loss: 0.028104389533731372
Epoch: 38 Idx: 5000 Loss: 0.010434937160485956
Epoch: 39 Idx: 0 Loss: 0.020411750874871015
Epoch: 39 Idx: 5000 Loss: 0.02245726628528434
Epoch: 40 Idx: 0 Loss: 0.03134795982218719
Epoch: 40 Idx: 5000 Loss: 0.017584508485420338
Epoch: 41 Idx: 0 Loss: 0.01659432542250801
Epoch: 41 Idx: 5000 Loss: 0.03460240197191913
Epoch: 42 Idx: 0 Loss: 0.011877302113140913
Epoch: 42 Idx: 5000 Loss: 0.015936452809286385
Epoch: 43 Idx: 0 Loss: 0.007453474775105022
Epoch: 43 Idx: 5000 Loss: 0.008698117137941773
Epoch: 44 Idx: 0 Loss: 0.012340520940418688
Epoch: 44 Idx: 5000 Loss: 0.020076514493073908
Epoch: 45 Idx: 0 Loss: 0.015821131318769718
Epoch: 45 Idx: 5000 Loss: 0.0057646158955862534
Epoch: 46 Idx: 0 Loss: 0.017221749150342174
Epoch: 46 Idx: 5000 Loss: 0.009463049082176309
Epoch: 47 Idx: 0 Loss: 0.020479938165244425
Epoch: 47 Idx: 5000 Loss: 0.014359938756265367
Epoch: 48 Idx: 0 Loss: 0.018746697778380443
Epoch: 48 Idx: 5000 Loss: 0.0330451180176
Epoch: 49 Idx: 0 Loss: 0.0057603691145768025
Epoch: 49 Idx: 5000 Loss: 0.01796483596998324
Len (direct inputs):  90
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.19665931901785652
Epoch: 0 Idx: 5000 Loss: 0.025927352377241916
Epoch: 1 Idx: 0 Loss: 0.013668610784279145
Epoch: 1 Idx: 5000 Loss: 0.0256259771774224
Epoch: 2 Idx: 0 Loss: 0.021227415595670936
Epoch: 2 Idx: 5000 Loss: 0.019630774993863863
Epoch: 3 Idx: 0 Loss: 0.01094108277282143
Epoch: 3 Idx: 5000 Loss: 0.030927024726576345
Epoch: 4 Idx: 0 Loss: 0.012165257554652767
Epoch: 4 Idx: 5000 Loss: 0.044429889165281095
Epoch: 5 Idx: 0 Loss: 0.013997218007714094
Epoch: 5 Idx: 5000 Loss: 0.018507204779437514
Epoch: 6 Idx: 0 Loss: 0.015498870555531234
Epoch: 6 Idx: 5000 Loss: 0.031125115662317078
Epoch: 7 Idx: 0 Loss: 0.031140584672095017
Epoch: 7 Idx: 5000 Loss: 0.026803722615416303
Epoch: 8 Idx: 0 Loss: 0.009712258485469027
Epoch: 8 Idx: 5000 Loss: 0.021659585554460053
Epoch: 9 Idx: 0 Loss: 0.02120316913110854
Epoch: 9 Idx: 5000 Loss: 0.014355010668693088
Epoch: 10 Idx: 0 Loss: 0.01516231347549845
Epoch: 10 Idx: 5000 Loss: 0.02735934574294286
Epoch: 11 Idx: 0 Loss: 0.011702887538661808
Epoch: 11 Idx: 5000 Loss: 0.016569132468848696
Epoch: 12 Idx: 0 Loss: 0.01650489320856888
Epoch: 12 Idx: 5000 Loss: 0.009977816516910824
Epoch: 13 Idx: 0 Loss: 0.01074082230561791
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
h: 15 Idx: 5000 Loss: 0.014414256041485447
Epoch: 16 Idx: 0 Loss: 0.025018901751782188
Epoch: 16 Idx: 5000 Loss: 0.015793883832127365
Epoch: 17 Idx: 0 Loss: 0.027332565960437316
Epoch: 17 Idx: 5000 Loss: 0.023909509819103735
Epoch: 18 Idx: 0 Loss: 0.0136149408474111
Epoch: 18 Idx: 5000 Loss: 0.012426752020476408
Epoch: 19 Idx: 0 Loss: 0.03038158036893718
Epoch: 19 Idx: 5000 Loss: 0.02824715618103952
Epoch: 20 Idx: 0 Loss: 0.012266005202778811
Epoch: 20 Idx: 5000 Loss: 0.02351163742818653
Epoch: 21 Idx: 0 Loss: 0.011435511263053763
Epoch: 21 Idx: 5000 Loss: 0.013882235381840432
Epoch: 22 Idx: 0 Loss: 0.032421805011871085
Epoch: 22 Idx: 5000 Loss: 0.023409093985709284
Epoch: 23 Idx: 0 Loss: 0.023443079815922613
Epoch: 23 Idx: 5000 Loss: 0.016876623451332538
Epoch: 24 Idx: 0 Loss: 0.02772973466432897
Epoch: 24 Idx: 5000 Loss: 0.016135237477380982
Epoch: 25 Idx: 0 Loss: 0.016535587760664057
Epoch: 25 Idx: 5000 Loss: 0.016333685893969774
Epoch: 26 Idx: 0 Loss: 0.01868507637023611
Epoch: 26 Idx: 5000 Loss: 0.023858961707949333
Epoch: 27 Idx: 0 Loss: 0.028512382804575034
Epoch: 27 Idx: 5000 Loss: 0.01257372637589593
Epoch: 28 Idx: 0 Loss: 0.020869045701125855
Epoch: 28 Idx: 5000 Loss: 0.024733305669275996
Epoch: 29 Idx: 0 Loss: 0.03180230537465895
Epoch: 29 Idx: 5000 Loss: 0.018027093579217383
Epoch: 30 Idx: 0 Loss: 0.008720269109422245
Epoch: 30 Idx: 5000 Loss: 0.02371204396267297
Epoch: 31 Idx: 0 Loss: 0.010782183909270979
Epoch: 31 Idx: 5000 Loss: 0.019022184563466243
Epoch: 32 Idx: 0 Loss: 0.01337588626209324
Epoch: 32 Idx: 5000 Loss: 0.028936785612252054
Epoch: 33 Idx: 0 Loss: 0.011817328562709572
Epoch: 33 Idx: 5000 Loss: 0.016713282165160756
Epoch: 34 Idx: 0 Loss: 0.01670654067828798
Epoch: 34 Idx: 5000 Loss: 0.02819152116254649
Epoch: 35 Idx: 0 Loss: 0.0316458171730288
Epoch: 35 Idx: 5000 Loss: 0.02070514895182867
Epoch: 36 Idx: 0 Loss: 0.02946131719979094
Epoch: 36 Idx: 5000 Loss: 0.03354761608873734
Epoch: 37 Idx: 0 Loss: 0.03112733842482997
Epoch: 37 Idx: 5000 Loss: 0.032494477611894064
Epoch: 38 Idx: 0 Loss: 0.018738271457648625
Epoch: 38 Idx: 5000 Loss: 0.02821310455722184
Epoch: 39 Idx: 0 Loss: 0.027088756225933465
Epoch: 39 Idx: 5000 Loss: 0.04738207846508564
Epoch: 40 Idx: 0 Loss: 0.01820050853371949
Epoch: 40 Idx: 5000 Loss: 0.010205353901847266
Epoch: 41 Idx: 0 Loss: 0.019837524796230816
Epoch: 41 Idx: 5000 Loss: 0.02348075608900186
Epoch: 42 Idx: 0 Loss: 0.01938410034510541
Epoch: 42 Idx: 5000 Loss: 0.01758076663686451
Epoch: 43 Idx: 0 Loss: 0.01927330165191292
Epoch: 43 Idx: 5000 Loss: 0.03100676820511239
Epoch: 44 Idx: 0 Loss: 0.03765555506627704
Epoch: 44 Idx: 5000 Loss: 0.009498850809782734
Epoch: 45 Idx: 0 Loss: 0.0320791609644703
Epoch: 45 Idx: 5000 Loss: 0.022441694595001685
Epoch: 46 Idx: 0 Loss: 0.022127812539864122
Epoch: 46 Idx: 5000 Loss: 0.029220456014186224
Epoch: 47 Idx: 0 Loss: 0.02687152189549632
Epoch: 47 Idx: 5000 Loss: 0.03314917541555522
Epoch: 48 Idx: 0 Loss: 0.010908053688614163
Epoch: 48 Idx: 5000 Loss: 0.015430359633108551
Epoch: 49 Idx: 0 Loss: 0.015939813821854214
Epoch: 49 Idx: 5000 Loss: 0.012113645327601999
Len (direct inputs):  108
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17629789852121938
Epoch: 0 Idx: 5000 Loss: 0.02936675069812597
Epoch: 1 Idx: 0 Loss: 0.02246168380837202
Epoch: 1 Idx: 5000 Loss: 0.028832563086319624
Epoch: 2 Idx: 0 Loss: 0.027090867777081095
Epoch: 2 Idx: 5000 Loss: 0.016566244805071045
Epoch: 3 Idx: 0 Loss: 0.013264125264750466
Epoch: 3 Idx: 5000 Loss: 0.011188577133034004
Epoch: 4 Idx: 0 Loss: 0.04087800898415479
Epoch: 4 Idx: 5000 Loss: 0.024783791734452276
Epoch: 5 Idx: 0 Loss: 0.02388663583033861
Epoch: 5 Idx: 5000 Loss: 0.02341684752742225
Epoch: 6 Idx: 0 Loss: 0.03165222099879051
Epoch: 6 Idx: 5000 Loss: 0.013773162340559897
Epoch: 7 Idx: 0 Loss: 0.018026233956768712
Epoch: 7 Idx: 5000 Loss: 0.025515845925793215
Epoch: 8 Idx: 0 Loss: 0.010848739784026565
Epoch: 8 Idx: 5000 Loss: 0.011568617662890048
Epoch: 9 Idx: 0 Loss: 0.01646429598936028
Epoch: 9 Idx: 5000 Loss: 0.025839947508162903
Epoch: 10 Idx: 0 Loss: 0.02069592709575125
Epoch: 10 Idx: 5000 Loss: 0.024840032010627063
Epoch: 11 Idx: 0 Loss: 0.014770053415152726
Epoch: 11 Idx: 5000 Loss: 0.030852608837622628
Epoch: 12 Idx: 0 Loss: 0.02855386078345381
Epoch: 12 Idx: 5000 Loss: 0.01998942664991757
Epoch: 13 Idx: 0 Loss: 0.026743955139439043
Epoch: 13 Idx: 5000 Loss: 0.020073554839612477
Epoch: 14 Idx: 0 Loss: 0.032425314886715954
Epoch: 14 Idx: 5000 Loss: 0.02483176585540799
Epoch: 15 Idx: 0 Loss: 0.020607966984068596
Epoch: 15 Idx: 5000 Loss: 0.016641930983595878
Epoch: 16 Idx: 0 Loss: 0.018347300668701137
Epoch: 16 Idx: 5000 Loss: 0.036044840995714325
Epoch: 17 Idx: 0 Loss: 0.018804307978882204
Epoch: 17 Idx: 5000 Loss: 0.0337589698902894
Epoch: 18 Idx: 0 Loss: 0.017152428913633243
Epoch: 18 Idx: 5000 Loss: 0.012475549505304868
Epoch: 19 Idx: 0 Loss: 0.013195059388858624
Epoch: 19 Idx: 5000 Loss: 0.019887398939295753
Epoch: 20 Idx: 0 Loss: 0.01606343771007642
Epoch: 20 Idx: 5000 Loss: 0.008071207262982664
Epoch: 21 Idx: 0 Loss: 0.03318605718857005
Epoch: 21 Idx: 5000 Loss: 0.024627412120202745
Epoch: 22 Idx: 0 Loss: 0.014597825138713843
Epoch: 22 Idx: 5000 Loss: 0.043346497466284994
Epoch: 23 Idx: 0 Loss: 0.017664863157588112
Epoch: 23 Idx: 5000 Loss: 0.030065264279176808
Epoch: 24 Idx: 0 Loss: 0.02227838342994534
Epoch: 24 Idx: 5000 Loss: 0.01587847642410473
Epoch: 25 Idx: 0 Loss: 0.014023220180283841
Epoch: 25 Idx: 5000 Loss: 0.01751193692174882
Epoch: 26 Idx: 0 Loss: 0.023368049724032145
Epoch: 26 Idx: 5000 Loss: 0.017324193549987828
Epoch: 27 Idx: 0 Loss: 0.036046565117089216
Epoch: 27 Idx: 5000 Loss: 0.029255128232031254
Epoch: 28 Idx: 0 Loss: 0.025553423106477673
Epoch: 28 Idx: 5000 Loss: 0.015135739040779646
Epoch: 29 Idx: 0 Loss: 0.01878494583392127
Epoch: 29 Idx: 5000 Loss: 0.02165884704875262
Epoch: 30 Idx: 0 Loss: 0.017717543498599345
Epoch: 30 Idx: 5000 Loss: 0.019211051936138562
Epoch: 31 Idx: 0 Loss: 0.03650637316481782
Epoch: 31 Idx: 5000 Loss: 0.01909156376719108
Epoch: 32 Idx: 0 Loss: 0.018953819148941068
Epoch: 32 Idx: 5000 Loss: 0.015482406698997146
Epoch: 33 Idx: 0 Loss: 0.020753822844588675
Epoch: 33 Idx: 5000 Loss: 0.02380144182860631
Epoch: 34 Idx: 0 Loss: 0.015709749210048203
Epoch: 34 Idx: 5000 Loss: 0.014727189889356979
Epoch: 35 Idx: 0 Loss: 0.026447439160996428
Epoch: 35 Idx: 5000 Loss: 0.019103813542813534
Epoch: 36 Idx: 0 Loss: 0.019923483145497634
Epoch: 36 Idx: 5000 Loss: 0.008317115777887341
Epoch: 37 Idx: 0 Loss: 0.023226067781935276
Epoch: 37 Idx: 5000 Loss: 0.01850469102909276
Epoch: 38 Idx: 0 Loss: 0.0328798949872402
Epoch: 38 Idx: 5000 Loss: 0.015382531468841875
Epoch: 39 Idx: 0 Loss: 0.025739704633679434
Epoch: 39 Idx: 5000 Loss: 0.05201075668924575
Epoch: 40 Idx: 0 Loss: 0.019596693519643126
Epoch: 40 Idx: 5000 Loss: 0.020591711629109784
Epoch: 41 Idx: 0 Loss: 0.011625247680405492
Epoch: 41 Idx: 5000 Loss: 0.022699628446990767
Epoch: 42 Idx: 0 Loss: 0.023031272743351643
Epoch: 42 Idx: 5000 Loss: 0.01578712800880016
Epoch: 43 Idx: 0 Loss: 0.022348407681260687
Epoch: 43 Idx: 5000 Loss: 0.016152306613053755
Epoch: 44 Idx: 0 Loss: 0.015472037651671398
Epoch: 44 Idx: 5000 Loss: 0.01864772650612071
Epoch: 45 Idx: 0 Loss: 0.02522560595575478
Epoch: 45 Idx: 5000 Loss: 0.031544080472962796
Epoch: 46 Idx: 0 Loss: 0.023863984356931645
Epoch: 46 Idx: 5000 Loss: 0.013439680639785628
Epoch: 47 Idx: 0 Loss: 0.018714412195896698
Epoch: 47 Idx: 5000 Loss: 0.03909183790019031
Epoch: 48 Idx: 0 Loss: 0.02923120365321817
Epoch: 48 Idx: 5000 Loss: 0.02145982139102799
Epoch: 49 Idx: 0 Loss: 0.01702177876159371
Epoch: 49 Idx: 5000 Loss: 0.03020229685921101
Len (direct inputs):  105
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.20252064526478564
Epoch: 0 Idx: 5000 Loss: 0.040806983583818686
Epoch: 1 Idx: 0 Loss: 0.028962145022895916
Epoch: 1 Idx: 5000 Loss: 0.013440128020377008
Epoch: 2 Idx: 0 Loss: 0.028001340047703334
Epoch: 2 Idx: 5000 Loss: 0.018028355377313662
Epoch: 3 Idx: 0 Loss: 0.011993894033785275
Epoch: 3 Idx: 5000 Loss: 0.014159705031499943
Epoch: 4 Idx: 0 Loss: 0.06629001628559067
Epoch: 4 Idx: 5000 Loss: 0.018740899720231213
Epoch: 5 Idx: 0 Loss: 0.02386682156607558
Epoch: 5 Idx: 5000 Loss: 0.019472304644479438
Epoch: 6 Idx: 0 Loss: 0.01597683519199495
Epoch: 6 Idx: 5000 Loss: 0.025570536004575994
Epoch: 7 Idx: 0 Loss: 0.017685419663647506
Epoch: 7 Idx: 5000 Loss: 0.019700206752487546
Epoch: 8 Idx: 0 Loss: 0.015609260703855429
Epoch: 8 Idx: 5000 Loss: 0.021320955519400976
Epoch: 9 Idx: 0 Loss: 0.014029270395447298
Epoch: 9 Idx: 5000 Loss: 0.015940484302816457
Epoch: 10 Idx: 0 Loss: 0.05065731986035253
Epoch: 10 Idx: 5000 Loss: 0.020232983232365886
Epoch: 11 Idx: 0 Loss: 0.015651685209669274
Epoch: 11 Idx: 5000 Loss: 0.023849422864298787
Epoch: 12 Idx: 0 Loss: 0.019255349563562202
Epoch: 12 Idx: 5000 Loss: 0.013759102648520637
Epoch: 13 Idx: 0 Loss: 0.0191970882603746
Epoch: 13 Idx: 5000 Loss: 0.015031710193100914
Epoch: 14 Idx: 0 Loss: 0.021867540001883824
Epoch: 14 Idx: 5000 Loss: 0.02853870686930461
Epoch: 15 Idx: 0 Loss: 0.018596131245375472
Epoch: 15 Idx: 5000 Loss: 0.02525512578324037
Epoch: 16 Idx: 0 Loss: 0.022611397772592315
Epoch: 16 Idx: 5000 Loss: 0.014089916444987494
Epoch: 17 Idx: 0 Loss: 0.007518392533678036
Epoch: 17 Idx: 5000 Loss: 0.021112255042302647
Epoch: 18 Idx: 0 Loss: 0.02503838771087559
Epoch: 18 Idx: 5000 Loss: 0.0074071341591105035
Epoch: 19 Idx: 0 Loss: 0.019278843898736006
Epoch: 19 Idx: 5000 Loss: 0.02568697421558557
Epoch: 20 Idx: 0 Loss: 0.03290079362517055
Epoch: 20 Idx: 5000 Loss: 0.019011660815797297
Epoch: 21 Idx: 0 Loss: 0.014602041450820056
Epoch: 21 Idx: 5000 Loss: 0.026996834782568972
Epoch: 22 Idx: 0 Loss: 0.014492065770836449
Epoch: 22 Idx: 5000 Loss: 0.018476467594929165
Epoch: 23 Idx: 0 Loss: 0.018778321058754766
Epoch: 23 Idx: 5000 Loss: 0.021041164639886097
Epoch: 24 Idx: 0 Loss: 0.02219324291628595
Epoch: 24 Idx: 5000 Loss: 0.02717695132485729
Epoch: 25 Idx: 0 Loss: 0.02781669031948661
Epoch: 25 Idx: 5000 Loss: 0.024456419068292136
Epoch: 26 Idx: 0 Loss: 0.013448008796448722
Epoch: 26 Idx: 5000 Loss: 0.01856046646048707
Epoch: 27 Idx: 0 Loss: 0.04286045501017274
Epoch: 27 Idx: 5000 Loss: 0.03024673021036897
Epoch: 28 Idx: 0 Loss: 0.02314358469048544
Epoch: 28 Idx: 5000 Loss: 0.0167430910425128
Epoch: 29 Idx: 0 Loss: 0.017664768006838222
Epoch: 29 Idx: 5000 Loss: 0.013421005932240554
Epoch: 30 Idx: 0 Loss: 0.018432081554007714
Epoch: 30 Idx: 5000 Loss: 0.015528875486196301
Epoch: 31 Idx: 0 Loss: 0.027387791708563425
Epoch: 31 Idx: 5000 Loss: 0.027392497787388205
Epoch: 32 Idx: 0 Loss: 0.02699772345290817
Epoch: 32 Idx: 5000 Loss: 0.012889185720174886
Epoch: 33 Idx: 0 Loss: 0.009548286219129287
Epoch: 33 Idx: 5000 Loss: 0.024937475099117454
Epoch: 34 Idx: 0 Loss: 0.027272902846497433
Epoch: 34 Idx: 5000 Loss: 0.015077029405763678
Epoch: 35 Idx: 0 Loss: 0.011732516908681349
Epoch: 35 Idx: 5000 Loss: 0.009287476054884489
Epoch: 36 Idx: 0 Loss: 0.01551004557666098
Epoch: 36 Idx: 5000 Loss: 0.020860128150889203
Epoch: 37 Idx: 0 Loss: 0.011918653606249382
Epoch: 37 Idx: 5000 Loss: 0.021400720730902996
Epoch: 38 Idx: 0 Loss: 0.015643882373717503
Epoch: 38 Idx: 5000 Loss: 0.014292771609972912
Epoch: 39 Idx: 0 Loss: 0.023131419209664803
Epoch: 39 Idx: 5000 Loss: 0.032692975870359145
Epoch: 40 Idx: 0 Loss: 0.022127695042007164
Epoch: 40 Idx: 5000 Loss: 0.028469254711080725
Epoch: 41 Idx: 0 Loss: 0.019873354860509385
Epoch: 41 Idx: 5000 Loss: 0.02441788238422217
Epoch: 42 Idx: 0 Loss: 0.03560027362305828
Epoch: 42 Idx: 5000 Loss: 0.02676247591737669
Epoch: 43 Idx: 0 Loss: 0.01069470810161296
Epoch: 43 Idx: 5000 Loss: 0.02226614141499135
Epoch: 44 Idx: 0 Loss: 0.024767079938536156
Epoch: 44 Idx: 5000 Loss: 0.03203676836989536
Epoch: 45 Idx: 0 Loss: 0.043152039137718194
Epoch: 45 Idx: 5000 Loss: 0.013797058125918265
Epoch: 46 Idx: 0 Loss: 0.01773416154371934
Epoch: 46 Idx: 5000 Loss: 0.025025054248368272
Epoch: 47 Idx: 0 Loss: 0.03274233042474744
Epoch: 47 Idx: 5000 Loss: 0.016098060944728168
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
on by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by 0.80.0.825915882609335
Parameter containing:
tensor([0.8259], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.2621458595867471
Epoch: 0 Idx: 5000 Loss: 0.03010287399164318
Epoch: 1 Idx: 0 Loss: 0.028921182502296718
Epoch: 1 Idx: 5000 Loss: 0.017839535931848612
Epoch: 2 Idx: 0 Loss: 0.014193128106039374
Epoch: 2 Idx: 5000 Loss: 0.02169117026465036
Epoch: 3 Idx: 0 Loss: 0.032578623002091744
Epoch: 3 Idx: 5000 Loss: 0.02049347083431733
Epoch: 4 Idx: 0 Loss: 0.02254285919117455
Epoch: 4 Idx: 5000 Loss: 0.014838831717002295
Epoch: 5 Idx: 0 Loss: 0.01591377702698822
Epoch: 5 Idx: 5000 Loss: 0.018099022061268948
Epoch: 6 Idx: 0 Loss: 0.017068130437249154
Epoch: 6 Idx: 5000 Loss: 0.03791763558582752
Epoch: 7 Idx: 0 Loss: 0.022736938626529174
Epoch: 7 Idx: 5000 Loss: 0.009491275300832418
Epoch: 8 Idx: 0 Loss: 0.04636689583591721
Epoch: 8 Idx: 5000 Loss: 0.0249442071352824
Epoch: 9 Idx: 0 Loss: 0.02497262507529545
Epoch: 9 Idx: 5000 Loss: 0.022870273207794764
Epoch: 10 Idx: 0 Loss: 0.025000994335356994
Epoch: 10 Idx: 5000 Loss: 0.03231368041582974
Epoch: 11 Idx: 0 Loss: 0.01726617432526452
Epoch: 11 Idx: 5000 Loss: 0.00802686852471426
Epoch: 12 Idx: 0 Loss: 0.02116109680652127
Epoch: 12 Idx: 5000 Loss: 0.021779845537929686
Epoch: 13 Idx: 0 Loss: 0.0294763888054649
Epoch: 13 Idx: 5000 Loss: 0.011327267124861356
Epoch: 14 Idx: 0 Loss: 0.010710069976322976
Epoch: 14 Idx: 5000 Loss: 0.02185469300112116
Epoch: 15 Idx: 0 Loss: 0.022276847445993564
Epoch: 15 Idx: 5000 Loss: 0.016353616178126897
Epoch: 16 Idx: 0 Loss: 0.02428501891840494
Epoch: 16 Idx: 5000 Loss: 0.027714497475764748
Epoch: 17 Idx: 0 Loss: 0.03119770706553234
Epoch: 17 Idx: 5000 Loss: 0.023951873984755092
Epoch: 18 Idx: 0 Loss: 0.03127359928994491
Epoch: 18 Idx: 5000 Loss: 0.014879140706293944
Epoch: 19 Idx: 0 Loss: 0.0141134725816583
Epoch: 19 Idx: 5000 Loss: 0.02014350313128004
Epoch: 20 Idx: 0 Loss: 0.012828279500714256
Epoch: 20 Idx: 5000 Loss: 0.021591636436244704
Epoch: 21 Idx: 0 Loss: 0.01612633778483872
Epoch: 21 Idx: 5000 Loss: 0.020706232571475802
Epoch: 22 Idx: 0 Loss: 0.02366349920596711
Epoch: 22 Idx: 5000 Loss: 0.016501214845953384
Epoch: 23 Idx: 0 Loss: 0.00943974851444121
Epoch: 23 Idx: 5000 Loss: 0.019731966036948895
Epoch: 24 Idx: 0 Loss: 0.02037701704421139
Epoch: 24 Idx: 5000 Loss: 0.012504362780260875
Epoch: 25 Idx: 0 Loss: 0.022803779002438896
Epoch: 25 Idx: 5000 Loss: 0.01693211355481221
Epoch: 26 Idx: 0 Loss: 0.02796914671198183
Epoch: 26 Idx: 5000 Loss: 0.015600843854783848
Epoch: 27 Idx: 0 Loss: 0.026913480394431775
Epoch: 27 Idx: 5000 Loss: 0.023090766217628778
Epoch: 28 Idx: 0 Loss: 0.035781341955534306
Epoch: 28 Idx: 5000 Loss: 0.012630719441038284
Epoch: 29 Idx: 0 Loss: 0.021285965156387924
Epoch: 29 Idx: 5000 Loss: 0.02493690432848525
Epoch: 30 Idx: 0 Loss: 0.031768576437931156
Epoch: 30 Idx: 5000 Loss: 0.02488966251291281
Epoch: 31 Idx: 0 Loss: 0.013361592790008838
Epoch: 31 Idx: 5000 Loss: 0.04107251204489261
Epoch: 32 Idx: 0 Loss: 0.021690608315633735
Epoch: 32 Idx: 5000 Loss: 0.049428296702863216
Epoch: 33 Idx: 0 Loss: 0.017572542403323302
Epoch: 33 Idx: 5000 Loss: 0.03333388381619898
Epoch: 34 Idx: 0 Loss: 0.03868023320584121
Epoch: 34 Idx: 5000 Loss: 0.019664810358592473
Epoch: 35 Idx: 0 Loss: 0.027820930064730305
Epoch: 35 Idx: 5000 Loss: 0.03125539893007459
Epoch: 36 Idx: 0 Loss: 0.021644009284766865
Epoch: 36 Idx: 5000 Loss: 0.01628854356094228
Epoch: 37 Idx: 0 Loss: 0.025402284772069215
Epoch: 37 Idx: 5000 Loss: 0.017717394291266342
Epoch: 38 Idx: 0 Loss: 0.024196817226070148
Epoch: 38 Idx: 5000 Loss: 0.02561643866095612
Epoch: 39 Idx: 0 Loss: 0.01284456478757644
Epoch: 39 Idx: 5000 Loss: 0.030528244242865637
Epoch: 40 Idx: 0 Loss: 0.021405366888299172
Epoch: 40 Idx: 5000 Loss: 0.016900527130222755
Epoch: 41 Idx: 0 Loss: 0.019065849778409017
Epoch: 41 Idx: 5000 Loss: 0.05406934383812702
Epoch: 42 Idx: 0 Loss: 0.017360623224074424
Epoch: 42 Idx: 5000 Loss: 0.01699986865533271
Epoch: 43 Idx: 0 Loss: 0.04143607245395578
Epoch: 43 Idx: 5000 Loss: 0.02778343745511771
Epoch: 44 Idx: 0 Loss: 0.03553198557528402
Epoch: 44 Idx: 5000 Loss: 0.021101708563169057
Epoch: 45 Idx: 0 Loss: 0.027952231043480213
Epoch: 45 Idx: 5000 Loss: 0.01417987713475953
Epoch: 46 Idx: 0 Loss: 0.027229376298240113
Epoch: 46 Idx: 5000 Loss: 0.018115383632106012
Epoch: 47 Idx: 0 Loss: 0.02541305155243939
Epoch: 47 Idx: 5000 Loss: 0.016237533443996432
Epoch: 48 Idx: 0 Loss: 0.011415236856338745
Epoch: 48 Idx: 5000 Loss: 0.02484565807728089
Epoch: 49 Idx: 0 Loss: 0.02783719621976915
Epoch: 49 Idx: 5000 Loss: 0.023227584158087317
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 476, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 24 failed with Disk quota exceeded
ed
led with Disk quota exceeded
---------------------------
Sender: LSF System <rer@dccxc238>
Subject: Job 3289834: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs_wtpath1_3.pkl Models/anatomy_aml_bagofnbrs_wtpath1_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs_wtpath1_3.pkl Models/anatomy_aml_bagofnbrs_wtpath1_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc238>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 20:29:06 2020
Results reported at Tue Sep  1 20:29:06 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs_wtpath1_3.pkl Models/anatomy_aml_bagofnbrs_wtpath1_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23648.03 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2723.06 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23719 sec.
    Turnaround time :                            23720 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc248>
Subject: Job 3289836: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs_wtpath2_3.pkl Models/anatomy_aml_bagofnbrs_wtpath2_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs_wtpath2_3.pkl Models/anatomy_aml_bagofnbrs_wtpath2_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc248>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 20:52:22 2020
Results reported at Tue Sep  1 20:52:22 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs_wtpath2_3.pkl Models/anatomy_aml_bagofnbrs_wtpath2_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25065.15 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2714.47 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25115 sec.
    Turnaround time :                            25116 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc236>
Subject: Job 3289838: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs_wtpath3_3.pkl Models/anatomy_aml_bagofnbrs_wtpath3_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs_wtpath3_3.pkl Models/anatomy_aml_bagofnbrs_wtpath3_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc236>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 21:02:16 2020
Results reported at Tue Sep  1 21:02:16 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs_wtpath3_3.pkl Models/anatomy_aml_bagofnbrs_wtpath3_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25694.00 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2725.08 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25709 sec.
    Turnaround time :                            25710 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc209>
Subject: Job 3289840: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs_wtpath4_3.pkl Models/anatomy_aml_bagofnbrs_wtpath4_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs_wtpath4_3.pkl Models/anatomy_aml_bagofnbrs_wtpath4_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc209>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 21:26:51 2020
Results reported at Tue Sep  1 21:26:51 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs_wtpath4_3.pkl Models/anatomy_aml_bagofnbrs_wtpath4_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27176.52 sec.
    Max Memory :                                 2934 MB
    Average Memory :                             2727.51 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40483.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27183 sec.
    Turnaround time :                            27184 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc245>
Subject: Job 3289842: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs_wtpath5_3.pkl Models/anatomy_aml_bagofnbrs_wtpath5_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs_wtpath5_3.pkl Models/anatomy_aml_bagofnbrs_wtpath5_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc245>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 21:33:43 2020
Results reported at Tue Sep  1 21:33:43 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs_wtpath5_3.pkl Models/anatomy_aml_bagofnbrs_wtpath5_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27510.31 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2726.59 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27608 sec.
    Turnaround time :                            27596 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc224>
Subject: Job 3289846: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs_wtpath8_3.pkl Models/anatomy_aml_bagofnbrs_wtpath8_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs_wtpath8_3.pkl Models/anatomy_aml_bagofnbrs_wtpath8_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc224>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 22:16:36 2020
Results reported at Tue Sep  1 22:16:36 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs_wtpath8_3.pkl Models/anatomy_aml_bagofnbrs_wtpath8_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30162.92 sec.
    Max Memory :                                 2929 MB
    Average Memory :                             2725.78 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40488.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30168 sec.
    Turnaround time :                            30169 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc216>
Subject: Job 3289870: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 3 Output/test_anatomy_aml_bagofnbrs_wtpath80_3.pkl Models/anatomy_aml_bagofnbrs_wtpath80_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 3 Output/test_anatomy_aml_bagofnbrs_wtpath80_3.pkl Models/anatomy_aml_bagofnbrs_wtpath80_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:49 2020
Job was executed on host(s) <dccxc216>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 3 Output/test_anatomy_aml_bagofnbrs_wtpath80_3.pkl Models/anatomy_aml_bagofnbrs_wtpath80_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80640.43 sec.
    Max Memory :                                 2726 MB
    Average Memory :                             2650.71 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40691.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80643 sec.
    Turnaround time :                            80700 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc231>
Subject: Job 3289872: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 3 Output/test_anatomy_aml_bagofnbrs_wtpath152_3.pkl Models/anatomy_aml_bagofnbrs_wtpath152_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 3 Output/test_anatomy_aml_bagofnbrs_wtpath152_3.pkl Models/anatomy_aml_bagofnbrs_wtpath152_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:49 2020
Job was executed on host(s) <dccxc231>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 3 Output/test_anatomy_aml_bagofnbrs_wtpath152_3.pkl Models/anatomy_aml_bagofnbrs_wtpath152_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80641.52 sec.
    Max Memory :                                 2725 MB
    Average Memory :                             2642.68 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40692.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80668 sec.
    Turnaround time :                            80700 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc227>
Subject: Job 3289868: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 3 Output/test_anatomy_aml_bagofnbrs_wtpath40_3.pkl Models/anatomy_aml_bagofnbrs_wtpath40_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 3 Output/test_anatomy_aml_bagofnbrs_wtpath40_3.pkl Models/anatomy_aml_bagofnbrs_wtpath40_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
Job was executed on host(s) <dccxc227>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:54:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 3 Output/test_anatomy_aml_bagofnbrs_wtpath40_3.pkl Models/anatomy_aml_bagofnbrs_wtpath40_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   80495.02 sec.
    Max Memory :                                 2741 MB
    Average Memory :                             2690.85 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40676.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   80668 sec.
    Turnaround time :                            80701 sec.

The output (if any) is above this job summary.

