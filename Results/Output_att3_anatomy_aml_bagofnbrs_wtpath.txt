Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.23878544540866664
Epoch: 0 Idx: 5000 Loss: 0.03206202710870101
Epoch: 1 Idx: 0 Loss: 0.03840293756639006
Epoch: 1 Idx: 5000 Loss: 0.05365854537352463
Epoch: 2 Idx: 0 Loss: 0.021636900476161826
Epoch: 2 Idx: 5000 Loss: 0.03098422601754382
Epoch: 3 Idx: 0 Loss: 0.03903707343437331
Epoch: 3 Idx: 5000 Loss: 0.017259222722169853
Epoch: 4 Idx: 0 Loss: 0.0138614140357499
Epoch: 4 Idx: 5000 Loss: 0.012606131644449486
Epoch: 5 Idx: 0 Loss: 0.01464950357369916
Epoch: 5 Idx: 5000 Loss: 0.00957253706016603
Epoch: 6 Idx: 0 Loss: 0.031535729418091955
Epoch: 6 Idx: 5000 Loss: 0.030708515408850695
Epoch: 7 Idx: 0 Loss: 0.031863245181433957
Epoch: 7 Idx: 5000 Loss: 0.009977237644358646
Epoch: 8 Idx: 0 Loss: 0.021889489464992556
Epoch: 8 Idx: 5000 Loss: 0.031703972632879676
Epoch: 9 Idx: 0 Loss: 0.014007488399220667
Epoch: 9 Idx: 5000 Loss: 0.028290633748938844
Epoch: 10 Idx: 0 Loss: 0.008297127532390081
Epoch: 10 Idx: 5000 Loss: 0.02776459238247266
Epoch: 11 Idx: 0 Loss: 0.019405874041186055
Epoch: 11 Idx: 5000 Loss: 0.025829467886495008
Epoch: 12 Idx: 0 Loss: 0.014853996057538206
Epoch: 12 Idx: 5000 Loss: 0.03457198198582854
Epoch: 13 Idx: 0 Loss: 0.026218695395603446
Epoch: 13 Idx: 5000 Loss: 0.022795179338566517
Epoch: 14 Idx: 0 Loss: 0.05356619364716246
Epoch: 14 Idx: 5000 Loss: 0.019128848711103097
Epoch: 15 Idx: 0 Loss: 0.020409288968562263
Epoch: 15 Idx: 5000 Loss: 0.015898114791798274
Epoch: 16 Idx: 0 Loss: 0.015331118565207384
Epoch: 16 Idx: 5000 Loss: 0.014590375109332856
Epoch: 17 Idx: 0 Loss: 0.016296173226106263
Epoch: 17 Idx: 5000 Loss: 0.028196431768154846
Epoch: 18 Idx: 0 Loss: 0.017489820137048945
Epoch: 18 Idx: 5000 Loss: 0.05684690890734807
Epoch: 19 Idx: 0 Loss: 0.01898408654290516
Epoch: 19 Idx: 5000 Loss: 0.021389668934145345
Epoch: 20 Idx: 0 Loss: 0.03291893545891511
Epoch: 20 Idx: 5000 Loss: 0.040707167926536614
Epoch: 21 Idx: 0 Loss: 0.02229417250133372
Epoch: 21 Idx: 5000 Loss: 0.017267571919762727
Epoch: 22 Idx: 0 Loss: 0.025004832224665645
Epoch: 22 Idx: 5000 Loss: 0.025496611069013887
Epoch: 23 Idx: 0 Loss: 0.025087070911679374
Epoch: 23 Idx: 5000 Loss: 0.01034982619113976
Epoch: 24 Idx: 0 Loss: 0.030686495367287583
Epoch: 24 Idx: 5000 Loss: 0.01704847277531213
Epoch: 25 Idx: 0 Loss: 0.022614305094514
Epoch: 25 Idx: 5000 Loss: 0.022347201911626237
Epoch: 26 Idx: 0 Loss: 0.01207744293876435
Epoch: 26 Idx: 5000 Loss: 0.015878627824337438
Epoch: 27 Idx: 0 Loss: 0.013128398589991527
Epoch: 27 Idx: 5000 Loss: 0.022698820991066165
Epoch: 28 Idx: 0 Loss: 0.022979763053169764
Epoch: 28 Idx: 5000 Loss: 0.04122088706454895
Epoch: 29 Idx: 0 Loss: 0.027560978359554088
Epoch: 29 Idx: 5000 Loss: 0.01703459765516646
Epoch: 30 Idx: 0 Loss: 0.02132462747669694
Epoch: 30 Idx: 5000 Loss: 0.018829837163785793
Epoch: 31 Idx: 0 Loss: 0.025175891409572604
Epoch: 31 Idx: 5000 Loss: 0.023883646808816064
Epoch: 32 Idx: 0 Loss: 0.011815012292442473
Epoch: 32 Idx: 5000 Loss: 0.012938139742720792
Epoch: 33 Idx: 0 Loss: 0.03197036942630703
Epoch: 33 Idx: 5000 Loss: 0.02639241189536413
Epoch: 34 Idx: 0 Loss: 0.00886320549969237
Epoch: 34 Idx: 5000 Loss: 0.03691840223001138
Epoch: 35 Idx: 0 Loss: 0.01331559728975041
Epoch: 35 Idx: 5000 Loss: 0.015995720817208813
Epoch: 36 Idx: 0 Loss: 0.01620198836093464
Epoch: 36 Idx: 5000 Loss: 0.015977307446964767
Epoch: 37 Idx: 0 Loss: 0.03355571018556296
Epoch: 37 Idx: 5000 Loss: 0.020105568051058686
Epoch: 38 Idx: 0 Loss: 0.022762597058509134
Epoch: 38 Idx: 5000 Loss: 0.01895612852192248
Epoch: 39 Idx: 0 Loss: 0.011176745522962191
Epoch: 39 Idx: 5000 Loss: 0.019428393779507898
Epoch: 40 Idx: 0 Loss: 0.014540040638077945
Epoch: 40 Idx: 5000 Loss: 0.03457766784640191
Epoch: 41 Idx: 0 Loss: 0.012162426903570568
Epoch: 41 Idx: 5000 Loss: 0.016715097338422337
Epoch: 42 Idx: 0 Loss: 0.040882726967291116
Epoch: 42 Idx: 5000 Loss: 0.02632074740988348
Epoch: 43 Idx: 0 Loss: 0.01752051710084308
Epoch: 43 Idx: 5000 Loss: 0.023942464178266412
Epoch: 44 Idx: 0 Loss: 0.03491193010316179
Epoch: 44 Idx: 5000 Loss: 0.014387321506507268
Epoch: 45 Idx: 0 Loss: 0.024612795196972376
Epoch: 45 Idx: 5000 Loss: 0.02427718437660311
Epoch: 46 Idx: 0 Loss: 0.013603678594540387
Epoch: 46 Idx: 5000 Loss: 0.008859055593414754
Epoch: 47 Idx: 0 Loss: 0.02979935431632813
Epoch: 47 Idx: 5000 Loss: 0.026872902999275538
Epoch: 48 Idx: 0 Loss: 0.037750068347955607
Epoch: 48 Idx: 5000 Loss: 0.026427073411593262
Epoch: 49 Idx: 0 Loss: 0.02088215149548424
Epoch: 49 Idx: 5000 Loss: 0.012926102399127209
Len (direct inputs):  94
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division byTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2594233852718045
Epoch: 0 Idx: 5000 Loss: 0.031023387599756144
Epoch: 1 Idx: 0 Loss: 0.03322873300739902
Epoch: 1 Idx: 5000 Loss: 0.050469841385783504
Epoch: 2 Idx: 0 Loss: 0.033098204125244994
Epoch: 2 Idx: 5000 Loss: 0.03202243109884944
Epoch: 3 Idx: 0 Loss: 0.032344883358193924
Epoch: 3 Idx: 5000 Loss: 0.028585647404606635
Epoch: 4 Idx: 0 Loss: 0.014030884490787168
Epoch: 4 Idx: 5000 Loss: 0.015758208592857356
Epoch: 5 Idx: 0 Loss: 0.015198194218508342
Epoch: 5 Idx: 5000 Loss: 0.021445391353922545
Epoch: 6 Idx: 0 Loss: 0.0244562673667816
Epoch: 6 Idx: 5000 Loss: 0.021061558595544976
Epoch: 7 Idx: 0 Loss: 0.010404016015570412
Epoch: 7 Idx: 5000 Loss: 0.04007775454701747
Epoch: 8 Idx: 0 Loss: 0.032531259408041295
Epoch: 8 Idx: 5000 Loss: 0.012404814735602497
Epoch: 9 Idx: 0 Loss: 0.013595173057771562
Epoch: 9 Idx: 5000 Loss: 0.018367354705003232
Epoch: 10 Idx: 0 Loss: 0.016351529927707747
Epoch: 10 Idx: 5000 Loss: 0.011733667612980003
Epoch: 11 Idx: 0 Loss: 0.021544139355657993
Epoch: 11 Idx: 5000 Loss: 0.022323514049166264
Epoch: 12 Idx: 0 Loss: 0.025192045246158166
Epoch: 12 Idx: 5000 Loss: 0.010560240764735263
Epoch: 13 Idx: 0 Loss: 0.02381626319499206
Epoch: 13 Idx: 5000 Loss: 0.01447326805313015
Epoch: 14 Idx: 0 Loss: 0.03816558702778004
Epoch: 14 Idx: 5000 Loss: 0.013682147607963994
Epoch: 15 Idx: 0 Loss: 0.03410960949693801
Epoch: 15 Idx: 5000 Loss: 0.024869724527406982
Epoch: 16 Idx: 0 Loss: 0.021949854090402253
Epoch: 16 Idx: 5000 Loss: 0.026743007447337937
Epoch: 17 Idx: 0 Loss: 0.02023559817576304
Epoch: 17 Idx: 5000 Loss: 0.022364240386756298
Epoch: 18 Idx: 0 Loss: 0.025836575367537425
Epoch: 18 Idx: 5000 Loss: 0.02789237333460701
Epoch: 19 Idx: 0 Loss: 0.012501219310210473
Epoch: 19 Idx: 5000 Loss: 0.020708514657776823
Epoch: 20 Idx: 0 Loss: 0.022542715821895206
Epoch: 20 Idx: 5000 Loss: 0.019150863963852786
Epoch: 21 Idx: 0 Loss: 0.01941537239296207
Epoch: 21 Idx: 5000 Loss: 0.020081866653680294
Epoch: 22 Idx: 0 Loss: 0.010643009446695755
Epoch: 22 Idx: 5000 Loss: 0.016584897232667165
Epoch: 23 Idx: 0 Loss: 0.029667471634724295
Epoch: 23 Idx: 5000 Loss: 0.022149377318391207
Epoch: 24 Idx: 0 Loss: 0.018990679198906797
Epoch: 24 Idx: 5000 Loss: 0.018760518731310922
Epoch: 25 Idx: 0 Loss: 0.01584409141322907
Epoch: 25 Idx: 5000 Loss: 0.03181965889180741
Epoch: 26 Idx: 0 Loss: 0.03491417340994858
Epoch: 26 Idx: 5000 Loss: 0.017078283329124422
Epoch: 27 Idx: 0 Loss: 0.02655281694251767
Epoch: 27 Idx: 5000 Loss: 0.021715467407733037
Epoch: 28 Idx: 0 Loss: 0.03974161304120873
Epoch: 28 Idx: 5000 Loss: 0.02654196827088737
Epoch: 29 Idx: 0 Loss: 0.015263717070272964
Epoch: 29 Idx: 5000 Loss: 0.021631608860688753
Epoch: 30 Idx: 0 Loss: 0.019450289425714487
Epoch: 30 Idx: 5000 Loss: 0.008973618560782898
Epoch: 31 Idx: 0 Loss: 0.023794813057640044
Epoch: 31 Idx: 5000 Loss: 0.02944600331883552
Epoch: 32 Idx: 0 Loss: 0.024315532993187356
Epoch: 32 Idx: 5000 Loss: 0.012957119970431485
Epoch: 33 Idx: 0 Loss: 0.04264773446647266
Epoch: 33 Idx: 5000 Loss: 0.03046339810945036
Epoch: 34 Idx: 0 Loss: 0.03464889885193612
Epoch: 34 Idx: 5000 Loss: 0.01622198521102674
Epoch: 35 Idx: 0 Loss: 0.012880803386128657
Epoch: 35 Idx: 5000 Loss: 0.027917560310934696
Epoch: 36 Idx: 0 Loss: 0.02578561252830399
Epoch: 36 Idx: 5000 Loss: 0.02289126568086466
Epoch: 37 Idx: 0 Loss: 0.01579149965150415
Epoch: 37 Idx: 5000 Loss: 0.04534808998338795
Epoch: 38 Idx: 0 Loss: 0.02047945210693987
Epoch: 38 Idx: 5000 Loss: 0.010628444829615294
Epoch: 39 Idx: 0 Loss: 0.016481931460196515
Epoch: 39 Idx: 5000 Loss: 0.023286606594105815
Epoch: 40 Idx: 0 Loss: 0.04021831726863635
Epoch: 40 Idx: 5000 Loss: 0.03138925873826277
Epoch: 41 Idx: 0 Loss: 0.011637025608702884
Epoch: 41 Idx: 5000 Loss: 0.022968142921993135
Epoch: 42 Idx: 0 Loss: 0.013535266846974849
Epoch: 42 Idx: 5000 Loss: 0.018902210888301317
Epoch: 43 Idx: 0 Loss: 0.030330106754602862
Epoch: 43 Idx: 5000 Loss: 0.02409311800881221
Epoch: 44 Idx: 0 Loss: 0.02719291344848432
Epoch: 44 Idx: 5000 Loss: 0.02052489585289341
Epoch: 45 Idx: 0 Loss: 0.014035166298222865
Epoch: 45 Idx: 5000 Loss: 0.02500487139251063
Epoch: 46 Idx: 0 Loss: 0.03666004520573931
Epoch: 46 Idx: 5000 Loss: 0.02565470948435143
Epoch: 47 Idx: 0 Loss: 0.01127063992199282
Epoch: 47 Idx: 5000 Loss: 0.016253394560151273
Epoch: 48 Idx: 0 Loss: 0.01139189977060618
Epoch: 48 Idx: 5000 Loss: 0.01876464910961837
Epoch: 49 Idx: 0 Loss: 0.02185151725178957
Epoch: 49 Idx: 5000 Loss: 0.020661093805912766
Len (direct inputs):  117
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18772790541617435
Epoch: 0 Idx: 5000 Loss: 0.014813967239445223
Epoch: 1 Idx: 0 Loss: 0.023394894283293742
Epoch: 1 Idx: 5000 Loss: 0.014775839536635032
Epoch: 2 Idx: 0 Loss: 0.028505670969823944
Epoch: 2 Idx: 5000 Loss: 0.032961691124454906
Epoch: 3 Idx: 0 Loss: 0.03797761357937053
Epoch: 3 Idx: 5000 Loss: 0.030617531272031526
Epoch: 4 Idx: 0 Loss: 0.01584384429033223
Epoch: 4 Idx: 5000 Loss: 0.04148838062284131
Epoch: 5 Idx: 0 Loss: 0.01814669921074829
Epoch: 5 Idx: 5000 Loss: 0.024483100878543468
Epoch: 6 Idx: 0 Loss: 0.024328146860671623
Epoch: 6 Idx: 5000 Loss: 0.005986596893065695
Epoch: 7 Idx: 0 Loss: 0.02407949050014027
Epoch: 7 Idx: 5000 Loss: 0.025562745983239064
Epoch: 8 Idx: 0 Loss: 0.03720385031516964
Epoch: 8 Idx: 5000 Loss: 0.0330666794665925
Epoch: 9 Idx: 0 Loss: 0.022681655914153442
Epoch: 9 Idx: 5000 Loss: 0.03865495600828145
Epoch: 10 Idx: 0 Loss: 0.01865965870606598
Epoch: 10 Idx: 5000 Loss: 0.021921045160721235
Epoch: 11 Idx: 0 Loss: 0.017856536874098432
Epoch: 11 Idx: 5000 Loss: 0.018062868927876355
Epoch: 12 Idx: 0 Loss: 0.034122188662850744
Epoch: 12 Idx: 5000 Loss: 0.009420647163450768
Epoch: 13 Idx: 0 Loss: 0.027101232616073417
Epoch: 13 Idx: 5000 Loss: 0.024017708512518533
Epoch: 14 Idx: 0 Loss: 0.02100979226344152
Epoch: 14 Idx: 5000 Loss: 0.0254579549608509
Epoch: 15 Idx: 0 Loss: 0.023912751742040883
Epoch: 15 Idx: 5000 Loss: 0.0119547042553768
Epoch: 16 Idx: 0 Loss: 0.02945453313864868
Epoch: 16 Idx: 5000 Loss: 0.015909333032884495
Epoch: 17 Idx: 0 Loss: 0.026781830750142614
Epoch: 17 Idx: 5000 Loss: 0.022116200218328556
Epoch: 18 Idx: 0 Loss: 0.020293267680004938
Epoch: 18 Idx: 5000 Loss: 0.023271118843389595
Epoch: 19 Idx: 0 Loss: 0.018055944346409385
Epoch: 19 Idx: 5000 Loss: 0.03255605612807549
Epoch: 20 Idx: 0 Loss: 0.028546340547187125
Epoch: 20 Idx: 5000 Loss: 0.026592385150667017
Epoch: 21 Idx: 0 Loss: 0.019811232153945202
Epoch: 21 Idx: 5000 Loss: 0.018618995345692148
Epoch: 22 Idx: 0 Loss: 0.01591123198237832
Epoch: 22 Idx: 5000 Loss: 0.018239549133873587
Epoch: 23 Idx: 0 Loss: 0.01985609023286586
Epoch: 23 Idx: 5000 Loss: 0.018297294383348532
Epoch: 24 Idx: 0 Loss: 0.06081268451463723
Epoch: 24 Idx: 5000 Loss: 0.036307759353223126
Epoch: 25 Idx: 0 Loss: 0.021082261136065473
Epoch: 25 Idx: 5000 Loss: 0.03591619457124
Epoch: 26 Idx: 0 Loss: 0.018833215859007164
Epoch: 26 Idx: 5000 Loss: 0.013704545515936431
Epoch: 27 Idx: 0 Loss: 0.013991806178781844
Epoch: 27 Idx: 5000 Loss: 0.03148395128994529
Epoch: 28 Idx: 0 Loss: 0.03097839563837148
Epoch: 28 Idx: 5000 Loss: 0.008104731867657608
Epoch: 29 Idx: 0 Loss: 0.025109667521075355
Epoch: 29 Idx: 5000 Loss: 0.046742271726956025
Epoch: 30 Idx: 0 Loss: 0.020010704157034275
Epoch: 30 Idx: 5000 Loss: 0.027051049532629504
Epoch: 31 Idx: 0 Loss: 0.013102056460226587
Epoch: 31 Idx: 5000 Loss: 0.030140967751396523
Epoch: 32 Idx: 0 Loss: 0.01330932535564983
Epoch: 32 Idx: 5000 Loss: 0.023652654269151294
Epoch: 33 Idx: 0 Loss: 0.022455267344452752
Epoch: 33 Idx: 5000 Loss: 0.011039920308178266
Epoch: 34 Idx: 0 Loss: 0.019485975021218585
Epoch: 34 Idx: 5000 Loss: 0.0318106482097103
Epoch: 35 Idx: 0 Loss: 0.022279557937323266
Epoch: 35 Idx: 5000 Loss: 0.012777631240902042
Epoch: 36 Idx: 0 Loss: 0.03287399391587571
Epoch: 36 Idx: 5000 Loss: 0.016179551731869014
Epoch: 37 Idx: 0 Loss: 0.024088456565332313
Epoch: 37 Idx: 5000 Loss: 0.027240639762415905
Epoch: 38 Idx: 0 Loss: 0.028104389533731372
Epoch: 38 Idx: 5000 Loss: 0.010434937160485956
Epoch: 39 Idx: 0 Loss: 0.020411750874871015
Epoch: 39 Idx: 5000 Loss: 0.02245726628528434
Epoch: 40 Idx: 0 Loss: 0.03134795982218719
Epoch: 40 Idx: 5000 Loss: 0.017584508485420338
Epoch: 41 Idx: 0 Loss: 0.01659432542250801
Epoch: 41 Idx: 5000 Loss: 0.03460240197191913
Epoch: 42 Idx: 0 Loss: 0.011877302113140913
Epoch: 42 Idx: 5000 Loss: 0.015936452809286385
Epoch: 43 Idx: 0 Loss: 0.007453474775105022
Epoch: 43 Idx: 5000 Loss: 0.008698117137941773
Epoch: 44 Idx: 0 Loss: 0.012340520940418688
Epoch: 44 Idx: 5000 Loss: 0.020076514493073908
Epoch: 45 Idx: 0 Loss: 0.015821131318769718
Epoch: 45 Idx: 5000 Loss: 0.0057646158955862534
Epoch: 46 Idx: 0 Loss: 0.017221749150342174
Epoch: 46 Idx: 5000 Loss: 0.009463049082176309
Epoch: 47 Idx: 0 Loss: 0.020479938165244425
Epoch: 47 Idx: 5000 Loss: 0.014359938756265367
Epoch: 48 Idx: 0 Loss: 0.018746697778380443
Epoch: 48 Idx: 5000 Loss: 0.0330451180176
Epoch: 49 Idx: 0 Loss: 0.0057603691145768025
Epoch: 49 Idx: 5000 Loss: 0.01796483596998324
Len (direct inputs):  90
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
TrainiTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1861340276587193
Epoch: 0 Idx: 5000 Loss: 0.01600980898007127
Epoch: 1 Idx: 0 Loss: 0.016607206286839704
Epoch: 1 Idx: 5000 Loss: 0.03364686456009581
Epoch: 2 Idx: 0 Loss: 0.013272937760323341
Epoch: 2 Idx: 5000 Loss: 0.028342894415514526
Epoch: 3 Idx: 0 Loss: 0.005264456323878685
Epoch: 3 Idx: 5000 Loss: 0.01608164268595246
Epoch: 4 Idx: 0 Loss: 0.01991763592063435
Epoch: 4 Idx: 5000 Loss: 0.010525088517980258
Epoch: 5 Idx: 0 Loss: 0.018796317274243107
Epoch: 5 Idx: 5000 Loss: 0.014902675838155225
Epoch: 6 Idx: 0 Loss: 0.03038694924862962
Epoch: 6 Idx: 5000 Loss: 0.03242881835419256
Epoch: 7 Idx: 0 Loss: 0.027334430088759073
Epoch: 7 Idx: 5000 Loss: 0.021609561882691926
Epoch: 8 Idx: 0 Loss: 0.007961771536873184
Epoch: 8 Idx: 5000 Loss: 0.018807743319986914
Epoch: 9 Idx: 0 Loss: 0.01691193093353622
Epoch: 9 Idx: 5000 Loss: 0.018202520468218485
Epoch: 10 Idx: 0 Loss: 0.015382666366873552
Epoch: 10 Idx: 5000 Loss: 0.023217123581771495
Epoch: 11 Idx: 0 Loss: 0.024350239642680703
Epoch: 11 Idx: 5000 Loss: 0.01952189015104986
Epoch: 12 Idx: 0 Loss: 0.020882920047208924
Epoch: 12 Idx: 5000 Loss: 0.01737693589298178
Epoch: 13 Idx: 0 Loss: 0.013690704632060919
Epoch: 13 Idx: 5000 Loss: 0.021264492306624985
Epoch: 14 Idx: 0 Loss: 0.019137664722346515
Epoch: 14 Idx: 5000 Loss: 0.019963740035156437
Epoch: 15 Idx: 0 Loss: 0.02279505282674204
Epoch: 15 Idx: 5000 Loss: 0.014414256041485447
Epoch: 16 Idx: 0 Loss: 0.025018901751782188
Epoch: 16 Idx: 5000 Loss: 0.015793883832127365
Epoch: 17 Idx: 0 Loss: 0.027332565960437316
Epoch: 17 Idx: 5000 Loss: 0.023909509819103735
Epoch: 18 Idx: 0 Loss: 0.0136149408474111
Epoch: 18 Idx: 5000 Loss: 0.012426752020476408
Epoch: 19 Idx: 0 Loss: 0.03038158036893718
Epoch: 19 Idx: 5000 Loss: 0.02824715618103952
Epoch: 20 Idx: 0 Loss: 0.012266005202778811
Epoch: 20 Idx: 5000 Loss: 0.02351163742818653
Epoch: 21 Idx: 0 Loss: 0.011435511263053763
Epoch: 21 Idx: 5000 Loss: 0.013882235381840432
Epoch: 22 Idx: 0 Loss: 0.032421805011871085
Epoch: 22 Idx: 5000 Loss: 0.023409093985709284
Epoch: 23 Idx: 0 Loss: 0.023443079815922613
Epoch: 23 Idx: 5000 Loss: 0.016876623451332538
Epoch: 24 Idx: 0 Loss: 0.02772973466432897
Epoch: 24 Idx: 5000 Loss: 0.016135237477380982
Epoch: 25 Idx: 0 Loss: 0.016535587760664057
Epoch: 25 Idx: 5000 Loss: 0.016333685893969774
Epoch: 26 Idx: 0 Loss: 0.01868507637023611
Epoch: 26 Idx: 5000 Loss: 0.023858961707949333
Epoch: 27 Idx: 0 Loss: 0.028512382804575034
Epoch: 27 Idx: 5000 Loss: 0.01257372637589593
Epoch: 28 Idx: 0 Loss: 0.020869045701125855
Epoch: 28 Idx: 5000 Loss: 0.024733305669275996
Epoch: 29 Idx: 0 Loss: 0.03180230537465895
Epoch: 29 Idx: 5000 Loss: 0.018027093579217383
Epoch: 30 Idx: 0 Loss: 0.008720269109422245
Epoch: 30 Idx: 5000 Loss: 0.02371204396267297
Epoch: 31 Idx: 0 Loss: 0.010782183909270979
Epoch: 31 Idx: 5000 Loss: 0.019022184563466243
Epoch: 32 Idx: 0 Loss: 0.01337588626209324
Epoch: 32 Idx: 5000 Loss: 0.028936785612252054
Epoch: 33 Idx: 0 Loss: 0.011817328562709572
Epoch: 33 Idx: 5000 Loss: 0.016713282165160756
Epoch: 34 Idx: 0 Loss: 0.01670654067828798
Epoch: 34 Idx: 5000 Loss: 0.02819152116254649
Epoch: 35 Idx: 0 Loss: 0.0316458171730288
Epoch: 35 Idx: 5000 Loss: 0.02070514895182867
Epoch: 36 Idx: 0 Loss: 0.02946131719979094
Epoch: 36 Idx: 5000 Loss: 0.03354761608873734
Epoch: 37 Idx: 0 Loss: 0.03112733842482997
Epoch: 37 Idx: 5000 Loss: 0.032494477611894064
Epoch: 38 Idx: 0 Loss: 0.018738271457648625
Epoch: 38 Idx: 5000 Loss: 0.02821310455722184
Epoch: 39 Idx: 0 Loss: 0.027088756225933465
Epoch: 39 Idx: 5000 Loss: 0.04738207846508564
Epoch: 40 Idx: 0 Loss: 0.01820050853371949
Epoch: 40 Idx: 5000 Loss: 0.010205353901847266
Epoch: 41 Idx: 0 Loss: 0.019837524796230816
Epoch: 41 Idx: 5000 Loss: 0.02348075608900186
Epoch: 42 Idx: 0 Loss: 0.01938410034510541
Epoch: 42 Idx: 5000 Loss: 0.01758076663686451
Epoch: 43 Idx: 0 Loss: 0.01927330165191292
Epoch: 43 Idx: 5000 Loss: 0.03100676820511239
Epoch: 44 Idx: 0 Loss: 0.03765555506627704
Epoch: 44 Idx: 5000 Loss: 0.009498850809782734
Epoch: 45 Idx: 0 Loss: 0.0320791609644703
Epoch: 45 Idx: 5000 Loss: 0.022441694595001685
Epoch: 46 Idx: 0 Loss: 0.022127812539864122
Epoch: 46 Idx: 5000 Loss: 0.029220456014186224
Epoch: 47 Idx: 0 Loss: 0.02687152189549632
Epoch: 47 Idx: 5000 Loss: 0.03314917541555522
Epoch: 48 Idx: 0 Loss: 0.010908053688614163
Epoch: 48 Idx: 5000 Loss: 0.015430359633108551
Epoch: 49 Idx: 0 Loss: 0.015939813821854214
Epoch: 49 Idx: 5000 Loss: 0.012113645327601999
Len (direct inputs):  108
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.17629789852121938
Epoch: 0 Idx: 5000 Loss: 0.02936675069812597
Epoch: 1 Idx: 0 Loss: 0.02246168380837202
Epoch: 1 Idx: 5000 Loss: 0.028832563086319624
Epoch: 2 Idx: 0 Loss: 0.027090867777081095
Epoch: 2 Idx: 5000 Loss: 0.016566244805071045
Epoch: 3 Idx: 0 Loss: 0.013264125264750466
Epoch: 3 Idx: 5000 Loss: 0.011188577133034004
Epoch: 4 Idx: 0 Loss: 0.04087800898415479
Epoch: 4 Idx: 5000 Loss: 0.024783791734452276
Epoch: 5 Idx: 0 Loss: 0.02388663583033861
Epoch: 5 Idx: 5000 Loss: 0.02341684752742225
Epoch: 6 Idx: 0 Loss: 0.03165222099879051
Epoch: 6 Idx: 5000 Loss: 0.013773162340559897
Epoch: 7 Idx: 0 Loss: 0.018026233956768712
Epoch: 7 Idx: 5000 Loss: 0.025515845925793215
Epoch: 8 Idx: 0 Loss: 0.010848739784026565
Epoch: 8 Idx: 5000 Loss: 0.011568617662890048
Epoch: 9 Idx: 0 Loss: 0.01646429598936028
Epoch: 9 Idx: 5000 Loss: 0.025839947508162903
Epoch: 10 Idx: 0 Loss: 0.02069592709575125
Epoch: 10 Idx: 5000 Loss: 0.024840032010627063
Epoch: 11 Idx: 0 Loss: 0.014770053415152726
Epoch: 11 Idx: 5000 Loss: 0.030852608837622628
Epoch: 12 Idx: 0 Loss: 0.02855386078345381
Epoch: 12 Idx: 5000 Loss: 0.01998942664991757
Epoch: 13 Idx: 0 Loss: 0.026743955139439043
Epoch: 13 Idx: 5000 Loss: 0.020073554839612477
Epoch: 14 Idx: 0 Loss: 0.032425314886715954
Epoch: 14 Idx: 5000 Loss: 0.02483176585540799
Epoch: 15 Idx: 0 Loss: 0.020607966984068596
Epoch: 15 Idx: 5000 Loss: 0.016641930983595878
Epoch: 16 Idx: 0 Loss: 0.018347300668701137
Epoch: 16 Idx: 5000 Loss: 0.036044840995714325
Epoch: 17 Idx: 0 Loss: 0.018804307978882204
Epoch: 17 Idx: 5000 Loss: 0.0337589698902894
Epoch: 18 Idx: 0 Loss: 0.017152428913633243
Epoch: 18 Idx: 5000 Loss: 0.012475549505304868
Epoch: 19 Idx: 0 Loss: 0.013195059388858624
Epoch: 19 Idx: 5000 Loss: 0.019887398939295753
Epoch: 20 Idx: 0 Loss: 0.01606343771007642
Epoch: 20 Idx: 5000 Loss: 0.008071207262982664
Epoch: 21 Idx: 0 Loss: 0.03318605718857005
Epoch: 21 Idx: 5000 Loss: 0.024627412120202745
Epoch: 22 Idx: 0 Loss: 0.014597825138713843
Epoch: 22 Idx: 5000 Loss: 0.043346497466284994
Epoch: 23 Idx: 0 Loss: 0.017664863157588112
Epoch: 23 Idx: 5000 Loss: 0.030065264279176808
Epoch: 24 Idx: 0 Loss: 0.02227838342994534
Epoch: 24 Idx: 5000 Loss: 0.01587847642410473
Epoch: 25 Idx: 0 Loss: 0.014023220180283841
Epoch: 25 Idx: 5000 Loss: 0.01751193692174882
Epoch: 26 Idx: 0 Loss: 0.023368049724032145
Epoch: 26 Idx: 5000 Loss: 0.017324193549987828
Epoch: 27 Idx: 0 Loss: 0.036046565117089216
Epoch: 27 Idx: 5000 Loss: 0.029255128232031254
Epoch: 28 Idx: 0 Loss: 0.025553423106477673
Epoch: 28 Idx: 5000 Loss: 0.015135739040779646
Epoch: 29 Idx: 0 Loss: 0.01878494583392127
Epoch: 29 Idx: 5000 Loss: 0.02165884704875262
Epoch: 30 Idx: 0 Loss: 0.017717543498599345
Epoch: 30 Idx: 5000 Loss: 0.019211051936138562
Epoch: 31 Idx: 0 Loss: 0.03650637316481782
Epoch: 31 Idx: 5000 Loss: 0.01909156376719108
Epoch: 32 Idx: 0 Loss: 0.018953819148941068
Epoch: 32 Idx: 5000 Loss: 0.015482406698997146
Epoch: 33 Idx: 0 Loss: 0.020753822844588675
Epoch: 33 Idx: 5000 Loss: 0.02380144182860631
Epoch: 34 Idx: 0 Loss: 0.015709749210048203
Epoch: 34 Idx: 5000 Loss: 0.014727189889356979
Epoch: 35 Idx: 0 Loss: 0.026447439160996428
Epoch: 35 Idx: 5000 Loss: 0.019103813542813534
Epoch: 36 Idx: 0 Loss: 0.019923483145497634
Epoch: 36 Idx: 5000 Loss: 0.008317115777887341
Epoch: 37 Idx: 0 Loss: 0.023226067781935276
Epoch: 37 Idx: 5000 Loss: 0.01850469102909276
Epoch: 38 Idx: 0 Loss: 0.0328798949872402
Epoch: 38 Idx: 5000 Loss: 0.015382531468841875
Epoch: 39 Idx: 0 Loss: 0.025739704633679434
Epoch: 39 Idx: 5000 Loss: 0.05201075668924575
Epoch: 40 Idx: 0 Loss: 0.019596693519643126
Epoch: 40 Idx: 5000 Loss: 0.020591711629109784
Epoch: 41 Idx: 0 Loss: 0.011625247680405492
Epoch: 41 Idx: 5000 Loss: 0.022699628446990767
Epoch: 42 Idx: 0 Loss: 0.023031272743351643
Epoch: 42 Idx: 5000 Loss: 0.01578712800880016
Epoch: 43 Idx: 0 Loss: 0.022348407681260687
Epoch: 43 Idx: 5000 Loss: 0.016152306613053755
Epoch: 44 Idx: 0 Loss: 0.015472037651671398
Epoch: 44 Idx: 5000 Loss: 0.01864772650612071
Epoch: 45 Idx: 0 Loss: 0.02522560595575478
Epoch: 45 Idx: 5000 Loss: 0.031544080472962796
Epoch: 46 Idx: 0 Loss: 0.023863984356931645
Epoch: 46 Idx: 5000 Loss: 0.013439680639785628
Epoch: 47 Idx: 0 Loss: 0.018714412195896698
Epoch: 47 Idx: 5000 Loss: 0.03909183790019031
Epoch: 48 Idx: 0 Loss: 0.02923120365321817
Epoch: 48 Idx: 5000 Loss: 0.02145982139102799
Epoch: 49 Idx: 0 Loss: 0.01702177876159371
Epoch: 49 Idx: 5000 Loss: 0.03020229685921101
Len (direct inputs):  105
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
ainingTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.1983474853277357
Epoch: 0 Idx: 5000 Loss: 0.026717098379631394
Epoch: 1 Idx: 0 Loss: 0.029042708485658347
Epoch: 1 Idx: 5000 Loss: 0.015814868911588793
Epoch: 2 Idx: 0 Loss: 0.013331768665557641
Epoch: 2 Idx: 5000 Loss: 0.026683738624086954
Epoch: 3 Idx: 0 Loss: 0.03456447588476124
Epoch: 3 Idx: 5000 Loss: 0.016077685985197255
Epoch: 4 Idx: 0 Loss: 0.018939868073727443
Epoch: 4 Idx: 5000 Loss: 0.034953895323220255
Epoch: 5 Idx: 0 Loss: 0.017361987858669686
Epoch: 5 Idx: 5000 Loss: 0.015948387260108877
Epoch: 6 Idx: 0 Loss: 0.02440904906557561
Epoch: 6 Idx: 5000 Loss: 0.035752991121121624
Epoch: 7 Idx: 0 Loss: 0.010850334595491905
Epoch: 7 Idx: 5000 Loss: 0.010902883404691015
Epoch: 8 Idx: 0 Loss: 0.021684186783879046
Epoch: 8 Idx: 5000 Loss: 0.03245556793866558
Epoch: 9 Idx: 0 Loss: 0.02344336252881795
Epoch: 9 Idx: 5000 Loss: 0.026453214533149
Epoch: 10 Idx: 0 Loss: 0.019670291824155764
Epoch: 10 Idx: 5000 Loss: 0.014988436626607758
Epoch: 11 Idx: 0 Loss: 0.021189265897347515
Epoch: 11 Idx: 5000 Loss: 0.018951784245324132
Epoch: 12 Idx: 0 Loss: 0.01702574149905397
Epoch: 12 Idx: 5000 Loss: 0.05391416290708435
Epoch: 13 Idx: 0 Loss: 0.03576162471037156
Epoch: 13 Idx: 5000 Loss: 0.029948187000019017
Epoch: 14 Idx: 0 Loss: 0.017848914755960007
Epoch: 14 Idx: 5000 Loss: 0.037166590993421525
Epoch: 15 Idx: 0 Loss: 0.024551463770383114
Epoch: 15 Idx: 5000 Loss: 0.018882114004031413
Epoch: 16 Idx: 0 Loss: 0.014859310935906846
Epoch: 16 Idx: 5000 Loss: 0.014523866015385179
Epoch: 17 Idx: 0 Loss: 0.02237765233001135
Epoch: 17 Idx: 5000 Loss: 0.016157420227411203
Epoch: 18 Idx: 0 Loss: 0.032968494411097414
Epoch: 18 Idx: 5000 Loss: 0.019116997983582892
Epoch: 19 Idx: 0 Loss: 0.010664759796507555
Epoch: 19 Idx: 5000 Loss: 0.021156077113468924
Epoch: 20 Idx: 0 Loss: 0.02003246737799231
Epoch: 20 Idx: 5000 Loss: 0.01350392897243866
Epoch: 21 Idx: 0 Loss: 0.03248625534968595
Epoch: 21 Idx: 5000 Loss: 0.03225092208742571
Epoch: 22 Idx: 0 Loss: 0.020531170682335433
Epoch: 22 Idx: 5000 Loss: 0.02260001875873272
Epoch: 23 Idx: 0 Loss: 0.03506860389475868
Epoch: 23 Idx: 5000 Loss: 0.0300746021124058
Epoch: 24 Idx: 0 Loss: 0.027175155258735807
Epoch: 24 Idx: 5000 Loss: 0.012400392709710436
Epoch: 25 Idx: 0 Loss: 0.018963148937967914
Epoch: 25 Idx: 5000 Loss: 0.012541414392516995
Epoch: 26 Idx: 0 Loss: 0.028273000162370478
Epoch: 26 Idx: 5000 Loss: 0.012893602343403405
Epoch: 27 Idx: 0 Loss: 0.02218878810926126
Epoch: 27 Idx: 5000 Loss: 0.021834709086014176
Epoch: 28 Idx: 0 Loss: 0.018785323161557305
Epoch: 28 Idx: 5000 Loss: 0.014388473812280098
Epoch: 29 Idx: 0 Loss: 0.03398306901174104
Epoch: 29 Idx: 5000 Loss: 0.011176535607596065
Epoch: 30 Idx: 0 Loss: 0.020124174100437063
Epoch: 30 Idx: 5000 Loss: 0.04002205943436695
Epoch: 31 Idx: 0 Loss: 0.03144955990291572
Epoch: 31 Idx: 5000 Loss: 0.012549501756075526
Epoch: 32 Idx: 0 Loss: 0.026098625394724732
Epoch: 32 Idx: 5000 Loss: 0.02278436300890401
Epoch: 33 Idx: 0 Loss: 0.047214057113670986
Epoch: 33 Idx: 5000 Loss: 0.03681977198074485
Epoch: 34 Idx: 0 Loss: 0.0277820693334979
Epoch: 34 Idx: 5000 Loss: 0.0204718750905745
Epoch: 35 Idx: 0 Loss: 0.018995221147227674
Epoch: 35 Idx: 5000 Loss: 0.015096435435776167
Epoch: 36 Idx: 0 Loss: 0.018937430016401018
Epoch: 36 Idx: 5000 Loss: 0.01926473405773432
Epoch: 37 Idx: 0 Loss: 0.023677694278835003
Epoch: 37 Idx: 5000 Loss: 0.01741308807902648
Epoch: 38 Idx: 0 Loss: 0.024284098918117666
Epoch: 38 Idx: 5000 Loss: 0.024230783145544384
Epoch: 39 Idx: 0 Loss: 0.009247307684652567
Epoch: 39 Idx: 5000 Loss: 0.019905485949997897
Epoch: 40 Idx: 0 Loss: 0.009708747608615724
Epoch: 40 Idx: 5000 Loss: 0.03860832263043679
Epoch: 41 Idx: 0 Loss: 0.02049719659706674
Epoch: 41 Idx: 5000 Loss: 0.03717029088816891
Epoch: 42 Idx: 0 Loss: 0.013757024181650545
Epoch: 42 Idx: 5000 Loss: 0.012521783816792708
Epoch: 43 Idx: 0 Loss: 0.021919747429641638
Epoch: 43 Idx: 5000 Loss: 0.019562560569060854
Epoch: 44 Idx: 0 Loss: 0.02728861128075144
Epoch: 44 Idx: 5000 Loss: 0.027837846992007878
Epoch: 45 Idx: 0 Loss: 0.013264226182755615
Epoch: 45 Idx: 5000 Loss: 0.025827213980382097
Epoch: 46 Idx: 0 Loss: 0.01427763608419169
Epoch: 46 Idx: 5000 Loss: 0.01622678944959326
Epoch: 47 Idx: 0 Loss: 0.020609507112592095
Epoch: 47 Idx: 5000 Loss: 0.022456575717032298
Epoch: 48 Idx: 0 Loss: 0.03196499880767374
Epoch: 48 Idx: 5000 Loss: 0.02644957363695609
Epoch: 49 Idx: 0 Loss: 0.019161901197784346
Epoch: 49 Idx: 5000 Loss: 0.026459127289936143
Len (direct inputs):  107
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by 0.80.0.825915882609335
Parameter containing:
tensor([0.8259], device='cuda:0')
Epoch: 0 Idx: 0 Loss: 0.2621458595867471
Epoch: 0 Idx: 5000 Loss: 0.03010287399164318
Epoch: 1 Idx: 0 Loss: 0.028921182502296718
Epoch: 1 Idx: 5000 Loss: 0.017839535931848612
Epoch: 2 Idx: 0 Loss: 0.014193128106039374
Epoch: 2 Idx: 5000 Loss: 0.02169117026465036
Epoch: 3 Idx: 0 Loss: 0.032578623002091744
Epoch: 3 Idx: 5000 Loss: 0.02049347083431733
Epoch: 4 Idx: 0 Loss: 0.02254285919117455
Epoch: 4 Idx: 5000 Loss: 0.014838831717002295
Epoch: 5 Idx: 0 Loss: 0.01591377702698822
Epoch: 5 Idx: 5000 Loss: 0.018099022061268948
Epoch: 6 Idx: 0 Loss: 0.017068130437249154
Epoch: 6 Idx: 5000 Loss: 0.03791763558582752
Epoch: 7 Idx: 0 Loss: 0.022736938626529174
Epoch: 7 Idx: 5000 Loss: 0.009491275300832418
Epoch: 8 Idx: 0 Loss: 0.04636689583591721
Epoch: 8 Idx: 5000 Loss: 0.0249442071352824
Epoch: 9 Idx: 0 Loss: 0.02497262507529545
Epoch: 9 Idx: 5000 Loss: 0.022870273207794764
Epoch: 10 Idx: 0 Loss: 0.025000994335356994
Epoch: 10 Idx: 5000 Loss: 0.03231368041582974
Epoch: 11 Idx: 0 Loss: 0.01726617432526452
Epoch: 11 Idx: 5000 Loss: 0.00802686852471426
Epoch: 12 Idx: 0 Loss: 0.02116109680652127
Epoch: 12 Idx: 5000 Loss: 0.021779845537929686
Epoch: 13 Idx: 0 Loss: 0.0294763888054649
Epoch: 13 Idx: 5000 Loss: 0.011327267124861356
Epoch: 14 Idx: 0 Loss: 0.010710069976322976
Epoch: 14 Idx: 5000 Loss: 0.02185469300112116
Epoch: 15 Idx: 0 Loss: 0.022276847445993564
Epoch: 15 Idx: 5000 Loss: 0.016353616178126897
Epoch: 16 Idx: 0 Loss: 0.02428501891840494
Epoch: 16 Idx: 5000 Loss: 0.027714497475764748
Epoch: 17 Idx: 0 Loss: 0.03119770706553234
Epoch: 17 Idx: 5000 Loss: 0.023951873984755092
Epoch: 18 Idx: 0 Loss: 0.03127359928994491
Epoch: 18 Idx: 5000 Loss: 0.014879140706293944
Epoch: 19 Idx: 0 Loss: 0.0141134725816583
Epoch: 19 Idx: 5000 Loss: 0.02014350313128004
Epoch: 20 Idx: 0 Loss: 0.012828279500714256
Epoch: 20 Idx: 5000 Loss: 0.021591636436244704
Epoch: 21 Idx: 0 Loss: 0.01612633778483872
Epoch: 21 Idx: 5000 Loss: 0.020706232571475802
Epoch: 22 Idx: 0 Loss: 0.02366349920596711
Epoch: 22 Idx: 5000 Loss: 0.016501214845953384
Epoch: 23 Idx: 0 Loss: 0.00943974851444121
Epoch: 23 Idx: 5000 Loss: 0.019731966036948895
Epoch: 24 Idx: 0 Loss: 0.02037701704421139
Epoch: 24 Idx: 5000 Loss: 0.012504362780260875
Epoch: 25 Idx: 0 Loss: 0.022803779002438896
Epoch: 25 Idx: 5000 Loss: 0.01693211355481221
Epoch: 26 Idx: 0 Loss: 0.02796914671198183
Epoch: 26 Idx: 5000 Loss: 0.015600843854783848
Epoch: 27 Idx: 0 Loss: 0.026913480394431775
Epoch: 27 Idx: 5000 Loss: 0.023090766217628778
Epoch: 28 Idx: 0 Loss: 0.035781341955534306
Epoch: 28 Idx: 5000 Loss: 0.012630719441038284
Epoch: 29 Idx: 0 Loss: 0.021285965156387924
Epoch: 29 Idx: 5000 Loss: 0.02493690432848525
Epoch: 30 Idx: 0 Loss: 0.031768576437931156
Epoch: 30 Idx: 5000 Loss: 0.02488966251291281
Epoch: 31 Idx: 0 Loss: 0.013361592790008838
Epoch: 31 Idx: 5000 Loss: 0.04107251204489261
Epoch: 32 Idx: 0 Loss: 0.021690608315633735
Epoch: 32 Idx: 5000 Loss: 0.049428296702863216
Epoch: 33 Idx: 0 Loss: 0.017572542403323302
Epoch: 33 Idx: 5000 Loss: 0.03333388381619898
Epoch: 34 Idx: 0 Loss: 0.03868023320584121
Epoch: 34 Idx: 5000 Loss: 0.019664810358592473
Epoch: 35 Idx: 0 Loss: 0.027820930064730305
Epoch: 35 Idx: 5000 Loss: 0.03125539893007459
Epoch: 36 Idx: 0 Loss: 0.021644009284766865
Epoch: 36 Idx: 5000 Loss: 0.01628854356094228
Epoch: 37 Idx: 0 Loss: 0.025402284772069215
Epoch: 37 Idx: 5000 Loss: 0.017717394291266342
Epoch: 38 Idx: 0 Loss: 0.024196817226070148
Epoch: 38 Idx: 5000 Loss: 0.02561643866095612
Epoch: 39 Idx: 0 Loss: 0.01284456478757644
Epoch: 39 Idx: 5000 Loss: 0.030528244242865637
Epoch: 40 Idx: 0 Loss: 0.021405366888299172
Epoch: 40 Idx: 5000 Loss: 0.016900527130222755
Epoch: 41 Idx: 0 Loss: 0.019065849778409017
Epoch: 41 Idx: 5000 Loss: 0.05406934383812702
Epoch: 42 Idx: 0 Loss: 0.017360623224074424
Epoch: 42 Idx: 5000 Loss: 0.01699986865533271
Epoch: 43 Idx: 0 Loss: 0.04143607245395578
Epoch: 43 Idx: 5000 Loss: 0.02778343745511771
Epoch: 44 Idx: 0 Loss: 0.03553198557528402
Epoch: 44 Idx: 5000 Loss: 0.021101708563169057
Epoch: 45 Idx: 0 Loss: 0.027952231043480213
Epoch: 45 Idx: 5000 Loss: 0.01417987713475953
Epoch: 46 Idx: 0 Loss: 0.027229376298240113
Epoch: 46 Idx: 5000 Loss: 0.018115383632106012
Epoch: 47 Idx: 0 Loss: 0.02541305155243939
Epoch: 47 Idx: 5000 Loss: 0.016237533443996432
Epoch: 48 Idx: 0 Loss: 0.011415236856338745
Epoch: 48 Idx: 5000 Loss: 0.02484565807728089
Epoch: 49 Idx: 0 Loss: 0.02783719621976915
Epoch: 49 Idx: 5000 Loss: 0.023227584158087317
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 476, in <module>
    torch.save(model.state_dict(), sys.argv[5])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in save
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 149, in _with_file_like
    return body(f)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 224, in <lambda>
    return _with_file_like(f, "wb", lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/serialization.py", line 302, in _save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
RuntimeError: write(): fd 24 failed with Disk quota exceeded
ed
led with Disk quota exceeded
---------------------------
Sender: LSF System <rer@dccxc238>
Subject: Job 3289834: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs_wtpath1_3.pkl Models/anatomy_aml_bagofnbrs_wtpath1_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs_wtpath1_3.pkl Models/anatomy_aml_bagofnbrs_wtpath1_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc238>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 20:29:06 2020
Results reported at Tue Sep  1 20:29:06 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 1 3 Output/test_anatomy_aml_bagofnbrs_wtpath1_3.pkl Models/anatomy_aml_bagofnbrs_wtpath1_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23648.03 sec.
    Max Memory :                                 2927 MB
    Average Memory :                             2723.06 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40490.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   23719 sec.
    Turnaround time :                            23720 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc248>
Subject: Job 3289836: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs_wtpath2_3.pkl Models/anatomy_aml_bagofnbrs_wtpath2_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs_wtpath2_3.pkl Models/anatomy_aml_bagofnbrs_wtpath2_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc248>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 20:52:22 2020
Results reported at Tue Sep  1 20:52:22 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 2 3 Output/test_anatomy_aml_bagofnbrs_wtpath2_3.pkl Models/anatomy_aml_bagofnbrs_wtpath2_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25065.15 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2714.47 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25115 sec.
    Turnaround time :                            25116 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc236>
Subject: Job 3289838: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs_wtpath3_3.pkl Models/anatomy_aml_bagofnbrs_wtpath3_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs_wtpath3_3.pkl Models/anatomy_aml_bagofnbrs_wtpath3_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:46 2020
Job was executed on host(s) <dccxc236>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:47 2020
Terminated at Tue Sep  1 21:02:16 2020
Results reported at Tue Sep  1 21:02:16 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 3 3 Output/test_anatomy_aml_bagofnbrs_wtpath3_3.pkl Models/anatomy_aml_bagofnbrs_wtpath3_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   25694.00 sec.
    Max Memory :                                 2924 MB
    Average Memory :                             2725.08 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40493.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   25709 sec.
    Turnaround time :                            25710 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc209>
Subject: Job 3289840: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs_wtpath4_3.pkl Models/anatomy_aml_bagofnbrs_wtpath4_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs_wtpath4_3.pkl Models/anatomy_aml_bagofnbrs_wtpath4_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc209>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 21:26:51 2020
Results reported at Tue Sep  1 21:26:51 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 4 3 Output/test_anatomy_aml_bagofnbrs_wtpath4_3.pkl Models/anatomy_aml_bagofnbrs_wtpath4_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27176.52 sec.
    Max Memory :                                 2934 MB
    Average Memory :                             2727.51 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40483.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27183 sec.
    Turnaround time :                            27184 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc245>
Subject: Job 3289842: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs_wtpath5_3.pkl Models/anatomy_aml_bagofnbrs_wtpath5_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs_wtpath5_3.pkl Models/anatomy_aml_bagofnbrs_wtpath5_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc245>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 21:33:43 2020
Results reported at Tue Sep  1 21:33:43 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 5 3 Output/test_anatomy_aml_bagofnbrs_wtpath5_3.pkl Models/anatomy_aml_bagofnbrs_wtpath5_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   27510.31 sec.
    Max Memory :                                 2931 MB
    Average Memory :                             2726.59 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   27608 sec.
    Turnaround time :                            27596 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc224>
Subject: Job 3289846: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs_wtpath8_3.pkl Models/anatomy_aml_bagofnbrs_wtpath8_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs_wtpath8_3.pkl Models/anatomy_aml_bagofnbrs_wtpath8_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:47 2020
Job was executed on host(s) <dccxc224>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 13:53:48 2020
Terminated at Tue Sep  1 22:16:36 2020
Results reported at Tue Sep  1 22:16:36 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 8 3 Output/test_anatomy_aml_bagofnbrs_wtpath8_3.pkl Models/anatomy_aml_bagofnbrs_wtpath8_3.pt
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   30162.92 sec.
    Max Memory :                                 2929 MB
    Average Memory :                             2725.78 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40488.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   30168 sec.
    Turnaround time :                            30169 sec.

The output (if any) is above this job summary.

