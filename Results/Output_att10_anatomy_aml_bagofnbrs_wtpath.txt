Max number of nodes in a path: Input/data_anatomy_oaei_bagofnbrs.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.21249641579775486
Epoch: 0 Idx: 5000 Loss: 0.028235152276537584
Epoch: 1 Idx: 0 Loss: 0.04195444472296479
Epoch: 1 Idx: 5000 Loss: 0.04282699361440838
Epoch: 2 Idx: 0 Loss: 0.02962911803206802
Epoch: 2 Idx: 5000 Loss: 0.011442940254217812
Epoch: 3 Idx: 0 Loss: 0.008552164138856786
Epoch: 3 Idx: 5000 Loss: 0.02492117726932452
Epoch: 4 Idx: 0 Loss: 0.01580811190467568
Epoch: 4 Idx: 5000 Loss: 0.01716557113704373
Epoch: 5 Idx: 0 Loss: 0.014441645886841907
Epoch: 5 Idx: 5000 Loss: 0.018741977020926205
Epoch: 6 Idx: 0 Loss: 0.027710158306996552
Epoch: 6 Idx: 5000 Loss: 0.01941137153836997
Epoch: 7 Idx: 0 Loss: 0.02283304462991784
Epoch: 7 Idx: 5000 Loss: 0.014749034144786632
Epoch: 8 Idx: 0 Loss: 0.027436488683056367
Epoch: 8 Idx: 5000 Loss: 0.019306242227532744
Epoch: 9 Idx: 0 Loss: 0.018970585966617284
Epoch: 9 Idx: 5000 Loss: 0.052302600014643576
Epoch: 10 Idx: 0 Loss: 0.01836753833433371
Epoch: 10 Idx: 5000 Loss: 0.02763192741756428
Epoch: 11 Idx: 0 Loss: 0.019354682044368753
Epoch: 11 Idx: 5000 Loss: 0.02273556550956762
Epoch: 12 Idx: 0 Loss: 0.032263309517616266
Epoch: 12 Idx: 5000 Loss: 0.014465929334782726
Epoch: 13 Idx: 0 Loss: 0.014201193208675657
Epoch: 13 Idx: 5000 Loss: 0.027347513067225982
Epoch: 14 Idx: 0 Loss: 0.03323190484133699
Epoch: 14 Idx: 5000 Loss: 0.013802742202062528
Epoch: 15 Idx: 0 Loss: 0.01520401843566246
Epoch: 15 Idx: 5000 Loss: 0.024306902940881787
Epoch: 16 Idx: 0 Loss: 0.016802836216031777
Epoch: 16 Idx: 5000 Loss: 0.017733694883454207
Epoch: 17 Idx: 0 Loss: 0.03453112134426137
Epoch: 17 Idx: 5000 Loss: 0.018687741259988188
Epoch: 18 Idx: 0 Loss: 0.024691811469908363
Epoch: 18 Idx: 5000 Loss: 0.013261037662392214
Epoch: 19 Idx: 0 Loss: 0.017656950210488867
Epoch: 19 Idx: 5000 Loss: 0.01639531134784433
Epoch: 20 Idx: 0 Loss: 0.020727414058716375
Epoch: 20 Idx: 5000 Loss: 0.017946480735267223
Epoch: 21 Idx: 0 Loss: 0.017189510553833755
Epoch: 21 Idx: 5000 Loss: 0.020421353834223117
Epoch: 22 Idx: 0 Loss: 0.020067583585796897
Epoch: 22 Idx: 5000 Loss: 0.012422256453471824
Epoch: 23 Idx: 0 Loss: 0.021039084557150485
Epoch: 23 Idx: 5000 Loss: 0.01957825684886995
Epoch: 24 Idx: 0 Loss: 0.029566927930857162
Epoch: 24 Idx: 5000 Loss: 0.014275750251978296
Epoch: 25 Idx: 0 Loss: 0.016746066982058893
Epoch: 25 Idx: 5000 Loss: 0.015938435068987105
Epoch: 26 Idx: 0 Loss: 0.025462218364245344
Epoch: 26 Idx: 5000 Loss: 0.0200655036336471
Epoch: 27 Idx: 0 Loss: 0.01814998170079909
Epoch: 27 Idx: 5000 Loss: 0.02066300137270106
Epoch: 28 Idx: 0 Loss: 0.015152428054219909
Epoch: 28 Idx: 5000 Loss: 0.017703241298645152
Epoch: 29 Idx: 0 Loss: 0.009014439611882244
Epoch: 29 Idx: 5000 Loss: 0.02176637555673756
Epoch: 30 Idx: 0 Loss: 0.012542540443721665
Epoch: 30 Idx: 5000 Loss: 0.019848303887990793
Epoch: 31 Idx: 0 Loss: 0.015654828059303693
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
45
Epoch: 35 Idx: 5000 Loss: 0.013086475785359868
Epoch: 36 Idx: 0 Loss: 0.015425689610473595
Epoch: 36 Idx: 5000 Loss: 0.009103767873275253
Epoch: 37 Idx: 0 Loss: 0.02336786510254231
Epoch: 37 Idx: 5000 Loss: 0.008582927825434546
Epoch: 38 Idx: 0 Loss: 0.022090502669482724
Epoch: 38 Idx: 5000 Loss: 0.018405705397719185
Epoch: 39 Idx: 0 Loss: 0.023535823058855723
Epoch: 39 Idx: 5000 Loss: 0.006220190486835975
Epoch: 40 Idx: 0 Loss: 0.021286271920963863
Epoch: 40 Idx: 5000 Loss: 0.012856520260344433
Epoch: 41 Idx: 0 Loss: 0.021163753466932347
Epoch: 41 Idx: 5000 Loss: 0.00843595369098767
Epoch: 42 Idx: 0 Loss: 0.01660512595941114
Epoch: 42 Idx: 5000 Loss: 0.01613543519166118
Epoch: 43 Idx: 0 Loss: 0.0159690948353294
Epoch: 43 Idx: 5000 Loss: 0.03266161680372218
Epoch: 44 Idx: 0 Loss: 0.034387449575927334
Epoch: 44 Idx: 5000 Loss: 0.013186057751981927
Epoch: 45 Idx: 0 Loss: 0.017380979253738032
Epoch: 45 Idx: 5000 Loss: 0.014144648891192428
Epoch: 46 Idx: 0 Loss: 0.01992475115319746
Epoch: 46 Idx: 5000 Loss: 0.029367063807167297
Epoch: 47 Idx: 0 Loss: 0.018172659360034808
Epoch: 47 Idx: 5000 Loss: 0.016468299134063873
Epoch: 48 Idx: 0 Loss: 0.023967144107563385
Epoch: 48 Idx: 5000 Loss: 0.017869410008549087
Epoch: 49 Idx: 0 Loss: 0.02133088400405929
Epoch: 49 Idx: 5000 Loss: 0.01761596814439847
Len (direct inputs):  90
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
diviTraining sizeTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.207923958289252
Epoch: 0 Idx: 5000 Loss: 0.018933781039955196
Epoch: 1 Idx: 0 Loss: 0.020876026741827383
Epoch: 1 Idx: 5000 Loss: 0.01993273283139779
Epoch: 2 Idx: 0 Loss: 0.022668560014678735
Epoch: 2 Idx: 5000 Loss: 0.009257007571799581
Epoch: 3 Idx: 0 Loss: 0.02051238976946615
Epoch: 3 Idx: 5000 Loss: 0.02846670491535983
Epoch: 4 Idx: 0 Loss: 0.0245225889300524
Epoch: 4 Idx: 5000 Loss: 0.034055216725922446
Epoch: 5 Idx: 0 Loss: 0.024794124924917272
Epoch: 5 Idx: 5000 Loss: 0.042696092836270204
Epoch: 6 Idx: 0 Loss: 0.027808623748060955
Epoch: 6 Idx: 5000 Loss: 0.012983619409112463
Epoch: 7 Idx: 0 Loss: 0.020132819191924542
Epoch: 7 Idx: 5000 Loss: 0.01511170283091072
Epoch: 8 Idx: 0 Loss: 0.017040115457580225
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
KeyboardInterrupt
: 0.021856961204920672
Epoch: 11 Idx: 0 Loss: 0.012860763590329501
Epoch: 11 Idx: 5000 Loss: 0.015903371044556264
Epoch: 12 Idx: 0 Loss: 0.02344758304817234
Epoch: 12 Idx: 5000 Loss: 0.021144676929303215
Epoch: 13 Idx: 0 Loss: 0.013556246958624996
Epoch: 13 Idx: 5000 Loss: 0.024884504777775547
Epoch: 14 Idx: 0 Loss: 0.011379778411288986
Epoch: 14 Idx: 5000 Loss: 0.03831646825253678
Epoch: 15 Idx: 0 Loss: 0.027348360298893336
Epoch: 15 Idx: 5000 Loss: 0.010902993351496765
Epoch: 16 Idx: 0 Loss: 0.02563215699924571
Epoch: 16 Idx: 5000 Loss: 0.02001488894835084
Epoch: 17 Idx: 0 Loss: 0.02671948437129298
Epoch: 17 Idx: 5000 Loss: 0.019037722283448348
Epoch: 18 Idx: 0 Loss: 0.017291291492289203
Epoch: 18 Idx: 5000 Loss: 0.0185347090624359
Epoch: 19 Idx: 0 Loss: 0.01040598000404682
Epoch: 19 Idx: 5000 Loss: 0.015591074726052475
Epoch: 20 Idx: 0 Loss: 0.02923940780964534
Epoch: 20 Idx: 5000 Loss: 0.012350812926995119
Epoch: 21 Idx: 0 Loss: 0.015256197273117103
Epoch: 21 Idx: 5000 Loss: 0.021199053708680155
Epoch: 22 Idx: 0 Loss: 0.012041579007324362
Epoch: 22 Idx: 5000 Loss: 0.027890889841642192
Epoch: 23 Idx: 0 Loss: 0.02633331970860591
Epoch: 23 Idx: 5000 Loss: 0.03374360608865888
Epoch: 24 Idx: 0 Loss: 0.01275092025187664
Epoch: 24 Idx: 5000 Loss: 0.018618469249723383
Epoch: 25 Idx: 0 Loss: 0.01543898956347819
Epoch: 25 Idx: 5000 Loss: 0.023747182750131036
Epoch: 26 Idx: 0 Loss: 0.025400073608569336
Epoch: 26 Idx: 5000 Loss: 0.01881742031482926
Epoch: 27 Idx: 0 Loss: 0.023821494247370366
Epoch: 27 Idx: 5000 Loss: 0.017423981904606766
Epoch: 28 Idx: 0 Loss: 0.018756729499836762
Epoch: 28 Idx: 5000 Loss: 0.030276182400514388
Epoch: 29 Idx: 0 Loss: 0.016369544021482742
Epoch: 29 Idx: 5000 Loss: 0.03827239241522564
Epoch: 30 Idx: 0 Loss: 0.0457602211495179
Epoch: 30 Idx: 5000 Loss: 0.01238675484164158
Epoch: 31 Idx: 0 Loss: 0.020330468946050034
Epoch: 31 Idx: 5000 Loss: 0.025405216243509957
Epoch: 32 Idx: 0 Loss: 0.02485875517066121
Epoch: 32 Idx: 5000 Loss: 0.012264641526617208
Epoch: 33 Idx: 0 Loss: 0.015793769726466764
Epoch: 33 Idx: 5000 Loss: 0.01564209420301967
Epoch: 34 Idx: 0 Loss: 0.03387585733555787
Epoch: 34 Idx: 5000 Loss: 0.03634778179528825
Epoch: 35 Idx: 0 Loss: 0.028235177475802294
Epoch: 35 Idx: 5000 Loss: 0.031568323573606796
Epoch: 36 Idx: 0 Loss: 0.02252207262957589
Epoch: 36 Idx: 5000 Loss: 0.021517731303469727
Epoch: 37 Idx: 0 Loss: 0.017333857698291783
Epoch: 37 Idx: 5000 Loss: 0.02381124001360837
Epoch: 38 Idx: 0 Loss: 0.015275549830661563
Epoch: 38 Idx: 5000 Loss: 0.020177532913214318
Epoch: 39 Idx: 0 Loss: 0.016164421562507192
Epoch: 39 Idx: 5000 Loss: 0.018648298858729484
Epoch: 40 Idx: 0 Loss: 0.020435040351490838
Epoch: 40 Idx: 5000 Loss: 0.011772890770835678
Epoch: 41 Idx: 0 Loss: 0.020754323982566325
Epoch: 41 Idx: 5000 Loss: 0.01394891226450893
Epoch: 42 Idx: 0 Loss: 0.031070950041780513
Epoch: 42 Idx: 5000 Loss: 0.013093318421382661
Epoch: 43 Idx: 0 Loss: 0.02284656448766098
Epoch: 43 Idx: 5000 Loss: 0.021688493135750494
Epoch: 44 Idx: 0 Loss: 0.023096644255575914
Epoch: 44 Idx: 5000 Loss: 0.0199837003179347
Epoch: 45 Idx: 0 Loss: 0.025446796135042735
Epoch: 45 Idx: 5000 Loss: 0.03452283027758429
Epoch: 46 Idx: 0 Loss: 0.035546795270430996
Epoch: 46 Idx: 5000 Loss: 0.034548284261831534
Epoch: 47 Idx: 0 Loss: 0.024819247700034373
Epoch: 47 Idx: 5000 Loss: 0.013001142683693116
Epoch: 48 Idx: 0 Loss: 0.03731868561652382
Epoch: 48 Idx: 5000 Loss: 0.028752848422856914
Epoch: 49 Idx: 0 Loss: 0.019189029302033346
Epoch: 49 Idx: 5000 Loss: 0.029014425802856073
Len (direct inputs):  72
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
TrainingTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18508265987295314
Epoch: 0 Idx: 5000 Loss: 0.018128618650967893
Epoch: 1 Idx: 0 Loss: 0.013407024033759776
Epoch: 1 Idx: 5000 Loss: 0.01751088386309907
Epoch: 2 Idx: 0 Loss: 0.04757408926005791
Epoch: 2 Idx: 5000 Loss: 0.030577284258127824
Epoch: 3 Idx: 0 Loss: 0.03539366132391274
Epoch: 3 Idx: 5000 Loss: 0.02410518084860958
Epoch: 4 Idx: 0 Loss: 0.02071527289362351
Epoch: 4 Idx: 5000 Loss: 0.015188064325681174
Epoch: 5 Idx: 0 Loss: 0.03679879919538684
Epoch: 5 Idx: 5000 Loss: 0.022515228857186254
Epoch: 6 Idx: 0 Loss: 0.018960945662169507
Epoch: 6 Idx: 5000 Loss: 0.021851684113872394
Epoch: 7 Idx: 0 Loss: 0.015596689136648325
Epoch: 7 Idx: 5000 Loss: 0.013372917721456037
Epoch: 8 Idx: 0 Loss: 0.01922215933433377
Epoch: 8 Idx: 5000 Loss: 0.01349740739817086
Epoch: 9 Idx: 0 Loss: 0.03345379360173073
Epoch: 9 Idx: 5000 Loss: 0.014837561107141553
Epoch: 10 Idx: 0 Loss: 0.01674232899468399
Epoch: 10 Idx: 5000 Loss: 0.019277829245830283
Epoch: 11 Idx: 0 Loss: 0.01628704166022174
Epoch: 11 Idx: 5000 Loss: 0.014597797380410206
Epoch: 12 Idx: 0 Loss: 0.010593597600387049
Epoch: 12 Idx: 5000 Loss: 0.013509688412198552
Epoch: 13 Idx: 0 Loss: 0.032542047507745434
Epoch: 13 Idx: 5000 Loss: 0.010257990442575233
Epoch: 14 Idx: 0 Loss: 0.018076473338962586
Epoch: 14 Idx: 5000 Loss: 0.024314533813850025
Epoch: 15 Idx: 0 Loss: 0.016275457387940312
Epoch: 15 Idx: 5000 Loss: 0.020537808062636942
Epoch: 16 Idx: 0 Loss: 0.02447147083543747
Epoch: 16 Idx: 5000 Loss: 0.00797067361848097
Epoch: 17 Idx: 0 Loss: 0.04453832213960211
Epoch: 17 Idx: 5000 Loss: 0.01260985943728964
Epoch: 18 Idx: 0 Loss: 0.015465594033978451
Epoch: 18 Idx: 5000 Loss: 0.01886441026793329
Epoch: 19 Idx: 0 Loss: 0.011178069289659578
Epoch: 19 Idx: 5000 Loss: 0.010341528652477259
Epoch: 20 Idx: 0 Loss: 0.02358662332275699
Epoch: 20 Idx: 5000 Loss: 0.016749713231013128
Epoch: 21 Idx: 0 Loss: 0.015692386161782763
Epoch: 21 Idx: 5000 Loss: 0.03176406462944777
Epoch: 22 Idx: 0 Loss: 0.04099922314371222
Epoch: 22 Idx: 5000 Loss: 0.024683085892173853
Epoch: 23 Idx: 0 Loss: 0.027377208896261086
Epoch: 23 Idx: 5000 Loss: 0.01920957090214942
Epoch: 24 Idx: 0 Loss: 0.026228952121618226
Epoch: 24 Idx: 5000 Loss: 0.03750027946007618
Epoch: 25 Idx: 0 Loss: 0.03449324455299914
Epoch: 25 Idx: 5000 Loss: 0.03729607061420657
Epoch: 26 Idx: 0 Loss: 0.045820128324328246
Epoch: 26 Idx: 5000 Loss: 0.014579463888596418
Epoch: 27 Idx: 0 Loss: 0.017368338699690452
Epoch: 27 Idx: 5000 Loss: 0.01632108994772952
Epoch: 28 Idx: 0 Loss: 0.021604983554331846
Epoch: 28 Idx: 5000 Loss: 0.020997530086689115
Epoch: 29 Idx: 0 Loss: 0.03598296139706512
Epoch: 29 Idx: 5000 Loss: 0.022725094774293797
Epoch: 30 Idx: 0 Loss: 0.0201850035291404
Epoch: 30 Idx: 5000 Loss: 0.0412100935124367
Epoch: 31 Idx: 0 Loss: 0.013628827746362777
Epoch: 31 Idx: 5000 Loss: 0.01794480257933725
Epoch: 32 Idx: 0 Loss: 0.036140422423084594
Epoch: 32 Idx: 5000 Loss: 0.01864547747272082
Epoch: 33 Idx: 0 Loss: 0.018268879746038483
Epoch: 33 Idx: 5000 Loss: 0.012401339223604441
Epoch: 34 Idx: 0 Loss: 0.02035761411750171
Epoch: 34 Idx: 5000 Loss: 0.013295460722398513
Epoch: 35 Idx: 0 Loss: 0.022612854363889368
Epoch: 35 Idx: 5000 Loss: 0.031595869122848105
Epoch: 36 Idx: 0 Loss: 0.010821550484672031
Epoch: 36 Idx: 5000 Loss: 0.027322862044890914
Epoch: 37 Idx: 0 Loss: 0.018119555834669076
Epoch: 37 Idx: 5000 Loss: 0.011567966355921284
Epoch: 38 Idx: 0 Loss: 0.02502331022992453
Epoch: 38 Idx: 5000 Loss: 0.025471384914395677
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt
43 Idx: 0 Loss: 0.022714892828730435
Epoch: 43 Idx: 5000 Loss: 0.022740245821663244
Epoch: 44 Idx: 0 Loss: 0.022130563885695387
Epoch: 44 Idx: 5000 Loss: 0.03254917985776343
Epoch: 45 Idx: 0 Loss: 0.02286052001022202
Epoch: 45 Idx: 5000 Loss: 0.020470570629982858
Epoch: 46 Idx: 0 Loss: 0.012691444197268852
Epoch: 46 Idx: 5000 Loss: 0.025186687873454076
Epoch: 47 Idx: 0 Loss: 0.025449066489697397
Epoch: 47 Idx: 5000 Loss: 0.015504330590645161
Epoch: 48 Idx: 0 Loss: 0.014580256784928768
Epoch: 48 Idx: 5000 Loss: 0.018317506422309917
Epoch: 49 Idx: 0 Loss: 0.02894265229125389
Epoch: 49 Idx: 5000 Loss: 0.019040074826616616
Len (direct inputs):  111
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by TrainTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.15225438389039153
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 397, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "Attention_anatomy_aml_weighted.py", line 287, in forward
    sim = self.cosine_sim_layer(results[0], results[1])
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/distance.py", line 68, in forward
    return F.cosine_similarity(x1, x2, self.dim, self.eps)
KeyboardInterrupt
ch: 9 Idx: 0 Loss: 0.024831431555433138
Epoch: 9 Idx: 5000 Loss: 0.035254794891397684
Epoch: 10 Idx: 0 Loss: 0.011444132993577025
Epoch: 10 Idx: 5000 Loss: 0.015819234299761592
Epoch: 11 Idx: 0 Loss: 0.015019051504575005
Epoch: 11 Idx: 5000 Loss: 0.021206575166444436
Epoch: 12 Idx: 0 Loss: 0.013858364649090026
Epoch: 12 Idx: 5000 Loss: 0.013846024054768258
Epoch: 13 Idx: 0 Loss: 0.023338376750890615
Epoch: 13 Idx: 5000 Loss: 0.019936044000504194
Epoch: 14 Idx: 0 Loss: 0.01892128416061921
Epoch: 14 Idx: 5000 Loss: 0.024469654344586648
Epoch: 15 Idx: 0 Loss: 0.03239905682375541
Epoch: 15 Idx: 5000 Loss: 0.01854659822448616
Epoch: 16 Idx: 0 Loss: 0.05236025587471892
Epoch: 16 Idx: 5000 Loss: 0.03675876140427678
Epoch: 17 Idx: 0 Loss: 0.010366579969100632
Epoch: 17 Idx: 5000 Loss: 0.01358022859323629
Epoch: 18 Idx: 0 Loss: 0.01644883532172642
Epoch: 18 Idx: 5000 Loss: 0.028790958209248086
Epoch: 19 Idx: 0 Loss: 0.019543696877791818
Epoch: 19 Idx: 5000 Loss: 0.015070063639464983
Epoch: 20 Idx: 0 Loss: 0.023446456999494304
Epoch: 20 Idx: 5000 Loss: 0.051133175457911795
Epoch: 21 Idx: 0 Loss: 0.017404117611635374
Epoch: 21 Idx: 5000 Loss: 0.01705008050208699
Epoch: 22 Idx: 0 Loss: 0.00814345091714688
Epoch: 22 Idx: 5000 Loss: 0.021240613007859715
Epoch: 23 Idx: 0 Loss: 0.02320053127157607
Epoch: 23 Idx: 5000 Loss: 0.026579477920782586
Epoch: 24 Idx: 0 Loss: 0.01951243551852807
Epoch: 24 Idx: 5000 Loss: 0.007068538222782455
Epoch: 25 Idx: 0 Loss: 0.02113241109866827
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 313, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
KeyboardInterrupt
4
Epoch: 33 Idx: 5000 Loss: 0.019956052523679907
Epoch: 34 Idx: 0 Loss: 0.01823064666987046
Epoch: 34 Idx: 5000 Loss: 0.010824556075760293
Epoch: 35 Idx: 0 Loss: 0.031241396335386994
Epoch: 35 Idx: 5000 Loss: 0.023113779884398795
Epoch: 36 Idx: 0 Loss: 0.016283602782057833
Epoch: 36 Idx: 5000 Loss: 0.013175531096324437
Epoch: 37 Idx: 0 Loss: 0.02880267127306735
Epoch: 37 Idx: 5000 Loss: 0.031231903662612005
Epoch: 38 Idx: 0 Loss: 0.02186360670031045
Epoch: 38 Idx: 5000 Loss: 0.01563303716673915
Epoch: 39 Idx: 0 Loss: 0.018318058813722657
Epoch: 39 Idx: 5000 Loss: 0.024590116456480286
Epoch: 40 Idx: 0 Loss: 0.047454181749687116
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 388, in <module>
    inputs = np.array(to_feature(inputs_all[batch_start: batch_end]))
  File "Attention_anatomy_aml_weighted.py", line 315, in to_feature
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 315, in <listcomp>
    for elem in inputs_lenpadded]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 314, in <listcomp>
    for nbr_type in ent] for ent in elem]
  File "Attention_anatomy_aml_weighted.py", line 313, in <listcomp>
    for i in range(max_paths - len(nbr_type))]
  File "Attention_anatomy_aml_weighted.py", line 312, in <listcomp>
    inputs_pathpadded = [[[nbr_type + [[0 for j in range(max_pathlen)]
KeyboardInterrupt
ero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2679814509821934
Epoch: 0 Idx: 5000 Loss: 0.03098433803595122
Epoch: 1 Idx: 0 Loss: 0.034907967882644664
Epoch: 1 Idx: 5000 Loss: 0.01888292075533593
Epoch: 2 Idx: 0 Loss: 0.01166752917260256
Epoch: 2 Idx: 5000 Loss: 0.028120794307120382
Epoch: 3 Idx: 0 Loss: 0.015553247371625124
Epoch: 3 Idx: 5000 Loss: 0.014616202842206667
Epoch: 4 Idx: 0 Loss: 0.02513701616084571
Epoch: 4 Idx: 5000 Loss: 0.014849750162692845
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc249>
Subject: Job 3290022: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 10 Output/test_anatomy_aml_bagofnbrs_wtpath26_10.pkl Models/anatomy_aml_bagofnbrs_wtpath26_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 10 Output/test_anatomy_aml_bagofnbrs_wtpath26_10.pkl Models/anatomy_aml_bagofnbrs_wtpath26_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc249>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:43:18 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:43:18 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 26 10 Output/test_anatomy_aml_bagofnbrs_wtpath26_10.pkl Models/anatomy_aml_bagofnbrs_wtpath26_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45254.00 sec.
    Max Memory :                                 2736 MB
    Average Memory :                             2664.46 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40681.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45330 sec.
    Turnaround time :                            80689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc250>
Subject: Job 3290032: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 10 Output/test_anatomy_aml_bagofnbrs_wtpath152_10.pkl Models/anatomy_aml_bagofnbrs_wtpath152_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 10 Output/test_anatomy_aml_bagofnbrs_wtpath152_10.pkl Models/anatomy_aml_bagofnbrs_wtpath152_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc250>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 00:07:00 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 00:07:00 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 152 10 Output/test_anatomy_aml_bagofnbrs_wtpath152_10.pkl Models/anatomy_aml_bagofnbrs_wtpath152_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   43831.52 sec.
    Max Memory :                                 2675 MB
    Average Memory :                             2567.90 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40742.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   43908 sec.
    Turnaround time :                            80689 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3290030: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 10 Output/test_anatomy_aml_bagofnbrs_wtpath80_10.pkl Models/anatomy_aml_bagofnbrs_wtpath80_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 10 Output/test_anatomy_aml_bagofnbrs_wtpath80_10.pkl Models/anatomy_aml_bagofnbrs_wtpath80_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 00:03:19 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 00:03:19 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 80 10 Output/test_anatomy_aml_bagofnbrs_wtpath80_10.pkl Models/anatomy_aml_bagofnbrs_wtpath80_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   44076.00 sec.
    Max Memory :                                 2722 MB
    Average Memory :                             2603.01 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40695.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   44130 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc244>
Subject: Job 3290028: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 10 Output/test_anatomy_aml_bagofnbrs_wtpath40_10.pkl Models/anatomy_aml_bagofnbrs_wtpath40_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 10 Output/test_anatomy_aml_bagofnbrs_wtpath40_10.pkl Models/anatomy_aml_bagofnbrs_wtpath40_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc244>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 00:01:37 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 00:01:37 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 40 10 Output/test_anatomy_aml_bagofnbrs_wtpath40_10.pkl Models/anatomy_aml_bagofnbrs_wtpath40_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   44225.92 sec.
    Max Memory :                                 2729 MB
    Average Memory :                             2637.23 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40688.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   44232 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc246>
Subject: Job 3290016: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 20 10 Output/test_anatomy_aml_bagofnbrs_wtpath20_10.pkl Models/anatomy_aml_bagofnbrs_wtpath20_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 20 10 Output/test_anatomy_aml_bagofnbrs_wtpath20_10.pkl Models/anatomy_aml_bagofnbrs_wtpath20_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:58 2020
Job was executed on host(s) <dccxc246>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:19:02 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:19:02 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 20 10 Output/test_anatomy_aml_bagofnbrs_wtpath20_10.pkl Models/anatomy_aml_bagofnbrs_wtpath20_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46658.13 sec.
    Max Memory :                                 2737 MB
    Average Memory :                             2671.78 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40680.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   46786 sec.
    Turnaround time :                            80691 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc276>
Subject: Job 3290024: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 10 Output/test_anatomy_aml_bagofnbrs_wtpath30_10.pkl Models/anatomy_aml_bagofnbrs_wtpath30_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 10 Output/test_anatomy_aml_bagofnbrs_wtpath30_10.pkl Models/anatomy_aml_bagofnbrs_wtpath30_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc276>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:47:08 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:47:08 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 30 10 Output/test_anatomy_aml_bagofnbrs_wtpath30_10.pkl Models/anatomy_aml_bagofnbrs_wtpath30_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45041.03 sec.
    Max Memory :                                 2737 MB
    Average Memory :                             2654.29 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40680.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45116 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc276>
Subject: Job 3290020: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 24 10 Output/test_anatomy_aml_bagofnbrs_wtpath24_10.pkl Models/anatomy_aml_bagofnbrs_wtpath24_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 24 10 Output/test_anatomy_aml_bagofnbrs_wtpath24_10.pkl Models/anatomy_aml_bagofnbrs_wtpath24_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc276>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:37:47 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:37:47 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 24 10 Output/test_anatomy_aml_bagofnbrs_wtpath24_10.pkl Models/anatomy_aml_bagofnbrs_wtpath24_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45654.45 sec.
    Max Memory :                                 2735 MB
    Average Memory :                             2662.91 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40682.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45678 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc259>
Subject: Job 3290014: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 18 10 Output/test_anatomy_aml_bagofnbrs_wtpath18_10.pkl Models/anatomy_aml_bagofnbrs_wtpath18_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 18 10 Output/test_anatomy_aml_bagofnbrs_wtpath18_10.pkl Models/anatomy_aml_bagofnbrs_wtpath18_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:58 2020
Job was executed on host(s) <dccxc259>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:16:46 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:16:46 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 18 10 Output/test_anatomy_aml_bagofnbrs_wtpath18_10.pkl Models/anatomy_aml_bagofnbrs_wtpath18_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   46836.76 sec.
    Max Memory :                                 2740 MB
    Average Memory :                             2669.12 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40677.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   46923 sec.
    Turnaround time :                            80691 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc206>
Subject: Job 3290026: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 10 Output/test_anatomy_aml_bagofnbrs_wtpath32_10.pkl Models/anatomy_aml_bagofnbrs_wtpath32_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 10 Output/test_anatomy_aml_bagofnbrs_wtpath32_10.pkl Models/anatomy_aml_bagofnbrs_wtpath32_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:59 2020
Job was executed on host(s) <dccxc206>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:59:54 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:59:54 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 32 10 Output/test_anatomy_aml_bagofnbrs_wtpath32_10.pkl Models/anatomy_aml_bagofnbrs_wtpath32_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   43952.69 sec.
    Max Memory :                                 2731 MB
    Average Memory :                             2647.96 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40686.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   44355 sec.
    Turnaround time :                            80690 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc214>
Subject: Job 3290018: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 22 10 Output/test_anatomy_aml_bagofnbrs_wtpath22_10.pkl Models/anatomy_aml_bagofnbrs_wtpath22_10.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 22 10 Output/test_anatomy_aml_bagofnbrs_wtpath22_10.pkl Models/anatomy_aml_bagofnbrs_wtpath22_10.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:53:58 2020
Job was executed on host(s) <dccxc214>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Tue Sep  1 23:33:24 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Tue Sep  1 23:33:24 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei_bagofnbrs.pkl 22 10 Output/test_anatomy_aml_bagofnbrs_wtpath22_10.pkl Models/anatomy_aml_bagofnbrs_wtpath22_10.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   45837.45 sec.
    Max Memory :                                 2737 MB
    Average Memory :                             2668.57 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40680.00 MB
    Max Swap :                                   4 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   45925 sec.
    Turnaround time :                            80691 sec.

The output (if any) is above this job summary.

