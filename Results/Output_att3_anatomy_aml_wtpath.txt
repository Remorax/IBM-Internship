Max number of nodes in a path: Input/data_anatomy_oaei.pkl
Number of entities: 150000
Training size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.18981552973954963
Epoch: 0 Idx: 5000 Loss: 0.018903474729998364
Epoch: 1 Idx: 0 Loss: 0.014913646167462921
Epoch: 1 Idx: 5000 Loss: 0.04021569819864769
Epoch: 2 Idx: 0 Loss: 0.024574860743384174
Epoch: 2 Idx: 5000 Loss: 0.01626038766840457
Epoch: 3 Idx: 0 Loss: 0.03306098683931494
Epoch: 3 Idx: 5000 Loss: 0.024045561037681545
Epoch: 4 Idx: 0 Loss: 0.022077828560128847
Epoch: 4 Idx: 5000 Loss: 0.035617628808145556
Epoch: 5 Idx: 0 Loss: 0.01006737932428792
Epoch: 5 Idx: 5000 Loss: 0.03207728670153631
Epoch: 6 Idx: 0 Loss: 0.04970132318612386
Epoch: 6 Idx: 5000 Loss: 0.029410129718908197
Epoch: 7 Idx: 0 Loss: 0.01879410428735531
Epoch: 7 Idx: 5000 Loss: 0.021614563699652093
Epoch: 8 Idx: 0 Loss: 0.025368924025527016
Epoch: 8 Idx: 5000 Loss: 0.017650991402869687
Epoch: 9 Idx: 0 Loss: 0.023646968732921304
Epoch: 9 Idx: 5000 Loss: 0.021446713681175122
Epoch: 10 Idx: 0 Loss: 0.014167817222525903
Epoch: 10 Idx: 5000 Loss: 0.01693657519060531
Epoch: 11 Idx: 0 Loss: 0.03376115467549322
Epoch: 11 Idx: 5000 Loss: 0.021236065961827498
Epoch: 12 Idx: 0 Loss: 0.019272751778319074
Epoch: 12 Idx: 5000 Loss: 0.02042056302272632
Epoch: 13 Idx: 0 Loss: 0.025315894736340938
Epoch: 13 Idx: 5000 Loss: 0.033749046303345076
Epoch: 14 Idx: 0 Loss: 0.02549847623455813
Epoch: 14 Idx: 5000 Loss: 0.024616726327130113
Epoch: 15 Idx: 0 Loss: 0.031600894016975384
Epoch: 15 Idx: 5000 Loss: 0.03255994188592698
Epoch: 16 Idx: 0 Loss: 0.014403176403716004
Epoch: 16 Idx: 5000 Loss: 0.016293404031144806
Epoch: 17 Idx: 0 Loss: 0.02255074549375954
Epoch: 17 Idx: 5000 Loss: 0.020485731903928696
Epoch: 18 Idx: 0 Loss: 0.018116000489809818
Epoch: 18 Idx: 5000 Loss: 0.007174780636186073
Epoch: 19 Idx: 0 Loss: 0.027486563823376622
Epoch: 19 Idx: 5000 Loss: 0.017722568329196907
Epoch: 20 Idx: 0 Loss: 0.019091603141687195
Epoch: 20 Idx: 5000 Loss: 0.010435318292042091
Epoch: 21 Idx: 0 Loss: 0.015347928209814541
Epoch: 21 Idx: 5000 Loss: 0.023534360332375964
Epoch: 22 Idx: 0 Loss: 0.0186496452884206
Epoch: 22 Idx: 5000 Loss: 0.015398724731467334
Epoch: 23 Idx: 0 Loss: 0.02352851056172931
Epoch: 23 Idx: 5000 Loss: 0.02068933440806337
Epoch: 24 Idx: 0 Loss: 0.010962671356293328
Epoch: 24 Idx: 5000 Loss: 0.012680003472417942
Epoch: 25 Idx: 0 Loss: 0.014053553914434157
Epoch: 25 Idx: 5000 Loss: 0.02548548242733986
Epoch: 26 Idx: 0 Loss: 0.017307724381642967
Epoch: 26 Idx: 5000 Loss: 0.02990616130832614
Epoch: 27 Idx: 0 Loss: 0.026628542715773397
Epoch: 27 Idx: 5000 Loss: 0.030426742836647317
Epoch: 28 Idx: 0 Loss: 0.030205475098045977
Epoch: 28 Idx: 5000 Loss: 0.03078228612123607
Epoch: 29 Idx: 0 Loss: 0.022083566730717256
Epoch: 29 Idx: 5000 Loss: 0.011370341740679359
Epoch: 30 Idx: 0 Loss: 0.027709675182052012
Epoch: 30 Idx: 5000 Loss: 0.01184089267234503
Epoch: 31 Idx: 0 Loss: 0.04263089009736839
Epoch: 31 Idx: 5000 Loss: 0.025326273483813593
Epoch: 32 Idx: 0 Loss: 0.019220623997183238
Epoch: 32 Idx: 5000 Loss: 0.02466207968090464
Epoch: 33 Idx: 0 Loss: 0.009032159600123818
Epoch: 33 Idx: 5000 Loss: 0.02554512939489171
Epoch: 34 Idx: 0 Loss: 0.020891792772605385
Epoch: 34 Idx: 5000 Loss: 0.0169322804953443
Epoch: 35 Idx: 0 Loss: 0.03252409560291034
Epoch: 35 Idx: 5000 Loss: 0.02075484611899225
Epoch: 36 Idx: 0 Loss: 0.024765340208455156
Epoch: 36 Idx: 5000 Loss: 0.023873978127783455
Epoch: 37 Idx: 0 Loss: 0.012825669973327118
Epoch: 37 Idx: 5000 Loss: 0.026215081903202944
Epoch: 38 Idx: 0 Loss: 0.018426264475294156
Epoch: 38 Idx: 5000 Loss: 0.016648224964254194
Epoch: 39 Idx: 0 Loss: 0.028065387134274762
Epoch: 39 Idx: 5000 Loss: 0.0074481857767739225
Epoch: 40 Idx: 0 Loss: 0.029672954981698384
Epoch: 40 Idx: 5000 Loss: 0.01575325278289319
Epoch: 41 Idx: 0 Loss: 0.03129081148895525
Epoch: 41 Idx: 5000 Loss: 0.0194525959329723
Epoch: 42 Idx: 0 Loss: 0.013933743854103068
Epoch: 42 Idx: 5000 Loss: 0.009106636191631646
Epoch: 43 Idx: 0 Loss: 0.020708929327972775
Epoch: 43 Idx: 5000 Loss: 0.024725505081432703
Epoch: 44 Idx: 0 Loss: 0.011346045268465804
Epoch: 44 Idx: 5000 Loss: 0.014178607619281393
Epoch: 45 Idx: 0 Loss: 0.030984689424687416
Epoch: 45 Idx: 5000 Loss: 0.01223636971676573
Epoch: 46 Idx: 0 Loss: 0.02334330546091347
Epoch: 46 Idx: 5000 Loss: 0.029409075147387304
Epoch: 47 Idx: 0 Loss: 0.033150886449776144
Epoch: 47 Idx: 5000 Loss: 0.025440401399549416
Epoch: 48 Idx: 0 Loss: 0.012467237511823586
Epoch: 48 Idx: 5000 Loss: 0.029312765630137322
Epoch: 49 Idx: 0 Loss: 0.020155332894604853
Epoch: 49 Idx: 5000 Loss: 0.025074776988808355
Len (direct inputs):  110
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisioTraiTrainiTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.277184330258075
Epoch: 0 Idx: 5000 Loss: 0.01761598377175606
Epoch: 1 Idx: 0 Loss: 0.02326794589781103
Epoch: 1 Idx: 5000 Loss: 0.033609779110063576
Epoch: 2 Idx: 0 Loss: 0.01629660640490528
Epoch: 2 Idx: 5000 Loss: 0.02774464510825786
Epoch: 3 Idx: 0 Loss: 0.015829962277296116
Epoch: 3 Idx: 5000 Loss: 0.027630395705837325
Epoch: 4 Idx: 0 Loss: 0.018515466760689516
Epoch: 4 Idx: 5000 Loss: 0.016054161539888378
Epoch: 5 Idx: 0 Loss: 0.01673294762633329
Epoch: 5 Idx: 5000 Loss: 0.012325752290605499
Epoch: 6 Idx: 0 Loss: 0.036265688710245575
Epoch: 6 Idx: 5000 Loss: 0.021429399500164533
Epoch: 7 Idx: 0 Loss: 0.022042344746155282
Epoch: 7 Idx: 5000 Loss: 0.027167356508585268
Epoch: 8 Idx: 0 Loss: 0.012943912728699374
Epoch: 8 Idx: 5000 Loss: 0.011990459348260894
Epoch: 9 Idx: 0 Loss: 0.016317740229817426
Epoch: 9 Idx: 5000 Loss: 0.02425081957285421
Epoch: 10 Idx: 0 Loss: 0.027094048967579595
Epoch: 10 Idx: 5000 Loss: 0.01345735631026165
Epoch: 11 Idx: 0 Loss: 0.020774777661617075
Epoch: 11 Idx: 5000 Loss: 0.018095687154809997
Epoch: 12 Idx: 0 Loss: 0.020562660593382805
Epoch: 12 Idx: 5000 Loss: 0.010435526474669738
Epoch: 13 Idx: 0 Loss: 0.014986553008989885
Epoch: 13 Idx: 5000 Loss: 0.027485944234792302
Epoch: 14 Idx: 0 Loss: 0.013933866438692297
Epoch: 14 Idx: 5000 Loss: 0.018677140457083817
Epoch: 15 Idx: 0 Loss: 0.030966358884597926
Epoch: 15 Idx: 5000 Loss: 0.01903570282473811
Epoch: 16 Idx: 0 Loss: 0.02931028933339964
Epoch: 16 Idx: 5000 Loss: 0.01934786442354841
Epoch: 17 Idx: 0 Loss: 0.015319803147643526
Epoch: 17 Idx: 5000 Loss: 0.014175914454373249
Epoch: 18 Idx: 0 Loss: 0.0291801193111261
Epoch: 18 Idx: 5000 Loss: 0.0224999019246401
Epoch: 19 Idx: 0 Loss: 0.03765237757112862
Epoch: 19 Idx: 5000 Loss: 0.013973842341823613
Epoch: 20 Idx: 0 Loss: 0.031530830081050086
Epoch: 20 Idx: 5000 Loss: 0.02048663900479378
Epoch: 21 Idx: 0 Loss: 0.025153941628304997
Epoch: 21 Idx: 5000 Loss: 0.017740757741638337
Epoch: 22 Idx: 0 Loss: 0.020394469462360097
Epoch: 22 Idx: 5000 Loss: 0.029788320197429427
Epoch: 23 Idx: 0 Loss: 0.021304718605074675
Epoch: 23 Idx: 5000 Loss: 0.03310226718016819
Epoch: 24 Idx: 0 Loss: 0.034156523383503286
Epoch: 24 Idx: 5000 Loss: 0.029250413701979137
Epoch: 25 Idx: 0 Loss: 0.032458789988409495
Epoch: 25 Idx: 5000 Loss: 0.02862723686149371
Epoch: 26 Idx: 0 Loss: 0.020035058296230212
Epoch: 26 Idx: 5000 Loss: 0.02620710444721602
Epoch: 27 Idx: 0 Loss: 0.022196485514132634
Epoch: 27 Idx: 5000 Loss: 0.031576573218596304
Epoch: 28 Idx: 0 Loss: 0.02256357179428552
Epoch: 28 Idx: 5000 Loss: 0.013529741001020255
Epoch: 29 Idx: 0 Loss: 0.033774159569197236
Epoch: 29 Idx: 5000 Loss: 0.029532126809260657
Epoch: 30 Idx: 0 Loss: 0.017890995777410948
Epoch: 30 Idx: 5000 Loss: 0.0067314031601103524
Epoch: 31 Idx: 0 Loss: 0.01472676864574032
Epoch: 31 Idx: 5000 Loss: 0.014593975061088327
Epoch: 32 Idx: 0 Loss: 0.04155097545554472
Epoch: 32 Idx: 5000 Loss: 0.020770686142669435
Epoch: 33 Idx: 0 Loss: 0.01764974513681409
Epoch: 33 Idx: 5000 Loss: 0.023458108260615954
Epoch: 34 Idx: 0 Loss: 0.027197756041957812
Epoch: 34 Idx: 5000 Loss: 0.02179953615155008
Epoch: 35 Idx: 0 Loss: 0.013735275832701111
Epoch: 35 Idx: 5000 Loss: 0.024004376108177787
Epoch: 36 Idx: 0 Loss: 0.02397398969505047
Epoch: 36 Idx: 5000 Loss: 0.010602813604137786
Epoch: 37 Idx: 0 Loss: 0.017314069306874465
Epoch: 37 Idx: 5000 Loss: 0.021263885957010127
Epoch: 38 Idx: 0 Loss: 0.014651940918360244
Epoch: 38 Idx: 5000 Loss: 0.010037889819185502
Epoch: 39 Idx: 0 Loss: 0.025720811922779998
Epoch: 39 Idx: 5000 Loss: 0.029804752578502472
Epoch: 40 Idx: 0 Loss: 0.013728251009020009
Epoch: 40 Idx: 5000 Loss: 0.02578300027524416
Epoch: 41 Idx: 0 Loss: 0.019904098752214106
Epoch: 41 Idx: 5000 Loss: 0.021873554700710264
Epoch: 42 Idx: 0 Loss: 0.0181530653136975
Epoch: 42 Idx: 5000 Loss: 0.010878886058682948
Epoch: 43 Idx: 0 Loss: 0.03515453810565193
Epoch: 43 Idx: 5000 Loss: 0.018445499985500935
Epoch: 44 Idx: 0 Loss: 0.019072612277235054
Epoch: 44 Idx: 5000 Loss: 0.02541012491856655
Epoch: 45 Idx: 0 Loss: 0.03632210534224079
Epoch: 45 Idx: 5000 Loss: 0.03339229323060047
Epoch: 46 Idx: 0 Loss: 0.027810129954834634
Epoch: 46 Idx: 5000 Loss: 0.028249843191069025
Epoch: 47 Idx: 0 Loss: 0.018024209996447185
Epoch: 47 Idx: 5000 Loss: 0.019607036340682178
Epoch: 48 Idx: 0 Loss: 0.03042443581021324
Epoch: 48 Idx: 5000 Loss: 0.01703421369127049
Epoch: 49 Idx: 0 Loss: 0.01774690764950338
Epoch: 49 Idx: 5000 Loss: 0.029304901865312198
Len (direct inputs):  114
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division bTraining sizeTraTrainingTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.2886604257120637
Epoch: 0 Idx: 5000 Loss: 0.02043942063124469
Epoch: 1 Idx: 0 Loss: 0.034190177329567195
Epoch: 1 Idx: 5000 Loss: 0.023418326287527436
Epoch: 2 Idx: 0 Loss: 0.014462859251075965
Epoch: 2 Idx: 5000 Loss: 0.024816158644914474
Epoch: 3 Idx: 0 Loss: 0.01992065201324245
Epoch: 3 Idx: 5000 Loss: 0.020259822235091672
Epoch: 4 Idx: 0 Loss: 0.0127702565483904
Epoch: 4 Idx: 5000 Loss: 0.017683531598659256
Epoch: 5 Idx: 0 Loss: 0.01791176066540682
Epoch: 5 Idx: 5000 Loss: 0.027289061506786762
Epoch: 6 Idx: 0 Loss: 0.027155387735735413
Epoch: 6 Idx: 5000 Loss: 0.027543239703139592
Epoch: 7 Idx: 0 Loss: 0.013406303762226665
Epoch: 7 Idx: 5000 Loss: 0.015151490661271475
Epoch: 8 Idx: 0 Loss: 0.022317446963941348
Epoch: 8 Idx: 5000 Loss: 0.02720558427797052
Epoch: 9 Idx: 0 Loss: 0.028789651068399858
Epoch: 9 Idx: 5000 Loss: 0.010263321462474861
Epoch: 10 Idx: 0 Loss: 0.0116665970970965
Epoch: 10 Idx: 5000 Loss: 0.011250733508080714
Epoch: 11 Idx: 0 Loss: 0.03276362289238037
Epoch: 11 Idx: 5000 Loss: 0.017743311839648147
Epoch: 12 Idx: 0 Loss: 0.015815060567994955
Epoch: 12 Idx: 5000 Loss: 0.01402246335127174
Epoch: 13 Idx: 0 Loss: 0.021815783139119294
Epoch: 13 Idx: 5000 Loss: 0.015630139949226747
Epoch: 14 Idx: 0 Loss: 0.015025826528320716
Epoch: 14 Idx: 5000 Loss: 0.01084889889571318
Epoch: 15 Idx: 0 Loss: 0.017138361338679554
Epoch: 15 Idx: 5000 Loss: 0.02048953481114655
Epoch: 16 Idx: 0 Loss: 0.030239957753333168
Epoch: 16 Idx: 5000 Loss: 0.013959340665654274
Epoch: 17 Idx: 0 Loss: 0.012547731155599508
Epoch: 17 Idx: 5000 Loss: 0.016819824559072143
Epoch: 18 Idx: 0 Loss: 0.026901679177743342
Epoch: 18 Idx: 5000 Loss: 0.019467890151243637
Epoch: 19 Idx: 0 Loss: 0.021504047577366264
Epoch: 19 Idx: 5000 Loss: 0.024573375081310966
Epoch: 20 Idx: 0 Loss: 0.027276612622187987
Epoch: 20 Idx: 5000 Loss: 0.010800968495853599
Epoch: 21 Idx: 0 Loss: 0.01964917705801379
Epoch: 21 Idx: 5000 Loss: 0.01912660099690084
Epoch: 22 Idx: 0 Loss: 0.011682690813411
Epoch: 22 Idx: 5000 Loss: 0.030213283969112104
Epoch: 23 Idx: 0 Loss: 0.01814712806285755
Epoch: 23 Idx: 5000 Loss: 0.02149871458614367
Epoch: 24 Idx: 0 Loss: 0.01297281410221926
Epoch: 24 Idx: 5000 Loss: 0.016211582482028662
Epoch: 25 Idx: 0 Loss: 0.015065621488753959
Epoch: 25 Idx: 5000 Loss: 0.022251151233251023
Epoch: 26 Idx: 0 Loss: 0.05081203955302173
Epoch: 26 Idx: 5000 Loss: 0.024161123775909774
Epoch: 27 Idx: 0 Loss: 0.02068551032992163
Epoch: 27 Idx: 5000 Loss: 0.0159828903816231
Epoch: 28 Idx: 0 Loss: 0.03637536105729396
Epoch: 28 Idx: 5000 Loss: 0.026899016547754517
Epoch: 29 Idx: 0 Loss: 0.009955272550837519
Epoch: 29 Idx: 5000 Loss: 0.03192344669200728
Epoch: 30 Idx: 0 Loss: 0.02700405228220293
Epoch: 30 Idx: 5000 Loss: 0.01511795852368307
Epoch: 31 Idx: 0 Loss: 0.013617583038235398
Epoch: 31 Idx: 5000 Loss: 0.03092972659742021
Epoch: 32 Idx: 0 Loss: 0.027402585969310656
Epoch: 32 Idx: 5000 Loss: 0.019828669395920803
Epoch: 33 Idx: 0 Loss: 0.01924375757471996
Epoch: 33 Idx: 5000 Loss: 0.02337016977102002
Epoch: 34 Idx: 0 Loss: 0.02955958918004237
Epoch: 34 Idx: 5000 Loss: 0.027477049330829912
Epoch: 35 Idx: 0 Loss: 0.010923956821845385
Epoch: 35 Idx: 5000 Loss: 0.014859422629393737
Epoch: 36 Idx: 0 Loss: 0.011670166281092353
Epoch: 36 Idx: 5000 Loss: 0.014430838570892873
Epoch: 37 Idx: 0 Loss: 0.017461273070157945
Epoch: 37 Idx: 5000 Loss: 0.02333120786757901
Epoch: 38 Idx: 0 Loss: 0.017679712654900016
Epoch: 38 Idx: 5000 Loss: 0.020983416597875462
Epoch: 39 Idx: 0 Loss: 0.011266610493921438
Epoch: 39 Idx: 5000 Loss: 0.01671931072533828
Epoch: 40 Idx: 0 Loss: 0.026282987107289872
Epoch: 40 Idx: 5000 Loss: 0.015083625892463193
Epoch: 41 Idx: 0 Loss: 0.022129509089765173
Epoch: 41 Idx: 5000 Loss: 0.02146871591987018
Epoch: 42 Idx: 0 Loss: 0.026767755449944176
Epoch: 42 Idx: 5000 Loss: 0.024912110509151344
Epoch: 43 Idx: 0 Loss: 0.053312820261876115
Epoch: 43 Idx: 5000 Loss: 0.019058929664802313
Epoch: 44 Idx: 0 Loss: 0.02000983887774517
Epoch: 44 Idx: 5000 Loss: 0.023051079765069205
Epoch: 45 Idx: 0 Loss: 0.013643962813625982
Epoch: 45 Idx: 5000 Loss: 0.026183771928643018
Epoch: 46 Idx: 0 Loss: 0.009792372651155611
Epoch: 46 Idx: 5000 Loss: 0.02011688479208429
Epoch: 47 Idx: 0 Loss: 0.014459052695397576
Epoch: 47 Idx: 5000 Loss: 0.015491335897217571
Epoch: 48 Idx: 0 Loss: 0.020961116880449243
Epoch: 48 Idx: 5000 Loss: 0.01869688533095872
Epoch: 49 Idx: 0 Loss: 0.02915828042211191
Epoch: 49 Idx: 5000 Loss: 0.015010623958515226
Len (direct inputs):  90
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
division by zero
divisionTraining size: 127500 ValiTraining size: 127500 Validation size: 22500
Epoch: 0 Idx: 0 Loss: 0.20604956773463728
Epoch: 0 Idx: 5000 Loss: 0.01900045349635833
Epoch: 1 Idx: 0 Loss: 0.009949027222823592
Epoch: 1 Idx: 5000 Loss: 0.012656984195367996
Epoch: 2 Idx: 0 Loss: 0.025056709103365524
Epoch: 2 Idx: 5000 Loss: 0.020807592922629174
Epoch: 3 Idx: 0 Loss: 0.016058849965058977
Epoch: 3 Idx: 5000 Loss: 0.0314761493587679
Epoch: 4 Idx: 0 Loss: 0.015035553482270852
Epoch: 4 Idx: 5000 Loss: 0.03221056397310236
Epoch: 5 Idx: 0 Loss: 0.025212933341272656
Epoch: 5 Idx: 5000 Loss: 0.015866426612888996
Epoch: 6 Idx: 0 Loss: 0.04185883036581371
Epoch: 6 Idx: 5000 Loss: 0.0076280505780193944
Epoch: 7 Idx: 0 Loss: 0.013296736212374587
Epoch: 7 Idx: 5000 Loss: 0.023789629441942343
Epoch: 8 Idx: 0 Loss: 0.013681603460357544
Epoch: 8 Idx: 5000 Loss: 0.015640740859704277
Epoch: 9 Idx: 0 Loss: 0.03101734137412851
Epoch: 9 Idx: 5000 Loss: 0.03245323255493938
Epoch: 10 Idx: 0 Loss: 0.028126899343977763
Epoch: 10 Idx: 5000 Loss: 0.024724451584102615
Epoch: 11 Idx: 0 Loss: 0.043407895155234806
Epoch: 11 Idx: 5000 Loss: 0.01632501624040684
Epoch: 12 Idx: 0 Loss: 0.029980722238542916
Epoch: 12 Idx: 5000 Loss: 0.02594015514590614
Epoch: 13 Idx: 0 Loss: 0.023909175597121052
Epoch: 13 Idx: 5000 Loss: 0.014677118630310052
Epoch: 14 Idx: 0 Loss: 0.03027521112742675
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 397, in <module>
    outputs = model(node_elems, inp_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "Attention_anatomy_aml_weighted.py", line 274, in forward
    node_weights = masked_softmax(node_weights.squeeze(1).reshape(-1, self.n_neighbours, self.max_pathlen)) # batch_size * 4 * max_pathlen
  File "Attention_anatomy_aml_weighted.py", line 219, in masked_softmax
    mask = ((inp != 0).double() - 1) * 9999  # for -inf
KeyboardInterrupt
h: 21 Idx: 5000 Loss: 0.026235741522543276
Epoch: 22 Idx: 0 Loss: 0.0076285495332168605
Epoch: 22 Idx: 5000 Loss: 0.01977114571142689
Epoch: 23 Idx: 0 Loss: 0.025961584003482814
Epoch: 23 Idx: 5000 Loss: 0.013901885866722231
Epoch: 24 Idx: 0 Loss: 0.013541084348764243
Epoch: 24 Idx: 5000 Loss: 0.01924045339959415
Epoch: 25 Idx: 0 Loss: 0.013247555255897447
Epoch: 25 Idx: 5000 Loss: 0.008628511545867157
Epoch: 26 Idx: 0 Loss: 0.02475145615361617
Epoch: 26 Idx: 5000 Loss: 0.022916377257421516
Epoch: 27 Idx: 0 Loss: 0.019862748929598658
Epoch: 27 Idx: 5000 Loss: 0.04493377090072623
Epoch: 28 Idx: 0 Loss: 0.02258560953023408
Epoch: 28 Idx: 5000 Loss: 0.0392069560725012
Epoch: 29 Idx: 0 Loss: 0.011645547019251547
Epoch: 29 Idx: 5000 Loss: 0.03338845001243279
Epoch: 30 Idx: 0 Loss: 0.014964190680547801
Epoch: 30 Idx: 5000 Loss: 0.026723020905754936
Epoch: 31 Idx: 0 Loss: 0.0197221371544902
Epoch: 31 Idx: 5000 Loss: 0.036715992050245956
Epoch: 32 Idx: 0 Loss: 0.021574651330360392
Epoch: 32 Idx: 5000 Loss: 0.010363255432077095
Epoch: 33 Idx: 0 Loss: 0.022175772643533806
Epoch: 33 Idx: 5000 Loss: 0.011518223893063912
Epoch: 34 Idx: 0 Loss: 0.0321887920306353
Epoch: 34 Idx: 5000 Loss: 0.040512315561640116
Epoch: 35 Idx: 0 Loss: 0.02808116835420571
Epoch: 35 Idx: 5000 Loss: 0.018993024051740942
Epoch: 36 Idx: 0 Loss: 0.007427945785244147
Epoch: 36 Idx: 5000 Loss: 0.029098190702623348
Epoch: 37 Idx: 0 Loss: 0.02985109621833218
Epoch: 37 Idx: 5000 Loss: 0.024389090595096804
Epoch: 38 Idx: 0 Loss: 0.024049680264466966
Epoch: 38 Idx: 5000 Loss: 0.0284578753021341
Epoch: 39 Idx: 0 Loss: 0.0254401400988371
Epoch: 39 Idx: 5000 Loss: 0.017943991666652337
Epoch: 40 Idx: 0 Loss: 0.012040025657739041
Epoch: 40 Idx: 5000 Loss: 0.0154780221095571
Traceback (most recent call last):
  File "Attention_anatomy_aml_weighted.py", line 399, in <module>
    loss = F.mse_loss(outputs, targ_elems)
  File "/u/harshitk/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 2190, in mse_loss
    ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
KeyboardInterrupt

------------------------------------------------------------
Sender: LSF System <rer@dccxc215>
Subject: Job 3290138: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc215>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 06:56:11 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 06:56:11 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   19353.32 sec.
    Max Memory :                                 2650 MB
    Average Memory :                             2592.62 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40767.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   19362 sec.
    Turnaround time :                            80682 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc265>
Subject: Job 3290144: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc265>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 07:03:16 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 07:03:16 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18929.22 sec.
    Max Memory :                                 2659 MB
    Average Memory :                             2592.89 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40758.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   18953 sec.
    Turnaround time :                            80682 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc235>
Subject: Job 3290148: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc235>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 07:07:36 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 07:07:36 2020
Terminated at Wed Sep  2 12:18:48 2020
Results reported at Wed Sep  2 12:18:48 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18668.91 sec.
    Max Memory :                                 2646 MB
    Average Memory :                             2579.90 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40771.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   18676 sec.
    Turnaround time :                            80682 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc249>
Subject: Job 3290142: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc249>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 07:00:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 07:00:48 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   19076.03 sec.
    Max Memory :                                 2654 MB
    Average Memory :                             2589.05 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40763.00 MB
    Max Swap :                                   84 MB
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   19106 sec.
    Turnaround time :                            80683 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc226>
Subject: Job 3290152: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:07 2020
Job was executed on host(s) <dccxc226>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 07:09:34 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 07:09:34 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18406.63 sec.
    Max Memory :                                 2644 MB
    Average Memory :                             2581.10 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40773.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   18555 sec.
    Turnaround time :                            80682 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc233>
Subject: Job 3290146: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:06 2020
Job was executed on host(s) <dccxc233>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 07:06:05 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 07:06:05 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18760.67 sec.
    Max Memory :                                 2653 MB
    Average Memory :                             2578.13 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40764.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   18784 sec.
    Turnaround time :                            80683 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <rer@dccxc278>
Subject: Job 3290150: </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> in cluster <dcc> Exited

Job </u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt> was submitted from host <dccxl001> by user <harshitk> in cluster <dcc> at Tue Sep  1 13:54:07 2020
Job was executed on host(s) <dccxc278>, in queue <x86_24h>, as user <harshitk> in cluster <dcc> at Wed Sep  2 07:08:48 2020
</u/harshitk> was used as the home directory.
</dccstor/cogfin/arvind/da/IBM-Internship> was used as the working directory.
Started at Wed Sep  2 07:08:48 2020
Terminated at Wed Sep  2 12:18:49 2020
Results reported at Wed Sep  2 12:18:49 2020

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
/u/harshitk/anaconda2/envs/myenv/bin/python3.6 Attention_anatomy_aml_weighted.py Input/data_anatomy_oaei.pkl 15 3 Output/test_anatomy_aml_wtpath15_3.pkl Models/anatomy_aml_wtpath15_3.pt
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18598.75 sec.
    Max Memory :                                 2647 MB
    Average Memory :                             2573.56 MB
    Total Requested Memory :                     43417.00 MB
    Delta Memory :                               40770.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   18625 sec.
    Turnaround time :                            80682 sec.

The output (if any) is above this job summary.

