{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of dataset\n",
    "\n",
    "import os, itertools, time, pickle, sys, glob, requests\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from orderedset import OrderedSet\n",
    "from copy import deepcopy\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "class Ontology():\n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.ontology_obj = minidom.parse(ontology)\n",
    "        self.root = self.ontology_obj.documentElement\n",
    "        self.construct_mapping_dict()\n",
    "        \n",
    "        self.parents_dict = {}\n",
    "        self.subclasses = self.parse_subclasses()\n",
    "        self.object_properties = self.parse_object_properties()\n",
    "        self.data_properties = self.parse_data_properties()\n",
    "        self.triples = self.parse_triples()\n",
    "        self.classes = self.parse_classes()        \n",
    "    \n",
    "    def construct_mapping_dict(self):\n",
    "        self.mapping_dict = {self.extract_ID(el, False): self.get_child_node(el, \"rdfs:label\")[0].firstChild.nodeValue for el in self.root.getElementsByTagName(\"owl:Class\") if self.get_child_node(el, \"rdfs:label\")}\n",
    "        self.mapping_dict_inv = {self.mapping_dict[key]: key for key in self.mapping_dict}\n",
    "        return\n",
    "        \n",
    "    def get_child_node(self, element, tag):\n",
    "        return [e for e in element._get_childNodes() if type(e)==minidom.Element and e._get_tagName() == tag]\n",
    "        \n",
    "    def has_attribute_value(self, element, attribute, value):\n",
    "        return True if element.getAttribute(attribute).split(\"#\")[-1] == value else False\n",
    "    \n",
    "    def get_subclass_triples(self, rootpath=False):\n",
    "        subclasses = self.get_subclasses(rootpath=False)\n",
    "        return [(b,a,c,d) for (a,b,c,d) in subclasses]\n",
    "    \n",
    "    def parse_triples(self, union_flag=0, subclass_of=True, rootpath=False):\n",
    "        obj_props = [(prop, \"Object Property\") for prop in self.object_properties]\n",
    "        data_props = [(prop, \"Datatype Property\") for prop in self.data_properties]\n",
    "        props = obj_props + data_props\n",
    "        all_triples = []\n",
    "        for prop, prop_type in props:\n",
    "            domain_children = self.get_child_node(prop, \"rdfs:domain\")\n",
    "            range_children = self.get_child_node(prop, \"rdfs:range\")\n",
    "            domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children])\n",
    "            range_prop = self.filter_null([self.extract_ID(el) for el in range_children])\n",
    "            if not domain_children or not range_children:\n",
    "                continue\n",
    "            if not domain_prop:\n",
    "                domain_prop = self.filter_null([self.extract_ID(el) for el in domain_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if not range_prop:\n",
    "                range_prop = self.filter_null([self.extract_ID(el) for el in range_children[0].getElementsByTagName(\"owl:Class\")])\n",
    "            if domain_prop and range_prop:\n",
    "                if union_flag == 0:\n",
    "                    all_triples.extend([(el[0], el[1], self.extract_ID(prop), prop_type) for el in list(itertools.product(domain_prop, range_prop))])\n",
    "                else:\n",
    "                    all_triples.append((\"###\".join(domain_prop), \"###\".join(range_prop), self.extract_ID(prop), prop_type))\n",
    "        if subclass_of:\n",
    "            all_triples.extend(self.get_subclass_triples(rootpath))\n",
    "        return list(set(all_triples))\n",
    "    \n",
    "    def get_triples(self, union_flag=0, subclass_of=True, rootpath=False):\n",
    "        return self.parse_triples(union_flag, subclass_of, rootpath)\n",
    "\n",
    "    def parse_subclasses(self, union_flag=0):\n",
    "        subclasses = self.root.getElementsByTagName(\"rdfs:subClassOf\")\n",
    "        subclass_pairs = []\n",
    "        for el in subclasses:\n",
    "            inline_subclasses = self.extract_ID(el)\n",
    "            if inline_subclasses:\n",
    "                subclass_pairs.append((el, el.parentNode, \"subclass_of\", \"Subclass\"))\n",
    "            else:\n",
    "                level1_class = self.get_child_node(el, \"owl:Class\")\n",
    "                if not level1_class:\n",
    "                    restriction = el.getElementsByTagName(\"owl:Restriction\")\n",
    "                    if not restriction:\n",
    "                        continue\n",
    "                    prop = self.get_child_node(restriction[0], \"owl:onProperty\")\n",
    "                    some_vals = self.get_child_node(restriction[0], \"owl:someValuesFrom\")\n",
    "                    \n",
    "                    if not prop or not some_vals:\n",
    "                        continue\n",
    "#                     print(self.extract_ID(el), \"**\", self.extract_ID(some_vals[0]), \"**\", self.extract_ID(prop[0]))\n",
    "                    try:\n",
    "                        if self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
    "                        elif self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
    "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop[0]), \"Object Property\"))\n",
    "                        elif not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
    "                            subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
    "                        else:\n",
    "                            prop_vals = self.get_child_node(prop[0], \"owl:ObjectProperty\")\n",
    "                            class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                            subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Object Property\"))\n",
    "                    except:\n",
    "                        try:\n",
    "                            if not self.extract_ID(prop[0]) and self.extract_ID(some_vals[0]):\n",
    "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
    "                                subclass_pairs.append((el.parentNode, some_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
    "                            elif not self.extract_ID(prop[0]) and not self.extract_ID(some_vals[0]):\n",
    "                                prop_vals = self.get_child_node(prop[0], \"owl:DatatypeProperty\")\n",
    "                                class_vals = self.get_child_node(some_vals[0], \"owl:Class\")\n",
    "                                subclass_pairs.append((el.parentNode, class_vals[0], self.extract_ID(prop_vals[0]), \"Datatype Property\"))\n",
    "                        except Exception as e:\n",
    "                            print (e)\n",
    "                            continue\n",
    "                else:\n",
    "                    if self.extract_ID(level1_class[0]):\n",
    "                        subclass_pairs.append((level1_class[0], el.parentNode, \"subclass_of\", \"Subclass\"))\n",
    "                    else:\n",
    "#                         level2classes = level1_class[0].getElementsByTagName(\"owl:Class\")\n",
    "#                         subclass_pairs.extend([(elem, el.parentNode, \"subclass_of\", \"Subclass\") for elem in level2classes if self.extract_ID(elem)])\n",
    "                        continue\n",
    "        return subclass_pairs\n",
    "        \n",
    "    def get_subclasses(self, rootpath=False):\n",
    "        subclasses = [(self.extract_ID(a, not rootpath), self.extract_ID(b, not rootpath), c, d) for (a,b,c,d) in self.subclasses]\n",
    "        self.parents_dict = {}\n",
    "        for (a,b,c,d) in subclasses:\n",
    "            if c == \"subclass_of\" and a!=\"Thing\" and b!=\"Thing\":\n",
    "                if b not in self.parents_dict:\n",
    "                    self.parents_dict[b] = [a]\n",
    "                else:\n",
    "                    self.parents_dict[b].append(a)\n",
    "        return [el for el in subclasses if el[0] and el[1] and el[2] and el[0]!=\"Thing\" and el[1]!=\"Thing\"]\n",
    "    \n",
    "    def filter_null(self, data):\n",
    "        return [el for el in data if el]\n",
    "    \n",
    "    def extract_ID(self, element, check_coded = True):\n",
    "        element_id = element.getAttribute(\"rdf:ID\") or element.getAttribute(\"rdf:resource\") or element.getAttribute(\"rdf:about\")\n",
    "        element_id = element_id.split(\"#\")[-1]\n",
    "        if len(list(filter(str.isdigit, element_id))) >= 3 and \"_\" in element_id and check_coded:\n",
    "            return self.mapping_dict[element_id]\n",
    "        return element_id.replace(\"UNDEFINED_\", \"\").replace(\"DO_\", \"\")\n",
    "    \n",
    "    def parse_classes(self):\n",
    "        class_elems = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        subclass_classes = list(set(flatten([el[:-1] for el in self.triples])))\n",
    "        return list(set(self.filter_null(class_elems + subclass_classes)))\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n",
    "    def get_entities(self):\n",
    "        entities = [self.extract_ID(el) for el in self.root.getElementsByTagName(\"owl:Class\")]\n",
    "        return list(set(self.filter_null(entities)))\n",
    "\n",
    "    def parse_data_properties(self):\n",
    "        data_properties = [el for el in self.get_child_node(self.root, 'owl:DatatypeProperty')]\n",
    "        fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_data_properties = [el for el in fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        inv_fn_data_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_data_properties = [el for el in inv_fn_data_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"DatatypeProperty\")]]\n",
    "        return data_properties + fn_data_properties + inv_fn_data_properties\n",
    "        \n",
    "    def parse_object_properties(self):\n",
    "        obj_properties = [el for el in self.get_child_node(self.root, 'owl:ObjectProperty')]\n",
    "        fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:FunctionalProperty') if el]\n",
    "        fn_obj_properties = [el for el in fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        inv_fn_obj_properties = [el for el in self.get_child_node(self.root, 'owl:InverseFunctionalProperty') if el]\n",
    "        inv_fn_obj_properties = [el for el in inv_fn_obj_properties if type(el)==minidom.Element and \n",
    "            [el for el in self.get_child_node(el, \"rdf:type\") if \n",
    "             self.has_attribute_value(el, \"rdf:resource\", \"ObjectProperty\")]]\n",
    "        return obj_properties + fn_obj_properties + inv_fn_obj_properties\n",
    "    \n",
    "    def get_object_properties(self):\n",
    "        obj_props = [self.extract_ID(el) for el in self.object_properties]\n",
    "        return list(set(self.filter_null(obj_props)))\n",
    "    \n",
    "    def get_data_properties(self):\n",
    "        data_props = [self.extract_ID(el) for el in self.data_properties]\n",
    "        return list(set(self.filter_null(data_props)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_folder = \"german_datasets/mapping freizeit/\"\n",
    "\n",
    "ontologies_in_alignment = []\n",
    "\n",
    "# Load reference alignments \n",
    "def load_alignments(folder):\n",
    "    gt = []\n",
    "    path = [folder + l for l in os.listdir(folder) if l.endswith(\".txt\")][0]\n",
    "    mappings = [content.strip() for content in open(path).read().split(\"--------------------------------------------------------\") if content.strip()]\n",
    "    for mapping in mappings:\n",
    "        src = [line.split(\":\")[-1].strip() for line in mapping.split(\"\\n\") if line.startswith(\" + Source: \")][0]\n",
    "        targ = [line.split(\":\")[-1].strip() for line in mapping.split(\"\\n\") if line.startswith(\" + Target: \")][0]\n",
    "        ontologies_in_alignment.append((folder + src, folder + targ))\n",
    "        src = src.rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "        targ = targ.rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "        lines = [[\"_\".join(row.strip().split(\":\")[0].replace(\",-\", \"_\").split(\".\")) for row in line.split(\"-\",1)[1].strip().split(\"<->\")]\n",
    "                 for line in mapping.split(\"\\n\") if line.startswith(\" -\")]\n",
    "        lines = [[src + \"#\" + line[0], targ + \"#\" + line[1]] for line in lines]\n",
    "        gt.extend(lines)\n",
    "    return gt\n",
    "\n",
    "# Extracting USE embeddings\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3?tf-hub-format=compressed\")\n",
    "    embeds = model(words)\n",
    "    return embeds.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "\n",
    "reference_alignments = load_alignments(alignment_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('german_datasets/mapping lebensmittel/Google.Lebensmittel.owl',\n",
       "  'german_datasets/mapping lebensmittel/web.Lebensmittel.owl')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontologies_in_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmoz_freizeit google_freizeit\n"
     ]
    }
   ],
   "source": [
    "# Combinatorial mapping generation\n",
    "all_mappings = []\n",
    "for l in ontologies_in_alignment:\n",
    "    ont1 = Ontology(l[0])\n",
    "    ont2 = Ontology(l[1])\n",
    "    \n",
    "    ent1 = ont1.get_classes()\n",
    "    ent2 = ont2.get_classes()\n",
    "    \n",
    "    obj1 = ont1.get_object_properties()\n",
    "    obj2 = ont2.get_object_properties()\n",
    "    \n",
    "    data1 = ont1.get_data_properties()\n",
    "    data2 = ont2.get_data_properties()\n",
    "\n",
    "    mappings = list(itertools.product(ent1, ent2)) + list(itertools.product(obj1, obj2)) + list(itertools.product(data1, data2))\n",
    "    \n",
    "    pre1 = l[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    pre2 = l[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    print (pre1, pre2)\n",
    "    all_mappings.extend([(pre1 + \"#\" + el[0], pre2 + \"#\" + el[1]) for el in mappings])\n",
    "    \n",
    "\n",
    "data = {mapping: False for mapping in all_mappings}\n",
    "reference_alignments = [tuple(alignment) for alignment in reference_alignments]\n",
    "s = set(all_mappings)\n",
    "for mapping in set(reference_alignments):\n",
    "    if mapping in s:\n",
    "        data[mapping] = True\n",
    "    else:\n",
    "        mapping = tuple([el.replace(\",-\", \"_\") for el in mapping])\n",
    "        if mapping in s:\n",
    "            data[mapping] = True\n",
    "        else:\n",
    "            print (mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies_in_alignment = [('german_datasets_copy/freizeit/dmoz.Freizeit.owl',\n",
    "  'german_datasets_copy/freizeit/Google.Freizeit.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/google.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/web.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl')]\n",
    "filtered_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_mappings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-73c22fcb98f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfinal_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_mappings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mis_abb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[A-Z][A-Z]+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_mappings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "# Abbrevation resolution preprocessing\n",
    "def parse(word):\n",
    "    return \" \".join(flatten([el.split(\"_\") for el in camel_case_split(word)]))\n",
    "\n",
    "abbreviations_dict = {}\n",
    "final_dict = {}\n",
    "\n",
    "for mapping in all_mappings:\n",
    "    mapping = tuple([el.split(\"#\")[1] for el in mapping])\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[0])\n",
    "    if is_abb:\n",
    "        \n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\"_\") if el])\n",
    "        if is_abb.group() in abbreviation:\n",
    "            \n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[1].split(\"_\")[start:end])\n",
    "            print (\"left\", mapping, abbreviation, fullform)\n",
    "            \n",
    "            rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[1].split(\"_\")[:start] + mapping[1].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[1])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\"_\") if el])\n",
    "        \n",
    "        if is_abb.group() in abbreviation:\n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[0].split(\"_\")[start:end])\n",
    "            print (\"right\", mapping, abbreviation, fullform)\n",
    "\n",
    "            rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[0].split(\"_\")[:start] + mapping[0].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings([parse(el) for el in keys])))\n",
    "\n",
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
    "filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of extracted unique classes and properties from entire RA set:  3174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "# Abbrevation resolution preprocessing\n",
    "def parse(word):\n",
    "    return \" \".join(flatten([el.split(\"_\") for el in camel_case_split(word)]))\n",
    "\n",
    "# Extracting USE embeddings\n",
    "\n",
    "def extractUSEEmbeddings(words):\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3?tf-hub-format=compressed\")\n",
    "    embeds = model(words)\n",
    "    return embeds.numpy()\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "extracted_elems = []\n",
    "mapping_ont = {}\n",
    "for ont_name in list(set(flatten(ontologies_in_alignment))):\n",
    "    ont = Ontology(ont_name)\n",
    "    entities = ont.get_classes()\n",
    "    props = ont.get_object_properties() + ont.get_data_properties()\n",
    "    triples = list(set(flatten(ont.get_triples())))\n",
    "    ont_name = ont_name.split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    mapping_ont[ont_name] = ont\n",
    "    extracted_elems.extend([ont_name + \"#\" + elem for elem in entities + props + triples])\n",
    "\n",
    "extracted_elems = list(set(extracted_elems))\n",
    "inp = []\n",
    "for word in extracted_elems:\n",
    "    ont_name = word.split(\"#\")[0]\n",
    "    elem = word.split(\"#\")[1]\n",
    "    inp.append(parse(mapping_ont[ont_name].mapping_dict.get(elem, elem)))\n",
    "\n",
    "print (\"Total number of extracted unique classes and properties from entire RA set: \", len(extracted_elems))\n",
    "\n",
    "extracted_elems = [\"<UNK>\"] + extracted_elems\n",
    "\n",
    "embeds = np.array(extractUSEEmbeddings(inp))\n",
    "embeds = np.array([np.zeros(embeds.shape[1],)] + list(embeds))\n",
    "# embeds = np.array([np.zeros(512,)] + list(extractUSEEmbeddings(inp_spellchecked)))\n",
    "embeddings = dict(zip(extracted_elems, embeds))\n",
    "\n",
    "\n",
    "emb_vals = list(embeddings.values())\n",
    "emb_indexer = {key: i for i, key in enumerate(list(embeddings.keys()))}\n",
    "emb_indexer_inv = {i: key for i, key in enumerate(list(embeddings.keys()))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_datasets_copy/freizeit/dmoz.Freizeit.owl\n",
      "german_datasets_copy/webdirectory/yahoo.small.owl\n",
      "german_datasets_copy/freizeit/Google.Freizeit.owl\n",
      "german_datasets_copy/webdirectory/dmoz.owl\n",
      "german_datasets_copy/webdirectory/google.owl\n",
      "german_datasets_copy/webdirectory/web.owl\n"
     ]
    }
   ],
   "source": [
    "def path_to_root(elem, ont_mappings, curr = [], rootpath=[]):\n",
    "    curr.append(elem)\n",
    "    if elem not in ont_mappings or not ont_mappings[elem]:\n",
    "        rootpath.append(curr)\n",
    "        return\n",
    "    for node in ont_mappings[elem]:\n",
    "        curr_orig = deepcopy(curr)\n",
    "        _ = path_to_root(node, ont_mappings, curr, rootpath)\n",
    "        curr = curr_orig\n",
    "    return rootpath\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    ont_obj = Ontology(ont)\n",
    "    triples = ont_obj.get_triples(rootpath=True)\n",
    "    entities = [(a,b) for (a,b,c,d) in triples]\n",
    "    neighbours_dict = {elem: [[] for i in range(4)] for elem in list(set(flatten(entities)))}\n",
    "    print (ont)\n",
    "    for (e1, e2, p, d) in triples:\n",
    "        if e1==e2:\n",
    "            continue\n",
    "        if d == \"Object Property\":\n",
    "            neighbours_dict[e1][2].append([e2])\n",
    "            neighbours_dict[e2][2].append([e1])\n",
    "        elif d == \"Datatype Property\":\n",
    "            neighbours_dict[e1][3].append([e2])\n",
    "            neighbours_dict[e2][3].append([e1])\n",
    "        elif d == \"Subclass\":\n",
    "            neighbours_dict[e2][1].append([e1])\n",
    "        else:\n",
    "            print (\"Error wrong value of d: \", d)\n",
    "    \n",
    "    rootpath_dict = ont_obj.parents_dict\n",
    "    rootpath_dict_new = {}\n",
    "    for elem in rootpath_dict:\n",
    "#         print (\"Done for \", elem)\n",
    "        rootpath_dict_new[elem] = path_to_root(elem, rootpath_dict, [], [])\n",
    "    ont = ont.split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "\n",
    "    \n",
    "    for entity in neighbours_dict:\n",
    "        if entity in rootpath_dict_new and len(rootpath_dict_new[entity]) > 0:\n",
    "            neighbours_dict[entity][0].extend(rootpath_dict_new[entity])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "#     prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "#     neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "#     for e1, e2, p in prop_triples:\n",
    "#         neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "#     neighbours_dict = {elem: [key[:1] + sorted(list(set(key[1:]))) for key in neighbours_dict[elem]]\n",
    "#                        for elem in neighbours_dict}\n",
    "#     neighbours_dict = {el: neighbours_dict[el][:23] for el in neighbours_dict if len( neighbours_dict[el]) > 2}\n",
    "#     ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "    neighbours_dict = {ont + \"#\" + el: [[tuple([ont + \"#\" + node for node in path]) for path in nbr_type]\n",
    "                                        for nbr_type in neighbours_dict[el]] \n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: [[list(path) for path in nbr_type] for nbr_type in neighbours_dict[el]]\n",
    "                       for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "# neighbours_dicts = {ont.split(\"/\")[-1].split(\".\")[0]: get_one_hop_neighbours(ont) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "neighbours_dicts = {}\n",
    "for ont in list(set(flatten(ontologies_in_alignment))):\n",
    "    neighbours_dicts = {**neighbours_dicts, **get_one_hop_neighbours(ont)}\n",
    "max_types = np.max([len([nbr_type for nbr_type in elem if nbr_type]) for elem in neighbours_dicts.values()])\n",
    "max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "\n",
    "# max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "# max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "# neighbours_dicts_lenpadded = {elem: [[path + [\"<UNK>\" for i in range(max_pathlen -len(path))] for path in nbr_type]\n",
    "#                                 for nbr_type in neighbours_dicts[elem]] for elem in neighbours_dicts}\n",
    "# neighbours_dicts_pathpadded = {elem: [nbr_type + [[\"<UNK>\" for j in range(max_pathlen)] for i in range(max_paths - len(nbr_type))]\n",
    "#                                 for k,nbr_type in enumerate(neighbours_dicts_lenpadded[elem])] for elem in neighbours_dicts_lenpadded}\n",
    "# neighbours_dicts_pathpadded = {elem: np.array(neighbours_dicts_pathpadded[elem]) for elem in neighbours_dicts_pathpadded}\n",
    "# data_items = np.array(list(data.items()))\n",
    "\n",
    "# data_shuffled_t = [elem for elem in data_items if elem[1]]\n",
    "# data_shuffled_f = [elem for elem in data_items if not elem[1]]\n",
    "# np.random.shuffle(data_items)\n",
    "# data_shuffled_f = data_shuffled_f[:150000-len(data_shuffled_t)]\n",
    "\n",
    "# indices = np.random.permutation(len(data_shuffled_t) + len(data_shuffled_f[:130000-len(data_shuffled_t)]))\n",
    "\n",
    "# data_shuffled = data_shuffled_t + data_shuffled_f\n",
    "# indices = np.random.permutation(len(data_shuffled))\n",
    "\n",
    "# indices = np.random.permutation(len(data_shuffled))\n",
    "\n",
    "# data = OrderedDict(data_items)\n",
    "\n",
    "# ontologies_in_alignment_rev = [[el.split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() for el in ont] for ont in ontologies_in_alignment]\n",
    "\n",
    "# f = open(\"Input/data_freizeit.pkl\", \"wb\")\n",
    "# pickle.dump([data, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts], f)\n",
    "# f = open(\"data_aml_uniqpath.pkl\", \"wb\")\n",
    "# pickle.dump([data, aml_data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts_pathpadded, ontologies_in_alignment], f)\n",
    "# # # # # neighbours_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../Input/data_webdir_fre.pkl\", \"wb\")\n",
    "pickle.dump([pickle.load(open(\"Input/data_freizeit.pkl\", \"rb\"))[0],\n",
    "             pickle.load(open(\"../Input/data_webdir.pkl\", \"rb\"))[0],\n",
    "             emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german_datasets_copy/lebensmittel/web.Lebensmittel.owl\n",
      "german_datasets_copy/webdirectory/yahoo.small.owl\n",
      "german_datasets_copy/webdirectory/dmoz.owl\n",
      "german_datasets_copy/lebensmittel/Google.Lebensmittel.owl\n",
      "german_datasets_copy/webdirectory/google.owl\n",
      "german_datasets_copy/webdirectory/web.owl\n"
     ]
    }
   ],
   "source": [
    "def path_to_root(elem, ont_mappings, curr = [], rootpath=[]):\n",
    "    curr.append(elem)\n",
    "    if elem not in ont_mappings or not ont_mappings[elem]:\n",
    "        rootpath.append(curr)\n",
    "        return\n",
    "    for node in ont_mappings[elem]:\n",
    "        curr_orig = deepcopy(curr)\n",
    "        _ = path_to_root(node, ont_mappings, curr, rootpath)\n",
    "        curr = curr_orig\n",
    "    return rootpath\n",
    "\n",
    "def get_one_hop_neighbours(ont, K=1):\n",
    "    ont_obj = Ontology(ont)\n",
    "    triples = ont_obj.get_triples(rootpath=True)\n",
    "    entities = [(a,b) for (a,b,c,d) in triples]\n",
    "    neighbours_dict = {elem: [[] for i in range(4)] for elem in list(set(flatten(entities)))}\n",
    "    print (ont)\n",
    "    for (e1, e2, p, d) in triples:\n",
    "        if e1==e2:\n",
    "            continue\n",
    "        if d == \"Object Property\":\n",
    "            neighbours_dict[e1][2].append(e2)\n",
    "            neighbours_dict[e2][2].append(e1)\n",
    "        elif d == \"Datatype Property\":\n",
    "            neighbours_dict[e1][3].append(e2)\n",
    "            neighbours_dict[e2][3].append(e1)\n",
    "        elif d == \"Subclass\":\n",
    "            neighbours_dict[e2][1].append(e1)\n",
    "        else:\n",
    "            print (\"Error wrong value of d: \", d)\n",
    "    \n",
    "    rootpath_dict = ont_obj.parents_dict\n",
    "    rootpath_dict_new = {}\n",
    "    for elem in rootpath_dict:\n",
    "#         print (\"Done for \", elem)\n",
    "        rootpath_dict_new[elem] = path_to_root(elem, rootpath_dict, [], [])\n",
    "    ont = ont.split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "\n",
    "    for entity in neighbours_dict:\n",
    "        neighbours_dict[entity][1] = [neighbours_dict[entity][1]]\n",
    "        neighbours_dict[entity][2] = [neighbours_dict[entity][2]]\n",
    "        neighbours_dict[entity][3] = [neighbours_dict[entity][3]]\n",
    "        if entity in rootpath_dict_new and len(rootpath_dict_new[entity]) > 0:\n",
    "            neighbours_dict[entity][0].extend(rootpath_dict_new[entity])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "#     prop_triples = ont_obj.get_triples(subclass_of=False)\n",
    "#     neighbours_dict_props = {c: [c] for a,b,c in prop_triples}\n",
    "#     for e1, e2, p in prop_triples:\n",
    "#         neighbours_dict_props[p].extend([e1, e2])\n",
    "\n",
    "    #neighbours_dict = {**neighbours_dict, **neighbours_dict_props}\n",
    "    \n",
    "    # for elem in ont_obj.get_entities() + ont_obj.get_object_properties() + ont_obj.get_data_properties():\n",
    "    #     if elem not in neighbours_dict:\n",
    "    #         neighbours_dict[elem] = [elem]\n",
    "\n",
    "#     neighbours_dict = {elem: [key[:1] + sorted(list(set(key[1:]))) for key in neighbours_dict[elem]]\n",
    "#                        for elem in neighbours_dict}\n",
    "#     neighbours_dict = {el: neighbours_dict[el][:23] for el in neighbours_dict if len( neighbours_dict[el]) > 2}\n",
    "#     ont = ont.split(\"/\")[-1].split(\".\")[0]\n",
    "    neighbours_dict = {ont + \"#\" + el: [[tuple([ont + \"#\" + node for node in path]) for path in nbr_type]\n",
    "                                        for nbr_type in neighbours_dict[el]] \n",
    "                       for el in neighbours_dict}\n",
    "    neighbours_dict = {el: [[list(path) for path in nbr_type] for nbr_type in neighbours_dict[el]]\n",
    "                       for el in neighbours_dict}\n",
    "    return neighbours_dict\n",
    "\n",
    "# neighbours_dicts = {ont.split(\"/\")[-1].split(\".\")[0]: get_one_hop_neighbours(ont) for ont in list(set(flatten(ontologies_in_alignment)))}\n",
    "neighbours_dicts = {}\n",
    "for ont in list(set(flatten(ontologies_in_alignment))):\n",
    "    neighbours_dicts = {**neighbours_dicts, **get_one_hop_neighbours(ont)}\n",
    "max_types = np.max([len([nbr_type for nbr_type in elem if nbr_type]) for elem in neighbours_dicts.values()])\n",
    "max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "\n",
    "# max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts.values()])\n",
    "# max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts.values()]), axis=0)\n",
    "# neighbours_dicts_lenpadded = {elem: [[path + [\"<UNK>\" for i in range(max_pathlen -len(path))] for path in nbr_type]\n",
    "#                                 for nbr_type in neighbours_dicts[elem]] for elem in neighbours_dicts}\n",
    "# neighbours_dicts_pathpadded = {elem: [nbr_type + [[\"<UNK>\" for j in range(max_pathlen)] for i in range(max_paths - len(nbr_type))]\n",
    "#                                 for k,nbr_type in enumerate(neighbours_dicts_lenpadded[elem])] for elem in neighbours_dicts_lenpadded}\n",
    "# neighbours_dicts_pathpadded = {elem: np.array(neighbours_dicts_pathpadded[elem]) for elem in neighbours_dicts_pathpadded}\n",
    "# data_items = np.array(list(data.items()))\n",
    "\n",
    "# data_shuffled_t = [elem for elem in data_items if elem[1]]\n",
    "# data_shuffled_f = [elem for elem in data_items if not elem[1]]\n",
    "# np.random.shuffle(data_items)\n",
    "# data_shuffled_f = data_shuffled_f[:150000-len(data_shuffled_t)]\n",
    "\n",
    "# indices = np.random.permutation(len(data_shuffled_t) + len(data_shuffled_f[:130000-len(data_shuffled_t)]))\n",
    "\n",
    "# data_shuffled = data_shuffled_t + data_shuffled_f\n",
    "# indices = np.random.permutation(len(data_shuffled))\n",
    "\n",
    "# indices = np.random.permutation(len(data_shuffled))\n",
    "\n",
    "# data = OrderedDict(data_items)\n",
    "\n",
    "# ontologies_in_alignment_rev = [[el.split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() for el in ont] for ont in ontologies_in_alignment]\n",
    "\n",
    "# f = open(\"Input/data_freizeit_bagofnbrs.pkl\", \"wb\")\n",
    "# pickle.dump([data, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts], f)\n",
    "# f = open(\"data_aml_uniqpath.pkl\", \"wb\")\n",
    "# pickle.dump([data, aml_data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts_pathpadded, ontologies_in_alignment], f)\n",
    "# # # # # neighbours_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "leb_data = pickle.load(open(\"Input/data_freizeit.pkl\", \"rb\"))[0]\n",
    "data, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts = pickle.load(open(\"Input/data_webdir.pkl\", \"rb\"))\n",
    "pickle.dump([leb_data, data, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts], open(\"Input/data_fre_webdir_bagofnbrs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "leb_data, data, emb_indexer, emb_indexer_inv, emb_vals, _ = pickle.load(open(\"../Input/data_webdir_leb.pkl\", \"rb\"))\n",
    "pickle.dump([leb_data, data, emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts], open(\"../Input/data_webdir_leb_bagofnbrs.pkl\", \"wb\"))\n",
    "# pickle.dump([pickle.load(open(\"Input/data_freizeit.pkl\", \"rb\"))[0],\n",
    "#              pickle.load(open(\"../Input/data_webdir.pkl\", \"rb\"))[0],\n",
    "#              emb_indexer, emb_indexer_inv, emb_vals, neighbours_dicts], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies_in_alignment = [('german_datasets_copy/lebensmittel/Google.Lebensmittel.owl',\n",
    "  'german_datasets_copy/lebensmittel/web.Lebensmittel.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/google.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/dmoz.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/web.owl'),\n",
    " ('german_datasets_copy/webdirectory/google.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl'),\n",
    " ('german_datasets_copy/webdirectory/web.owl',\n",
    "  'german_datasets_copy/webdirectory/yahoo.small.owl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_paths = np.max([[len(nbr_type) for nbr_type in elem] for elem in neighbours_dicts1.values()])\n",
    "max_pathlen = np.max(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in neighbours_dicts1.values()]), axis=0)\n",
    "neighbours_dicts_lenpadded = {elem: [[path + [\"<UNK>\" for i in range(max_pathlen -len(path))] for path in nbr_type]\n",
    "                                for nbr_type in neighbours_dicts1[elem]] for elem in neighbours_dicts1}\n",
    "neighbours_dicts_pathpadded = {elem: [nbr_type + [[\"<UNK>\" for j in range(max_pathlen)] for i in range(max_paths - len(nbr_type))]\n",
    "                                for k,nbr_type in enumerate(neighbours_dicts_lenpadded[elem])] for elem in neighbours_dicts_lenpadded}\n",
    "neighbours_dicts_pathpadded = {elem: np.array(neighbours_dicts_pathpadded[elem]) for elem in neighbours_dicts_pathpadded}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 8658),\n",
       " (1, 3486),\n",
       " (2, 148),\n",
       " (3, 99),\n",
       " (4, 69),\n",
       " (5, 33),\n",
       " (6, 32),\n",
       " (7, 37),\n",
       " (8, 12),\n",
       " (9, 11),\n",
       " (10, 6),\n",
       " (11, 6),\n",
       " (12, 8),\n",
       " (13, 7),\n",
       " (15, 4),\n",
       " (16, 3),\n",
       " (17, 3),\n",
       " (18, 1),\n",
       " (19, 2),\n",
       " (20, 4),\n",
       " (21, 5),\n",
       " (22, 3),\n",
       " (23, 1),\n",
       " (24, 2),\n",
       " (26, 3),\n",
       " (38, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (48, 1),\n",
       " (49, 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_paths\n",
    "sorted(Counter(flatten([[len(nbr_type) for nbr_type in elem] for elem in pickle.load(open(\"../Input/data_webdir_fre.pkl\", \"rb\"))[-1].values()])).items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3156),\n",
       " (2, 25),\n",
       " (3, 426),\n",
       " (4, 1243),\n",
       " (5, 1054),\n",
       " (6, 329),\n",
       " (7, 53),\n",
       " (8, 26)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(Counter(flatten([flatten([[len(path) for path in nbr_type] for nbr_type in elem]) for elem in pickle.load(open(\"../Input/data_webdir_fre.pkl\", \"rb\"))[-1].values()])).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "name_embedding = nn.Embedding(len(emb_vals), embedding_dim)\n",
    "name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "name_embedding.weight.requires_grad = False\n",
    "\n",
    "cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "output = nn.Linear(2*embedding_dim, 300)\n",
    "max_pathlen = np.array(list(neighbours_dicts_pathpadded.values())).shape[3]\n",
    "v = nn.Parameter(torch.DoubleTensor([1/(max_pathlen) for i in range(max_pathlen)]))\n",
    "\n",
    "nodes = torch.randint(0, len(emb_vals), size=(10,2))\n",
    "features= torch.randint(0, len(emb_vals), size=(10,2,4,22,6))\n",
    "\n",
    "nodes = nodes.permute(1,0) # 2 * batch_size\n",
    "features = features.permute(1,0,2,3,4) # 2 * batch_size * 4 * max_paths * max_pathlen\n",
    "for i in range(2):\n",
    "    node_emb = name_embedding(nodes[i]) # batch_size * 512\n",
    "    feature_emb = name_embedding(features[i]) #  batch_size * 4 * max_paths * max_pathlen * 512\n",
    "\n",
    "    path_weights = torch.sum(torch.sum(node_emb[:, None, None, None, :] * feature_emb, dim=-1), dim=-1)\n",
    "    best_path_indices = torch.max(path_weights, dim=2)[1][(..., ) + (None, ) * 3]\n",
    "    best_path_indices = best_path_indices.expand(-1, -1, -1, max_pathlen,  embedding_dim)\n",
    "    best_path = torch.gather(feature_emb, 2, best_path_indices).squeeze(2) # batch_size * 4 * max_pathlen * 512\n",
    "    # Another way: \n",
    "    # path_weights = masked_softmax(path_weights)\n",
    "    # best_path = torch.sum(path_weights.unsqueeze(-1) * feature_emb, dim=2)\n",
    "\n",
    "    node_weights = torch.sum(node_emb[:, None, None, :] * best_path, dim=-1).unsqueeze(-1)\n",
    "    torch.matmul(v, node_weights * best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "a = torch.randn((10,4,22,6,512))\n",
    "b = torch.randn((10,4,22,6,512))\n",
    "\n",
    "attended_path = torch.bmm(a.reshape(-1, 1, 512), b.reshape(-1, 512, 1))\n",
    "attended_path = attended_path.reshape(-1, 4, 22, 6)\n",
    "path_weights = masked_softmax(torch.sum(attended_path, dim=-1))\n",
    "path_weights.shape\n",
    "# best_path = torch.sum(path_weights[:, :, :, None, None] * a, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = d[(..., ) + (None, ) * 3].expand(-1, -1, -1, 6, 512)\n",
    "e = torch.gather(b, 2, indices).squeeze(2)\n",
    "\n",
    "f = torch.sum(a[:,None,None,:] *e,dim=-1).unsqueeze(-1)\n",
    "\n",
    "g = (f*e)\n",
    "\n",
    "h = torch.sum((v[None,None,:,None] * g), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn((10,4,22,6,512))\n",
    "b = torch.randn((10,4,1,6,512))\n",
    "d = b * c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = torch.bmm(c.permute(0,1,3,4,2).reshape(-1, 22, 1), b.permute(0,1,3,4,2).reshape(-1, 1, 1)).squeeze(-1).reshape(-1,4,6,512,22).permute(0,1,4,2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3318178"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctimes, dtimes = [], []\n",
    "for i in range(100):\n",
    "    a = torch.randn((10,4,22,6,512))\n",
    "    b = torch.randn((10,4,1,6,512))\n",
    "    t = time.time()\n",
    "    c = a * b\n",
    "    ctimes.append(time.time()-t)\n",
    "    t = time.time()\n",
    "    d = torch.bmm(c.permute(0,1,3,4,2).reshape(-1, 22, 1), b.permute(0,1,3,4,2).reshape(-1, 1, 1)).squeeze(-1).reshape(-1,4,6,512,22).permute(0,1,4,2,3)\n",
    "    dtimes.append(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((10,4,22,6,512))\n",
    "b = torch.randn((10,4,22,6,512))\n",
    "c = a * torch.sum(b,dim=2).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-edec9b890059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                      \u001b[0;34m\" file:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data/Vivek/IBM/IBM-Internship/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/ false\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_command\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mpred_logmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/logmap2_mappings.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arr = [('german_datasets_copy/lebensmittel/Google.Lebensmittel.owl',\n",
    "  'german_datasets_copy/lebensmittel/web.Lebensmittel.owl')]\n",
    "for ont_pair in arr:\n",
    "    a, b, c = ont_pair[0], ont_pair[1], ont_pair[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"-\" + ont_pair[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "    !rm -rf $c\n",
    "    os.mkdir(c)\n",
    "    java_command = \"java -jar logmap-matcher/target/logmap-matcher-4.0.jar MATCHER file:\" +  os.path.abspath(a) + \\\n",
    "                     \" file:\" + os.path.abspath(b) + \" \" + \"/data/Vivek/IBM/IBM-Internship/\" + c + \"/ false\"\n",
    "    process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "pred_logmap = [[el.split(\"/\")[-1] for el in l.split(\"\\t\")[:-1]] for l in open(os.path.abspath(c + \"/\") + \"/logmap2_mappings.tsv\",  \"r\").read().split(\"\\n\")[:-1] if not l.startswith(\"Optional\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mappings = []\n",
    "for elem in pred_logmap:\n",
    "    gt_mappings.append(tuple([el.split(\"#\")[0].replace(\".v2\", \"\").rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"#\" + el.split(\"#\")[1] for el in elem]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = pickle.load(open(\"../Input/data_lebensmittel.pkl\", \"rb\"))[0]\n",
    "data_logmap = {}\n",
    "for key in data_orig:\n",
    "    data_logmap[key] = False\n",
    "s = set(list(data_logmap.keys()))\n",
    "gt_mappings = [tuple(pair) for pair in gt_mappings]\n",
    "for mapping in gt_mappings:\n",
    "    \n",
    "    if mapping in s:\n",
    "        data_logmap[mapping] = True\n",
    "    else:\n",
    "        mapping = tuple([el.replace(\",-\", \"_\") for el in mapping])\n",
    "        if mapping in s:\n",
    "            data_logmap[mapping] = True\n",
    "        else:\n",
    "            print (mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "def return_test_data(data, i):\n",
    "    data_t = {elem: data[elem] for elem in data if data[elem]}\n",
    "    data_f = {elem: data[elem] for elem in data if not data[elem]}\n",
    "\n",
    "    data_t_items = list(data_t.keys())\n",
    "    data_f_items = list(data_f.keys())\n",
    "\n",
    "    test_data_t = data_t_items[int((0.2*i)*len(data_t)):int((0.2*i + 0.2)*len(data_t))]\n",
    "    test_data_f = data_f_items[int((0.2*i)*len(data_f)):int((0.2*i + 0.2)*len(data_f))]\n",
    "    \n",
    "    test_data = {}\n",
    "    for elem in test_data_t:\n",
    "        test_data[elem] = True\n",
    "    for elem in test_data_f:\n",
    "        test_data[elem] = False\n",
    "    return test_data\n",
    "\n",
    "for i in range(5):\n",
    "    test_gt = return_test_data(data_orig, i)\n",
    "    test_logmap = {elem: data_logmap[elem] for elem in test_gt}\n",
    "    tp = len([elem for elem in test_gt if test_gt[elem] and test_logmap[elem]])\n",
    "    fp = len([elem for elem in test_logmap if not test_gt[elem] and test_logmap[elem]])\n",
    "    fn = len([elem for elem in test_logmap if test_gt[elem] and not test_logmap[elem]])\n",
    "    \n",
    "    try:\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1score = 2 * precision * recall / (precision + recall)\n",
    "        f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "        f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        continue\n",
    "    all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_metrics, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AML test\n",
    "def is_test(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) in test_onto\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in list(range(0, ontologies_in_alignment, 3)):\n",
    "    test_onto = all_ont_pairs[i:i+3]\n",
    "    for ont_pair in test_onto:\n",
    "        print (ont_pair)\n",
    "        a, b, c = ont_pair[0], ont_pair[1], ont_pair[0].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower() + \"-\" + ont_pair[1].split(\"/\")[-1].rsplit(\".\",1)[0].replace(\".\", \"_\").lower()\n",
    "        java_command = \"java -jar logmap-matcher/target/logmap-matcher-4.0.jar MATCHER file:\" +  os.path.abspath(a)\n",
    "                     \" file:\" + os.path.abspath(b) + \" \" + \"/data/Vivek/IBM/IBM-Internship/\" + c + \"/ false\"\n",
    "        process = subprocess.Popen(java_command.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "    print (os.listdir(\"AML-test-results/\"))\n",
    "    pred_aml = load_alignments(\"AML-test-results/\")\n",
    "    pred_aml = [tuple([el.split(\"/\")[-1] for el in key]) for key in pred_aml]\n",
    "    tp = len([elem for elem in pred_aml if data[elem]])\n",
    "    fn = len([key for key in gt_mappings if key not in set(pred_aml) and is_test(test_onto, key)])\n",
    "    fp = len([elem for elem in pred_aml if not data[elem]])\n",
    "\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1score = 2 * precision * recall / (precision + recall)\n",
    "    f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "    f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "    print (precision, recall, f1score, f2score, f0_5score)\n",
    "    \n",
    "    metrics = [precision, recall, f1score, f2score, f0_5score]\n",
    "    results.append(metrics)\n",
    "    \n",
    "    _ = [os.remove(f) for f in glob.glob('AML-test-results/*')]\n",
    "    \n",
    "print (\"Final Results:\", np.mean(results, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies_in_alignment = [[el.split(\"/\")[1].split(\".\")[0] for el in ont] for ont in ontologies_in_alignment][:-1] + [[\"human\", \"mouse\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75201952"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = \"\"\"Results/Output_att1_12_twostep_freizeit_bagofnbrs.txt:Final Results: [0.542      0.92417582 0.68105967 0.80767428 0.58992116]\n",
    "Results/Output_att1_12_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.52598246 0.92417582 0.66882445 0.80097117 0.57492009]\n",
    "Results/Output_att1_1_twostep_freizeit_bagofnbrs.txt:Final Results: [0.57167043 0.92417582 0.70512605 0.82142405 0.61834253]\n",
    "Results/Output_att1_1_twostep_freizeit.txt:Final Results: [0.50311111 0.92417582 0.64688563 0.78647655 0.55167552]\n",
    "Results/Output_att1_1_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.51432553 0.92417582 0.65897912 0.79496616 0.56364267]\n",
    "Results/Output_att1_1_twostep_freizeit_weighted.txt:Final Results: [0.57963956 0.92417582 0.71082327 0.82437393 0.62563432]\n",
    "Results/Output_att12_1_twostep_freizeit.txt:Final Results: [0.53903704 0.90879121 0.67388022 0.79623202 0.58559036]\n",
    "Results/Output_att12_1_twostep_freizeit_weighted.txt:Final Results: [0.51465517 0.90879121 0.65373548 0.78442191 0.5621159 ]\n",
    "Results/Output_att12_3_twostep_freizeit.txt:Final Results: [0.56498489 0.89340659 0.69039897 0.79861746 0.60900172]\n",
    "Results/Output_att12_3_twostep_freizeit_weighted.txt:Final Results: [0.57821282 0.90879121 0.70505182 0.81391719 0.62280816]\n",
    "Results/Output_att12_4_twostep_freizeit.txt:Final Results: [0.54722222 0.90879121 0.68170157 0.80130952 0.59391393]\n",
    "Results/Output_att12_4_twostep_freizeit_weighted.txt:Final Results: [0.57501783 0.89340659 0.69811195 0.8028339  0.61842965]\n",
    "Results/Output_att12_6_twostep_freizeit.txt:Final Results: [0.59621212 0.92417582 0.72157902 0.82952344 0.64027935]\n",
    "Results/Output_att1_3_twostep_freizeit_bagofnbrs.txt:Final Results: [0.51762963 0.90879121 0.65683667 0.78660887 0.56523113]\n",
    "Results/Output_att1_3_twostep_freizeit.txt:Final Results: [0.51785549 0.92417582 0.66100113 0.79578016 0.56665496]\n",
    "Results/Output_att1_3_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.56383437 0.92417582 0.69866489 0.81771726 0.61078083]\n",
    "Results/Output_att1_3_twostep_freizeit_weighted.txt:Final Results: [0.59771645 0.89340659 0.71446924 0.81138126 0.63924466]\n",
    "Results/Output_att1_4_twostep_freizeit_bagofnbrs.txt:Final Results: [0.59218285 0.89340659 0.71026503 0.80910927 0.63406523]\n",
    "Results/Output_att1_4_twostep_freizeit.txt:Final Results: [0.60666667 0.90879121 0.72682751 0.82572581 0.64952195]\n",
    "Results/Output_att1_4_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.64106194 0.89340659 0.74530104 0.82717047 0.67887095]\n",
    "Results/Output_att1_4_twostep_freizeit_weighted.txt:Final Results: [0.50324302 0.92417582 0.64684922 0.78639771 0.55174072]\n",
    "Results/Output_att1_5_twostep_freizeit_bagofnbrs.txt:Final Results: [0.51136654 0.92417582 0.65472039 0.79171294 0.56000765]\n",
    "Results/Output_att1_5_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.50180057 0.92417582 0.64869407 0.78905705 0.5515873 ]\n",
    "Results/Output_att1_6_twostep_freizeit.txt:Final Results: [0.5372932  0.92417582 0.67706093 0.80527473 0.58534458]\n",
    "Results/Output_att1_6_twostep_freizeit_weighted.txt:Final Results: [0.58917749 0.90879121 0.71409174 0.8190805  0.63339739]\n",
    "Results/Output_att1_7_twostep_freizeit_bagofnbrs.txt:Final Results: [0.63929825 0.87912088 0.73849312 0.81627242 0.67530232]\n",
    "Results/Output_att1_7_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.55472464 0.90879121 0.6874599  0.8044529  0.60095983]\n",
    "Results/Output_att1_9_twostep_freizeit_bagofnbrs.txt:Final Results: [0.49393268 0.92417582 0.63924518 0.78203409 0.54280113]\n",
    "Results/Output_att1_9_twostep_freizeit_weighted_bagofnbrs.txt:Final Results: [0.50063406 0.92417582 0.64672295 0.78739794 0.55005   ]\n",
    "Results/Output_att2_1_twostep_freizeit.txt:Final Results: [0.55458227 0.92417582 0.68991939 0.81220306 0.60137083]\n",
    "Results/Output_att2_1_twostep_freizeit_weighted.txt:Final Results: [0.62138471 0.89340659 0.73157708 0.82025003 0.66100973]\n",
    "Results/Output_att2_3_twostep_freizeit.txt:Final Results: [0.65912281 0.87912088 0.75201952 0.82297985 0.69315286]\n",
    "Results/Output_att2_3_twostep_freizeit_weighted.txt:Final Results: [0.61805138 0.87912088 0.72451826 0.80948448 0.65645046]\n",
    "Results/Output_att2_4_twostep_freizeit.txt:Final Results: [0.56930171 0.90879121 0.69887131 0.81078803 0.61474568]\n",
    "Results/Output_att2_4_twostep_freizeit_weighted.txt:Final Results: [0.51536866 0.92417582 0.65973197 0.79540603 0.56458926]\n",
    "Results/Output_att2_6_twostep_freizeit.txt:Final Results: [0.54995969 0.92417582 0.68626566 0.81015788 0.59698432]\n",
    "Results/Output_att2_6_twostep_freizeit_weighted.txt:Final Results: [0.49077833 0.90879121 0.63467649 0.77331847 0.5394674 ]\n",
    "Results/Output_att3_1_twostep_freizeit.txt:Final Results: [0.56818182 0.92417582 0.70220713 0.8197619  0.6149478 ]\n",
    "Results/Output_att3_1_twostep_freizeit_weighted.txt:Final Results: [0.54965217 0.92417582 0.68749242 0.8114838  0.59733793]\n",
    "Results/Output_att3_3_twostep_freizeit.txt:Final Results: [0.55272288 0.92417582 0.6888738  0.81166064 0.59983564]\n",
    "Results/Output_att3_3_twostep_freizeit_weighted.txt:Final Results: [0.65444444 0.86373626 0.74355178 0.81090741 0.68720442]\n",
    "Results/Output_att3_4_twostep_freizeit.txt:Final Results: [0.51032719 0.92417582 0.65545176 0.79292867 0.55966429]\n",
    "Results/Output_att3_4_twostep_freizeit_weighted.txt:Final Results: [0.54702093 0.90879121 0.68095584 0.80058558 0.59348212]\n",
    "Results/Output_att3_6_twostep_freizeit.txt:Final Results: [0.52189719 0.90879121 0.66030046 0.78858689 0.56931986]\n",
    "Results/Output_att3_6_twostep_freizeit_weighted.txt:Final Results: [0.50337401 0.92417582 0.64948385 0.78917379 0.55291471]\n",
    "Results/Output_att4_1_twostep_freizeit.txt:Final Results: [0.54231079 0.92417582 0.68128865 0.80779192 0.59020972]\n",
    "Results/Output_att4_1_twostep_freizeit_weighted.txt:Final Results: [0.57839452 0.90879121 0.70428019 0.81306414 0.62258793]\n",
    "Results/Output_att4_3_twostep_freizeit.txt:Final Results: [0.63616332 0.87912088 0.73730681 0.81600164 0.67296192]\n",
    "Results/Output_att4_3_twostep_freizeit_weighted.txt:Final Results: [0.50699634 0.92417582 0.65271665 0.79122889 0.55649157]\n",
    "Results/Output_att4_4_twostep_freizeit.txt:Final Results: [0.60350649 0.92417582 0.72944188 0.83468686 0.64817588]\n",
    "Results/Output_att4_4_twostep_freizeit_weighted.txt:Final Results: [0.4919886  0.92417582 0.63970119 0.78322349 0.5418044 ]\n",
    "Results/Output_att4_6_twostep_freizeit.txt:Final Results: [0.52532194 0.90879121 0.66317174 0.79029258 0.57263394]\n",
    "Results/Output_att4_6_twostep_freizeit_weighted.txt:Final Results: [0.45627755 0.92417582 0.60909481 0.76452856 0.50701552]\n",
    "Results/Output_att7_1_twostep_freizeit.txt:Final Results: [0.57366836 0.90879121 0.70226849 0.81266203 0.61886689]\n",
    "Results/Output_att7_1_twostep_freizeit_weighted.txt:Final Results: [0.55151515 0.90879121 0.68448941 0.80256892 0.59773504]\n",
    "Results/Output_att7_3_twostep_freizeit.txt:Final Results: [0.54858919 0.90879121 0.6804129  0.79946531 0.59420015]\n",
    "Results/Output_att7_3_twostep_freizeit_weighted.txt:Final Results: [0.50021612 0.90879121 0.64397246 0.77975426 0.54911349]\n",
    "Results/Output_att7_4_twostep_freizeit.txt:Final Results: [0.49703704 0.92417582 0.64401462 0.78582569 0.5467229 ]\n",
    "Results/Output_att7_4_twostep_freizeit_weighted.txt:Final Results: [0.57362836 0.89340659 0.6956005  0.80091119 0.61645573]\n",
    "Results/Output_att7_6_twostep_freizeit.txt:Final Results: [0.51706394 0.89340659 0.65134503 0.77610459 0.56305571]\n",
    "Results/Output_att7_6_twostep_freizeit_weighted.txt:Final Results: [0.57698599 0.90879121 0.70358974 0.81288102 0.62143001]\"\"\"\n",
    "\n",
    "max([float(l.split(\":\")[2].strip().split()[2]) for l in output.split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data_unhas.pkl\", \"wb\")\n",
    "pickle.dump([data, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, ontologies_in_alignment], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:int(sys.argv[1])] for el in neighbours_dicts[ont]\n",
    "       if count_non_unk(neighbours_dicts[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add1(elem):\n",
    "    return elem + 1\n",
    "\n",
    "v = np.random.random((3,2,3))\n",
    "v, np.vectorize(add1)([[1,2,3],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://montanaflynn-spellcheck.p.rapidapi.com/check/\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"montanaflynn-spellcheck.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"9965b01207msh06291e57d6f2c55p1a6a16jsn0fb016da4a62\"\n",
    "    }\n",
    "\n",
    "# inp_spellchecked = []\n",
    "for concept in inp[731:]:\n",
    "    querystring = {\"text\": concept}\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=querystring).json()\n",
    "    if response[\"suggestion\"] != concept:\n",
    "        resolved = str(concept)\n",
    "        for word in response[\"corrections\"]:\n",
    "            if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "                resolved = resolved.replace(word, response[\"corrections\"][word][0])\n",
    "        \n",
    "        inp_spellchecked.append(resolved)\n",
    "        print (concept, resolved)\n",
    "    else:\n",
    "        inp_spellchecked.append(concept)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "querystring = {\"text\": \"technically Organised By\"}\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_spellchecked[730], inp[731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_spellchecked, fp_spellchecked = [dict(el) for el in pickle.load(open(\"test_v2.pkl\", \"rb\"))]\n",
    "fn_baseline, fp_baseline = [dict(el) for el in pickle.load(open(\"test_best.pkl\", \"rb\"))]\n",
    "fn_unhas, fp_unhas = [dict(el) for el in pickle.load(open(\"test_unhas.pkl\", \"rb\"))]\n",
    "fn_resolved, fp_resolved = [dict(el) for el in pickle.load(open(\"test_resolved.pkl\", \"rb\"))]\n",
    "\n",
    "fn_dict, fp_dict = {}, {}\n",
    "def create_comparison_file(file, idx):\n",
    "    fn, fp = [dict(el) for el in pickle.load(open(file, \"rb\"))]\n",
    "    \n",
    "    for key in fn:\n",
    "        if key in fn_dict:\n",
    "            fn_dict[key][idx] = fn[key]\n",
    "        else:\n",
    "            fn_dict[key] = [\"N/A\" for i in range(4)]\n",
    "            fn_dict[key][idx] = fn[key]\n",
    "    \n",
    "    for key in fp:\n",
    "        if key in fp_dict:\n",
    "            fp_dict[key][idx] = fp[key]\n",
    "        else:\n",
    "            fp_dict[key] = [\"N/A\" for i in range(4)]\n",
    "            fp_dict[key][idx] = fp[key]\n",
    "    \n",
    "\n",
    "create_comparison_file(\"test_best.pkl\", 0)\n",
    "create_comparison_file(\"test_unhas.pkl\", 1)\n",
    "create_comparison_file(\"test_v2.pkl\", 2)\n",
    "create_comparison_file(\"test_resolved.pkl\", 3)\n",
    "\n",
    "open(\"fn - comparison.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(el) for el in flatten(el)]) for el in fn_dict.items()]))\n",
    "open(\"fp - comparison.tsv\", \"w+\").write(\"\\n\".join([\"\\t\".join([str(el) for el in flatten(el)]) for el in fp_dict.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontologies_in_alignment = pickle.load(open(\"data_path.pkl\", \"rb\"))[-1]\n",
    "ontologies_in_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {('confOf#Organization', 'sigkdd#Organizator'): (1,2,3,4),\n",
    " ('iasted#Document', 'sigkdd#Document'): (5,6,78,8)}\n",
    "[[str(el) for el in flatten(el)] for el in d.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abbreviations_dict = {}\n",
    "final_dict = {}\n",
    "\n",
    "for mapping in all_mappings:\n",
    "    mapping = tuple([el.split(\"#\")[1] for el in mapping])\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[0])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[1].split(\"_\")])\n",
    "        if is_abb.group() in abbreviation:\n",
    "            \n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[1].split(\"_\")[start:end])\n",
    "            print (\"left\", mapping, abbreviation, fullform)\n",
    "            \n",
    "            rest_first = \" \".join([el for el in mapping[0].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[1].split(\"_\")[:start] + mapping[1].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "    is_abb = re.search(\"[A-Z][A-Z]+\", mapping[1])\n",
    "    if is_abb:\n",
    "        abbreviation = \"\".join([el[0].upper() for el in mapping[0].split(\"_\")])\n",
    "        \n",
    "        if is_abb.group() in abbreviation:\n",
    "            start = abbreviation.find(is_abb.group())\n",
    "            end = start + len(is_abb.group())\n",
    "            fullform = \"_\".join(mapping[0].split(\"_\")[start:end])\n",
    "            print (\"right\", mapping, abbreviation, fullform)\n",
    "\n",
    "            rest_first = \" \".join([el for el in mapping[1].replace(is_abb.group(), \"\").split(\"_\") if el]).lower()\n",
    "            rest_second = \" \".join(mapping[0].split(\"_\")[:start] + mapping[0].split(\"_\")[end:])\n",
    "            if is_abb.group() not in final_dict:\n",
    "                final_dict[is_abb.group()] = [(fullform, rest_first, rest_second)]\n",
    "            else:\n",
    "                final_dict[is_abb.group()].append((fullform, rest_first, rest_second))\n",
    "\n",
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n",
    "\n",
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n",
    "\n",
    "resolved_dict = {key: scored_dict[key][0] for key in scored_dict}\n",
    "filtered_dict = {key: \" \".join(resolved_dict[key][0].split(\"_\")) for key in resolved_dict if resolved_dict[key][-1] > 0.9}\n",
    "inp_resolved = []\n",
    "for concept in inp:\n",
    "    for key in filtered_dict:\n",
    "        concept = concept.replace(key, filtered_dict[key])\n",
    "    inp_resolved.append(concept)\n",
    "inp_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [el for el in list(set(flatten([flatten([tup[1:] for tup in final_dict[key]]) for key in final_dict]))) if el]\n",
    "abb_embeds = dict(zip(keys, extractUSEEmbeddings(keys)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(*extractUSEEmbeddings([\"Conference Banquet\", \"Dinner Banquet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_dict = {}\n",
    "for abbr in final_dict:\n",
    "    sim_list = [(tup[0], tup[1], tup[2], cos_sim(abb_embeds[tup[1]], abb_embeds[tup[2]])) if tup[1] and tup[2]\n",
    "                else (tup[0], tup[1], tup[2], 0) for tup in final_dict[abbr]]\n",
    "    scored_dict[abbr] = sorted(list(set(sim_list)), key=lambda x:x[-1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_case_handled = []\n",
    "for concept in inp:\n",
    "    final_list = []\n",
    "    for word in concept.split(\" \"):\n",
    "        if not re.search(\"[A-Z][A-Z]+\", concept):\n",
    "            final_list.append(word.lower())\n",
    "        else:\n",
    "            final_list.append(word)\n",
    "    case_resolved = \" \".join(final_list)\n",
    "    inp_case_handled.append(case_resolved)\n",
    "    \n",
    "inp_case_handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ontology(\"conference_ontologies/conference.owl\").triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
