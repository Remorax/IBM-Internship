{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [\"main\", 34, 1, \"data_prop.pkl\", \"test.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neighbours: 34\n",
      "Number of entities: 98688\n",
      "Training size: 87849 Testing size: 8977\n",
      "Training size: 21909 Testing size: 2296\n",
      "Properties with neighbours:  41694 Total:  43732\n",
      "Epoch: 0 Idx: 0 Loss: 0.20326958330792028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-67ca05305290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_elems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, itertools, time, pickle, operator\n",
    "import subprocess\n",
    "from xml.dom import minidom\n",
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re, sys\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from math import ceil, exp\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "f = open(sys.argv[3], \"rb\")\n",
    "data_ent, data_prop, emb_indexer, emb_indexer_inv, emb_vals, gt_mappings, neighbours_dicts, neighbours_dicts_prop, ontologies_in_alignment = pickle.load(f)\n",
    "ontologies_in_alignment = [tuple(pair) for pair in ontologies_in_alignment]\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "direct_inputs, direct_targets = [], []\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return 1 - spatial.distance.cosine(a,b)\n",
    "\n",
    "all_fn, all_fp = [], []\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "def test():\n",
    "    global batch_size, test_data_t, test_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()    \n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(test_data_t)\n",
    "        np.random.shuffle(test_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(test_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(test_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        inputs_pos_prop, targets_pos_prop = generate_input(test_data_prop_t, 1, True)\n",
    "        inputs_neg_prop, targets_neg_prop = generate_input(test_data_prop_f, 0, True)\n",
    "        \n",
    "        inputs_all_prop = list(inputs_pos_prop) + list(inputs_neg_prop)\n",
    "        targets_all_prop = list(targets_pos_prop) + list(targets_neg_prop)\n",
    "        \n",
    "        indices_all_prop = np.random.permutation(len(inputs_all_prop))\n",
    "        inputs_all_prop = np.array(inputs_all_prop)[indices_all_prop]\n",
    "        targets_all_prop = np.array(targets_all_prop)[indices_all_prop]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        \n",
    "        batch_size_prop = int(ceil(len(inputs_all_prop)/num_batches))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            batch_start_prop = batch_idx * batch_size_prop\n",
    "            batch_end_prop = (batch_idx+1) * batch_size_prop\n",
    "            \n",
    "            inputs_prop = inputs_all_prop[batch_start_prop: batch_end_prop]\n",
    "            targets_prop = targets_all_prop[batch_start_prop: batch_end_prop]\n",
    "\n",
    "            targets = list(targets) + list(targets_prop)\n",
    "\n",
    "            inp = np.array(list(inputs) + list(np.array(inputs_prop)[:,:,0,:])).transpose(1,0,2)\n",
    "\n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            inp_prop_elems = torch.LongTensor(inputs_prop).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            \n",
    "            outputs = model(inp_elems, inp_prop_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inp[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inp[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "    return (test_onto, all_results)\n",
    "\n",
    "def optimize_threshold():\n",
    "    global batch_size, val_data_t, val_data_prop_t, val_data_prop_f, val_data_f, model, optimizer, emb_indexer_inv, gt_mappings, all_metrics, direct_inputs, direct_targets, threshold_results\n",
    "    all_results = OrderedDict()\n",
    "    direct_inputs, direct_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        all_pred = []\n",
    "        \n",
    "        np.random.shuffle(val_data_t)\n",
    "        np.random.shuffle(val_data_f)\n",
    "\n",
    "        inputs_pos, targets_pos = generate_input(val_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(val_data_f, 0)\n",
    "\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        inputs_pos_prop, targets_pos_prop = generate_input(val_data_prop_t, 1, True)\n",
    "        inputs_neg_prop, targets_neg_prop = generate_input(val_data_prop_f, 0, True)\n",
    "        \n",
    "        inputs_all_prop = list(inputs_pos_prop) + list(inputs_neg_prop)\n",
    "        targets_all_prop = list(targets_pos_prop) + list(targets_neg_prop)\n",
    "        \n",
    "        indices_all_prop = np.random.permutation(len(inputs_all_prop))\n",
    "        inputs_all_prop = np.array(inputs_all_prop)[indices_all_prop]\n",
    "        targets_all_prop = np.array(targets_all_prop)[indices_all_prop]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "        \n",
    "        batch_size_prop = int(ceil(len(inputs_all_prop)/num_batches))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "\n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            batch_start_prop = batch_idx * batch_size_prop\n",
    "            batch_end_prop = (batch_idx+1) * batch_size_prop\n",
    "            \n",
    "            inputs_prop = inputs_all_prop[batch_start_prop: batch_end_prop]\n",
    "            targets_prop = targets_all_prop[batch_start_prop: batch_end_prop]\n",
    "\n",
    "            targets = list(targets) + list(targets_prop)\n",
    "\n",
    "            inp = np.array(list(inputs) + list(np.array(inputs_prop)[:,:,0,:])).transpose(1,0,2)\n",
    "\n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            inp_prop_elems = torch.LongTensor(inputs_prop).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            \n",
    "            outputs = model(inp_elems, inp_prop_elems)\n",
    "            outputs = [el.item() for el in outputs]\n",
    "\n",
    "            targets = [True if el.item() else False for el in targets]\n",
    "\n",
    "            for idx, pred_elem in enumerate(outputs):\n",
    "                ent1 = emb_indexer_inv[inp[0][idx][0]]\n",
    "                ent2 = emb_indexer_inv[inp[1][idx][0]]\n",
    "                if (ent1, ent2) in all_results:\n",
    "                    print (\"Error: \", ent1, ent2, \"already present\")\n",
    "                all_results[(ent1, ent2)] = (pred_elem, targets[idx])\n",
    "        \n",
    "        direct_targets = [True if el else False for el in direct_targets]\n",
    "        \n",
    "        print (\"Len (direct inputs): \", len(direct_inputs))\n",
    "        for idx, direct_input in enumerate(direct_inputs):\n",
    "            ent1 = emb_indexer_inv[direct_input[0]]\n",
    "            ent2 = emb_indexer_inv[direct_input[1]]\n",
    "            sim = cos_sim(emb_vals[direct_input[0]], emb_vals[direct_input[1]])\n",
    "            all_results[(ent1, ent2)] = (sim, direct_targets[idx])\n",
    "        \n",
    "        low_threshold = np.min([el[0] for el in all_results.values()]) - 0.02\n",
    "        high_threshold = np.max([el[0] for el in all_results.values()]) + 0.02\n",
    "        threshold = low_threshold\n",
    "        step = 0.001\n",
    "        while threshold < high_threshold:\n",
    "            res = []\n",
    "            for i,key in enumerate(all_results):\n",
    "                if all_results[key][0] > threshold:\n",
    "                    res.append(key)\n",
    "            fn_list = [(key, all_results[key][0]) for key in gt_mappings if key not in set(res) and not is_valid(val_onto, key)]\n",
    "            fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "            tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "            \n",
    "            tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "            exception = False\n",
    "            \n",
    "            try:\n",
    "                precision = tp/(tp+fp)\n",
    "                recall = tp/(tp+fn)\n",
    "                f1score = 2 * precision * recall / (precision + recall)\n",
    "                f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "                f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                exception = True\n",
    "                step = 0.001\n",
    "                threshold += step\n",
    "                continue\n",
    "            # print (\"Threshold: \", threshold, precision, recall, f1score, f2score, f0_5score)\n",
    "            if threshold in threshold_results:\n",
    "                threshold_results[threshold].append([precision, recall, f1score, f2score, f0_5score])\n",
    "            else:\n",
    "                threshold_results[threshold] = [[precision, recall, f1score, f2score, f0_5score]]\n",
    "\n",
    "            if threshold > 0.98 and not exception:\n",
    "                step = 0.0001\n",
    "            else:\n",
    "                step = 0.001\n",
    "            threshold += step \n",
    "        \n",
    "def calculate_performance():\n",
    "    global final_results\n",
    "    all_metrics = []\n",
    "    for (test_onto, all_results) in final_results:\n",
    "        res = []\n",
    "        for i,key in enumerate(all_results):\n",
    "            if all_results[key][0] > threshold:\n",
    "                res.append(key)\n",
    "        fn_list = [(key, all_results[key][0]) for key in gt_mappings if key not in set(res) and not is_valid(test_onto, key)]\n",
    "        fp_list = [(elem, all_results[elem][0]) for elem in res if not all_results[elem][1]]\n",
    "        tp_list = [(elem, all_results[elem][0]) for elem in res if all_results[elem][1]]\n",
    "        tp, fn, fp = len(tp_list), len(fn_list), len(fp_list)\n",
    "        \n",
    "        try:\n",
    "            precision = tp/(tp+fp)\n",
    "            recall = tp/(tp+fn)\n",
    "            f1score = 2 * precision * recall / (precision + recall)\n",
    "            f2score = 5 * precision * recall / (4 * precision + recall)\n",
    "            f0_5score = 1.25 * precision * recall / (0.25 * precision + recall)\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            continue\n",
    "        print (\"Performance for\", test_onto, \"is :\", (precision, recall, f1score, f2score, f0_5score))\n",
    "        all_metrics.append((precision, recall, f1score, f2score, f0_5score))\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "def masked_softmax(inp):\n",
    "    inp = inp.double()\n",
    "    mask = ((inp != 0).double() - 1) * 9999  # for -inf\n",
    "    return (inp + mask).softmax(dim=-1)\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.embedding_dim = np.array(emb_vals).shape[1]\n",
    "        \n",
    "        self.name_embedding = nn.Embedding(len(emb_vals), self.embedding_dim)\n",
    "        self.name_embedding.load_state_dict({'weight': torch.from_numpy(np.array(emb_vals))})\n",
    "        self.name_embedding.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.cosine_sim_layer = nn.CosineSimilarity(dim=1)\n",
    "        self.output = nn.Linear(2*self.embedding_dim, 300)\n",
    "        n = int(sys.argv[1])\n",
    "        self.v = nn.Parameter(torch.DoubleTensor([1/(n-1) for i in range(n-1)]))\n",
    "        self.w_prop = nn.Parameter(torch.DoubleTensor([1/3 for i in range(3)]))\n",
    " \n",
    "    def forward(self, inputs, inputs_prop):\n",
    "        results = []\n",
    "        inputs = inputs.permute(1,0,2)\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs[i])\n",
    "            node = x.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = x.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            att_weights = torch.bmm(neighbours, node.permute(0, 2, 1)).squeeze()\n",
    "            att_weights = masked_softmax(att_weights).unsqueeze(-1)\n",
    "            context = torch.matmul(self.v, att_weights * neighbours)\n",
    "\n",
    "            x = torch.cat((node.reshape(-1, self.embedding_dim), context.reshape(-1, self.embedding_dim)), dim=1)\n",
    "            x = self.output(x)\n",
    "            results.append(x)\n",
    "\n",
    "        inputs_prop = inputs_prop.permute(1,0,2,3)\n",
    "        \n",
    "        domains, ranges, props = [], [], []\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs_prop[i])\n",
    "            curr = x.permute(1,0,2,3)[1]\n",
    "            node = curr.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = curr.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            att_weights = torch.bmm(neighbours, node.permute(0, 2, 1)).squeeze()\n",
    "            att_weights = masked_softmax(att_weights).unsqueeze(-1)\n",
    "            context = torch.matmul(self.v, att_weights * neighbours)\n",
    "\n",
    "            x = torch.cat((node.reshape(-1, self.embedding_dim), context.reshape(-1, self.embedding_dim)), dim=1)\n",
    "            x = self.output(x)\n",
    "            domains.append(x)\n",
    "\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs_prop[i])\n",
    "            curr = x.permute(1,0,2,3)[2]\n",
    "            node = curr.permute(1,0,2)[:1].permute(1,0,2) # 3993 * 1 * 512\n",
    "            neighbours = curr.permute(1,0,2)[1:].permute(1,0,2) # 3993 * 9 * 512\n",
    "            \n",
    "            att_weights = torch.bmm(neighbours, node.permute(0, 2, 1)).squeeze()\n",
    "            att_weights = masked_softmax(att_weights).unsqueeze(-1)\n",
    "            context = torch.matmul(self.v, att_weights * neighbours)\n",
    "\n",
    "            x = torch.cat((node.reshape(-1, self.embedding_dim), context.reshape(-1, self.embedding_dim)), dim=1)\n",
    "            x = self.output(x)\n",
    "            ranges.append(x)\n",
    "\n",
    "        for i in range(2):\n",
    "            x = self.name_embedding(inputs_prop[i])\n",
    "            curr = x.permute(1,0,2,3)[0].permute(1,0,2)[0]\n",
    "            props.append(curr)\n",
    "\n",
    "        x_ent = self.cosine_sim_layer(results[0], results[1])\n",
    "        \n",
    "        x_prop = self.w_prop[0] * self.cosine_sim_layer(domains[0], domains[1]) + \\\n",
    "                self.w_prop[1] * self.cosine_sim_layer(ranges[0], ranges[1]) + \\\n",
    "                self.w_prop[2] * self.cosine_sim_layer(props[0], props[1])\n",
    "\n",
    "        return torch.cat((x_ent, x_prop))\n",
    "\n",
    "def is_valid(test_onto, key):\n",
    "    return tuple([el.split(\"#\")[0] for el in key]) not in test_onto\n",
    "\n",
    "def generate_data_neighbourless(elem_tuple):\n",
    "    op = np.array([emb_indexer[elem] for elem in elem_tuple])\n",
    "    return op\n",
    "\n",
    "def generate_data(elem_tuple, prop=False):\n",
    "    if prop:\n",
    "        return np.array([[[emb_indexer[el] for el in element] for element in neighbours_dicts_prop[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "    return np.array([[emb_indexer[el] for el in neighbours_dicts[elem.split(\"#\")[0]][elem]] for elem in elem_tuple])\n",
    "\n",
    "def generate_input(elems, target, prop=False):\n",
    "    inputs, targets = [], []\n",
    "    global direct_inputs, direct_targets\n",
    "    for elem in list(elems):\n",
    "        try:\n",
    "            inputs.append(generate_data(elem, prop))\n",
    "            targets.append(target)\n",
    "        except Exception as e:\n",
    "#             print (\"Error \", e)\n",
    "            direct_inputs.append(generate_data_neighbourless(elem))\n",
    "            direct_targets.append(target)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "print(\"Number of neighbours: \" + str(sys.argv[1]))\n",
    "\n",
    "def count_non_unk(elem):\n",
    "    return len([l for l in elem if l!=\"<UNK>\"])\n",
    "\n",
    "neighbours_dicts = {ont: {el: neighbours_dicts[ont][el][:int(sys.argv[1])] for el in neighbours_dicts[ont]\n",
    "       if count_non_unk(neighbours_dicts[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts}\n",
    "\n",
    "neighbours_dicts_prop = {ont: {el: [element[:int(sys.argv[1])] for element in neighbours_dicts_prop[ont][el]] for el in neighbours_dicts_prop[ont]\n",
    "       if count_non_unk(neighbours_dicts_prop[ont][el]) > int(sys.argv[2])} for ont in neighbours_dicts_prop}\n",
    "\n",
    "print (\"Number of entities:\", len(data_ent))\n",
    "\n",
    "all_metrics = []\n",
    "final_results = []\n",
    "for i in list(range(0, len(ontologies_in_alignment), 3)):\n",
    "    \n",
    "    test_onto = ontologies_in_alignment[i:i+3]\n",
    "    \n",
    "    train_data = {elem: data_ent[elem] for elem in data_ent if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "\n",
    "    val_onto = test_onto[:1]\n",
    "    test_onto = test_onto[1:]\n",
    "    val_data = {elem: data_ent[elem] for elem in data_ent if tuple([el.split(\"#\")[0] for el in elem]) in val_onto}\n",
    "    test_data = {elem: data_ent[elem] for elem in data_ent if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    print (\"Training size:\", len(train_data), \"Testing size:\", len(test_data))\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_t = [key for key in train_data if train_data[key]]\n",
    "    train_data_f = [key for key in train_data if not train_data[key]]\n",
    "    train_data_t = np.repeat(train_data_t, ceil(len(train_data_f)/len(train_data_t)), axis=0)\n",
    "    train_data_t = train_data_t[:len(train_data_f)].tolist()\n",
    "    #train_data_f = train_data_f[:int(len(train_data_t))]\n",
    "#     [:int(0.1*(len(train_data) - len(train_data_t)) )]\n",
    "    np.random.shuffle(train_data_f)\n",
    "\n",
    "    train_data_prop = {elem: data_prop[elem] for elem in data_prop if tuple([el.split(\"#\")[0] for el in elem]) not in test_onto}\n",
    "    val_data_prop = {elem: data_prop[elem] for elem in data_prop if tuple([el.split(\"#\")[0] for el in elem]) in val_onto}\n",
    "    test_data_prop = {elem: data_prop[elem] for elem in data_prop if tuple([el.split(\"#\")[0] for el in elem]) in test_onto}\n",
    "\n",
    "    print (\"Training size:\", len(train_data_prop), \"Testing size:\", len(test_data_prop))\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    train_test_split = 0.9\n",
    "\n",
    "    train_data_prop_t = [key for key in train_data_prop if train_data_prop[key]]\n",
    "    train_data_prop_f = [key for key in train_data_prop if not train_data_prop[key]]\n",
    "    train_data_prop_t = np.repeat(train_data_prop_t, ceil(len(train_data_prop_f)/len(train_data_prop_t)), axis=0)\n",
    "    train_data_prop_t = train_data_prop_t[:len(train_data_prop_f)].tolist()\n",
    "    #train_data_prop_f = train_data_prop_f[:int(len(train_data_prop_t))]\n",
    "#     [:int(0.1*(len(train_data_prop) - len(train_data_prop_t)) )]\n",
    "    np.random.shuffle(train_data_prop_f)\n",
    "    \n",
    "    lr = 0.001\n",
    "    num_epochs = 50\n",
    "    weight_decay = 0.001\n",
    "    batch_size = 10\n",
    "    dropout = 0.3\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SiameseNetwork().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        inputs_pos, targets_pos = generate_input(train_data_t, 1)\n",
    "        inputs_neg, targets_neg = generate_input(train_data_f, 0)\n",
    "        inputs_all = list(inputs_pos) + list(inputs_neg)\n",
    "        targets_all = list(targets_pos) + list(targets_neg)\n",
    "        \n",
    "        indices_all = np.random.permutation(len(inputs_all))\n",
    "        inputs_all = np.array(inputs_all)[indices_all]\n",
    "        targets_all = np.array(targets_all)[indices_all]\n",
    "\n",
    "        batch_size = min(batch_size, len(inputs_all))\n",
    "        num_batches = int(ceil(len(inputs_all)/batch_size))\n",
    "\n",
    "        inputs_pos_prop, targets_pos_prop = generate_input(train_data_prop_t, 1, True)\n",
    "        inputs_neg_prop, targets_neg_prop = generate_input(train_data_prop_f, 0, True)\n",
    "\n",
    "        print (\"Properties with neighbours: \", len(inputs_pos_prop)+len(inputs_neg_prop),\\\n",
    "         \"Total: \", len(train_data_prop_t)+len(train_data_prop_f))\n",
    "\n",
    "        inputs_all_prop = list(inputs_pos_prop) + list(inputs_neg_prop)\n",
    "        targets_all_prop = list(targets_pos_prop) + list(targets_neg_prop)\n",
    "        \n",
    "        indices_all_prop = np.random.permutation(len(inputs_all_prop))\n",
    "        inputs_all_prop = np.array(inputs_all_prop)[indices_all_prop]\n",
    "        targets_all_prop = np.array(targets_all_prop)[indices_all_prop]\n",
    "\n",
    "        batch_size_prop = int(ceil(len(inputs_all_prop)/num_batches))\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx+1) * batch_size\n",
    "            \n",
    "            inputs = inputs_all[batch_start: batch_end]\n",
    "            targets = targets_all[batch_start: batch_end]\n",
    "            \n",
    "            batch_start_prop = batch_idx * batch_size_prop\n",
    "            batch_end_prop = (batch_idx+1) * batch_size_prop\n",
    "            \n",
    "            inputs_prop = inputs_all_prop[batch_start_prop: batch_end_prop]\n",
    "            targets_prop = targets_all_prop[batch_start_prop: batch_end_prop]\n",
    "\n",
    "            targets = list(targets) + list(targets_prop)\n",
    "\n",
    "            inp_elems = torch.LongTensor(inputs).to(device)\n",
    "            inp_prop_elems = torch.LongTensor(inputs_prop).to(device)\n",
    "            targ_elems = torch.DoubleTensor(targets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inp_elems, inp_prop_elems)\n",
    "            loss = F.mse_loss(outputs, targ_elems)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx%5000 == 0:\n",
    "                print (\"Epoch: {} Idx: {} Loss: {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    val_data_t = [key for key in val_data if val_data[key]]\n",
    "    val_data_f = [key for key in val_data if not val_data[key]]\n",
    "    val_data_prop_t = [key for key in val_data_prop if val_data_prop[key]]\n",
    "    val_data_prop_f = [key for key in val_data_prop if not val_data_prop[key]]\n",
    "    \n",
    "    optimize_threshold()\n",
    "\n",
    "    test_data_t = [key for key in test_data if test_data[key]]\n",
    "    test_data_f = [key for key in test_data if not test_data[key]]\n",
    "    test_data_prop_t = [key for key in test_data_prop if test_data_prop[key]]\n",
    "    test_data_prop_f = [key for key in test_data_prop if not test_data_prop[key]]\n",
    "\n",
    "    final_results.append(test())\n",
    "\n",
    "threshold_results_mean = {el: np.mean(threshold_results[el], axis=0) for el in threshold_results}    \n",
    "threshold = max(threshold_results_mean.keys(), key=(lambda key: threshold_results_mean[key][2]))\n",
    "\n",
    "all_metrics = calculate_performance()\n",
    "\n",
    "print (\"Final Results: \" + str(np.mean(all_metrics, axis=0)))\n",
    "print (\"Threshold: \", threshold)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "neighbours_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('confOf', 'sigkdd'),\n",
       " ('iasted', 'sigkdd'),\n",
       " ('cmt', 'ekaw'),\n",
       " ('confOf', 'iasted'),\n",
       " ('conference', 'edas'),\n",
       " ('cmt', 'sigkdd'),\n",
       " ('ekaw', 'sigkdd'),\n",
       " ('conference', 'confOf'),\n",
       " ('conference', 'sigkdd'),\n",
       " ('confOf', 'edas'),\n",
       " ('cmt', 'conference'),\n",
       " ('edas', 'iasted'),\n",
       " ('conference', 'iasted'),\n",
       " ('edas', 'sigkdd'),\n",
       " ('ekaw', 'iasted'),\n",
       " ('cmt', 'edas'),\n",
       " ('edas', 'ekaw'),\n",
       " ('cmt', 'confOf'),\n",
       " ('confOf', 'ekaw'),\n",
       " ('conference', 'ekaw'),\n",
       " ('cmt', 'iasted')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontologies_in_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[455,   0,   0, ...,   0,   0,   0],\n",
       "         [573,  42,  18, ...,   0,   0,   0],\n",
       "         [431, 294, 828, ...,   0,   0,   0]],\n",
       "\n",
       "        [[625,   0,   0, ...,   0,   0,   0],\n",
       "         [811, 108, 486, ...,   0,   0,   0],\n",
       "         [108, 811, 488, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[446,   0,   0, ...,   0,   0,   0],\n",
       "         [573,  42,  18, ...,   0,   0,   0],\n",
       "         [623, 596, 741, ...,   0,   0,   0]],\n",
       "\n",
       "        [[293,   0,   0, ...,   0,   0,   0],\n",
       "         [669, 506, 670, ...,   0,   0,   0],\n",
       "         [339, 648,  76, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[535,   0,   0, ...,   0,   0,   0],\n",
       "         [573,  42,  18, ...,   0,   0,   0],\n",
       "         [363, 348, 431, ...,   0,   0,   0]],\n",
       "\n",
       "        [[247,   0,   0, ...,   0,   0,   0],\n",
       "         [404, 511, 733, ...,   0,   0,   0],\n",
       "         [473,  17, 404, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[413,   0,   0, ...,   0,   0,   0],\n",
       "         [616, 746, 380, ...,   0,   0,   0],\n",
       "         [380,   0,   0, ...,   0,   0,   0]],\n",
       "\n",
       "        [[268,   0,   0, ...,   0,   0,   0],\n",
       "         [ 97, 516, 642, ...,   0,   0,   0],\n",
       "         [516,   0,   0, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[440,   0,   0, ...,   0,   0,   0],\n",
       "         [332, 493, 702, ...,   0,   0,   0],\n",
       "         [192, 332, 473, ...,   0,   0,   0]],\n",
       "\n",
       "        [[713,   0,   0, ...,   0,   0,   0],\n",
       "         [591, 254, 496, ...,   0,   0,   0],\n",
       "         [539, 591, 302, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[315,   0,   0, ...,   0,   0,   0],\n",
       "         [803, 476, 516, ...,   0,   0,   0],\n",
       "         [206, 803,  97, ...,   0,   0,   0]],\n",
       "\n",
       "        [[763,   0,   0, ...,   0,   0,   0],\n",
       "         [332, 493, 702, ...,   0,   0,   0],\n",
       "         [690, 241, 332, ...,   0,   0,   0]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_all_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169534"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
